#ifndef matmul_256_32bit_batch_0025_sample_0002_SCHEDULE_H
#define matmul_256_32bit_batch_0025_sample_0002_SCHEDULE_H

// MACHINE GENERATED -- DO NOT EDIT
// This schedule was automatically generated by Adams2019
// for target=x86-64-linux-avx-avx2-avx512-avx512_sapphirerapids-avx512_skylake-disable_llvm_loop_opt-f16c-fma-sse41  // NOLINT
// with machine_params=10,16777216,40

#include "Halide.h"


inline void apply_schedule_matmul_256_32bit_batch_0025_sample_0002(
    ::Halide::Pipeline pipeline,
    ::Halide::Target target
) {
    using ::Halide::Func;
    using ::Halide::MemoryType;
    using ::Halide::RVar;
    using ::Halide::TailStrategy;
    using ::Halide::Var;
    Func res = pipeline.get_func(3);
    Func matrix_mul = pipeline.get_func(2);
    Var x(res.get_schedule().dims()[0].var);
    Var xi("xi");
    Var xii("xii");
    Var y(res.get_schedule().dims()[1].var);
    Var yi("yi");
    RVar r8_x(matrix_mul.update(0).get_schedule().dims()[0].var);
    res
        .split(y, y, yi, 4, TailStrategy::ShiftInwards)
        .split(x, x, xi, 64, TailStrategy::ShiftInwards)
        .split(xi, xi, xii, 32, TailStrategy::ShiftInwards)
        .unroll(xi)
        .unroll(yi)
        .vectorize(xii)
        .compute_root()
        .reorder({xii, xi, yi, x, y})
        .parallel(y);
    matrix_mul.update(0)
        .split(x, x, xi, 32, TailStrategy::GuardWithIf)
        .unroll(x)
        .unroll(y)
        .vectorize(xi)
        .reorder({xi, x, y, r8_x});
    matrix_mul
        .store_in(MemoryType::Stack)
        .split(x, x, xi, 32, TailStrategy::RoundUp)
        .unroll(x)
        .unroll(y)
        .vectorize(xi)
        .compute_at(res, x)
        .reorder({xi, x, y});

}

#endif  // matmul_256_32bit_batch_0025_sample_0002_SCHEDULE_H
