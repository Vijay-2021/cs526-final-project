; ModuleID = 'arm_wrappers.c.bc'
source_filename = "arm_wrappers.c"
target datalayout = "e-m:o-i64:64-i128:128-n32:64-S128"
target triple = "arm64-apple-macosx13.0.0"

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %add.i = add <8 x i8> %2, %3
  store <8 x i8> %add.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %add.i = add <16 x i8> %2, %3
  store <16 x i8> %add.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %add.i = add <4 x i16> %2, %3
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %add.i = add <8 x i16> %2, %3
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %add.i = add <2 x i32> %2, %3
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %add.i = add <4 x i32> %2, %3
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %add.i = add <2 x i64> %2, %3
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %add.i = add <8 x i8> %2, %3
  store <8 x i8> %add.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %add.i = add <16 x i8> %2, %3
  store <16 x i8> %add.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %add.i = add <4 x i16> %2, %3
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %add.i = add <8 x i16> %2, %3
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %add.i = add <2 x i32> %2, %3
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %add.i = add <4 x i32> %2, %3
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %add.i = add <2 x i64> %2, %3
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vadd_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %add.i = add <1 x i64> %2, %3
  store <1 x i64> %add.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vadd_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %add.i = add <1 x i64> %2, %3
  store <1 x i64> %add.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vaddd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vaddd.i = add i64 %2, %3
  store i64 %vaddd.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vaddd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vaddd.i = add i64 %2, %3
  store i64 %vaddd.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = sext <8 x i8> %3 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i2.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i4.i = sext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %vmovl.i4.i, <8 x i16>* %__ret.i3.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  %add.i = add <8 x i16> %4, %7
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %vmovl.i.i = sext <4 x i16> %3 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p0.addr.i2.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vmovl.i4.i = sext <4 x i16> %7 to <4 x i32>
  store <4 x i32> %vmovl.i4.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  %add.i = add <4 x i32> %5, %9
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %vmovl.i.i = sext <2 x i32> %3 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p0.addr.i2.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vmovl.i4.i = sext <2 x i32> %7 to <2 x i64>
  store <2 x i64> %vmovl.i4.i, <2 x i64>* %__ret.i3.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  %add.i = add <2 x i64> %5, %9
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddl_high_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <16 x i8>, align 16
  %__ret.i.i3.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i4.i = alloca <16 x i8>, align 16
  %__ret_612.i5.i = alloca <8 x i16>, align 16
  %__a1_612.i6.i = alloca <8 x i8>, align 8
  %__s0.i7.i = alloca <8 x i8>, align 8
  %__ret.i8.i = alloca <8 x i16>, align 16
  %tmp.i9.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i.i = alloca <16 x i8>, align 16
  %__ret_612.i.i = alloca <8 x i16>, align 16
  %__a1_612.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0_612.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0.addr.i.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %4, <16 x i8> %5, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__a1_612.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__a1_612.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__s0.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %9 = sext <8 x i8> %8 to <8 x i16>
  store <8 x i16> %9, <8 x i16>* %__ret.i.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %10, <8 x i16>* %tmp.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__ret_612.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret_612.i.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %13, <16 x i8>* %__p0_612.addr.i4.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i4.i, align 16
  store <16 x i8> %14, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %15 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %16 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <16 x i8> %15, <16 x i8> %16, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i10.i, <8 x i8>* %__ret.i.i3.i, align 8
  %17 = load <8 x i8>, <8 x i8>* %__ret.i.i3.i, align 8
  store <8 x i8> %17, <8 x i8>* %__a1_612.i6.i, align 8
  %18 = load <8 x i8>, <8 x i8>* %__a1_612.i6.i, align 8
  store <8 x i8> %18, <8 x i8>* %__s0.i7.i, align 8
  %19 = load <8 x i8>, <8 x i8>* %__s0.i7.i, align 8
  %20 = sext <8 x i8> %19 to <8 x i16>
  store <8 x i16> %20, <8 x i16>* %__ret.i8.i, align 16
  %21 = load <8 x i16>, <8 x i16>* %__ret.i8.i, align 16
  store <8 x i16> %21, <8 x i16>* %tmp.i9.i, align 16
  %22 = load <8 x i16>, <8 x i16>* %tmp.i9.i, align 16
  store <8 x i16> %22, <8 x i16>* %__ret_612.i5.i, align 16
  %23 = load <8 x i16>, <8 x i16>* %__ret_612.i5.i, align 16
  %add.i = add <8 x i16> %12, %23
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %24 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddl_high_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <8 x i16>, align 16
  %__ret.i.i3.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i4.i = alloca <8 x i16>, align 16
  %__ret_618.i5.i = alloca <4 x i32>, align 16
  %__a1_618.i6.i = alloca <4 x i16>, align 8
  %__s0.i7.i = alloca <4 x i16>, align 8
  %__ret.i8.i = alloca <4 x i32>, align 16
  %tmp.i9.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i.i = alloca <8 x i16>, align 16
  %__ret_618.i.i = alloca <4 x i32>, align 16
  %__a1_618.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0_618.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %4, <8 x i16> %5, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %6, <4 x i16>* %__a1_618.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__a1_618.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__s0.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = sext <4 x i16> %8 to <4 x i32>
  store <4 x i32> %10, <4 x i32>* %__ret.i.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %11, <4 x i32>* %tmp.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret_618.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret_618.i.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %14, <8 x i16>* %__p0_618.addr.i4.i, align 16
  %15 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i4.i, align 16
  store <8 x i16> %15, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <8 x i16> %16, <8 x i16> %17, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i10.i, <4 x i16>* %__ret.i.i3.i, align 8
  %18 = load <4 x i16>, <4 x i16>* %__ret.i.i3.i, align 8
  store <4 x i16> %18, <4 x i16>* %__a1_618.i6.i, align 8
  %19 = load <4 x i16>, <4 x i16>* %__a1_618.i6.i, align 8
  store <4 x i16> %19, <4 x i16>* %__s0.i7.i, align 8
  %20 = load <4 x i16>, <4 x i16>* %__s0.i7.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %22 = sext <4 x i16> %20 to <4 x i32>
  store <4 x i32> %22, <4 x i32>* %__ret.i8.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i8.i, align 16
  store <4 x i32> %23, <4 x i32>* %tmp.i9.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %tmp.i9.i, align 16
  store <4 x i32> %24, <4 x i32>* %__ret_618.i5.i, align 16
  %25 = load <4 x i32>, <4 x i32>* %__ret_618.i5.i, align 16
  %add.i = add <4 x i32> %13, %25
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %26 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddl_high_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <4 x i32>, align 16
  %__ret.i.i3.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i4.i = alloca <4 x i32>, align 16
  %__ret_615.i5.i = alloca <2 x i64>, align 16
  %__a1_615.i6.i = alloca <2 x i32>, align 8
  %__s0.i7.i = alloca <2 x i32>, align 8
  %__ret.i8.i = alloca <2 x i64>, align 16
  %tmp.i9.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i.i = alloca <4 x i32>, align 16
  %__ret_615.i.i = alloca <2 x i64>, align 16
  %__a1_615.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0_615.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %4, <4 x i32> %5, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %6, <2 x i32>* %__a1_615.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__a1_615.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__s0.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = sext <2 x i32> %8 to <2 x i64>
  store <2 x i64> %10, <2 x i64>* %__ret.i.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %11, <2 x i64>* %tmp.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %__ret_615.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret_615.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %14, <4 x i32>* %__p0_615.addr.i4.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i4.i, align 16
  store <4 x i32> %15, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <4 x i32> %16, <4 x i32> %17, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i10.i, <2 x i32>* %__ret.i.i3.i, align 8
  %18 = load <2 x i32>, <2 x i32>* %__ret.i.i3.i, align 8
  store <2 x i32> %18, <2 x i32>* %__a1_615.i6.i, align 8
  %19 = load <2 x i32>, <2 x i32>* %__a1_615.i6.i, align 8
  store <2 x i32> %19, <2 x i32>* %__s0.i7.i, align 8
  %20 = load <2 x i32>, <2 x i32>* %__s0.i7.i, align 8
  %21 = bitcast <2 x i32> %20 to <8 x i8>
  %22 = sext <2 x i32> %20 to <2 x i64>
  store <2 x i64> %22, <2 x i64>* %__ret.i8.i, align 16
  %23 = load <2 x i64>, <2 x i64>* %__ret.i8.i, align 16
  store <2 x i64> %23, <2 x i64>* %tmp.i9.i, align 16
  %24 = load <2 x i64>, <2 x i64>* %tmp.i9.i, align 16
  store <2 x i64> %24, <2 x i64>* %__ret_615.i5.i, align 16
  %25 = load <2 x i64>, <2 x i64>* %__ret_615.i5.i, align 16
  %add.i = add <2 x i64> %13, %25
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %26 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = zext <8 x i8> %3 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i2.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i4.i = zext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %vmovl.i4.i, <8 x i16>* %__ret.i3.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  %add.i = add <8 x i16> %4, %7
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %3 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p0.addr.i2.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vmovl.i4.i = zext <4 x i16> %7 to <4 x i32>
  store <4 x i32> %vmovl.i4.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  %add.i = add <4 x i32> %5, %9
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %3 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p0.addr.i2.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vmovl.i4.i = zext <2 x i32> %7 to <2 x i64>
  store <2 x i64> %vmovl.i4.i, <2 x i64>* %__ret.i3.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  %add.i = add <2 x i64> %5, %9
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddl_high_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <16 x i8>, align 16
  %__ret.i.i3.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i4.i = alloca <16 x i8>, align 16
  %__ret_603.i5.i = alloca <8 x i16>, align 16
  %__a1_603.i6.i = alloca <8 x i8>, align 8
  %__s0.i7.i = alloca <8 x i8>, align 8
  %__ret.i8.i = alloca <8 x i16>, align 16
  %tmp.i9.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i.i = alloca <16 x i8>, align 16
  %__ret_603.i.i = alloca <8 x i16>, align 16
  %__a1_603.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0_603.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0.addr.i.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %4, <16 x i8> %5, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__a1_603.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__a1_603.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__s0.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %9 = zext <8 x i8> %8 to <8 x i16>
  store <8 x i16> %9, <8 x i16>* %__ret.i.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %10, <8 x i16>* %tmp.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__ret_603.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret_603.i.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %13, <16 x i8>* %__p0_603.addr.i4.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i4.i, align 16
  store <16 x i8> %14, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %15 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %16 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <16 x i8> %15, <16 x i8> %16, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i10.i, <8 x i8>* %__ret.i.i3.i, align 8
  %17 = load <8 x i8>, <8 x i8>* %__ret.i.i3.i, align 8
  store <8 x i8> %17, <8 x i8>* %__a1_603.i6.i, align 8
  %18 = load <8 x i8>, <8 x i8>* %__a1_603.i6.i, align 8
  store <8 x i8> %18, <8 x i8>* %__s0.i7.i, align 8
  %19 = load <8 x i8>, <8 x i8>* %__s0.i7.i, align 8
  %20 = zext <8 x i8> %19 to <8 x i16>
  store <8 x i16> %20, <8 x i16>* %__ret.i8.i, align 16
  %21 = load <8 x i16>, <8 x i16>* %__ret.i8.i, align 16
  store <8 x i16> %21, <8 x i16>* %tmp.i9.i, align 16
  %22 = load <8 x i16>, <8 x i16>* %tmp.i9.i, align 16
  store <8 x i16> %22, <8 x i16>* %__ret_603.i5.i, align 16
  %23 = load <8 x i16>, <8 x i16>* %__ret_603.i5.i, align 16
  %add.i = add <8 x i16> %12, %23
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %24 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddl_high_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <8 x i16>, align 16
  %__ret.i.i3.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i4.i = alloca <8 x i16>, align 16
  %__ret_609.i5.i = alloca <4 x i32>, align 16
  %__a1_609.i6.i = alloca <4 x i16>, align 8
  %__s0.i7.i = alloca <4 x i16>, align 8
  %__ret.i8.i = alloca <4 x i32>, align 16
  %tmp.i9.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i.i = alloca <8 x i16>, align 16
  %__ret_609.i.i = alloca <4 x i32>, align 16
  %__a1_609.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0_609.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %4, <8 x i16> %5, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %6, <4 x i16>* %__a1_609.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__a1_609.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__s0.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = zext <4 x i16> %8 to <4 x i32>
  store <4 x i32> %10, <4 x i32>* %__ret.i.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %11, <4 x i32>* %tmp.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret_609.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret_609.i.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %14, <8 x i16>* %__p0_609.addr.i4.i, align 16
  %15 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i4.i, align 16
  store <8 x i16> %15, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <8 x i16> %16, <8 x i16> %17, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i10.i, <4 x i16>* %__ret.i.i3.i, align 8
  %18 = load <4 x i16>, <4 x i16>* %__ret.i.i3.i, align 8
  store <4 x i16> %18, <4 x i16>* %__a1_609.i6.i, align 8
  %19 = load <4 x i16>, <4 x i16>* %__a1_609.i6.i, align 8
  store <4 x i16> %19, <4 x i16>* %__s0.i7.i, align 8
  %20 = load <4 x i16>, <4 x i16>* %__s0.i7.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %22 = zext <4 x i16> %20 to <4 x i32>
  store <4 x i32> %22, <4 x i32>* %__ret.i8.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i8.i, align 16
  store <4 x i32> %23, <4 x i32>* %tmp.i9.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %tmp.i9.i, align 16
  store <4 x i32> %24, <4 x i32>* %__ret_609.i5.i, align 16
  %25 = load <4 x i32>, <4 x i32>* %__ret_609.i5.i, align 16
  %add.i = add <4 x i32> %13, %25
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %26 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddl_high_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <4 x i32>, align 16
  %__ret.i.i3.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i4.i = alloca <4 x i32>, align 16
  %__ret_606.i5.i = alloca <2 x i64>, align 16
  %__a1_606.i6.i = alloca <2 x i32>, align 8
  %__s0.i7.i = alloca <2 x i32>, align 8
  %__ret.i8.i = alloca <2 x i64>, align 16
  %tmp.i9.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i.i = alloca <4 x i32>, align 16
  %__ret_606.i.i = alloca <2 x i64>, align 16
  %__a1_606.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0_606.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %4, <4 x i32> %5, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %6, <2 x i32>* %__a1_606.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__a1_606.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__s0.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = zext <2 x i32> %8 to <2 x i64>
  store <2 x i64> %10, <2 x i64>* %__ret.i.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %11, <2 x i64>* %tmp.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %__ret_606.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret_606.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %14, <4 x i32>* %__p0_606.addr.i4.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i4.i, align 16
  store <4 x i32> %15, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <4 x i32> %16, <4 x i32> %17, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i10.i, <2 x i32>* %__ret.i.i3.i, align 8
  %18 = load <2 x i32>, <2 x i32>* %__ret.i.i3.i, align 8
  store <2 x i32> %18, <2 x i32>* %__a1_606.i6.i, align 8
  %19 = load <2 x i32>, <2 x i32>* %__a1_606.i6.i, align 8
  store <2 x i32> %19, <2 x i32>* %__s0.i7.i, align 8
  %20 = load <2 x i32>, <2 x i32>* %__s0.i7.i, align 8
  %21 = bitcast <2 x i32> %20 to <8 x i8>
  %22 = zext <2 x i32> %20 to <2 x i64>
  store <2 x i64> %22, <2 x i64>* %__ret.i8.i, align 16
  %23 = load <2 x i64>, <2 x i64>* %__ret.i8.i, align 16
  store <2 x i64> %23, <2 x i64>* %tmp.i9.i, align 16
  %24 = load <2 x i64>, <2 x i64>* %tmp.i9.i, align 16
  store <2 x i64> %24, <2 x i64>* %__ret_606.i5.i, align 16
  %25 = load <2 x i64>, <2 x i64>* %__ret_606.i5.i, align 16
  %add.i = add <2 x i64> %13, %25
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %26 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddw_s8_wrapper(<8 x i16> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = sext <8 x i8> %4 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %add.i = add <8 x i16> %2, %5
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddw_s16_wrapper(<4 x i32> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmovl.i.i = sext <4 x i16> %4 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %2, %6
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddw_s32_wrapper(<2 x i64> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmovl.i.i = sext <2 x i32> %4 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %2, %6
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %7 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddw_high_s8_wrapper(<8 x i16> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i.i = alloca <16 x i8>, align 16
  %__ret_612.i.i = alloca <8 x i16>, align 16
  %__a1_612.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0_612.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__a1_612.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__a1_612.i.i, align 8
  store <8 x i8> %8, <8 x i8>* %__s0.i.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %10 = sext <8 x i8> %9 to <8 x i16>
  store <8 x i16> %10, <8 x i16>* %__ret.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %tmp.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret_612.i.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret_612.i.i, align 16
  %add.i = add <8 x i16> %2, %13
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddw_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i.i = alloca <8 x i16>, align 16
  %__ret_618.i.i = alloca <4 x i32>, align 16
  %__a1_618.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0_618.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__a1_618.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__a1_618.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__s0.i.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %11 = sext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %11, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %tmp.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret_618.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret_618.i.i, align 16
  %add.i = add <4 x i32> %2, %14
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddw_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i.i = alloca <4 x i32>, align 16
  %__ret_615.i.i = alloca <2 x i64>, align 16
  %__a1_615.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0_615.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__a1_615.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__a1_615.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__s0.i.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %11 = sext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %11, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %tmp.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %13, <2 x i64>* %__ret_615.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret_615.i.i, align 16
  %add.i = add <2 x i64> %2, %14
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddw_u8_wrapper(<8 x i16> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = zext <8 x i8> %4 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %add.i = add <8 x i16> %2, %5
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddw_u16_wrapper(<4 x i32> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %4 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %2, %6
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddw_u32_wrapper(<2 x i64> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %4 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %2, %6
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %7 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddw_high_u8_wrapper(<8 x i16> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i.i = alloca <16 x i8>, align 16
  %__ret_603.i.i = alloca <8 x i16>, align 16
  %__a1_603.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0_603.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__a1_603.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__a1_603.i.i, align 8
  store <8 x i8> %8, <8 x i8>* %__s0.i.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %10 = zext <8 x i8> %9 to <8 x i16>
  store <8 x i16> %10, <8 x i16>* %__ret.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %tmp.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret_603.i.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret_603.i.i, align 16
  %add.i = add <8 x i16> %2, %13
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddw_high_u16_wrapper(<4 x i32> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i.i = alloca <8 x i16>, align 16
  %__ret_609.i.i = alloca <4 x i32>, align 16
  %__a1_609.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0_609.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__a1_609.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__a1_609.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__s0.i.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %11 = zext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %11, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %tmp.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret_609.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret_609.i.i, align 16
  %add.i = add <4 x i32> %2, %14
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vaddw_high_u32_wrapper(<2 x i64> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i.i = alloca <4 x i32>, align 16
  %__ret_606.i.i = alloca <2 x i64>, align 16
  %__a1_606.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0_606.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__a1_606.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__a1_606.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__s0.i.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %11 = zext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %11, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %tmp.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %13, <2 x i64>* %__ret_606.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret_606.i.i, align 16
  %add.i = add <2 x i64> %2, %14
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vhadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vhadd_v.i = call <8 x i8> @llvm.aarch64.neon.shadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vhadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vhaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vhaddq_v.i = call <16 x i8> @llvm.aarch64.neon.shadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vhaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vhadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vhadd_v2.i = call <4 x i16> @llvm.aarch64.neon.shadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vhadd_v3.i = bitcast <4 x i16> %vhadd_v2.i to <8 x i8>
  store <4 x i16> %vhadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vhaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vhaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.shadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vhaddq_v3.i = bitcast <8 x i16> %vhaddq_v2.i to <16 x i8>
  store <8 x i16> %vhaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vhadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vhadd_v2.i = call <2 x i32> @llvm.aarch64.neon.shadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vhadd_v3.i = bitcast <2 x i32> %vhadd_v2.i to <8 x i8>
  store <2 x i32> %vhadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vhaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vhaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.shadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vhaddq_v3.i = bitcast <4 x i32> %vhaddq_v2.i to <16 x i8>
  store <4 x i32> %vhaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vhadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vhadd_v.i = call <8 x i8> @llvm.aarch64.neon.uhadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vhadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vhaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vhaddq_v.i = call <16 x i8> @llvm.aarch64.neon.uhadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vhaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vhadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vhadd_v2.i = call <4 x i16> @llvm.aarch64.neon.uhadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vhadd_v3.i = bitcast <4 x i16> %vhadd_v2.i to <8 x i8>
  store <4 x i16> %vhadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vhaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vhaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.uhadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vhaddq_v3.i = bitcast <8 x i16> %vhaddq_v2.i to <16 x i8>
  store <8 x i16> %vhaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vhadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vhadd_v2.i = call <2 x i32> @llvm.aarch64.neon.uhadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vhadd_v3.i = bitcast <2 x i32> %vhadd_v2.i to <8 x i8>
  store <2 x i32> %vhadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vhaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vhaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.uhadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vhaddq_v3.i = bitcast <4 x i32> %vhaddq_v2.i to <16 x i8>
  store <4 x i32> %vhaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrhadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vrhadd_v.i = call <8 x i8> @llvm.aarch64.neon.srhadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vrhadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrhaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vrhaddq_v.i = call <16 x i8> @llvm.aarch64.neon.srhadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vrhaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrhadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vrhadd_v2.i = call <4 x i16> @llvm.aarch64.neon.srhadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vrhadd_v3.i = bitcast <4 x i16> %vrhadd_v2.i to <8 x i8>
  store <4 x i16> %vrhadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrhaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrhaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.srhadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vrhaddq_v3.i = bitcast <8 x i16> %vrhaddq_v2.i to <16 x i8>
  store <8 x i16> %vrhaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrhadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vrhadd_v2.i = call <2 x i32> @llvm.aarch64.neon.srhadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vrhadd_v3.i = bitcast <2 x i32> %vrhadd_v2.i to <8 x i8>
  store <2 x i32> %vrhadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrhaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrhaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.srhadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vrhaddq_v3.i = bitcast <4 x i32> %vrhaddq_v2.i to <16 x i8>
  store <4 x i32> %vrhaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrhadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vrhadd_v.i = call <8 x i8> @llvm.aarch64.neon.urhadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vrhadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrhaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vrhaddq_v.i = call <16 x i8> @llvm.aarch64.neon.urhadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vrhaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrhadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vrhadd_v2.i = call <4 x i16> @llvm.aarch64.neon.urhadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vrhadd_v3.i = bitcast <4 x i16> %vrhadd_v2.i to <8 x i8>
  store <4 x i16> %vrhadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrhaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrhaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.urhadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vrhaddq_v3.i = bitcast <8 x i16> %vrhaddq_v2.i to <16 x i8>
  store <8 x i16> %vrhaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrhadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vrhadd_v2.i = call <2 x i32> @llvm.aarch64.neon.urhadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vrhadd_v3.i = bitcast <2 x i32> %vrhadd_v2.i to <8 x i8>
  store <2 x i32> %vrhadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrhaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrhaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.urhadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vrhaddq_v3.i = bitcast <4 x i32> %vrhaddq_v2.i to <16 x i8>
  store <4 x i32> %vrhaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqadd_v.i = call <8 x i8> @llvm.aarch64.neon.sqadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqaddq_v.i = call <16 x i8> @llvm.aarch64.neon.sqadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqadd_v2.i = call <4 x i16> @llvm.aarch64.neon.sqadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqadd_v3.i = bitcast <4 x i16> %vqadd_v2.i to <8 x i8>
  store <4 x i16> %vqadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqaddq_v3.i = bitcast <8 x i16> %vqaddq_v2.i to <16 x i8>
  store <8 x i16> %vqaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqadd_v2.i = call <2 x i32> @llvm.aarch64.neon.sqadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqadd_v3.i = bitcast <2 x i32> %vqadd_v2.i to <8 x i8>
  store <2 x i32> %vqadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqaddq_v3.i = bitcast <4 x i32> %vqaddq_v2.i to <16 x i8>
  store <4 x i32> %vqaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqaddq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqaddq_v2.i = call <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqaddq_v3.i = bitcast <2 x i64> %vqaddq_v2.i to <16 x i8>
  store <2 x i64> %vqaddq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqadd_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqadd_v2.i = call <1 x i64> @llvm.aarch64.neon.sqadd.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqadd_v3.i = bitcast <1 x i64> %vqadd_v2.i to <8 x i8>
  store <1 x i64> %vqadd_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqaddb_s8_wrapper(i8 signext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqaddb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqadd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqaddb_s8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqaddh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqaddh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqadd.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqaddh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqadds_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqadds_s32.i = call i32 @llvm.aarch64.neon.sqadd.i32(i32 %2, i32 %3) #4
  store i32 %vqadds_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqaddd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqaddd_s64.i = call i64 @llvm.aarch64.neon.sqadd.i64(i64 %2, i64 %3) #4
  store i64 %vqaddd_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqadd_v.i = call <8 x i8> @llvm.aarch64.neon.uqadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqaddq_v.i = call <16 x i8> @llvm.aarch64.neon.uqadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqadd_v2.i = call <4 x i16> @llvm.aarch64.neon.uqadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqadd_v3.i = bitcast <4 x i16> %vqadd_v2.i to <8 x i8>
  store <4 x i16> %vqadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.uqadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqaddq_v3.i = bitcast <8 x i16> %vqaddq_v2.i to <16 x i8>
  store <8 x i16> %vqaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqadd_v2.i = call <2 x i32> @llvm.aarch64.neon.uqadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqadd_v3.i = bitcast <2 x i32> %vqadd_v2.i to <8 x i8>
  store <2 x i32> %vqadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.uqadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqaddq_v3.i = bitcast <4 x i32> %vqaddq_v2.i to <16 x i8>
  store <4 x i32> %vqaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqaddq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqaddq_v2.i = call <2 x i64> @llvm.aarch64.neon.uqadd.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqaddq_v3.i = bitcast <2 x i64> %vqaddq_v2.i to <16 x i8>
  store <2 x i64> %vqaddq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqadd_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqadd_v2.i = call <1 x i64> @llvm.aarch64.neon.uqadd.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqadd_v3.i = bitcast <1 x i64> %vqadd_v2.i to <8 x i8>
  store <1 x i64> %vqadd_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqaddb_u8_wrapper(i8 zeroext %a, i8 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqaddb_u8.i = call <8 x i8> @llvm.aarch64.neon.uqadd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqaddb_u8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqaddh_u16_wrapper(i16 zeroext %a, i16 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqaddh_u16.i = call <4 x i16> @llvm.aarch64.neon.uqadd.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqaddh_u16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqadds_u32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqadds_u32.i = call i32 @llvm.aarch64.neon.uqadd.i32(i32 %2, i32 %3) #4
  store i32 %vqadds_u32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqaddd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqaddd_u64.i = call i64 @llvm.aarch64.neon.uqadd.i64(i64 %2, i64 %3) #4
  store i64 %vqaddd_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vuqadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vuqadd.i = call <8 x i8> @llvm.aarch64.neon.suqadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vuqadd.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vuqaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vuqadd.i = call <16 x i8> @llvm.aarch64.neon.suqadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vuqadd.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vuqadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vuqadd2.i = call <4 x i16> @llvm.aarch64.neon.suqadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vuqadd2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vuqaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vuqadd2.i = call <8 x i16> @llvm.aarch64.neon.suqadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vuqadd2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vuqadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vuqadd2.i = call <2 x i32> @llvm.aarch64.neon.suqadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vuqadd2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vuqaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vuqadd2.i = call <4 x i32> @llvm.aarch64.neon.suqadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vuqadd2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vuqaddq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vuqadd2.i = call <2 x i64> @llvm.aarch64.neon.suqadd.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  store <2 x i64> %vuqadd2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vuqadd_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vuqadd2.i = call <1 x i64> @llvm.aarch64.neon.suqadd.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  store <1 x i64> %vuqadd2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vuqaddb_s8_wrapper(i8 signext %a, i8 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vuqaddb_s8.i = call <8 x i8> @llvm.aarch64.neon.suqadd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vuqaddb_s8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vuqaddh_s16_wrapper(i16 signext %a, i16 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vuqaddh_s16.i = call <4 x i16> @llvm.aarch64.neon.suqadd.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vuqaddh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vuqadds_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vuqadds_s32.i = call i32 @llvm.aarch64.neon.suqadd.i32(i32 %2, i32 %3) #4
  store i32 %vuqadds_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vuqaddd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vuqaddd_s64.i = call i64 @llvm.aarch64.neon.suqadd.i64(i64 %2, i64 %3) #4
  store i64 %vuqaddd_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vsqadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vsqadd.i = call <8 x i8> @llvm.aarch64.neon.usqadd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vsqadd.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vsqaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vsqadd.i = call <16 x i8> @llvm.aarch64.neon.usqadd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vsqadd.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vsqadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vsqadd2.i = call <4 x i16> @llvm.aarch64.neon.usqadd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vsqadd2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsqaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vsqadd2.i = call <8 x i16> @llvm.aarch64.neon.usqadd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vsqadd2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vsqadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vsqadd2.i = call <2 x i32> @llvm.aarch64.neon.usqadd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vsqadd2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsqaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vsqadd2.i = call <4 x i32> @llvm.aarch64.neon.usqadd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vsqadd2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsqaddq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vsqadd2.i = call <2 x i64> @llvm.aarch64.neon.usqadd.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  store <2 x i64> %vsqadd2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vsqadd_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vsqadd2.i = call <1 x i64> @llvm.aarch64.neon.usqadd.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  store <1 x i64> %vsqadd2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vsqaddb_u8_wrapper(i8 zeroext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vsqaddb_u8.i = call <8 x i8> @llvm.aarch64.neon.usqadd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vsqaddb_u8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vsqaddh_u16_wrapper(i16 zeroext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vsqaddh_u16.i = call <4 x i16> @llvm.aarch64.neon.usqadd.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vsqaddh_u16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vsqadds_u32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vsqadds_u32.i = call i32 @llvm.aarch64.neon.usqadd.i32(i32 %2, i32 %3) #4
  store i32 %vsqadds_u32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vsqaddd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vsqaddd_u64.i = call i64 @llvm.aarch64.neon.usqadd.i64(i64 %2, i64 %3) #4
  store i64 %vsqaddd_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vaddhn_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vaddhn.i = add <8 x i16> %2, %4
  %vaddhn1.i = lshr <8 x i16> %vaddhn.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vaddhn2.i = trunc <8 x i16> %vaddhn1.i to <8 x i8>
  store <8 x i8> %vaddhn2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vaddhn_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vaddhn.i = add <4 x i32> %2, %4
  %vaddhn1.i = lshr <4 x i32> %vaddhn.i, <i32 16, i32 16, i32 16, i32 16>
  %vaddhn2.i = trunc <4 x i32> %vaddhn1.i to <4 x i16>
  store <4 x i16> %vaddhn2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vaddhn_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vaddhn.i = add <2 x i64> %2, %4
  %vaddhn1.i = lshr <2 x i64> %vaddhn.i, <i64 32, i64 32>
  %vaddhn2.i = trunc <2 x i64> %vaddhn1.i to <2 x i32>
  store <2 x i32> %vaddhn2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vaddhn_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vaddhn.i = add <8 x i16> %2, %4
  %vaddhn1.i = lshr <8 x i16> %vaddhn.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vaddhn2.i = trunc <8 x i16> %vaddhn1.i to <8 x i8>
  store <8 x i8> %vaddhn2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vaddhn_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vaddhn.i = add <4 x i32> %2, %4
  %vaddhn1.i = lshr <4 x i32> %vaddhn.i, <i32 16, i32 16, i32 16, i32 16>
  %vaddhn2.i = trunc <4 x i32> %vaddhn1.i to <4 x i16>
  store <4 x i16> %vaddhn2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vaddhn_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vaddhn.i = add <2 x i64> %2, %4
  %vaddhn1.i = lshr <2 x i64> %vaddhn.i, <i64 32, i64 32>
  %vaddhn2.i = trunc <2 x i64> %vaddhn1.i to <2 x i32>
  store <2 x i32> %vaddhn2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vaddhn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vaddhn.i.i = add <8 x i16> %6, %8
  %vaddhn1.i.i = lshr <8 x i16> %vaddhn.i.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vaddhn2.i.i = trunc <8 x i16> %vaddhn1.i.i to <8 x i8>
  store <8 x i8> %vaddhn2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddhn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vaddhn.i.i = add <4 x i32> %6, %8
  %vaddhn1.i.i = lshr <4 x i32> %vaddhn.i.i, <i32 16, i32 16, i32 16, i32 16>
  %vaddhn2.i.i = trunc <4 x i32> %vaddhn1.i.i to <4 x i16>
  store <4 x i16> %vaddhn2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddhn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vaddhn.i.i = add <2 x i64> %6, %8
  %vaddhn1.i.i = lshr <2 x i64> %vaddhn.i.i, <i64 32, i64 32>
  %vaddhn2.i.i = trunc <2 x i64> %vaddhn1.i.i to <2 x i32>
  store <2 x i32> %vaddhn2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vaddhn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vaddhn.i.i = add <8 x i16> %6, %8
  %vaddhn1.i.i = lshr <8 x i16> %vaddhn.i.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vaddhn2.i.i = trunc <8 x i16> %vaddhn1.i.i to <8 x i8>
  store <8 x i8> %vaddhn2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vaddhn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vaddhn.i.i = add <4 x i32> %6, %8
  %vaddhn1.i.i = lshr <4 x i32> %vaddhn.i.i, <i32 16, i32 16, i32 16, i32 16>
  %vaddhn2.i.i = trunc <4 x i32> %vaddhn1.i.i to <4 x i16>
  store <4 x i16> %vaddhn2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vaddhn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vaddhn.i.i = add <2 x i64> %6, %8
  %vaddhn1.i.i = lshr <2 x i64> %vaddhn.i.i, <i64 32, i64 32>
  %vaddhn2.i.i = trunc <2 x i64> %vaddhn1.i.i to <2 x i32>
  store <2 x i32> %vaddhn2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vraddhn_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vraddhn_v2.i = call <8 x i8> @llvm.aarch64.neon.raddhn.v8i8(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i8> %vraddhn_v2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vraddhn_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vraddhn_v2.i = call <4 x i16> @llvm.aarch64.neon.raddhn.v4i16(<4 x i32> %2, <4 x i32> %4) #4
  %vraddhn_v3.i = bitcast <4 x i16> %vraddhn_v2.i to <8 x i8>
  store <4 x i16> %vraddhn_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vraddhn_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vraddhn_v2.i = call <2 x i32> @llvm.aarch64.neon.raddhn.v2i32(<2 x i64> %2, <2 x i64> %4) #4
  %vraddhn_v3.i = bitcast <2 x i32> %vraddhn_v2.i to <8 x i8>
  store <2 x i32> %vraddhn_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vraddhn_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vraddhn_v2.i = call <8 x i8> @llvm.aarch64.neon.raddhn.v8i8(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i8> %vraddhn_v2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vraddhn_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vraddhn_v2.i = call <4 x i16> @llvm.aarch64.neon.raddhn.v4i16(<4 x i32> %2, <4 x i32> %4) #4
  %vraddhn_v3.i = bitcast <4 x i16> %vraddhn_v2.i to <8 x i8>
  store <4 x i16> %vraddhn_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vraddhn_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vraddhn_v2.i = call <2 x i32> @llvm.aarch64.neon.raddhn.v2i32(<2 x i64> %2, <2 x i64> %4) #4
  %vraddhn_v3.i = bitcast <2 x i32> %vraddhn_v2.i to <8 x i8>
  store <2 x i32> %vraddhn_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vraddhn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <8 x i8> @llvm.aarch64.neon.raddhn.v8i8(<8 x i16> %6, <8 x i16> %8) #4
  store <8 x i8> %vraddhn_v2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vraddhn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <4 x i16> @llvm.aarch64.neon.raddhn.v4i16(<4 x i32> %6, <4 x i32> %8) #4
  %vraddhn_v3.i.i = bitcast <4 x i16> %vraddhn_v2.i.i to <8 x i8>
  store <4 x i16> %vraddhn_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vraddhn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <2 x i32> @llvm.aarch64.neon.raddhn.v2i32(<2 x i64> %6, <2 x i64> %8) #4
  %vraddhn_v3.i.i = bitcast <2 x i32> %vraddhn_v2.i.i to <8 x i8>
  store <2 x i32> %vraddhn_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vraddhn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <8 x i8> @llvm.aarch64.neon.raddhn.v8i8(<8 x i16> %6, <8 x i16> %8) #4
  store <8 x i8> %vraddhn_v2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vraddhn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <4 x i16> @llvm.aarch64.neon.raddhn.v4i16(<4 x i32> %6, <4 x i32> %8) #4
  %vraddhn_v3.i.i = bitcast <4 x i16> %vraddhn_v2.i.i to <8 x i8>
  store <4 x i16> %vraddhn_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vraddhn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vraddhn_v2.i.i = call <2 x i32> @llvm.aarch64.neon.raddhn.v2i32(<2 x i64> %6, <2 x i64> %8) #4
  %vraddhn_v3.i.i = bitcast <2 x i32> %vraddhn_v2.i.i to <8 x i8>
  store <2 x i32> %vraddhn_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmul_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %mul.i = mul <8 x i8> %2, %3
  store <8 x i8> %mul.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmulq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %mul.i = mul <16 x i8> %2, %3
  store <16 x i8> %mul.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmul_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %mul.i = mul <4 x i16> %2, %3
  store <4 x i16> %mul.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmulq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %mul.i = mul <8 x i16> %2, %3
  store <8 x i16> %mul.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmul_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %mul.i = mul <2 x i32> %2, %3
  store <2 x i32> %mul.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmulq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %mul.i = mul <4 x i32> %2, %3
  store <4 x i32> %mul.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmul_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %mul.i = mul <8 x i8> %2, %3
  store <8 x i8> %mul.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmulq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %mul.i = mul <16 x i8> %2, %3
  store <16 x i8> %mul.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmul_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %mul.i = mul <4 x i16> %2, %3
  store <4 x i16> %mul.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmulq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %mul.i = mul <8 x i16> %2, %3
  store <8 x i16> %mul.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmul_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %mul.i = mul <2 x i32> %2, %3
  store <2 x i32> %mul.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmulq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %mul.i = mul <4 x i32> %2, %3
  store <4 x i32> %mul.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmla_s8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %mul.i = mul <8 x i8> %4, %5
  %add.i = add <8 x i8> %3, %mul.i
  store <8 x i8> %add.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmlaq_s8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %mul.i = mul <16 x i8> %4, %5
  %add.i = add <16 x i8> %3, %mul.i
  store <16 x i8> %add.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmla_s16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %mul.i = mul <4 x i16> %4, %5
  %add.i = add <4 x i16> %3, %mul.i
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlaq_s16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %mul.i = mul <8 x i16> %4, %5
  %add.i = add <8 x i16> %3, %mul.i
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmla_s32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %mul.i = mul <2 x i32> %4, %5
  %add.i = add <2 x i32> %3, %mul.i
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlaq_s32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %mul.i = mul <4 x i32> %4, %5
  %add.i = add <4 x i32> %3, %mul.i
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmla_u8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %mul.i = mul <8 x i8> %4, %5
  %add.i = add <8 x i8> %3, %mul.i
  store <8 x i8> %add.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmlaq_u8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %mul.i = mul <16 x i8> %4, %5
  %add.i = add <16 x i8> %3, %mul.i
  store <16 x i8> %add.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmla_u16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %mul.i = mul <4 x i16> %4, %5
  %add.i = add <4 x i16> %3, %mul.i
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlaq_u16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %mul.i = mul <8 x i16> %4, %5
  %add.i = add <8 x i16> %3, %mul.i
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmla_u32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %mul.i = mul <2 x i32> %4, %5
  %add.i = add <2 x i32> %3, %mul.i
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlaq_u32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %mul.i = mul <4 x i32> %4, %5
  %add.i = add <4 x i32> %3, %mul.i
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlal_s8_wrapper(<8 x i16> %a, <8 x i8> %b, <8 x i8> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  store <8 x i8> %4, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p1.addr.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %6, <8 x i8> %7) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %add.i = add <8 x i16> %3, %8
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_s16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p1.addr.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6, <4 x i16> %8) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %3, %10
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_s32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %3, %10
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlal_high_s8_wrapper(<8 x i16> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i3.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__p2.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %8 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  store <16 x i8> %8, <16 x i8>* %__p0.addr.i5.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %9, <16 x i8> %10, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i3.i, align 16
  store <8 x i8> %7, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p2.addr.i.i, align 8
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i3.i, align 16
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__p2.addr.i.i, align 8
  store <8 x i8> %13, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p1.addr.i.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %16 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vmull.i.i.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %15, <8 x i8> %16) #4
  store <8 x i16> %vmull.i.i.i, <8 x i16>* %__ret.i.i.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i.i.i, align 16
  %add.i.i = add <8 x i16> %12, %17
  store <8 x i16> %add.i.i, <8 x i16>* %__ret.i4.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %18, <8 x i16>* %__ret.i, align 16
  %19 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  store <4 x i16> %13, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %14, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %17 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %15, <4 x i16> %17) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %add.i.i = add <4 x i32> %12, %19
  store <4 x i32> %add.i.i, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %14, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <2 x i32> %15 to <8 x i8>
  %17 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %15, <2 x i32> %17) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %add.i.i = add <2 x i64> %12, %19
  store <2 x i64> %add.i.i, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlal_u8_wrapper(<8 x i16> %a, <8 x i8> %b, <8 x i8> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  store <8 x i8> %4, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p1.addr.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %6, <8 x i8> %7) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %add.i = add <8 x i16> %3, %8
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_u16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p1.addr.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %6, <4 x i16> %8) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %3, %10
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_u32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %3, %10
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlal_high_u8_wrapper(<8 x i16> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i3.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__p2.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %8 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  store <16 x i8> %8, <16 x i8>* %__p0.addr.i5.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %9, <16 x i8> %10, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i3.i, align 16
  store <8 x i8> %7, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p2.addr.i.i, align 8
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i3.i, align 16
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__p2.addr.i.i, align 8
  store <8 x i8> %13, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p1.addr.i.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %16 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vmull.i.i.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %15, <8 x i8> %16) #4
  store <8 x i16> %vmull.i.i.i, <8 x i16>* %__ret.i.i.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i.i.i, align 16
  %add.i.i = add <8 x i16> %12, %17
  store <8 x i16> %add.i.i, <8 x i16>* %__ret.i4.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %18, <8 x i16>* %__ret.i, align 16
  %19 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_high_u16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  store <4 x i16> %13, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %14, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %17 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %15, <4 x i16> %17) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %add.i.i = add <4 x i32> %12, %19
  store <4 x i32> %add.i.i, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_high_u32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %14, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <2 x i32> %15 to <8 x i8>
  %17 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %15, <2 x i32> %17) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %add.i.i = add <2 x i64> %12, %19
  store <2 x i64> %add.i.i, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmls_s8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %mul.i = mul <8 x i8> %4, %5
  %sub.i = sub <8 x i8> %3, %mul.i
  store <8 x i8> %sub.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmlsq_s8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %mul.i = mul <16 x i8> %4, %5
  %sub.i = sub <16 x i8> %3, %mul.i
  store <16 x i8> %sub.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmls_s16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %mul.i = mul <4 x i16> %4, %5
  %sub.i = sub <4 x i16> %3, %mul.i
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsq_s16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %mul.i = mul <8 x i16> %4, %5
  %sub.i = sub <8 x i16> %3, %mul.i
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmls_s32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %mul.i = mul <2 x i32> %4, %5
  %sub.i = sub <2 x i32> %3, %mul.i
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsq_s32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %mul.i = mul <4 x i32> %4, %5
  %sub.i = sub <4 x i32> %3, %mul.i
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmls_u8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %mul.i = mul <8 x i8> %4, %5
  %sub.i = sub <8 x i8> %3, %mul.i
  store <8 x i8> %sub.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmlsq_u8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %mul.i = mul <16 x i8> %4, %5
  %sub.i = sub <16 x i8> %3, %mul.i
  store <16 x i8> %sub.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmls_u16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %mul.i = mul <4 x i16> %4, %5
  %sub.i = sub <4 x i16> %3, %mul.i
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsq_u16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %mul.i = mul <8 x i16> %4, %5
  %sub.i = sub <8 x i16> %3, %mul.i
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmls_u32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %mul.i = mul <2 x i32> %4, %5
  %sub.i = sub <2 x i32> %3, %mul.i
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsq_u32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %mul.i = mul <4 x i32> %4, %5
  %sub.i = sub <4 x i32> %3, %mul.i
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsl_s8_wrapper(<8 x i16> %a, <8 x i8> %b, <8 x i8> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  store <8 x i8> %4, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p1.addr.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %6, <8 x i8> %7) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %sub.i = sub <8 x i16> %3, %8
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_s16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p1.addr.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6, <4 x i16> %8) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %3, %10
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_s32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %3, %10
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsl_high_s8_wrapper(<8 x i16> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i3.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__p2.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %8 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  store <16 x i8> %8, <16 x i8>* %__p0.addr.i5.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %9, <16 x i8> %10, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i3.i, align 16
  store <8 x i8> %7, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p2.addr.i.i, align 8
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i3.i, align 16
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__p2.addr.i.i, align 8
  store <8 x i8> %13, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p1.addr.i.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %16 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vmull.i.i.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %15, <8 x i8> %16) #4
  store <8 x i16> %vmull.i.i.i, <8 x i16>* %__ret.i.i.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i.i.i, align 16
  %sub.i.i = sub <8 x i16> %12, %17
  store <8 x i16> %sub.i.i, <8 x i16>* %__ret.i4.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %18, <8 x i16>* %__ret.i, align 16
  %19 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  store <4 x i16> %13, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %14, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %17 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %15, <4 x i16> %17) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %sub.i.i = sub <4 x i32> %12, %19
  store <4 x i32> %sub.i.i, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %14, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <2 x i32> %15 to <8 x i8>
  %17 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %15, <2 x i32> %17) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %sub.i.i = sub <2 x i64> %12, %19
  store <2 x i64> %sub.i.i, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsl_u8_wrapper(<8 x i16> %a, <8 x i8> %b, <8 x i8> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  store <8 x i8> %4, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p1.addr.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %6, <8 x i8> %7) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %sub.i = sub <8 x i16> %3, %8
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_u16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p1.addr.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %6, <4 x i16> %8) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %3, %10
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_u32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %3, %10
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsl_high_u8_wrapper(<8 x i16> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i3.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__p2.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %8 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  store <16 x i8> %8, <16 x i8>* %__p0.addr.i5.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %9, <16 x i8> %10, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i3.i, align 16
  store <8 x i8> %7, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p2.addr.i.i, align 8
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i3.i, align 16
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__p2.addr.i.i, align 8
  store <8 x i8> %13, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p1.addr.i.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %16 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vmull.i.i.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %15, <8 x i8> %16) #4
  store <8 x i16> %vmull.i.i.i, <8 x i16>* %__ret.i.i.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i.i.i, align 16
  %sub.i.i = sub <8 x i16> %12, %17
  store <8 x i16> %sub.i.i, <8 x i16>* %__ret.i4.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %18, <8 x i16>* %__ret.i, align 16
  %19 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_high_u16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  store <4 x i16> %13, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %14, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %17 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %15, <4 x i16> %17) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %sub.i.i = sub <4 x i32> %12, %19
  store <4 x i32> %sub.i.i, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_high_u32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %14, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %16 = bitcast <2 x i32> %15 to <8 x i8>
  %17 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %15, <2 x i32> %17) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %sub.i.i = sub <2 x i64> %12, %19
  store <2 x i64> %sub.i.i, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqdmulh_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqdmulh_v2.i = call <4 x i16> @llvm.aarch64.neon.sqdmulh.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqdmulh_v3.i = bitcast <4 x i16> %vqdmulh_v2.i to <8 x i8>
  store <4 x i16> %vqdmulh_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqdmulhq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqdmulhq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqdmulh.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqdmulhq_v3.i = bitcast <8 x i16> %vqdmulhq_v2.i to <16 x i8>
  store <8 x i16> %vqdmulhq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqdmulh_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqdmulh_v2.i = call <2 x i32> @llvm.aarch64.neon.sqdmulh.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqdmulh_v3.i = bitcast <2 x i32> %vqdmulh_v2.i to <8 x i8>
  store <2 x i32> %vqdmulh_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmulhq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqdmulhq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqdmulh.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqdmulhq_v3.i = bitcast <4 x i32> %vqdmulhq_v2.i to <16 x i8>
  store <4 x i32> %vqdmulhq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqdmulhh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqdmulhh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqdmulh.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqdmulhh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqdmulhs_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqdmulhs_s32.i = call i32 @llvm.aarch64.neon.sqdmulh.i32(i32 %2, i32 %3) #4
  store i32 %vqdmulhs_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqrdmulh_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqrdmulh_v2.i = call <4 x i16> @llvm.aarch64.neon.sqrdmulh.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqrdmulh_v3.i = bitcast <4 x i16> %vqrdmulh_v2.i to <8 x i8>
  store <4 x i16> %vqrdmulh_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqrdmulhq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqrdmulhq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqrdmulh.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqrdmulhq_v3.i = bitcast <8 x i16> %vqrdmulhq_v2.i to <16 x i8>
  store <8 x i16> %vqrdmulhq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqrdmulh_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqrdmulh_v2.i = call <2 x i32> @llvm.aarch64.neon.sqrdmulh.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqrdmulh_v3.i = bitcast <2 x i32> %vqrdmulh_v2.i to <8 x i8>
  store <2 x i32> %vqrdmulh_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqrdmulhq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqrdmulhq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqrdmulhq_v3.i = bitcast <4 x i32> %vqrdmulhq_v2.i to <16 x i8>
  store <4 x i32> %vqrdmulhq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqrdmulhh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqrdmulhh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqrdmulh.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqrdmulhh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqrdmulhs_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqrdmulhs_s32.i = call i32 @llvm.aarch64.neon.sqrdmulh.i32(i32 %2, i32 %3) #4
  store i32 %vqrdmulhs_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlal_s16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = bitcast <4 x i32> %3 to <16 x i8>
  %5 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %6 = bitcast <4 x i16> %5 to <8 x i8>
  %7 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vqdmlal2.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %5, <4 x i16> %7) #4
  %vqdmlal_v3.i = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %3, <4 x i32> %vqdmlal2.i) #4
  store <4 x i32> %vqdmlal_v3.i, <4 x i32>* %__ret.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlal_s32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %6 = bitcast <2 x i32> %5 to <8 x i8>
  %7 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vqdmlal2.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %5, <2 x i32> %7) #4
  %vqdmlal_v3.i = call <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64> %3, <2 x i64> %vqdmlal2.i) #4
  store <2 x i64> %vqdmlal_v3.i, <2 x i64>* %__ret.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlal_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = bitcast <4 x i32> %12 to <16 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %16 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %vqdmlal2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %14, <4 x i16> %16) #4
  %vqdmlal_v3.i.i = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %12, <4 x i32> %vqdmlal2.i.i) #4
  store <4 x i32> %vqdmlal_v3.i.i, <4 x i32>* %__ret.i4.i, align 16
  %18 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %18, <4 x i32>* %__ret.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlal_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vqdmlal2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  %vqdmlal_v3.i.i = call <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64> %12, <2 x i64> %vqdmlal2.i.i) #4
  store <2 x i64> %vqdmlal_v3.i.i, <2 x i64>* %__ret.i4.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %18, <2 x i64>* %__ret.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqdmlalh_s16_wrapper(i32 %a, i16 signext %b, i16 signext %c) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i16, align 2
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i16, align 2
  %c.addr = alloca i16, align 2
  store i32 %a, i32* %a.addr, align 4
  store i16 %b, i16* %b.addr, align 2
  store i16 %c, i16* %c.addr, align 2
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i16, i16* %b.addr, align 2
  %2 = load i16, i16* %c.addr, align 2
  store i32 %0, i32* %__p0.addr.i, align 4
  store i16 %1, i16* %__p1.addr.i, align 2
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load i32, i32* %__p0.addr.i, align 4
  %4 = load i16, i16* %__p1.addr.i, align 2
  %5 = insertelement <4 x i16> undef, i16 %4, i64 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %7 = insertelement <4 x i16> undef, i16 %6, i64 0
  %vqdmlXl.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %5, <4 x i16> %7) #4
  %lane0.i = extractelement <4 x i32> %vqdmlXl.i, i64 0
  %vqdmlXl1.i = call i32 @llvm.aarch64.neon.sqadd.i32(i32 %3, i32 %lane0.i) #4
  store i32 %vqdmlXl1.i, i32* %__ret.i, align 4
  %8 = load i32, i32* %__ret.i, align 4
  ret i32 %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqdmlals_s32_wrapper(i64 %a, i32 %b, i32 %c) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i32, align 4
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i32, align 4
  %c.addr = alloca i32, align 4
  store i64 %a, i64* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  store i32 %c, i32* %c.addr, align 4
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  %2 = load i32, i32* %c.addr, align 4
  store i64 %0, i64* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load i64, i64* %__p0.addr.i, align 8
  %4 = load i32, i32* %__p1.addr.i, align 4
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vqdmlXl.i = call i64 @llvm.aarch64.neon.sqdmulls.scalar(i32 %4, i32 %5) #4
  %vqdmlXl1.i = call i64 @llvm.aarch64.neon.sqadd.i64(i64 %3, i64 %vqdmlXl.i) #4
  store i64 %vqdmlXl1.i, i64* %__ret.i, align 8
  %6 = load i64, i64* %__ret.i, align 8
  ret i64 %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlsl_s16_wrapper(<4 x i32> %a, <4 x i16> %b, <4 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = bitcast <4 x i32> %3 to <16 x i8>
  %5 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %6 = bitcast <4 x i16> %5 to <8 x i8>
  %7 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vqdmlal2.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %5, <4 x i16> %7) #4
  %vqdmlsl_v3.i = call <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32> %3, <4 x i32> %vqdmlal2.i) #4
  store <4 x i32> %vqdmlsl_v3.i, <4 x i32>* %__ret.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlsl_s32_wrapper(<2 x i64> %a, <2 x i32> %b, <2 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %6 = bitcast <2 x i32> %5 to <8 x i8>
  %7 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vqdmlal2.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %5, <2 x i32> %7) #4
  %vqdmlsl_v3.i = call <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64> %3, <2 x i64> %vqdmlal2.i) #4
  store <2 x i64> %vqdmlsl_v3.i, <2 x i64>* %__ret.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlsl_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %8, <8 x i16>* %__p0.addr.i5.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %9, <8 x i16> %10, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i3.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p2.addr.i.i, align 8
  %12 = load <4 x i32>, <4 x i32>* %__p0.addr.i3.i, align 16
  %13 = bitcast <4 x i32> %12 to <16 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %16 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %vqdmlal2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %14, <4 x i16> %16) #4
  %vqdmlsl_v3.i.i = call <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32> %12, <4 x i32> %vqdmlal2.i.i) #4
  store <4 x i32> %vqdmlsl_v3.i.i, <4 x i32>* %__ret.i4.i, align 16
  %18 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %18, <4 x i32>* %__ret.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlsl_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %8, <4 x i32>* %__p0.addr.i5.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %9, <4 x i32> %10, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i3.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p2.addr.i.i, align 8
  %12 = load <2 x i64>, <2 x i64>* %__p0.addr.i3.i, align 16
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vqdmlal2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  %vqdmlsl_v3.i.i = call <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64> %12, <2 x i64> %vqdmlal2.i.i) #4
  store <2 x i64> %vqdmlsl_v3.i.i, <2 x i64>* %__ret.i4.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %18, <2 x i64>* %__ret.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqdmlslh_s16_wrapper(i32 %a, i16 signext %b, i16 signext %c) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i16, align 2
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i16, align 2
  %c.addr = alloca i16, align 2
  store i32 %a, i32* %a.addr, align 4
  store i16 %b, i16* %b.addr, align 2
  store i16 %c, i16* %c.addr, align 2
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i16, i16* %b.addr, align 2
  %2 = load i16, i16* %c.addr, align 2
  store i32 %0, i32* %__p0.addr.i, align 4
  store i16 %1, i16* %__p1.addr.i, align 2
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load i32, i32* %__p0.addr.i, align 4
  %4 = load i16, i16* %__p1.addr.i, align 2
  %5 = insertelement <4 x i16> undef, i16 %4, i64 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %7 = insertelement <4 x i16> undef, i16 %6, i64 0
  %vqdmlXl.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %5, <4 x i16> %7) #4
  %lane0.i = extractelement <4 x i32> %vqdmlXl.i, i64 0
  %vqdmlXl1.i = call i32 @llvm.aarch64.neon.sqsub.i32(i32 %3, i32 %lane0.i) #4
  store i32 %vqdmlXl1.i, i32* %__ret.i, align 4
  %8 = load i32, i32* %__ret.i, align 4
  ret i32 %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqdmlsls_s32_wrapper(i64 %a, i32 %b, i32 %c) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i32, align 4
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i32, align 4
  %c.addr = alloca i32, align 4
  store i64 %a, i64* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  store i32 %c, i32* %c.addr, align 4
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  %2 = load i32, i32* %c.addr, align 4
  store i64 %0, i64* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load i64, i64* %__p0.addr.i, align 8
  %4 = load i32, i32* %__p1.addr.i, align 4
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vqdmlXl.i = call i64 @llvm.aarch64.neon.sqdmulls.scalar(i32 %4, i32 %5) #4
  %vqdmlXl1.i = call i64 @llvm.aarch64.neon.sqsub.i64(i64 %3, i64 %vqdmlXl.i) #4
  store i64 %vqdmlXl1.i, i64* %__ret.i, align 8
  %6 = load i64, i64* %__ret.i, align 8
  ret i64 %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmull_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmull.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i16> %vmull.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmull2.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i32> %vmull2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmull2.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i64> %vmull2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmull_high_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i3.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %3, <16 x i8> %4, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %6 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %6, <16 x i8>* %__p0.addr.i5.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %8 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %7, <16 x i8> %8, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i3.i, align 8
  store <8 x i8> %9, <8 x i8>* %__p1.addr.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__p0.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8> %10, <8 x i8> %11) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i4.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_high_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %6, <8 x i16>* %__p0.addr.i5.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %7, <8 x i16> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i3.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i3.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i4.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %14, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_high_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %6, <4 x i32>* %__p0.addr.i5.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %7, <4 x i32> %8, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i3.i, align 8
  store <2 x i32> %9, <2 x i32>* %__p1.addr.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__p0.addr.i3.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %10, <2 x i32> %12) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i4.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %14, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmull_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmull.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i16> %vmull.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmull2.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i32> %vmull2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmull2.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i64> %vmull2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmull_high_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i3.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %3, <16 x i8> %4, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %6 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %6, <16 x i8>* %__p0.addr.i5.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %8 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %7, <16 x i8> %8, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i3.i, align 8
  store <8 x i8> %9, <8 x i8>* %__p1.addr.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__p0.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vmull.i.i = call <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8> %10, <8 x i8> %11) #4
  store <8 x i16> %vmull.i.i, <8 x i16>* %__ret.i4.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_high_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %6, <8 x i16>* %__p0.addr.i5.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %7, <8 x i16> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i3.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i3.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i4.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %14, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_high_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %6, <4 x i32>* %__p0.addr.i5.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %7, <4 x i32> %8, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i3.i, align 8
  store <2 x i32> %9, <2 x i32>* %__p1.addr.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__p0.addr.i3.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %10, <2 x i32> %12) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i4.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %14, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmull_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqdmull_v2.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %2, <4 x i16> %4) #4
  %vqdmull_v3.i = bitcast <4 x i32> %vqdmull_v2.i to <16 x i8>
  store <4 x i32> %vqdmull_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmull_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqdmull_v2.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %2, <2 x i32> %4) #4
  %vqdmull_v3.i = bitcast <2 x i64> %vqdmull_v2.i to <16 x i8>
  store <2 x i64> %vqdmull_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmull_high_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %6, <8 x i16>* %__p0.addr.i5.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %7, <8 x i16> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i3.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i3.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vqdmull_v2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  %vqdmull_v3.i.i = bitcast <4 x i32> %vqdmull_v2.i.i to <16 x i8>
  store <4 x i32> %vqdmull_v2.i.i, <4 x i32>* %__ret.i4.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %14, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmull_high_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %6, <4 x i32>* %__p0.addr.i5.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %7, <4 x i32> %8, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i3.i, align 8
  store <2 x i32> %9, <2 x i32>* %__p1.addr.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__p0.addr.i3.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %vqdmull_v2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %10, <2 x i32> %12) #4
  %vqdmull_v3.i.i = bitcast <2 x i64> %vqdmull_v2.i.i to <16 x i8>
  store <2 x i64> %vqdmull_v2.i.i, <2 x i64>* %__ret.i4.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %14, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqdmullh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqdmullh_s16.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i32> %vqdmullh_s16.i, i64 0
  store i32 %6, i32* %__ret.i, align 4
  %7 = load i32, i32* %__ret.i, align 4
  ret i32 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqdmulls_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqdmulls_s32.i = call i64 @llvm.aarch64.neon.sqdmulls.scalar(i32 %2, i32 %3) #4
  store i64 %vqdmulls_s32.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vsub_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %sub.i = sub <8 x i8> %2, %3
  store <8 x i8> %sub.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vsubq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %sub.i = sub <16 x i8> %2, %3
  store <16 x i8> %sub.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vsub_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %sub.i = sub <4 x i16> %2, %3
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %sub.i = sub <8 x i16> %2, %3
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vsub_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %sub.i = sub <2 x i32> %2, %3
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %sub.i = sub <4 x i32> %2, %3
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %sub.i = sub <2 x i64> %2, %3
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vsub_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %sub.i = sub <8 x i8> %2, %3
  store <8 x i8> %sub.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vsubq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %sub.i = sub <16 x i8> %2, %3
  store <16 x i8> %sub.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vsub_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %sub.i = sub <4 x i16> %2, %3
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %sub.i = sub <8 x i16> %2, %3
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vsub_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %sub.i = sub <2 x i32> %2, %3
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %sub.i = sub <4 x i32> %2, %3
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %sub.i = sub <2 x i64> %2, %3
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vsub_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %sub.i = sub <1 x i64> %2, %3
  store <1 x i64> %sub.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vsub_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %sub.i = sub <1 x i64> %2, %3
  store <1 x i64> %sub.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vsubd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vsubd.i = sub i64 %2, %3
  store i64 %vsubd.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vsubd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vsubd.i = sub i64 %2, %3
  store i64 %vsubd.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = sext <8 x i8> %3 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i2.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i4.i = sext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %vmovl.i4.i, <8 x i16>* %__ret.i3.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  %sub.i = sub <8 x i16> %4, %7
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %vmovl.i.i = sext <4 x i16> %3 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p0.addr.i2.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vmovl.i4.i = sext <4 x i16> %7 to <4 x i32>
  store <4 x i32> %vmovl.i4.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  %sub.i = sub <4 x i32> %5, %9
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %vmovl.i.i = sext <2 x i32> %3 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p0.addr.i2.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vmovl.i4.i = sext <2 x i32> %7 to <2 x i64>
  store <2 x i64> %vmovl.i4.i, <2 x i64>* %__ret.i3.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  %sub.i = sub <2 x i64> %5, %9
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubl_high_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <16 x i8>, align 16
  %__ret.i.i3.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i4.i = alloca <16 x i8>, align 16
  %__ret_612.i5.i = alloca <8 x i16>, align 16
  %__a1_612.i6.i = alloca <8 x i8>, align 8
  %__s0.i7.i = alloca <8 x i8>, align 8
  %__ret.i8.i = alloca <8 x i16>, align 16
  %tmp.i9.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i.i = alloca <16 x i8>, align 16
  %__ret_612.i.i = alloca <8 x i16>, align 16
  %__a1_612.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0_612.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0.addr.i.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %4, <16 x i8> %5, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__a1_612.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__a1_612.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__s0.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %9 = sext <8 x i8> %8 to <8 x i16>
  store <8 x i16> %9, <8 x i16>* %__ret.i.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %10, <8 x i16>* %tmp.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__ret_612.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret_612.i.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %13, <16 x i8>* %__p0_612.addr.i4.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i4.i, align 16
  store <16 x i8> %14, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %15 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %16 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <16 x i8> %15, <16 x i8> %16, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i10.i, <8 x i8>* %__ret.i.i3.i, align 8
  %17 = load <8 x i8>, <8 x i8>* %__ret.i.i3.i, align 8
  store <8 x i8> %17, <8 x i8>* %__a1_612.i6.i, align 8
  %18 = load <8 x i8>, <8 x i8>* %__a1_612.i6.i, align 8
  store <8 x i8> %18, <8 x i8>* %__s0.i7.i, align 8
  %19 = load <8 x i8>, <8 x i8>* %__s0.i7.i, align 8
  %20 = sext <8 x i8> %19 to <8 x i16>
  store <8 x i16> %20, <8 x i16>* %__ret.i8.i, align 16
  %21 = load <8 x i16>, <8 x i16>* %__ret.i8.i, align 16
  store <8 x i16> %21, <8 x i16>* %tmp.i9.i, align 16
  %22 = load <8 x i16>, <8 x i16>* %tmp.i9.i, align 16
  store <8 x i16> %22, <8 x i16>* %__ret_612.i5.i, align 16
  %23 = load <8 x i16>, <8 x i16>* %__ret_612.i5.i, align 16
  %sub.i = sub <8 x i16> %12, %23
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %24 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubl_high_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <8 x i16>, align 16
  %__ret.i.i3.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i4.i = alloca <8 x i16>, align 16
  %__ret_618.i5.i = alloca <4 x i32>, align 16
  %__a1_618.i6.i = alloca <4 x i16>, align 8
  %__s0.i7.i = alloca <4 x i16>, align 8
  %__ret.i8.i = alloca <4 x i32>, align 16
  %tmp.i9.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i.i = alloca <8 x i16>, align 16
  %__ret_618.i.i = alloca <4 x i32>, align 16
  %__a1_618.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0_618.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %4, <8 x i16> %5, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %6, <4 x i16>* %__a1_618.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__a1_618.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__s0.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = sext <4 x i16> %8 to <4 x i32>
  store <4 x i32> %10, <4 x i32>* %__ret.i.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %11, <4 x i32>* %tmp.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret_618.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret_618.i.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %14, <8 x i16>* %__p0_618.addr.i4.i, align 16
  %15 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i4.i, align 16
  store <8 x i16> %15, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <8 x i16> %16, <8 x i16> %17, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i10.i, <4 x i16>* %__ret.i.i3.i, align 8
  %18 = load <4 x i16>, <4 x i16>* %__ret.i.i3.i, align 8
  store <4 x i16> %18, <4 x i16>* %__a1_618.i6.i, align 8
  %19 = load <4 x i16>, <4 x i16>* %__a1_618.i6.i, align 8
  store <4 x i16> %19, <4 x i16>* %__s0.i7.i, align 8
  %20 = load <4 x i16>, <4 x i16>* %__s0.i7.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %22 = sext <4 x i16> %20 to <4 x i32>
  store <4 x i32> %22, <4 x i32>* %__ret.i8.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i8.i, align 16
  store <4 x i32> %23, <4 x i32>* %tmp.i9.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %tmp.i9.i, align 16
  store <4 x i32> %24, <4 x i32>* %__ret_618.i5.i, align 16
  %25 = load <4 x i32>, <4 x i32>* %__ret_618.i5.i, align 16
  %sub.i = sub <4 x i32> %13, %25
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %26 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubl_high_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <4 x i32>, align 16
  %__ret.i.i3.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i4.i = alloca <4 x i32>, align 16
  %__ret_615.i5.i = alloca <2 x i64>, align 16
  %__a1_615.i6.i = alloca <2 x i32>, align 8
  %__s0.i7.i = alloca <2 x i32>, align 8
  %__ret.i8.i = alloca <2 x i64>, align 16
  %tmp.i9.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i.i = alloca <4 x i32>, align 16
  %__ret_615.i.i = alloca <2 x i64>, align 16
  %__a1_615.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0_615.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %4, <4 x i32> %5, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %6, <2 x i32>* %__a1_615.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__a1_615.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__s0.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = sext <2 x i32> %8 to <2 x i64>
  store <2 x i64> %10, <2 x i64>* %__ret.i.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %11, <2 x i64>* %tmp.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %__ret_615.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret_615.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %14, <4 x i32>* %__p0_615.addr.i4.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i4.i, align 16
  store <4 x i32> %15, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <4 x i32> %16, <4 x i32> %17, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i10.i, <2 x i32>* %__ret.i.i3.i, align 8
  %18 = load <2 x i32>, <2 x i32>* %__ret.i.i3.i, align 8
  store <2 x i32> %18, <2 x i32>* %__a1_615.i6.i, align 8
  %19 = load <2 x i32>, <2 x i32>* %__a1_615.i6.i, align 8
  store <2 x i32> %19, <2 x i32>* %__s0.i7.i, align 8
  %20 = load <2 x i32>, <2 x i32>* %__s0.i7.i, align 8
  %21 = bitcast <2 x i32> %20 to <8 x i8>
  %22 = sext <2 x i32> %20 to <2 x i64>
  store <2 x i64> %22, <2 x i64>* %__ret.i8.i, align 16
  %23 = load <2 x i64>, <2 x i64>* %__ret.i8.i, align 16
  store <2 x i64> %23, <2 x i64>* %tmp.i9.i, align 16
  %24 = load <2 x i64>, <2 x i64>* %tmp.i9.i, align 16
  store <2 x i64> %24, <2 x i64>* %__ret_615.i5.i, align 16
  %25 = load <2 x i64>, <2 x i64>* %__ret_615.i5.i, align 16
  %sub.i = sub <2 x i64> %13, %25
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %26 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = zext <8 x i8> %3 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i2.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i4.i = zext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %vmovl.i4.i, <8 x i16>* %__ret.i3.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  %sub.i = sub <8 x i16> %4, %7
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %3 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p0.addr.i2.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vmovl.i4.i = zext <4 x i16> %7 to <4 x i32>
  store <4 x i32> %vmovl.i4.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  %sub.i = sub <4 x i32> %5, %9
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %3 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p0.addr.i2.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vmovl.i4.i = zext <2 x i32> %7 to <2 x i64>
  store <2 x i64> %vmovl.i4.i, <2 x i64>* %__ret.i3.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  %sub.i = sub <2 x i64> %5, %9
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubl_high_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <16 x i8>, align 16
  %__ret.i.i3.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i4.i = alloca <16 x i8>, align 16
  %__ret_603.i5.i = alloca <8 x i16>, align 16
  %__a1_603.i6.i = alloca <8 x i8>, align 8
  %__s0.i7.i = alloca <8 x i8>, align 8
  %__ret.i8.i = alloca <8 x i16>, align 16
  %tmp.i9.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i.i = alloca <16 x i8>, align 16
  %__ret_603.i.i = alloca <8 x i16>, align 16
  %__a1_603.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0_603.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0.addr.i.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %4, <16 x i8> %5, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__a1_603.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__a1_603.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__s0.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %9 = zext <8 x i8> %8 to <8 x i16>
  store <8 x i16> %9, <8 x i16>* %__ret.i.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %10, <8 x i16>* %tmp.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__ret_603.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret_603.i.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %13, <16 x i8>* %__p0_603.addr.i4.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i4.i, align 16
  store <16 x i8> %14, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %15 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %16 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <16 x i8> %15, <16 x i8> %16, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i10.i, <8 x i8>* %__ret.i.i3.i, align 8
  %17 = load <8 x i8>, <8 x i8>* %__ret.i.i3.i, align 8
  store <8 x i8> %17, <8 x i8>* %__a1_603.i6.i, align 8
  %18 = load <8 x i8>, <8 x i8>* %__a1_603.i6.i, align 8
  store <8 x i8> %18, <8 x i8>* %__s0.i7.i, align 8
  %19 = load <8 x i8>, <8 x i8>* %__s0.i7.i, align 8
  %20 = zext <8 x i8> %19 to <8 x i16>
  store <8 x i16> %20, <8 x i16>* %__ret.i8.i, align 16
  %21 = load <8 x i16>, <8 x i16>* %__ret.i8.i, align 16
  store <8 x i16> %21, <8 x i16>* %tmp.i9.i, align 16
  %22 = load <8 x i16>, <8 x i16>* %tmp.i9.i, align 16
  store <8 x i16> %22, <8 x i16>* %__ret_603.i5.i, align 16
  %23 = load <8 x i16>, <8 x i16>* %__ret_603.i5.i, align 16
  %sub.i = sub <8 x i16> %12, %23
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %24 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubl_high_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <8 x i16>, align 16
  %__ret.i.i3.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i4.i = alloca <8 x i16>, align 16
  %__ret_609.i5.i = alloca <4 x i32>, align 16
  %__a1_609.i6.i = alloca <4 x i16>, align 8
  %__s0.i7.i = alloca <4 x i16>, align 8
  %__ret.i8.i = alloca <4 x i32>, align 16
  %tmp.i9.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i.i = alloca <8 x i16>, align 16
  %__ret_609.i.i = alloca <4 x i32>, align 16
  %__a1_609.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0_609.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %4, <8 x i16> %5, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %6, <4 x i16>* %__a1_609.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__a1_609.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__s0.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = zext <4 x i16> %8 to <4 x i32>
  store <4 x i32> %10, <4 x i32>* %__ret.i.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %11, <4 x i32>* %tmp.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret_609.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret_609.i.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %14, <8 x i16>* %__p0_609.addr.i4.i, align 16
  %15 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i4.i, align 16
  store <8 x i16> %15, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <8 x i16> %16, <8 x i16> %17, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i10.i, <4 x i16>* %__ret.i.i3.i, align 8
  %18 = load <4 x i16>, <4 x i16>* %__ret.i.i3.i, align 8
  store <4 x i16> %18, <4 x i16>* %__a1_609.i6.i, align 8
  %19 = load <4 x i16>, <4 x i16>* %__a1_609.i6.i, align 8
  store <4 x i16> %19, <4 x i16>* %__s0.i7.i, align 8
  %20 = load <4 x i16>, <4 x i16>* %__s0.i7.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %22 = zext <4 x i16> %20 to <4 x i32>
  store <4 x i32> %22, <4 x i32>* %__ret.i8.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i8.i, align 16
  store <4 x i32> %23, <4 x i32>* %tmp.i9.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %tmp.i9.i, align 16
  store <4 x i32> %24, <4 x i32>* %__ret_609.i5.i, align 16
  %25 = load <4 x i32>, <4 x i32>* %__ret_609.i5.i, align 16
  %sub.i = sub <4 x i32> %13, %25
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %26 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubl_high_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i2.i = alloca <4 x i32>, align 16
  %__ret.i.i3.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i4.i = alloca <4 x i32>, align 16
  %__ret_606.i5.i = alloca <2 x i64>, align 16
  %__a1_606.i6.i = alloca <2 x i32>, align 8
  %__s0.i7.i = alloca <2 x i32>, align 8
  %__ret.i8.i = alloca <2 x i64>, align 16
  %tmp.i9.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i.i = alloca <4 x i32>, align 16
  %__ret_606.i.i = alloca <2 x i64>, align 16
  %__a1_606.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0_606.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %4, <4 x i32> %5, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %6, <2 x i32>* %__a1_606.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__a1_606.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__s0.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = zext <2 x i32> %8 to <2 x i64>
  store <2 x i64> %10, <2 x i64>* %__ret.i.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %11, <2 x i64>* %tmp.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %__ret_606.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret_606.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %14, <4 x i32>* %__p0_606.addr.i4.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i4.i, align 16
  store <4 x i32> %15, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i2.i, align 16
  %shuffle.i.i10.i = shufflevector <4 x i32> %16, <4 x i32> %17, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i10.i, <2 x i32>* %__ret.i.i3.i, align 8
  %18 = load <2 x i32>, <2 x i32>* %__ret.i.i3.i, align 8
  store <2 x i32> %18, <2 x i32>* %__a1_606.i6.i, align 8
  %19 = load <2 x i32>, <2 x i32>* %__a1_606.i6.i, align 8
  store <2 x i32> %19, <2 x i32>* %__s0.i7.i, align 8
  %20 = load <2 x i32>, <2 x i32>* %__s0.i7.i, align 8
  %21 = bitcast <2 x i32> %20 to <8 x i8>
  %22 = zext <2 x i32> %20 to <2 x i64>
  store <2 x i64> %22, <2 x i64>* %__ret.i8.i, align 16
  %23 = load <2 x i64>, <2 x i64>* %__ret.i8.i, align 16
  store <2 x i64> %23, <2 x i64>* %tmp.i9.i, align 16
  %24 = load <2 x i64>, <2 x i64>* %tmp.i9.i, align 16
  store <2 x i64> %24, <2 x i64>* %__ret_606.i5.i, align 16
  %25 = load <2 x i64>, <2 x i64>* %__ret_606.i5.i, align 16
  %sub.i = sub <2 x i64> %13, %25
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %26 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %26
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubw_s8_wrapper(<8 x i16> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = sext <8 x i8> %4 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %sub.i = sub <8 x i16> %2, %5
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubw_s16_wrapper(<4 x i32> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmovl.i.i = sext <4 x i16> %4 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %2, %6
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubw_s32_wrapper(<2 x i64> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmovl.i.i = sext <2 x i32> %4 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %2, %6
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %7 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubw_high_s8_wrapper(<8 x i16> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i.i = alloca <16 x i8>, align 16
  %__ret_612.i.i = alloca <8 x i16>, align 16
  %__a1_612.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0_612.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__a1_612.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__a1_612.i.i, align 8
  store <8 x i8> %8, <8 x i8>* %__s0.i.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %10 = sext <8 x i8> %9 to <8 x i16>
  store <8 x i16> %10, <8 x i16>* %__ret.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %tmp.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret_612.i.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret_612.i.i, align 16
  %sub.i = sub <8 x i16> %2, %13
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubw_high_s16_wrapper(<4 x i32> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i.i = alloca <8 x i16>, align 16
  %__ret_618.i.i = alloca <4 x i32>, align 16
  %__a1_618.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0_618.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__a1_618.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__a1_618.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__s0.i.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %11 = sext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %11, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %tmp.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret_618.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret_618.i.i, align 16
  %sub.i = sub <4 x i32> %2, %14
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubw_high_s32_wrapper(<2 x i64> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i.i = alloca <4 x i32>, align 16
  %__ret_615.i.i = alloca <2 x i64>, align 16
  %__a1_615.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0_615.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__a1_615.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__a1_615.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__s0.i.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %11 = sext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %11, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %tmp.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %13, <2 x i64>* %__ret_615.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret_615.i.i, align 16
  %sub.i = sub <2 x i64> %2, %14
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubw_u8_wrapper(<8 x i16> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %vmovl.i.i = zext <8 x i8> %4 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  %sub.i = sub <8 x i16> %2, %5
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubw_u16_wrapper(<4 x i32> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %4 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %2, %6
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubw_u32_wrapper(<2 x i64> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %4 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %2, %6
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %7 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubw_high_u8_wrapper(<8 x i16> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <16 x i8>, align 16
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i.i = alloca <16 x i8>, align 16
  %__ret_603.i.i = alloca <8 x i16>, align 16
  %__a1_603.i.i = alloca <8 x i8>, align 8
  %__s0.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i16>, align 16
  %tmp.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %3, <16 x i8>* %__p0_603.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i.i, align 16
  store <16 x i8> %4, <16 x i8>* %__p0.addr.i.i.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <16 x i8> %5, <16 x i8> %6, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %7, <8 x i8>* %__a1_603.i.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__a1_603.i.i, align 8
  store <8 x i8> %8, <8 x i8>* %__s0.i.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__s0.i.i, align 8
  %10 = zext <8 x i8> %9 to <8 x i16>
  store <8 x i16> %10, <8 x i16>* %__ret.i.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %tmp.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %tmp.i.i, align 16
  store <8 x i16> %12, <8 x i16>* %__ret_603.i.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret_603.i.i, align 16
  %sub.i = sub <8 x i16> %2, %13
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubw_high_u16_wrapper(<4 x i32> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <8 x i16>, align 16
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i.i = alloca <8 x i16>, align 16
  %__ret_609.i.i = alloca <4 x i32>, align 16
  %__a1_609.i.i = alloca <4 x i16>, align 8
  %__s0.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %tmp.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0_609.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__a1_609.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__a1_609.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__s0.i.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__s0.i.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %11 = zext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %11, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %tmp.i.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %tmp.i.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret_609.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret_609.i.i, align 16
  %sub.i = sub <4 x i32> %2, %14
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vsubw_high_u32_wrapper(<2 x i64> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i.i = alloca <4 x i32>, align 16
  %__ret_606.i.i = alloca <2 x i64>, align 16
  %__a1_606.i.i = alloca <2 x i32>, align 8
  %__s0.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %tmp.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0_606.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %shuffle.i.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__a1_606.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__a1_606.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__s0.i.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__s0.i.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %11 = zext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %11, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %12, <2 x i64>* %tmp.i.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %tmp.i.i, align 16
  store <2 x i64> %13, <2 x i64>* %__ret_606.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret_606.i.i, align 16
  %sub.i = sub <2 x i64> %2, %14
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vhsub_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vhsub_v.i = call <8 x i8> @llvm.aarch64.neon.shsub.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vhsub_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vhsubq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vhsubq_v.i = call <16 x i8> @llvm.aarch64.neon.shsub.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vhsubq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vhsub_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vhsub_v2.i = call <4 x i16> @llvm.aarch64.neon.shsub.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vhsub_v3.i = bitcast <4 x i16> %vhsub_v2.i to <8 x i8>
  store <4 x i16> %vhsub_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vhsubq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vhsubq_v2.i = call <8 x i16> @llvm.aarch64.neon.shsub.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vhsubq_v3.i = bitcast <8 x i16> %vhsubq_v2.i to <16 x i8>
  store <8 x i16> %vhsubq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vhsub_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vhsub_v2.i = call <2 x i32> @llvm.aarch64.neon.shsub.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vhsub_v3.i = bitcast <2 x i32> %vhsub_v2.i to <8 x i8>
  store <2 x i32> %vhsub_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vhsubq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vhsubq_v2.i = call <4 x i32> @llvm.aarch64.neon.shsub.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vhsubq_v3.i = bitcast <4 x i32> %vhsubq_v2.i to <16 x i8>
  store <4 x i32> %vhsubq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vhsub_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vhsub_v.i = call <8 x i8> @llvm.aarch64.neon.uhsub.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vhsub_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vhsubq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vhsubq_v.i = call <16 x i8> @llvm.aarch64.neon.uhsub.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vhsubq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vhsub_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vhsub_v2.i = call <4 x i16> @llvm.aarch64.neon.uhsub.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vhsub_v3.i = bitcast <4 x i16> %vhsub_v2.i to <8 x i8>
  store <4 x i16> %vhsub_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vhsubq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vhsubq_v2.i = call <8 x i16> @llvm.aarch64.neon.uhsub.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vhsubq_v3.i = bitcast <8 x i16> %vhsubq_v2.i to <16 x i8>
  store <8 x i16> %vhsubq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vhsub_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vhsub_v2.i = call <2 x i32> @llvm.aarch64.neon.uhsub.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vhsub_v3.i = bitcast <2 x i32> %vhsub_v2.i to <8 x i8>
  store <2 x i32> %vhsub_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vhsubq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vhsubq_v2.i = call <4 x i32> @llvm.aarch64.neon.uhsub.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vhsubq_v3.i = bitcast <4 x i32> %vhsubq_v2.i to <16 x i8>
  store <4 x i32> %vhsubq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqsub_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqsub_v.i = call <8 x i8> @llvm.aarch64.neon.sqsub.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqsub_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqsubq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqsubq_v.i = call <16 x i8> @llvm.aarch64.neon.sqsub.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqsubq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqsub_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqsub_v2.i = call <4 x i16> @llvm.aarch64.neon.sqsub.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqsub_v3.i = bitcast <4 x i16> %vqsub_v2.i to <8 x i8>
  store <4 x i16> %vqsub_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqsubq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqsubq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqsub.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqsubq_v3.i = bitcast <8 x i16> %vqsubq_v2.i to <16 x i8>
  store <8 x i16> %vqsubq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqsub_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqsub_v2.i = call <2 x i32> @llvm.aarch64.neon.sqsub.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqsub_v3.i = bitcast <2 x i32> %vqsub_v2.i to <8 x i8>
  store <2 x i32> %vqsub_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqsubq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqsubq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqsubq_v3.i = bitcast <4 x i32> %vqsubq_v2.i to <16 x i8>
  store <4 x i32> %vqsubq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqsubq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqsubq_v2.i = call <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqsubq_v3.i = bitcast <2 x i64> %vqsubq_v2.i to <16 x i8>
  store <2 x i64> %vqsubq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqsub_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqsub_v2.i = call <1 x i64> @llvm.aarch64.neon.sqsub.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqsub_v3.i = bitcast <1 x i64> %vqsub_v2.i to <8 x i8>
  store <1 x i64> %vqsub_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqsubb_s8_wrapper(i8 signext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqsubb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqsub.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqsubb_s8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqsubh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqsubh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqsub.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqsubh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqsubs_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqsubs_s32.i = call i32 @llvm.aarch64.neon.sqsub.i32(i32 %2, i32 %3) #4
  store i32 %vqsubs_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqsubd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqsubd_s64.i = call i64 @llvm.aarch64.neon.sqsub.i64(i64 %2, i64 %3) #4
  store i64 %vqsubd_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqsub_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqsub_v.i = call <8 x i8> @llvm.aarch64.neon.uqsub.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqsub_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqsubq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqsubq_v.i = call <16 x i8> @llvm.aarch64.neon.uqsub.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqsubq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqsub_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqsub_v2.i = call <4 x i16> @llvm.aarch64.neon.uqsub.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqsub_v3.i = bitcast <4 x i16> %vqsub_v2.i to <8 x i8>
  store <4 x i16> %vqsub_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqsubq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqsubq_v2.i = call <8 x i16> @llvm.aarch64.neon.uqsub.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqsubq_v3.i = bitcast <8 x i16> %vqsubq_v2.i to <16 x i8>
  store <8 x i16> %vqsubq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqsub_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqsub_v2.i = call <2 x i32> @llvm.aarch64.neon.uqsub.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqsub_v3.i = bitcast <2 x i32> %vqsub_v2.i to <8 x i8>
  store <2 x i32> %vqsub_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqsubq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqsubq_v2.i = call <4 x i32> @llvm.aarch64.neon.uqsub.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqsubq_v3.i = bitcast <4 x i32> %vqsubq_v2.i to <16 x i8>
  store <4 x i32> %vqsubq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqsubq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqsubq_v2.i = call <2 x i64> @llvm.aarch64.neon.uqsub.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqsubq_v3.i = bitcast <2 x i64> %vqsubq_v2.i to <16 x i8>
  store <2 x i64> %vqsubq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqsub_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqsub_v2.i = call <1 x i64> @llvm.aarch64.neon.uqsub.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqsub_v3.i = bitcast <1 x i64> %vqsub_v2.i to <8 x i8>
  store <1 x i64> %vqsub_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqsubb_u8_wrapper(i8 zeroext %a, i8 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqsubb_u8.i = call <8 x i8> @llvm.aarch64.neon.uqsub.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqsubb_u8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqsubh_u16_wrapper(i16 zeroext %a, i16 zeroext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqsubh_u16.i = call <4 x i16> @llvm.aarch64.neon.uqsub.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqsubh_u16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqsubs_u32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqsubs_u32.i = call i32 @llvm.aarch64.neon.uqsub.i32(i32 %2, i32 %3) #4
  store i32 %vqsubs_u32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqsubd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqsubd_u64.i = call i64 @llvm.aarch64.neon.uqsub.i64(i64 %2, i64 %3) #4
  store i64 %vqsubd_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vsubhn_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vsubhn.i = sub <8 x i16> %2, %4
  %vsubhn1.i = lshr <8 x i16> %vsubhn.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vsubhn2.i = trunc <8 x i16> %vsubhn1.i to <8 x i8>
  store <8 x i8> %vsubhn2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vsubhn_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vsubhn.i = sub <4 x i32> %2, %4
  %vsubhn1.i = lshr <4 x i32> %vsubhn.i, <i32 16, i32 16, i32 16, i32 16>
  %vsubhn2.i = trunc <4 x i32> %vsubhn1.i to <4 x i16>
  store <4 x i16> %vsubhn2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vsubhn_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vsubhn.i = sub <2 x i64> %2, %4
  %vsubhn1.i = lshr <2 x i64> %vsubhn.i, <i64 32, i64 32>
  %vsubhn2.i = trunc <2 x i64> %vsubhn1.i to <2 x i32>
  store <2 x i32> %vsubhn2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vsubhn_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vsubhn.i = sub <8 x i16> %2, %4
  %vsubhn1.i = lshr <8 x i16> %vsubhn.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vsubhn2.i = trunc <8 x i16> %vsubhn1.i to <8 x i8>
  store <8 x i8> %vsubhn2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vsubhn_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vsubhn.i = sub <4 x i32> %2, %4
  %vsubhn1.i = lshr <4 x i32> %vsubhn.i, <i32 16, i32 16, i32 16, i32 16>
  %vsubhn2.i = trunc <4 x i32> %vsubhn1.i to <4 x i16>
  store <4 x i16> %vsubhn2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vsubhn_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vsubhn.i = sub <2 x i64> %2, %4
  %vsubhn1.i = lshr <2 x i64> %vsubhn.i, <i64 32, i64 32>
  %vsubhn2.i = trunc <2 x i64> %vsubhn1.i to <2 x i32>
  store <2 x i32> %vsubhn2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vsubhn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vsubhn.i.i = sub <8 x i16> %6, %8
  %vsubhn1.i.i = lshr <8 x i16> %vsubhn.i.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vsubhn2.i.i = trunc <8 x i16> %vsubhn1.i.i to <8 x i8>
  store <8 x i8> %vsubhn2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubhn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vsubhn.i.i = sub <4 x i32> %6, %8
  %vsubhn1.i.i = lshr <4 x i32> %vsubhn.i.i, <i32 16, i32 16, i32 16, i32 16>
  %vsubhn2.i.i = trunc <4 x i32> %vsubhn1.i.i to <4 x i16>
  store <4 x i16> %vsubhn2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubhn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vsubhn.i.i = sub <2 x i64> %6, %8
  %vsubhn1.i.i = lshr <2 x i64> %vsubhn.i.i, <i64 32, i64 32>
  %vsubhn2.i.i = trunc <2 x i64> %vsubhn1.i.i to <2 x i32>
  store <2 x i32> %vsubhn2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vsubhn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vsubhn.i.i = sub <8 x i16> %6, %8
  %vsubhn1.i.i = lshr <8 x i16> %vsubhn.i.i, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %vsubhn2.i.i = trunc <8 x i16> %vsubhn1.i.i to <8 x i8>
  store <8 x i8> %vsubhn2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vsubhn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vsubhn.i.i = sub <4 x i32> %6, %8
  %vsubhn1.i.i = lshr <4 x i32> %vsubhn.i.i, <i32 16, i32 16, i32 16, i32 16>
  %vsubhn2.i.i = trunc <4 x i32> %vsubhn1.i.i to <4 x i16>
  store <4 x i16> %vsubhn2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vsubhn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vsubhn.i.i = sub <2 x i64> %6, %8
  %vsubhn1.i.i = lshr <2 x i64> %vsubhn.i.i, <i64 32, i64 32>
  %vsubhn2.i.i = trunc <2 x i64> %vsubhn1.i.i to <2 x i32>
  store <2 x i32> %vsubhn2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrsubhn_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrsubhn_v2.i = call <8 x i8> @llvm.aarch64.neon.rsubhn.v8i8(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i8> %vrsubhn_v2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrsubhn_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrsubhn_v2.i = call <4 x i16> @llvm.aarch64.neon.rsubhn.v4i16(<4 x i32> %2, <4 x i32> %4) #4
  %vrsubhn_v3.i = bitcast <4 x i16> %vrsubhn_v2.i to <8 x i8>
  store <4 x i16> %vrsubhn_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrsubhn_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vrsubhn_v2.i = call <2 x i32> @llvm.aarch64.neon.rsubhn.v2i32(<2 x i64> %2, <2 x i64> %4) #4
  %vrsubhn_v3.i = bitcast <2 x i32> %vrsubhn_v2.i to <8 x i8>
  store <2 x i32> %vrsubhn_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrsubhn_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrsubhn_v2.i = call <8 x i8> @llvm.aarch64.neon.rsubhn.v8i8(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i8> %vrsubhn_v2.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrsubhn_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrsubhn_v2.i = call <4 x i16> @llvm.aarch64.neon.rsubhn.v4i16(<4 x i32> %2, <4 x i32> %4) #4
  %vrsubhn_v3.i = bitcast <4 x i16> %vrsubhn_v2.i to <8 x i8>
  store <4 x i16> %vrsubhn_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrsubhn_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vrsubhn_v2.i = call <2 x i32> @llvm.aarch64.neon.rsubhn.v2i32(<2 x i64> %2, <2 x i64> %4) #4
  %vrsubhn_v3.i = bitcast <2 x i32> %vrsubhn_v2.i to <8 x i8>
  store <2 x i32> %vrsubhn_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrsubhn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <8 x i8> @llvm.aarch64.neon.rsubhn.v8i8(<8 x i16> %6, <8 x i16> %8) #4
  store <8 x i8> %vrsubhn_v2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrsubhn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <4 x i16> @llvm.aarch64.neon.rsubhn.v4i16(<4 x i32> %6, <4 x i32> %8) #4
  %vrsubhn_v3.i.i = bitcast <4 x i16> %vrsubhn_v2.i.i to <8 x i8>
  store <4 x i16> %vrsubhn_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrsubhn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <2 x i32> @llvm.aarch64.neon.rsubhn.v2i32(<2 x i64> %6, <2 x i64> %8) #4
  %vrsubhn_v3.i.i = bitcast <2 x i32> %vrsubhn_v2.i.i to <8 x i8>
  store <2 x i32> %vrsubhn_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrsubhn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i3.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %5, <8 x i16>* %__p1.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %9 = bitcast <8 x i16> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <8 x i8> @llvm.aarch64.neon.rsubhn.v8i8(<8 x i16> %6, <8 x i16> %8) #4
  store <8 x i8> %vrsubhn_v2.i.i, <8 x i8>* %__ret.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p1.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %11, <8 x i8> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i4.i, align 16
  %13 = load <16 x i8>, <16 x i8>* %__ret.i4.i, align 16
  store <16 x i8> %13, <16 x i8>* %__ret.i, align 16
  %14 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrsubhn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i3.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %5, <4 x i32>* %__p1.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %7 = bitcast <4 x i32> %6 to <16 x i8>
  %8 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <4 x i16> @llvm.aarch64.neon.rsubhn.v4i16(<4 x i32> %6, <4 x i32> %8) #4
  %vrsubhn_v3.i.i = bitcast <4 x i16> %vrsubhn_v2.i.i to <8 x i8>
  store <4 x i16> %vrsubhn_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %11, <4 x i16> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i4.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %13, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrsubhn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i3.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  store <2 x i64> %4, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i64> %5, <2 x i64>* %__p1.addr.i.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, <2 x i64>* %__p1.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %vrsubhn_v2.i.i = call <2 x i32> @llvm.aarch64.neon.rsubhn.v2i32(<2 x i64> %6, <2 x i64> %8) #4
  %vrsubhn_v3.i.i = bitcast <2 x i32> %vrsubhn_v2.i.i to <8 x i8>
  store <2 x i32> %vrsubhn_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p1.addr.i3.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %11, <2 x i32> %12, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i4.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %13, <4 x i32>* %__ret.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vceq_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vceqq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vceq_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vceqq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vceq_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vceqq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vceq_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vceqq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vceq_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vceqq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vceq_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vceqq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vceqq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vceqq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp eq <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vceq_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vceq_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp eq <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vceqd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp eq i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vceqd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp eq i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vceqz_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp eq <8 x i8> %1, zeroinitializer
  %vceqz.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vceqz.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vceqzq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp eq <16 x i8> %1, zeroinitializer
  %vceqz.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vceqz.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vceqz_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp eq <4 x i16> %1, zeroinitializer
  %vceqz.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vceqz.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vceqzq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp eq <8 x i16> %1, zeroinitializer
  %vceqz.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vceqz.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vceqz_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp eq <2 x i32> %1, zeroinitializer
  %vceqz.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vceqz.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vceqzq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp eq <4 x i32> %1, zeroinitializer
  %vceqz.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vceqz.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vceqz_u8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp eq <8 x i8> %1, zeroinitializer
  %vceqz.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vceqz.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vceqzq_u8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp eq <16 x i8> %1, zeroinitializer
  %vceqz.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vceqz.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vceqz_u16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp eq <4 x i16> %1, zeroinitializer
  %vceqz.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vceqz.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vceqzq_u16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp eq <8 x i16> %1, zeroinitializer
  %vceqz.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vceqz.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vceqz_u32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp eq <2 x i32> %1, zeroinitializer
  %vceqz.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vceqz.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vceqzq_u32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp eq <4 x i32> %1, zeroinitializer
  %vceqz.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vceqz.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vceqzq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp eq <2 x i64> %1, zeroinitializer
  %vceqz.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vceqz.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vceqzq_u64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp eq <2 x i64> %1, zeroinitializer
  %vceqz.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vceqz.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vceqz_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp eq <1 x i64> %1, zeroinitializer
  %vceqz.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vceqz.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vceqz_u64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp eq <1 x i64> %1, zeroinitializer
  %vceqz.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vceqz.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vceqzd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp eq i64 %1, 0
  %vceqz.i = sext i1 %2 to i64
  store i64 %vceqz.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vceqzd_u64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp eq i64 %1, 0
  %vceqzd.i = sext i1 %2 to i64
  store i64 %vceqzd.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcge_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp sge <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgeq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp sge <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcge_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp sge <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgeq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp sge <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcge_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp sge <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgeq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp sge <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgeq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp sge <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcle_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp sle <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcleq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp sle <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcle_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp sle <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcleq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp sle <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcle_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp sle <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcleq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp sle <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcleq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp sle <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcge_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp uge <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgeq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp uge <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcge_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp uge <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgeq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp uge <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcge_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp uge <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgeq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp uge <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgeq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp uge <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcle_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp ule <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcleq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp ule <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcle_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp ule <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcleq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp ule <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcle_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp ule <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcleq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp ule <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcleq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp ule <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcge_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp sge <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcged_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp sge i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcle_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp sle <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcled_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp sle i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcge_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp uge <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcged_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp uge i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcle_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp ule <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcled_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp ule i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcgez_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp sge <8 x i8> %1, zeroinitializer
  %vcgez.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vcgez.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgezq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp sge <16 x i8> %1, zeroinitializer
  %vcgez.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vcgez.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcgez_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp sge <4 x i16> %1, zeroinitializer
  %vcgez.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vcgez.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgezq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp sge <8 x i16> %1, zeroinitializer
  %vcgez.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vcgez.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcgez_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp sge <2 x i32> %1, zeroinitializer
  %vcgez.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vcgez.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgezq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp sge <4 x i32> %1, zeroinitializer
  %vcgez.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vcgez.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgezq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp sge <2 x i64> %1, zeroinitializer
  %vcgez.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vcgez.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcgez_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp sge <1 x i64> %1, zeroinitializer
  %vcgez.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vcgez.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcgezd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp sge i64 %1, 0
  %vcgez.i = sext i1 %2 to i64
  store i64 %vcgez.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vclez_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp sle <8 x i8> %1, zeroinitializer
  %vclez.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vclez.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vclezq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp sle <16 x i8> %1, zeroinitializer
  %vclez.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vclez.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vclez_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp sle <4 x i16> %1, zeroinitializer
  %vclez.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vclez.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vclezq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp sle <8 x i16> %1, zeroinitializer
  %vclez.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vclez.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vclez_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp sle <2 x i32> %1, zeroinitializer
  %vclez.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vclez.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vclezq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp sle <4 x i32> %1, zeroinitializer
  %vclez.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vclez.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vclezq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp sle <2 x i64> %1, zeroinitializer
  %vclez.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vclez.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vclez_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp sle <1 x i64> %1, zeroinitializer
  %vclez.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vclez.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vclezd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp sle i64 %1, 0
  %vclez.i = sext i1 %2 to i64
  store i64 %vclez.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcgt_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp sgt <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgtq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp sgt <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcgt_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp sgt <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgtq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp sgt <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcgt_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp sgt <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgtq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp sgt <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgtq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp sgt <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vclt_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp slt <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcltq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp slt <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vclt_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp slt <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcltq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp slt <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vclt_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp slt <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcltq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp slt <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcltq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp slt <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcgt_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp ugt <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgtq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp ugt <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcgt_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp ugt <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgtq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp ugt <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcgt_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp ugt <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgtq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp ugt <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgtq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp ugt <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vclt_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %cmp.i = icmp ult <8 x i8> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i8>
  store <8 x i8> %sext.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcltq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %cmp.i = icmp ult <16 x i8> %2, %3
  %sext.i = sext <16 x i1> %cmp.i to <16 x i8>
  store <16 x i8> %sext.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vclt_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %cmp.i = icmp ult <4 x i16> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i16>
  store <4 x i16> %sext.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcltq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %cmp.i = icmp ult <8 x i16> %2, %3
  %sext.i = sext <8 x i1> %cmp.i to <8 x i16>
  store <8 x i16> %sext.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vclt_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %cmp.i = icmp ult <2 x i32> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i32>
  store <2 x i32> %sext.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcltq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %cmp.i = icmp ult <4 x i32> %2, %3
  %sext.i = sext <4 x i1> %cmp.i to <4 x i32>
  store <4 x i32> %sext.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcltq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %cmp.i = icmp ult <2 x i64> %2, %3
  %sext.i = sext <2 x i1> %cmp.i to <2 x i64>
  store <2 x i64> %sext.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcgt_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp sgt <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcgtd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp sgt i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vclt_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp slt <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcltd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp slt i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcgt_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp ugt <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcgtd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp ugt i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vclt_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %cmp.i = icmp ult <1 x i64> %2, %3
  %sext.i = sext <1 x i1> %cmp.i to <1 x i64>
  store <1 x i64> %sext.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcltd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = icmp ult i64 %2, %3
  %vceqd.i = sext i1 %4 to i64
  store i64 %vceqd.i, i64* %__ret.i, align 8
  %5 = load i64, i64* %__ret.i, align 8
  ret i64 %5
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcgtz_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp sgt <8 x i8> %1, zeroinitializer
  %vcgtz.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vcgtz.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcgtzq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp sgt <16 x i8> %1, zeroinitializer
  %vcgtz.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vcgtz.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcgtz_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp sgt <4 x i16> %1, zeroinitializer
  %vcgtz.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vcgtz.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcgtzq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp sgt <8 x i16> %1, zeroinitializer
  %vcgtz.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vcgtz.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcgtz_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp sgt <2 x i32> %1, zeroinitializer
  %vcgtz.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vcgtz.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcgtzq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp sgt <4 x i32> %1, zeroinitializer
  %vcgtz.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vcgtz.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcgtzq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp sgt <2 x i64> %1, zeroinitializer
  %vcgtz.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vcgtz.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcgtz_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp sgt <1 x i64> %1, zeroinitializer
  %vcgtz.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vcgtz.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcgtzd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp sgt i64 %1, 0
  %vcgtz.i = sext i1 %2 to i64
  store i64 %vcgtz.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vcltz_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %2 = icmp slt <8 x i8> %1, zeroinitializer
  %vcltz.i = sext <8 x i1> %2 to <8 x i8>
  store <8 x i8> %vcltz.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vcltzq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %2 = icmp slt <16 x i8> %1, zeroinitializer
  %vcltz.i = sext <16 x i1> %2 to <16 x i8>
  store <16 x i8> %vcltz.i, <16 x i8>* %__ret.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vcltz_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %3 = icmp slt <4 x i16> %1, zeroinitializer
  %vcltz.i = sext <4 x i1> %3 to <4 x i16>
  store <4 x i16> %vcltz.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vcltzq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %3 = icmp slt <8 x i16> %1, zeroinitializer
  %vcltz.i = sext <8 x i1> %3 to <8 x i16>
  store <8 x i16> %vcltz.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vcltz_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %3 = icmp slt <2 x i32> %1, zeroinitializer
  %vcltz.i = sext <2 x i1> %3 to <2 x i32>
  store <2 x i32> %vcltz.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vcltzq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %3 = icmp slt <4 x i32> %1, zeroinitializer
  %vcltz.i = sext <4 x i1> %3 to <4 x i32>
  store <4 x i32> %vcltz.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vcltzq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = icmp slt <2 x i64> %1, zeroinitializer
  %vcltz.i = sext <2 x i1> %3 to <2 x i64>
  store <2 x i64> %vcltz.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vcltz_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %3 = icmp slt <1 x i64> %1, zeroinitializer
  %vcltz.i = sext <1 x i1> %3 to <1 x i64>
  store <1 x i64> %vcltz.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vcltzd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %2 = icmp slt i64 %1, 0
  %vcltz.i = sext i1 %2 to i64
  store i64 %vcltz.i, i64* %__ret.i, align 8
  %3 = load i64, i64* %__ret.i, align 8
  ret i64 %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtst_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %4 = and <8 x i8> %2, %3
  %5 = icmp ne <8 x i8> %4, zeroinitializer
  %vtst.i = sext <8 x i1> %5 to <8 x i8>
  store <8 x i8> %vtst.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtstq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %4 = and <16 x i8> %2, %3
  %5 = icmp ne <16 x i8> %4, zeroinitializer
  %vtst.i = sext <16 x i1> %5 to <16 x i8>
  store <16 x i8> %vtst.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtst_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %6 = and <4 x i16> %2, %4
  %7 = icmp ne <4 x i16> %6, zeroinitializer
  %vtst.i = sext <4 x i1> %7 to <4 x i16>
  store <4 x i16> %vtst.i, <4 x i16>* %__ret.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtstq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %6 = and <8 x i16> %2, %4
  %7 = icmp ne <8 x i16> %6, zeroinitializer
  %vtst.i = sext <8 x i1> %7 to <8 x i16>
  store <8 x i16> %vtst.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtst_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %6 = and <2 x i32> %2, %4
  %7 = icmp ne <2 x i32> %6, zeroinitializer
  %vtst.i = sext <2 x i1> %7 to <2 x i32>
  store <2 x i32> %vtst.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtstq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %6 = and <4 x i32> %2, %4
  %7 = icmp ne <4 x i32> %6, zeroinitializer
  %vtst.i = sext <4 x i1> %7 to <4 x i32>
  store <4 x i32> %vtst.i, <4 x i32>* %__ret.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtst_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %4 = and <8 x i8> %2, %3
  %5 = icmp ne <8 x i8> %4, zeroinitializer
  %vtst.i = sext <8 x i1> %5 to <8 x i8>
  store <8 x i8> %vtst.i, <8 x i8>* %__ret.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtstq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %4 = and <16 x i8> %2, %3
  %5 = icmp ne <16 x i8> %4, zeroinitializer
  %vtst.i = sext <16 x i1> %5 to <16 x i8>
  store <16 x i8> %vtst.i, <16 x i8>* %__ret.i, align 16
  %6 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtst_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %6 = and <4 x i16> %2, %4
  %7 = icmp ne <4 x i16> %6, zeroinitializer
  %vtst.i = sext <4 x i1> %7 to <4 x i16>
  store <4 x i16> %vtst.i, <4 x i16>* %__ret.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtstq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %6 = and <8 x i16> %2, %4
  %7 = icmp ne <8 x i16> %6, zeroinitializer
  %vtst.i = sext <8 x i1> %7 to <8 x i16>
  store <8 x i16> %vtst.i, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtst_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %6 = and <2 x i32> %2, %4
  %7 = icmp ne <2 x i32> %6, zeroinitializer
  %vtst.i = sext <2 x i1> %7 to <2 x i32>
  store <2 x i32> %vtst.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtstq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %6 = and <4 x i32> %2, %4
  %7 = icmp ne <4 x i32> %6, zeroinitializer
  %vtst.i = sext <4 x i1> %7 to <4 x i32>
  store <4 x i32> %vtst.i, <4 x i32>* %__ret.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtstq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %6 = and <2 x i64> %2, %4
  %7 = icmp ne <2 x i64> %6, zeroinitializer
  %vtst.i = sext <2 x i1> %7 to <2 x i64>
  store <2 x i64> %vtst.i, <2 x i64>* %__ret.i, align 16
  %8 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtstq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %6 = and <2 x i64> %2, %4
  %7 = icmp ne <2 x i64> %6, zeroinitializer
  %vtst.i = sext <2 x i1> %7 to <2 x i64>
  store <2 x i64> %vtst.i, <2 x i64>* %__ret.i, align 16
  %8 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vtst_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %6 = and <1 x i64> %2, %4
  %7 = icmp ne <1 x i64> %6, zeroinitializer
  %vtst.i = sext <1 x i1> %7 to <1 x i64>
  store <1 x i64> %vtst.i, <1 x i64>* %__ret.i, align 8
  %8 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vtst_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %6 = and <1 x i64> %2, %4
  %7 = icmp ne <1 x i64> %6, zeroinitializer
  %vtst.i = sext <1 x i1> %7 to <1 x i64>
  store <1 x i64> %vtst.i, <1 x i64>* %__ret.i, align 8
  %8 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vtstd_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = and i64 %2, %3
  %5 = icmp ne i64 %4, 0
  %vtstd.i = sext i1 %5 to i64
  store i64 %vtstd.i, i64* %__ret.i, align 8
  %6 = load i64, i64* %__ret.i, align 8
  ret i64 %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vtstd_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %4 = and i64 %2, %3
  %5 = icmp ne i64 %4, 0
  %vtstd.i = sext i1 %5 to i64
  store i64 %vtstd.i, i64* %__ret.i, align 8
  %6 = load i64, i64* %__ret.i, align 8
  ret i64 %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vabd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vabd.i = call <8 x i8> @llvm.aarch64.neon.sabd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vabd.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vabdq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vabd.i = call <16 x i8> @llvm.aarch64.neon.sabd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vabd.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vabd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vabd2.i = call <4 x i16> @llvm.aarch64.neon.sabd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vabd2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vabd2.i = call <8 x i16> @llvm.aarch64.neon.sabd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vabd2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vabd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vabd2.i = call <2 x i32> @llvm.aarch64.neon.sabd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vabd2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vabd2.i = call <4 x i32> @llvm.aarch64.neon.sabd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vabd2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vabd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vabd.i = call <8 x i8> @llvm.aarch64.neon.uabd.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vabd.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vabdq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vabd.i = call <16 x i8> @llvm.aarch64.neon.uabd.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vabd.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vabd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vabd2.i = call <4 x i16> @llvm.aarch64.neon.uabd.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vabd2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vabd2.i = call <8 x i16> @llvm.aarch64.neon.uabd.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vabd2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vabd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vabd2.i = call <2 x i32> @llvm.aarch64.neon.uabd.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vabd2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vabd2.i = call <4 x i32> @llvm.aarch64.neon.uabd.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vabd2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p1.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vabd.i.i = call <8 x i8> @llvm.aarch64.neon.sabd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  store <8 x i8> %vabd.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p0.addr.i2.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i.i = zext <8 x i8> %7 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i3.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %8, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p1.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %vabd2.i.i = call <4 x i16> @llvm.aarch64.neon.sabd.v4i16(<4 x i16> %4, <4 x i16> %6) #4
  store <4 x i16> %vabd2.i.i, <4 x i16>* %__ret.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__p0.addr.i2.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i3.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %11, <4 x i32>* %__ret.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vabdl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p1.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %vabd2.i.i = call <2 x i32> @llvm.aarch64.neon.sabd.v2i32(<2 x i32> %4, <2 x i32> %6) #4
  store <2 x i32> %vabd2.i.i, <2 x i32>* %__ret.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__p0.addr.i2.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i3.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %11, <2 x i64>* %__ret.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdl_high_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i2.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i3.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %3, <16 x i8> %4, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %6 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %6, <16 x i8>* %__p0.addr.i5.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %8 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %7, <16 x i8> %8, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i3.i, align 8
  store <8 x i8> %9, <8 x i8>* %__p1.addr.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__p0.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p1.addr.i.i.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vabd.i.i.i = call <8 x i8> @llvm.aarch64.neon.sabd.v8i8(<8 x i8> %12, <8 x i8> %13) #4
  store <8 x i8> %vabd.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p0.addr.i2.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i.i, align 8
  %vmovl.i.i.i = zext <8 x i8> %15 to <8 x i16>
  store <8 x i16> %vmovl.i.i.i, <8 x i16>* %__ret.i3.i.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__ret.i3.i.i, align 16
  store <8 x i16> %16, <8 x i16>* %__ret.i4.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %17, <8 x i16>* %__ret.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdl_high_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i2.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %6, <8 x i16>* %__p0.addr.i5.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %7, <8 x i16> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i3.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p1.addr.i.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %vabd2.i.i.i = call <4 x i16> @llvm.aarch64.neon.sabd.v4i16(<4 x i16> %12, <4 x i16> %14) #4
  store <4 x i16> %vabd2.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %16, <4 x i16>* %__p0.addr.i2.i.i, align 8
  %17 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmovl.i.i.i = zext <4 x i16> %17 to <4 x i32>
  store <4 x i32> %vmovl.i.i.i, <4 x i32>* %__ret.i3.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i3.i.i, align 16
  store <4 x i32> %19, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vabdl_high_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i2.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %6, <4 x i32>* %__p0.addr.i5.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %7, <4 x i32> %8, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i3.i, align 8
  store <2 x i32> %9, <2 x i32>* %__p1.addr.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__p0.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p1.addr.i.i.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %14 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %vabd2.i.i.i = call <2 x i32> @llvm.aarch64.neon.sabd.v2i32(<2 x i32> %12, <2 x i32> %14) #4
  store <2 x i32> %vabd2.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %16 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %16, <2 x i32>* %__p0.addr.i2.i.i, align 8
  %17 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmovl.i.i.i = zext <2 x i32> %17 to <2 x i64>
  store <2 x i64> %vmovl.i.i.i, <2 x i64>* %__ret.i3.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i.i, align 8
  store <8 x i8> %3, <8 x i8>* %__p1.addr.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %vabd.i.i = call <8 x i8> @llvm.aarch64.neon.uabd.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  store <8 x i8> %vabd.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p0.addr.i2.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %vmovl.i.i = zext <8 x i8> %7 to <8 x i16>
  store <8 x i16> %vmovl.i.i, <8 x i16>* %__ret.i3.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %8, <8 x i16>* %__ret.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %9
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %3, <4 x i16>* %__p1.addr.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %6 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %vabd2.i.i = call <4 x i16> @llvm.aarch64.neon.uabd.v4i16(<4 x i16> %4, <4 x i16> %6) #4
  store <4 x i16> %vabd2.i.i, <4 x i16>* %__ret.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %8, <4 x i16>* %__p0.addr.i2.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %10 = bitcast <4 x i16> %9 to <8 x i8>
  %vmovl.i.i = zext <4 x i16> %9 to <4 x i32>
  store <4 x i32> %vmovl.i.i, <4 x i32>* %__ret.i3.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %11, <4 x i32>* %__ret.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vabdl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %3, <2 x i32>* %__p1.addr.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %6 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %vabd2.i.i = call <2 x i32> @llvm.aarch64.neon.uabd.v2i32(<2 x i32> %4, <2 x i32> %6) #4
  store <2 x i32> %vabd2.i.i, <2 x i32>* %__ret.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %8, <2 x i32>* %__p0.addr.i2.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %10 = bitcast <2 x i32> %9 to <8 x i8>
  %vmovl.i.i = zext <2 x i32> %9 to <2 x i64>
  store <2 x i64> %vmovl.i.i, <2 x i64>* %__ret.i3.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %11, <2 x i64>* %__ret.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabdl_high_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <16 x i8>, align 16
  %__ret.i6.i = alloca <8 x i8>, align 8
  %__p0.addr.i2.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i.i = alloca <8 x i8>, align 8
  %__ret.i.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i3.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i4.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %3, <16 x i8> %4, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  %6 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %6, <16 x i8>* %__p0.addr.i5.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %8 = load <16 x i8>, <16 x i8>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <16 x i8> %7, <16 x i8> %8, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i7.i, <8 x i8>* %__ret.i6.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %__ret.i6.i, align 8
  store <8 x i8> %5, <8 x i8>* %__p0.addr.i3.i, align 8
  store <8 x i8> %9, <8 x i8>* %__p1.addr.i.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__p0.addr.i3.i, align 8
  %11 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  store <8 x i8> %10, <8 x i8>* %__p0.addr.i.i.i, align 8
  store <8 x i8> %11, <8 x i8>* %__p1.addr.i.i.i, align 8
  %12 = load <8 x i8>, <8 x i8>* %__p0.addr.i.i.i, align 8
  %13 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i.i, align 8
  %vabd.i.i.i = call <8 x i8> @llvm.aarch64.neon.uabd.v8i8(<8 x i8> %12, <8 x i8> %13) #4
  store <8 x i8> %vabd.i.i.i, <8 x i8>* %__ret.i.i.i, align 8
  %14 = load <8 x i8>, <8 x i8>* %__ret.i.i.i, align 8
  store <8 x i8> %14, <8 x i8>* %__p0.addr.i2.i.i, align 8
  %15 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i.i, align 8
  %vmovl.i.i.i = zext <8 x i8> %15 to <8 x i16>
  store <8 x i16> %vmovl.i.i.i, <8 x i16>* %__ret.i3.i.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__ret.i3.i.i, align 16
  store <8 x i16> %16, <8 x i16>* %__ret.i4.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i4.i, align 16
  store <8 x i16> %17, <8 x i16>* %__ret.i, align 16
  %18 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabdl_high_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <8 x i16>, align 16
  %__ret.i6.i = alloca <4 x i16>, align 8
  %__p0.addr.i2.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i3.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i4.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %6, <8 x i16>* %__p0.addr.i5.i, align 16
  %7 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <8 x i16> %7, <8 x i16> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i7.i, <4 x i16>* %__ret.i6.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %__ret.i6.i, align 8
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i3.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i3.i, align 8
  %11 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %11, <4 x i16>* %__p1.addr.i.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %vabd2.i.i.i = call <4 x i16> @llvm.aarch64.neon.uabd.v4i16(<4 x i16> %12, <4 x i16> %14) #4
  store <4 x i16> %vabd2.i.i.i, <4 x i16>* %__ret.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__ret.i.i.i, align 8
  store <4 x i16> %16, <4 x i16>* %__p0.addr.i2.i.i, align 8
  %17 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i.i, align 8
  %18 = bitcast <4 x i16> %17 to <8 x i8>
  %vmovl.i.i.i = zext <4 x i16> %17 to <4 x i32>
  store <4 x i32> %vmovl.i.i.i, <4 x i32>* %__ret.i3.i.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i3.i.i, align 16
  store <4 x i32> %19, <4 x i32>* %__ret.i4.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i4.i, align 16
  store <4 x i32> %20, <4 x i32>* %__ret.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vabdl_high_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i5.i = alloca <4 x i32>, align 16
  %__ret.i6.i = alloca <2 x i32>, align 8
  %__p0.addr.i2.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i3.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i4.i = alloca <2 x i64>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %6, <4 x i32>* %__p0.addr.i5.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i5.i, align 16
  %shuffle.i7.i = shufflevector <4 x i32> %7, <4 x i32> %8, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i7.i, <2 x i32>* %__ret.i6.i, align 8
  %9 = load <2 x i32>, <2 x i32>* %__ret.i6.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i3.i, align 8
  store <2 x i32> %9, <2 x i32>* %__p1.addr.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__p0.addr.i3.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %11, <2 x i32>* %__p1.addr.i.i.i, align 8
  %12 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %14 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %vabd2.i.i.i = call <2 x i32> @llvm.aarch64.neon.uabd.v2i32(<2 x i32> %12, <2 x i32> %14) #4
  store <2 x i32> %vabd2.i.i.i, <2 x i32>* %__ret.i.i.i, align 8
  %16 = load <2 x i32>, <2 x i32>* %__ret.i.i.i, align 8
  store <2 x i32> %16, <2 x i32>* %__p0.addr.i2.i.i, align 8
  %17 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i.i, align 8
  %18 = bitcast <2 x i32> %17 to <8 x i8>
  %vmovl.i.i.i = zext <2 x i32> %17 to <2 x i64>
  store <2 x i64> %vmovl.i.i.i, <2 x i64>* %__ret.i3.i.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i4.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i4.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %21
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmax_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmax.i = call <8 x i8> @llvm.aarch64.neon.smax.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vmax.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmaxq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vmax.i = call <16 x i8> @llvm.aarch64.neon.smax.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vmax.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmax_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmax2.i = call <4 x i16> @llvm.aarch64.neon.smax.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vmax2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmaxq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmax2.i = call <8 x i16> @llvm.aarch64.neon.smax.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vmax2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmax_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmax2.i = call <2 x i32> @llvm.aarch64.neon.smax.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vmax2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmaxq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmax2.i = call <4 x i32> @llvm.aarch64.neon.smax.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vmax2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmax_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmax.i = call <8 x i8> @llvm.aarch64.neon.umax.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vmax.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmaxq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vmax.i = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vmax.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmax_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmax2.i = call <4 x i16> @llvm.aarch64.neon.umax.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vmax2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmaxq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmax2.i = call <8 x i16> @llvm.aarch64.neon.umax.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vmax2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmax_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmax2.i = call <2 x i32> @llvm.aarch64.neon.umax.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vmax2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmaxq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmax2.i = call <4 x i32> @llvm.aarch64.neon.umax.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vmax2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmin_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmin.i = call <8 x i8> @llvm.aarch64.neon.smin.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vmin.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vminq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vmin.i = call <16 x i8> @llvm.aarch64.neon.smin.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vmin.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmin_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmin2.i = call <4 x i16> @llvm.aarch64.neon.smin.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vmin2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vminq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmin2.i = call <8 x i16> @llvm.aarch64.neon.smin.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vmin2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmin_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmin2.i = call <2 x i32> @llvm.aarch64.neon.smin.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vmin2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vminq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmin2.i = call <4 x i32> @llvm.aarch64.neon.smin.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vmin2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmin_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vmin.i = call <8 x i8> @llvm.aarch64.neon.umin.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vmin.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vminq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vmin.i = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vmin.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmin_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vmin2.i = call <4 x i16> @llvm.aarch64.neon.umin.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vmin2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vminq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmin2.i = call <8 x i16> @llvm.aarch64.neon.umin.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vmin2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmin_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vmin2.i = call <2 x i32> @llvm.aarch64.neon.umin.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vmin2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vminq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmin2.i = call <4 x i32> @llvm.aarch64.neon.umin.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vmin2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vshl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vshl_v.i = call <8 x i8> @llvm.aarch64.neon.sshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vshlq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vshlq_v.i = call <16 x i8> @llvm.aarch64.neon.sshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vshl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vshl_v2.i = call <4 x i16> @llvm.aarch64.neon.sshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vshl_v3.i = bitcast <4 x i16> %vshl_v2.i to <8 x i8>
  store <4 x i16> %vshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vshlq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.sshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vshlq_v3.i = bitcast <8 x i16> %vshlq_v2.i to <16 x i8>
  store <8 x i16> %vshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vshl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vshl_v2.i = call <2 x i32> @llvm.aarch64.neon.sshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vshl_v3.i = bitcast <2 x i32> %vshl_v2.i to <8 x i8>
  store <2 x i32> %vshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vshlq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vshlq_v3.i = bitcast <4 x i32> %vshlq_v2.i to <16 x i8>
  store <4 x i32> %vshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vshlq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.sshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vshlq_v3.i = bitcast <2 x i64> %vshlq_v2.i to <16 x i8>
  store <2 x i64> %vshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vshl_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vshl_v2.i = call <1 x i64> @llvm.aarch64.neon.sshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vshl_v3.i = bitcast <1 x i64> %vshl_v2.i to <8 x i8>
  store <1 x i64> %vshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vshld_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vshld_s64.i = call i64 @llvm.aarch64.neon.sshl.i64(i64 %2, i64 %3) #4
  store i64 %vshld_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vshl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vshl_v.i = call <8 x i8> @llvm.aarch64.neon.ushl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vshlq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vshlq_v.i = call <16 x i8> @llvm.aarch64.neon.ushl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vshl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vshl_v2.i = call <4 x i16> @llvm.aarch64.neon.ushl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vshl_v3.i = bitcast <4 x i16> %vshl_v2.i to <8 x i8>
  store <4 x i16> %vshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vshlq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.ushl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vshlq_v3.i = bitcast <8 x i16> %vshlq_v2.i to <16 x i8>
  store <8 x i16> %vshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vshl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vshl_v2.i = call <2 x i32> @llvm.aarch64.neon.ushl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vshl_v3.i = bitcast <2 x i32> %vshl_v2.i to <8 x i8>
  store <2 x i32> %vshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vshlq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.ushl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vshlq_v3.i = bitcast <4 x i32> %vshlq_v2.i to <16 x i8>
  store <4 x i32> %vshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vshlq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.ushl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vshlq_v3.i = bitcast <2 x i64> %vshlq_v2.i to <16 x i8>
  store <2 x i64> %vshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vshl_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vshl_v2.i = call <1 x i64> @llvm.aarch64.neon.ushl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vshl_v3.i = bitcast <1 x i64> %vshl_v2.i to <8 x i8>
  store <1 x i64> %vshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vshld_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vshld_u64.i = call i64 @llvm.aarch64.neon.ushl.i64(i64 %2, i64 %3) #4
  store i64 %vshld_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqshl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqshl_v.i = call <8 x i8> @llvm.aarch64.neon.sqshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqshlq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqshlq_v.i = call <16 x i8> @llvm.aarch64.neon.sqshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqshl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqshl_v2.i = call <4 x i16> @llvm.aarch64.neon.sqshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqshl_v3.i = bitcast <4 x i16> %vqshl_v2.i to <8 x i8>
  store <4 x i16> %vqshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqshlq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqshlq_v3.i = bitcast <8 x i16> %vqshlq_v2.i to <16 x i8>
  store <8 x i16> %vqshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqshl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqshl_v2.i = call <2 x i32> @llvm.aarch64.neon.sqshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqshl_v3.i = bitcast <2 x i32> %vqshl_v2.i to <8 x i8>
  store <2 x i32> %vqshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqshlq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqshlq_v3.i = bitcast <4 x i32> %vqshlq_v2.i to <16 x i8>
  store <4 x i32> %vqshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqshlq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.sqshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqshlq_v3.i = bitcast <2 x i64> %vqshlq_v2.i to <16 x i8>
  store <2 x i64> %vqshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqshl_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqshl_v2.i = call <1 x i64> @llvm.aarch64.neon.sqshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqshl_v3.i = bitcast <1 x i64> %vqshl_v2.i to <8 x i8>
  store <1 x i64> %vqshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqshlb_s8_wrapper(i8 signext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqshlb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqshl.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqshlb_s8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqshlh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqshlh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqshl.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqshlh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqshls_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqshls_s32.i = call i32 @llvm.aarch64.neon.sqshl.i32(i32 %2, i32 %3) #4
  store i32 %vqshls_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqshld_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqshld_s64.i = call i64 @llvm.aarch64.neon.sqshl.i64(i64 %2, i64 %3) #4
  store i64 %vqshld_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqshl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqshl_v.i = call <8 x i8> @llvm.aarch64.neon.uqshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqshlq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqshlq_v.i = call <16 x i8> @llvm.aarch64.neon.uqshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqshl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqshl_v2.i = call <4 x i16> @llvm.aarch64.neon.uqshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqshl_v3.i = bitcast <4 x i16> %vqshl_v2.i to <8 x i8>
  store <4 x i16> %vqshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqshlq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.uqshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqshlq_v3.i = bitcast <8 x i16> %vqshlq_v2.i to <16 x i8>
  store <8 x i16> %vqshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqshl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqshl_v2.i = call <2 x i32> @llvm.aarch64.neon.uqshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqshl_v3.i = bitcast <2 x i32> %vqshl_v2.i to <8 x i8>
  store <2 x i32> %vqshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqshlq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.uqshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqshlq_v3.i = bitcast <4 x i32> %vqshlq_v2.i to <16 x i8>
  store <4 x i32> %vqshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqshlq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.uqshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqshlq_v3.i = bitcast <2 x i64> %vqshlq_v2.i to <16 x i8>
  store <2 x i64> %vqshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqshl_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqshl_v2.i = call <1 x i64> @llvm.aarch64.neon.uqshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqshl_v3.i = bitcast <1 x i64> %vqshl_v2.i to <8 x i8>
  store <1 x i64> %vqshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqshlb_u8_wrapper(i8 zeroext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqshlb_u8.i = call <8 x i8> @llvm.aarch64.neon.uqshl.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqshlb_u8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqshlh_u16_wrapper(i16 zeroext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqshlh_u16.i = call <4 x i16> @llvm.aarch64.neon.uqshl.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqshlh_u16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqshls_u32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqshls_u32.i = call i32 @llvm.aarch64.neon.uqshl.i32(i32 %2, i32 %3) #4
  store i32 %vqshls_u32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqshld_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqshld_u64.i = call i64 @llvm.aarch64.neon.uqshl.i64(i64 %2, i64 %3) #4
  store i64 %vqshld_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrshl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vrshl_v.i = call <8 x i8> @llvm.aarch64.neon.srshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vrshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrshlq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vrshlq_v.i = call <16 x i8> @llvm.aarch64.neon.srshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vrshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrshl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vrshl_v2.i = call <4 x i16> @llvm.aarch64.neon.srshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vrshl_v3.i = bitcast <4 x i16> %vrshl_v2.i to <8 x i8>
  store <4 x i16> %vrshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrshlq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.srshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vrshlq_v3.i = bitcast <8 x i16> %vrshlq_v2.i to <16 x i8>
  store <8 x i16> %vrshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrshl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vrshl_v2.i = call <2 x i32> @llvm.aarch64.neon.srshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vrshl_v3.i = bitcast <2 x i32> %vrshl_v2.i to <8 x i8>
  store <2 x i32> %vrshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrshlq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vrshlq_v3.i = bitcast <4 x i32> %vrshlq_v2.i to <16 x i8>
  store <4 x i32> %vrshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vrshlq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vrshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.srshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vrshlq_v3.i = bitcast <2 x i64> %vrshlq_v2.i to <16 x i8>
  store <2 x i64> %vrshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vrshl_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vrshl_v2.i = call <1 x i64> @llvm.aarch64.neon.srshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vrshl_v3.i = bitcast <1 x i64> %vrshl_v2.i to <8 x i8>
  store <1 x i64> %vrshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vrshld_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vrshld_s64.i = call i64 @llvm.aarch64.neon.srshl.i64(i64 %2, i64 %3) #4
  store i64 %vrshld_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vrshl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vrshl_v.i = call <8 x i8> @llvm.aarch64.neon.urshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vrshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vrshlq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vrshlq_v.i = call <16 x i8> @llvm.aarch64.neon.urshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vrshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vrshl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vrshl_v2.i = call <4 x i16> @llvm.aarch64.neon.urshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vrshl_v3.i = bitcast <4 x i16> %vrshl_v2.i to <8 x i8>
  store <4 x i16> %vrshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vrshlq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vrshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.urshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vrshlq_v3.i = bitcast <8 x i16> %vrshlq_v2.i to <16 x i8>
  store <8 x i16> %vrshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vrshl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vrshl_v2.i = call <2 x i32> @llvm.aarch64.neon.urshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vrshl_v3.i = bitcast <2 x i32> %vrshl_v2.i to <8 x i8>
  store <2 x i32> %vrshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vrshlq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vrshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.urshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vrshlq_v3.i = bitcast <4 x i32> %vrshlq_v2.i to <16 x i8>
  store <4 x i32> %vrshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vrshlq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vrshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.urshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vrshlq_v3.i = bitcast <2 x i64> %vrshlq_v2.i to <16 x i8>
  store <2 x i64> %vrshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vrshl_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vrshl_v2.i = call <1 x i64> @llvm.aarch64.neon.urshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vrshl_v3.i = bitcast <1 x i64> %vrshl_v2.i to <8 x i8>
  store <1 x i64> %vrshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vrshld_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vrshld_u64.i = call i64 @llvm.aarch64.neon.urshl.i64(i64 %2, i64 %3) #4
  store i64 %vrshld_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqrshl_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqrshl_v.i = call <8 x i8> @llvm.aarch64.neon.sqrshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqrshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqrshlq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqrshlq_v.i = call <16 x i8> @llvm.aarch64.neon.sqrshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqrshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqrshl_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqrshl_v2.i = call <4 x i16> @llvm.aarch64.neon.sqrshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqrshl_v3.i = bitcast <4 x i16> %vqrshl_v2.i to <8 x i8>
  store <4 x i16> %vqrshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqrshlq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqrshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.sqrshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqrshlq_v3.i = bitcast <8 x i16> %vqrshlq_v2.i to <16 x i8>
  store <8 x i16> %vqrshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqrshl_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqrshl_v2.i = call <2 x i32> @llvm.aarch64.neon.sqrshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqrshl_v3.i = bitcast <2 x i32> %vqrshl_v2.i to <8 x i8>
  store <2 x i32> %vqrshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqrshlq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqrshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.sqrshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqrshlq_v3.i = bitcast <4 x i32> %vqrshlq_v2.i to <16 x i8>
  store <4 x i32> %vqrshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqrshlq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqrshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.sqrshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqrshlq_v3.i = bitcast <2 x i64> %vqrshlq_v2.i to <16 x i8>
  store <2 x i64> %vqrshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqrshl_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqrshl_v2.i = call <1 x i64> @llvm.aarch64.neon.sqrshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqrshl_v3.i = bitcast <1 x i64> %vqrshl_v2.i to <8 x i8>
  store <1 x i64> %vqrshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqrshlb_s8_wrapper(i8 signext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqrshlb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqrshl.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqrshlb_s8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqrshlh_s16_wrapper(i16 signext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqrshlh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqrshl.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqrshlh_s16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqrshls_s32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqrshls_s32.i = call i32 @llvm.aarch64.neon.sqrshl.i32(i32 %2, i32 %3) #4
  store i32 %vqrshls_s32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqrshld_s64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqrshld_s64.i = call i64 @llvm.aarch64.neon.sqrshl.i64(i64 %2, i64 %3) #4
  store i64 %vqrshld_s64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqrshl_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vqrshl_v.i = call <8 x i8> @llvm.aarch64.neon.uqrshl.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vqrshl_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqrshlq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vqrshlq_v.i = call <16 x i8> @llvm.aarch64.neon.uqrshl.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vqrshlq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqrshl_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vqrshl_v2.i = call <4 x i16> @llvm.aarch64.neon.uqrshl.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vqrshl_v3.i = bitcast <4 x i16> %vqrshl_v2.i to <8 x i8>
  store <4 x i16> %vqrshl_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqrshlq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqrshlq_v2.i = call <8 x i16> @llvm.aarch64.neon.uqrshl.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vqrshlq_v3.i = bitcast <8 x i16> %vqrshlq_v2.i to <16 x i8>
  store <8 x i16> %vqrshlq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqrshl_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vqrshl_v2.i = call <2 x i32> @llvm.aarch64.neon.uqrshl.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vqrshl_v3.i = bitcast <2 x i32> %vqrshl_v2.i to <8 x i8>
  store <2 x i32> %vqrshl_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqrshlq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqrshlq_v2.i = call <4 x i32> @llvm.aarch64.neon.uqrshl.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vqrshlq_v3.i = bitcast <4 x i32> %vqrshlq_v2.i to <16 x i8>
  store <4 x i32> %vqrshlq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqrshlq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqrshlq_v2.i = call <2 x i64> @llvm.aarch64.neon.uqrshl.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vqrshlq_v3.i = bitcast <2 x i64> %vqrshlq_v2.i to <16 x i8>
  store <2 x i64> %vqrshlq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqrshl_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = bitcast <1 x i64> %2 to <8 x i8>
  %4 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %5 = bitcast <1 x i64> %4 to <8 x i8>
  %vqrshl_v2.i = call <1 x i64> @llvm.aarch64.neon.uqrshl.v1i64(<1 x i64> %2, <1 x i64> %4) #4
  %vqrshl_v3.i = bitcast <1 x i64> %vqrshl_v2.i to <8 x i8>
  store <1 x i64> %vqrshl_v2.i, <1 x i64>* %__ret.i, align 8
  %6 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqrshlb_u8_wrapper(i8 zeroext %a, i8 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__p1.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  %b.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  store i8 %b, i8* %b.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  %1 = load i8, i8* %b.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  store i8 %1, i8* %__p1.addr.i, align 1
  %2 = load i8, i8* %__p0.addr.i, align 1
  %3 = load i8, i8* %__p1.addr.i, align 1
  %4 = insertelement <8 x i8> undef, i8 %2, i64 0
  %5 = insertelement <8 x i8> undef, i8 %3, i64 0
  %vqrshlb_u8.i = call <8 x i8> @llvm.aarch64.neon.uqrshl.v8i8(<8 x i8> %4, <8 x i8> %5) #4
  %6 = extractelement <8 x i8> %vqrshlb_u8.i, i64 0
  store i8 %6, i8* %__ret.i, align 1
  %7 = load i8, i8* %__ret.i, align 1
  ret i8 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqrshlh_u16_wrapper(i16 zeroext %a, i16 signext %b) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  %b.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  store i16 %b, i16* %b.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  %1 = load i16, i16* %b.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load i16, i16* %__p0.addr.i, align 2
  %3 = load i16, i16* %__p1.addr.i, align 2
  %4 = insertelement <4 x i16> undef, i16 %2, i64 0
  %5 = insertelement <4 x i16> undef, i16 %3, i64 0
  %vqrshlh_u16.i = call <4 x i16> @llvm.aarch64.neon.uqrshl.v4i16(<4 x i16> %4, <4 x i16> %5) #4
  %6 = extractelement <4 x i16> %vqrshlh_u16.i, i64 0
  store i16 %6, i16* %__ret.i, align 2
  %7 = load i16, i16* %__ret.i, align 2
  ret i16 %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqrshls_u32_wrapper(i32 %a, i32 %b) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  %b.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  store i32 %b, i32* %b.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  %1 = load i32, i32* %b.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load i32, i32* %__p0.addr.i, align 4
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vqrshls_u32.i = call i32 @llvm.aarch64.neon.uqrshl.i32(i32 %2, i32 %3) #4
  store i32 %vqrshls_u32.i, i32* %__ret.i, align 4
  %4 = load i32, i32* %__ret.i, align 4
  ret i32 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqrshld_u64_wrapper(i64 %a, i64 %b) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__p1.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  %b.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  store i64 %b, i64* %b.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  %1 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  store i64 %1, i64* %__p1.addr.i, align 8
  %2 = load i64, i64* %__p0.addr.i, align 8
  %3 = load i64, i64* %__p1.addr.i, align 8
  %vqrshld_u64.i = call i64 @llvm.aarch64.neon.uqrshl.i64(i64 %2, i64 %3) #4
  store i64 %vqrshld_u64.i, i64* %__ret.i, align 8
  %4 = load i64, i64* %__ret.i, align 8
  ret i64 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovl_s8_wrapper(<8 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %vmovl.i = sext <8 x i8> %1 to <8 x i16>
  store <8 x i16> %vmovl.i, <8 x i16>* %__ret.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovl_s16_wrapper(<4 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %vmovl.i = sext <4 x i16> %1 to <4 x i32>
  store <4 x i32> %vmovl.i, <4 x i32>* %__ret.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovl_s32_wrapper(<2 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %vmovl.i = sext <2 x i32> %1 to <2 x i64>
  store <2 x i64> %vmovl.i, <2 x i64>* %__ret.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovl_high_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0_612.addr.i = alloca <16 x i8>, align 16
  %__ret_612.i = alloca <8 x i16>, align 16
  %__a1_612.i = alloca <8 x i8>, align 8
  %__s0.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %tmp.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0_612.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0_612.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p0.addr.i.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %2, <16 x i8> %3, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %4, <8 x i8>* %__a1_612.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__a1_612.i, align 8
  store <8 x i8> %5, <8 x i8>* %__s0.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__s0.i, align 8
  %7 = sext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %7, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  store <8 x i16> %8, <8 x i16>* %tmp.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %tmp.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret_612.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret_612.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovl_high_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0_618.addr.i = alloca <8 x i16>, align 16
  %__ret_618.i = alloca <4 x i32>, align 16
  %__a1_618.i = alloca <4 x i16>, align 8
  %__s0.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %tmp.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0_618.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0_618.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p0.addr.i.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %2, <8 x i16> %3, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %4, <4 x i16>* %__a1_618.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__a1_618.i, align 8
  store <4 x i16> %5, <4 x i16>* %__s0.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__s0.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = sext <4 x i16> %6 to <4 x i32>
  store <4 x i32> %8, <4 x i32>* %__ret.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  store <4 x i32> %9, <4 x i32>* %tmp.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %tmp.i, align 16
  store <4 x i32> %10, <4 x i32>* %__ret_618.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret_618.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovl_high_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0_615.addr.i = alloca <4 x i32>, align 16
  %__ret_615.i = alloca <2 x i64>, align 16
  %__a1_615.i = alloca <2 x i32>, align 8
  %__s0.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %tmp.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0_615.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0_615.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p0.addr.i.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %2, <4 x i32> %3, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %4, <2 x i32>* %__a1_615.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__a1_615.i, align 8
  store <2 x i32> %5, <2 x i32>* %__s0.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__s0.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = sext <2 x i32> %6 to <2 x i64>
  store <2 x i64> %8, <2 x i64>* %__ret.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  store <2 x i64> %9, <2 x i64>* %tmp.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %tmp.i, align 16
  store <2 x i64> %10, <2 x i64>* %__ret_615.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret_615.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovl_u8_wrapper(<8 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %vmovl.i = zext <8 x i8> %1 to <8 x i16>
  store <8 x i16> %vmovl.i, <8 x i16>* %__ret.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovl_u16_wrapper(<4 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %vmovl.i = zext <4 x i16> %1 to <4 x i32>
  store <4 x i32> %vmovl.i, <4 x i32>* %__ret.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovl_u32_wrapper(<2 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %vmovl.i = zext <2 x i32> %1 to <2 x i64>
  store <2 x i64> %vmovl.i, <2 x i64>* %__ret.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovl_high_u8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <16 x i8>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0_603.addr.i = alloca <16 x i8>, align 16
  %__ret_603.i = alloca <8 x i16>, align 16
  %__a1_603.i = alloca <8 x i8>, align 8
  %__s0.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i16>, align 16
  %tmp.i = alloca <8 x i16>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0_603.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0_603.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p0.addr.i.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <16 x i8> %2, <16 x i8> %3, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <8 x i8> %shuffle.i.i, <8 x i8>* %__ret.i.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %4, <8 x i8>* %__a1_603.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__a1_603.i, align 8
  store <8 x i8> %5, <8 x i8>* %__s0.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__s0.i, align 8
  %7 = zext <8 x i8> %6 to <8 x i16>
  store <8 x i16> %7, <8 x i16>* %__ret.i, align 16
  %8 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  store <8 x i16> %8, <8 x i16>* %tmp.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %tmp.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret_603.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret_603.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovl_high_u16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0_609.addr.i = alloca <8 x i16>, align 16
  %__ret_609.i = alloca <4 x i32>, align 16
  %__a1_609.i = alloca <4 x i16>, align 8
  %__s0.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i32>, align 16
  %tmp.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0_609.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0_609.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p0.addr.i.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %2, <8 x i16> %3, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %4, <4 x i16>* %__a1_609.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__a1_609.i, align 8
  store <4 x i16> %5, <4 x i16>* %__s0.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__s0.i, align 8
  %7 = bitcast <4 x i16> %6 to <8 x i8>
  %8 = zext <4 x i16> %6 to <4 x i32>
  store <4 x i32> %8, <4 x i32>* %__ret.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  store <4 x i32> %9, <4 x i32>* %tmp.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %tmp.i, align 16
  store <4 x i32> %10, <4 x i32>* %__ret_609.i, align 16
  %11 = load <4 x i32>, <4 x i32>* %__ret_609.i, align 16
  ret <4 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovl_high_u32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0_606.addr.i = alloca <4 x i32>, align 16
  %__ret_606.i = alloca <2 x i64>, align 16
  %__a1_606.i = alloca <2 x i32>, align 8
  %__s0.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %tmp.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0_606.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0_606.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p0.addr.i.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %2, <4 x i32> %3, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %4, <2 x i32>* %__a1_606.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__a1_606.i, align 8
  store <2 x i32> %5, <2 x i32>* %__s0.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__s0.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = zext <2 x i32> %6 to <2 x i64>
  store <2 x i64> %8, <2 x i64>* %__ret.i, align 16
  %9 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  store <2 x i64> %9, <2 x i64>* %tmp.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %tmp.i, align 16
  store <2 x i64> %10, <2 x i64>* %__ret_606.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret_606.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmovn_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vmovn.i = trunc <8 x i16> %1 to <8 x i8>
  store <8 x i8> %vmovn.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmovn_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vmovn.i = trunc <4 x i32> %1 to <4 x i16>
  store <4 x i16> %vmovn.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmovn_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vmovn.i = trunc <2 x i64> %1 to <2 x i32>
  store <2 x i32> %vmovn.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmovn_u16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vmovn.i = trunc <8 x i16> %1 to <8 x i8>
  store <8 x i8> %vmovn.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmovn_u32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vmovn.i = trunc <4 x i32> %1 to <4 x i16>
  store <4 x i16> %vmovn.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmovn_u64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vmovn.i = trunc <2 x i64> %1 to <2 x i32>
  store <2 x i32> %vmovn.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmovn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmovn.i.i = trunc <8 x i16> %4 to <8 x i8>
  store <8 x i8> %vmovn.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p1.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %7, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i3.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__ret.i3.i, align 16
  store <16 x i8> %9, <16 x i8>* %__ret.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmovn.i.i = trunc <4 x i32> %4 to <4 x i16>
  store <4 x i16> %vmovn.i.i, <4 x i16>* %__ret.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %7, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i3.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vmovn.i.i = trunc <2 x i64> %4 to <2 x i32>
  store <2 x i32> %vmovn.i.i, <2 x i32>* %__ret.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %7, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %9, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmovn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vmovn.i.i = trunc <8 x i16> %4 to <8 x i8>
  store <8 x i8> %vmovn.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p1.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %7, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i3.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__ret.i3.i, align 16
  store <16 x i8> %9, <16 x i8>* %__ret.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vmovn.i.i = trunc <4 x i32> %4 to <4 x i16>
  store <4 x i16> %vmovn.i.i, <4 x i16>* %__ret.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %7, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i3.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vmovn.i.i = trunc <2 x i64> %4 to <2 x i32>
  store <2 x i32> %vmovn.i.i, <2 x i32>* %__ret.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %7, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %9, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqmovn_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vqmovn_v1.i = call <8 x i8> @llvm.aarch64.neon.sqxtn.v8i8(<8 x i16> %1) #4
  store <8 x i8> %vqmovn_v1.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqmovn_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vqmovn_v1.i = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1) #4
  %vqmovn_v2.i = bitcast <4 x i16> %vqmovn_v1.i to <8 x i8>
  store <4 x i16> %vqmovn_v1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqmovn_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vqmovn_v1.i = call <2 x i32> @llvm.aarch64.neon.sqxtn.v2i32(<2 x i64> %1) #4
  %vqmovn_v2.i = bitcast <2 x i32> %vqmovn_v1.i to <8 x i8>
  store <2 x i32> %vqmovn_v1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqmovn_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <8 x i8> @llvm.aarch64.neon.sqxtn.v8i8(<8 x i16> %4) #4
  store <8 x i8> %vqmovn_v1.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p1.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %7, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i3.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__ret.i3.i, align 16
  store <16 x i8> %9, <16 x i8>* %__ret.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqmovn_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4) #4
  %vqmovn_v2.i.i = bitcast <4 x i16> %vqmovn_v1.i.i to <8 x i8>
  store <4 x i16> %vqmovn_v1.i.i, <4 x i16>* %__ret.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %7, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i3.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqmovn_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <2 x i32> @llvm.aarch64.neon.sqxtn.v2i32(<2 x i64> %4) #4
  %vqmovn_v2.i.i = bitcast <2 x i32> %vqmovn_v1.i.i to <8 x i8>
  store <2 x i32> %vqmovn_v1.i.i, <2 x i32>* %__ret.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %7, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %9, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqmovn_u16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vqmovn_v1.i = call <8 x i8> @llvm.aarch64.neon.uqxtn.v8i8(<8 x i16> %1) #4
  store <8 x i8> %vqmovn_v1.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqmovn_u32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vqmovn_v1.i = call <4 x i16> @llvm.aarch64.neon.uqxtn.v4i16(<4 x i32> %1) #4
  %vqmovn_v2.i = bitcast <4 x i16> %vqmovn_v1.i to <8 x i8>
  store <4 x i16> %vqmovn_v1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqmovn_u64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vqmovn_v1.i = call <2 x i32> @llvm.aarch64.neon.uqxtn.v2i32(<2 x i64> %1) #4
  %vqmovn_v2.i = bitcast <2 x i32> %vqmovn_v1.i to <8 x i8>
  store <2 x i32> %vqmovn_v1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqmovn_high_u16_wrapper(<8 x i8> %r, <8 x i16> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <8 x i8> @llvm.aarch64.neon.uqxtn.v8i8(<8 x i16> %4) #4
  store <8 x i8> %vqmovn_v1.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p1.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %7, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i3.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__ret.i3.i, align 16
  store <16 x i8> %9, <16 x i8>* %__ret.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqmovn_high_u32_wrapper(<4 x i16> %r, <4 x i32> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <4 x i16> @llvm.aarch64.neon.uqxtn.v4i16(<4 x i32> %4) #4
  %vqmovn_v2.i.i = bitcast <4 x i16> %vqmovn_v1.i.i to <8 x i8>
  store <4 x i16> %vqmovn_v1.i.i, <4 x i16>* %__ret.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %7, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i3.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqmovn_high_u64_wrapper(<2 x i32> %r, <2 x i64> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqmovn_v1.i.i = call <2 x i32> @llvm.aarch64.neon.uqxtn.v2i32(<2 x i64> %4) #4
  %vqmovn_v2.i.i = bitcast <2 x i32> %vqmovn_v1.i.i to <8 x i8>
  store <2 x i32> %vqmovn_v1.i.i, <2 x i32>* %__ret.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %7, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %9, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqmovnh_s16_wrapper(i16 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %2 = insertelement <8 x i16> undef, i16 %1, i64 0
  %vqmovnh_s16.i = call <8 x i8> @llvm.aarch64.neon.sqxtn.v8i8(<8 x i16> %2) #4
  %3 = extractelement <8 x i8> %vqmovnh_s16.i, i64 0
  store i8 %3, i8* %__ret.i, align 1
  %4 = load i8, i8* %__ret.i, align 1
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqmovns_s32_wrapper(i32 %a) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %2 = insertelement <4 x i32> undef, i32 %1, i64 0
  %vqmovns_s32.i = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #4
  %3 = extractelement <4 x i16> %vqmovns_s32.i, i64 0
  store i16 %3, i16* %__ret.i, align 2
  %4 = load i16, i16* %__ret.i, align 2
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqmovnd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vqmovnd_s64.i = call i32 @llvm.aarch64.neon.scalar.sqxtn.i32.i64(i64 %1) #4
  store i32 %vqmovnd_s64.i, i32* %__ret.i, align 4
  %2 = load i32, i32* %__ret.i, align 4
  ret i32 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqmovnh_u16_wrapper(i16 zeroext %a) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %2 = insertelement <8 x i16> undef, i16 %1, i64 0
  %vqmovnh_u16.i = call <8 x i8> @llvm.aarch64.neon.uqxtn.v8i8(<8 x i16> %2) #4
  %3 = extractelement <8 x i8> %vqmovnh_u16.i, i64 0
  store i8 %3, i8* %__ret.i, align 1
  %4 = load i8, i8* %__ret.i, align 1
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqmovns_u32_wrapper(i32 %a) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %2 = insertelement <4 x i32> undef, i32 %1, i64 0
  %vqmovns_u32.i = call <4 x i16> @llvm.aarch64.neon.uqxtn.v4i16(<4 x i32> %2) #4
  %3 = extractelement <4 x i16> %vqmovns_u32.i, i64 0
  store i16 %3, i16* %__ret.i, align 2
  %4 = load i16, i16* %__ret.i, align 2
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqmovnd_u64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vqmovnd_u64.i = call i32 @llvm.aarch64.neon.scalar.uqxtn.i32.i64(i64 %1) #4
  store i32 %vqmovnd_u64.i, i32* %__ret.i, align 4
  %2 = load i32, i32* %__ret.i, align 4
  ret i32 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqmovun_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vqmovun_v1.i = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1) #4
  store <8 x i8> %vqmovun_v1.i, <8 x i8>* %__ret.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqmovun_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vqmovun_v1.i = call <4 x i16> @llvm.aarch64.neon.sqxtun.v4i16(<4 x i32> %1) #4
  %vqmovun_v2.i = bitcast <4 x i16> %vqmovun_v1.i to <8 x i8>
  store <4 x i16> %vqmovun_v1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqmovun_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vqmovun_v1.i = call <2 x i32> @llvm.aarch64.neon.sqxtun.v2i32(<2 x i64> %1) #4
  %vqmovun_v2.i = bitcast <2 x i32> %vqmovun_v1.i to <8 x i8>
  store <2 x i32> %vqmovun_v1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqmovun_high_s16_wrapper(<8 x i8> %r, <8 x i16> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <8 x i8>, align 8
  %__p1.addr.i.i = alloca <8 x i8>, align 8
  %__ret.i3.i = alloca <16 x i8>, align 16
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i8>, align 8
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %r.addr = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i8> %r, <8 x i8>* %r.addr, align 8
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i8>, <8 x i8>* %r.addr, align 8
  %1 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %3, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vqmovun_v1.i.i = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %4) #4
  store <8 x i8> %vqmovun_v1.i.i, <8 x i8>* %__ret.i.i, align 8
  %6 = load <8 x i8>, <8 x i8>* %__ret.i.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p0.addr.i2.i, align 8
  store <8 x i8> %6, <8 x i8>* %__p1.addr.i.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__p0.addr.i2.i, align 8
  %8 = load <8 x i8>, <8 x i8>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <8 x i8> %7, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  store <16 x i8> %shuffle.i.i, <16 x i8>* %__ret.i3.i, align 16
  %9 = load <16 x i8>, <16 x i8>* %__ret.i3.i, align 16
  store <16 x i8> %9, <16 x i8>* %__ret.i, align 16
  %10 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqmovun_high_s32_wrapper(<4 x i16> %r, <4 x i32> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i3.i = alloca <8 x i16>, align 16
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %r.addr = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i16> %r, <4 x i16>* %r.addr, align 8
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i16>, <4 x i16>* %r.addr, align 8
  %1 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vqmovun_v1.i.i = call <4 x i16> @llvm.aarch64.neon.sqxtun.v4i16(<4 x i32> %4) #4
  %vqmovun_v2.i.i = bitcast <4 x i16> %vqmovun_v1.i.i to <8 x i8>
  store <4 x i16> %vqmovun_v1.i.i, <4 x i16>* %__ret.i.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i2.i, align 8
  store <4 x i16> %6, <4 x i16>* %__p1.addr.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <4 x i16> %7, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  store <8 x i16> %shuffle.i.i, <8 x i16>* %__ret.i3.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %__ret.i3.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqmovun_high_s64_wrapper(<2 x i32> %r, <2 x i64> %a) #1 {
entry:
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i3.i = alloca <4 x i32>, align 16
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %r.addr = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i32> %r, <2 x i32>* %r.addr, align 8
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i32>, <2 x i32>* %r.addr, align 8
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vqmovun_v1.i.i = call <2 x i32> @llvm.aarch64.neon.sqxtun.v2i32(<2 x i64> %4) #4
  %vqmovun_v2.i.i = bitcast <2 x i32> %vqmovun_v1.i.i to <8 x i8>
  store <2 x i32> %vqmovun_v1.i.i, <2 x i32>* %__ret.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i2.i, align 8
  store <2 x i32> %6, <2 x i32>* %__p1.addr.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %shuffle.i.i = shufflevector <2 x i32> %7, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %shuffle.i.i, <4 x i32>* %__ret.i3.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %9, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i8 @vqmovunh_s16_wrapper(i16 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %2 = insertelement <8 x i16> undef, i16 %1, i64 0
  %vqmovunh_s16.i = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %2) #4
  %3 = extractelement <8 x i8> %vqmovunh_s16.i, i64 0
  store i8 %3, i8* %__ret.i, align 1
  %4 = load i8, i8* %__ret.i, align 1
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local zeroext i16 @vqmovuns_s32_wrapper(i32 %a) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %2 = insertelement <4 x i32> undef, i32 %1, i64 0
  %vqmovuns_s32.i = call <4 x i16> @llvm.aarch64.neon.sqxtun.v4i16(<4 x i32> %2) #4
  %3 = extractelement <4 x i16> %vqmovuns_s32.i, i64 0
  store i16 %3, i16* %__ret.i, align 2
  %4 = load i16, i16* %__ret.i, align 2
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqmovund_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vqmovund_s64.i = call i32 @llvm.aarch64.neon.scalar.sqxtun.i32.i64(i64 %1) #4
  store i32 %vqmovund_s64.i, i32* %__ret.i, align 4
  %2 = load i32, i32* %__ret.i, align 4
  ret i32 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmla_n_s16_wrapper(<4 x i16> %a, <4 x i16> %b, i16 signext %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %4, %9
  %add.i = add <4 x i16> %3, %mul.i
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlaq_n_s16_wrapper(<8 x i16> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %8, i32 3
  %9 = load i16, i16* %__p2.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %9, i32 4
  %10 = load i16, i16* %__p2.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %10, i32 5
  %11 = load i16, i16* %__p2.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %11, i32 6
  %12 = load i16, i16* %__p2.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %12, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %4, %13
  %add.i = add <8 x i16> %3, %mul.i
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmla_n_s32_wrapper(<2 x i32> %a, <2 x i32> %b, i32 %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %4, %7
  %add.i = add <2 x i32> %3, %mul.i
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlaq_n_s32_wrapper(<4 x i32> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %6, i32 1
  %7 = load i32, i32* %__p2.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %7, i32 2
  %8 = load i32, i32* %__p2.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %8, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %4, %9
  %add.i = add <4 x i32> %3, %mul.i
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmla_n_u16_wrapper(<4 x i16> %a, <4 x i16> %b, i16 zeroext %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %4, %9
  %add.i = add <4 x i16> %3, %mul.i
  store <4 x i16> %add.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlaq_n_u16_wrapper(<8 x i16> %a, <8 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %8, i32 3
  %9 = load i16, i16* %__p2.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %9, i32 4
  %10 = load i16, i16* %__p2.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %10, i32 5
  %11 = load i16, i16* %__p2.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %11, i32 6
  %12 = load i16, i16* %__p2.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %12, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %4, %13
  %add.i = add <8 x i16> %3, %mul.i
  store <8 x i16> %add.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmla_n_u32_wrapper(<2 x i32> %a, <2 x i32> %b, i32 %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %4, %7
  %add.i = add <2 x i32> %3, %mul.i
  store <2 x i32> %add.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlaq_n_u32_wrapper(<4 x i32> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %6, i32 1
  %7 = load i32, i32* %__p2.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %7, i32 2
  %8 = load i32, i32* %__p2.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %8, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %4, %9
  %add.i = add <4 x i32> %3, %mul.i
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_n_s16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %3, %14
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_n_s32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %8, <2 x i32> %10) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %3, %12
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_high_n_s16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %16, <4 x i16> %18) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %add.i.i = add <4 x i32> %9, %20
  store <4 x i32> %add.i.i, <4 x i32>* %__ret.i3.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %21, <4 x i32>* %__ret.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_high_n_s32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %add.i.i = add <2 x i64> %9, %18
  store <2 x i64> %add.i.i, <2 x i64>* %__ret.i3.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %20
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_n_u16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %add.i = add <4 x i32> %3, %14
  store <4 x i32> %add.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_n_u32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %8, <2 x i32> %10) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %add.i = add <2 x i64> %3, %12
  store <2 x i64> %add.i, <2 x i64>* %__ret.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlal_high_n_u16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %16, <4 x i16> %18) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %add.i.i = add <4 x i32> %9, %20
  store <4 x i32> %add.i.i, <4 x i32>* %__ret.i3.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %21, <4 x i32>* %__ret.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlal_high_n_u32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %add.i.i = add <2 x i64> %9, %18
  store <2 x i64> %add.i.i, <2 x i64>* %__ret.i3.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %20
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlal_n_s16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i16> %4, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p2.addr.i.i, align 8
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %11 = bitcast <4 x i32> %10 to <16 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %vqdmlal2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %12, <4 x i16> %14) #4
  %vqdmlal_v3.i.i = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %10, <4 x i32> %vqdmlal2.i.i) #4
  store <4 x i32> %vqdmlal_v3.i.i, <4 x i32>* %__ret.i.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %16, <4 x i32>* %__ret.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlal_n_s32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i32> %4, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p2.addr.i.i, align 8
  %8 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %12 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %vqdmlal2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %10, <2 x i32> %12) #4
  %vqdmlal_v3.i.i = call <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64> %8, <2 x i64> %vqdmlal2.i.i) #4
  store <2 x i64> %vqdmlal_v3.i.i, <2 x i64>* %__ret.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %14, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlal_high_n_s16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i32> %9, <4 x i32>* %__p0.addr.i.i.i, align 16
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p2.addr.i.i.i, align 8
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %17 = bitcast <4 x i32> %16 to <16 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %20 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %vqdmlal2.i.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %18, <4 x i16> %20) #4
  %vqdmlal_v3.i.i.i = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16, <4 x i32> %vqdmlal2.i.i.i) #4
  store <4 x i32> %vqdmlal_v3.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  store <4 x i32> %22, <4 x i32>* %__ret.i3.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %23, <4 x i32>* %__ret.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlal_high_n_s32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i64> %9, <2 x i64>* %__p0.addr.i.i.i, align 16
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p2.addr.i.i.i, align 8
  %14 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i.i, align 16
  %15 = bitcast <2 x i64> %14 to <16 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %18 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i.i, align 8
  %19 = bitcast <2 x i32> %18 to <8 x i8>
  %vqdmlal2.i.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %16, <2 x i32> %18) #4
  %vqdmlal_v3.i.i.i = call <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64> %14, <2 x i64> %vqdmlal2.i.i.i) #4
  store <2 x i64> %vqdmlal_v3.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i3.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %21, <2 x i64>* %__ret.i, align 16
  %22 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmls_n_s16_wrapper(<4 x i16> %a, <4 x i16> %b, i16 signext %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %4, %9
  %sub.i = sub <4 x i16> %3, %mul.i
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsq_n_s16_wrapper(<8 x i16> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %8, i32 3
  %9 = load i16, i16* %__p2.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %9, i32 4
  %10 = load i16, i16* %__p2.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %10, i32 5
  %11 = load i16, i16* %__p2.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %11, i32 6
  %12 = load i16, i16* %__p2.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %12, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %4, %13
  %sub.i = sub <8 x i16> %3, %mul.i
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmls_n_s32_wrapper(<2 x i32> %a, <2 x i32> %b, i32 %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %4, %7
  %sub.i = sub <2 x i32> %3, %mul.i
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsq_n_s32_wrapper(<4 x i32> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %6, i32 1
  %7 = load i32, i32* %__p2.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %7, i32 2
  %8 = load i32, i32* %__p2.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %8, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %4, %9
  %sub.i = sub <4 x i32> %3, %mul.i
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmls_n_u16_wrapper(<4 x i16> %a, <4 x i16> %b, i16 zeroext %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %4, %9
  %sub.i = sub <4 x i16> %3, %mul.i
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmlsq_n_u16_wrapper(<8 x i16> %a, <8 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %8, i32 3
  %9 = load i16, i16* %__p2.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %9, i32 4
  %10 = load i16, i16* %__p2.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %10, i32 5
  %11 = load i16, i16* %__p2.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %11, i32 6
  %12 = load i16, i16* %__p2.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %12, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %13 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %4, %13
  %sub.i = sub <8 x i16> %3, %mul.i
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %14 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %14
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmls_n_u32_wrapper(<2 x i32> %a, <2 x i32> %b, i32 %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %4, %7
  %sub.i = sub <2 x i32> %3, %mul.i
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsq_n_u32_wrapper(<4 x i32> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %6, i32 1
  %7 = load i32, i32* %__p2.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %7, i32 2
  %8 = load i32, i32* %__p2.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %8, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %9 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %4, %9
  %sub.i = sub <4 x i32> %3, %mul.i
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_n_s16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %3, %14
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_n_s32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %8, <2 x i32> %10) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %3, %12
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_high_n_s16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %16, <4 x i16> %18) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %sub.i.i = sub <4 x i32> %9, %20
  store <4 x i32> %sub.i.i, <4 x i32>* %__ret.i3.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %21, <4 x i32>* %__ret.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_high_n_s32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %sub.i.i = sub <2 x i64> %9, %18
  store <2 x i64> %sub.i.i, <2 x i64>* %__ret.i3.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %20
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_n_u16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %4, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p1.addr.i.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %10, <4 x i16> %12) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %14 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  %sub.i = sub <4 x i32> %3, %14
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %15 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_n_u32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %4, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  %8 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %8, <2 x i32> %10) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %12 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  %sub.i = sub <2 x i64> %3, %12
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %13 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmlsl_high_n_u16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 zeroext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %10, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %17 = bitcast <4 x i16> %16 to <8 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %16, <4 x i16> %18) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %20 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  %sub.i.i = sub <4 x i32> %9, %20
  store <4 x i32> %sub.i.i, <4 x i32>* %__ret.i3.i, align 16
  %21 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %21, <4 x i32>* %__ret.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmlsl_high_n_u32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %15 = bitcast <2 x i32> %14 to <8 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %14, <2 x i32> %16) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %18 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  %sub.i.i = sub <2 x i64> %9, %18
  store <2 x i64> %sub.i.i, <2 x i64>* %__ret.i3.i, align 16
  %19 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %19, <2 x i64>* %__ret.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %20
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlsl_n_s16_wrapper(<4 x i32> %a, <4 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = load i16, i16* %__p2.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %5, i32 0
  %6 = load i16, i16* %__p2.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %6, i32 1
  %7 = load i16, i16* %__p2.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %7, i32 2
  %8 = load i16, i16* %__p2.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %8, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %9 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i16> %4, <4 x i16>* %__p1.addr.i.i, align 8
  store <4 x i16> %9, <4 x i16>* %__p2.addr.i.i, align 8
  %10 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %11 = bitcast <4 x i32> %10 to <16 x i8>
  %12 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %13 = bitcast <4 x i16> %12 to <8 x i8>
  %14 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i, align 8
  %15 = bitcast <4 x i16> %14 to <8 x i8>
  %vqdmlal2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %12, <4 x i16> %14) #4
  %vqdmlsl_v3.i.i = call <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32> %10, <4 x i32> %vqdmlal2.i.i) #4
  store <4 x i32> %vqdmlsl_v3.i.i, <4 x i32>* %__ret.i.i, align 16
  %16 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %16, <4 x i32>* %__ret.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlsl_n_s32_wrapper(<2 x i64> %a, <2 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = load i32, i32* %__p2.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %5, i32 0
  %6 = load i32, i32* %__p2.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %6, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i.i, align 16
  store <2 x i32> %4, <2 x i32>* %__p1.addr.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p2.addr.i.i, align 8
  %8 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = bitcast <2 x i32> %10 to <8 x i8>
  %12 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i, align 8
  %13 = bitcast <2 x i32> %12 to <8 x i8>
  %vqdmlal2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %10, <2 x i32> %12) #4
  %vqdmlsl_v3.i.i = call <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64> %8, <2 x i64> %vqdmlal2.i.i) #4
  store <2 x i64> %vqdmlsl_v3.i.i, <2 x i64>* %__ret.i.i, align 16
  %14 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %14, <2 x i64>* %__ret.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %15
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmlsl_high_n_s16_wrapper(<4 x i32> %a, <8 x i16> %b, i16 signext %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__p2.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca i16, align 2
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store i16 %c, i16* %c.addr, align 2
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load i16, i16* %c.addr, align 2
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store i16 %2, i16* %__p2.addr.i, align 2
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %4, <8 x i16>* %__p0.addr.i.i, align 16
  %5 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %5, <8 x i16> %6, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %8 = load i16, i16* %__p2.addr.i, align 2
  store <4 x i32> %3, <4 x i32>* %__p0.addr.i2.i, align 16
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  store i16 %8, i16* %__p2.addr.i.i, align 2
  %9 = load <4 x i32>, <4 x i32>* %__p0.addr.i2.i, align 16
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %11, i32 0
  %12 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %12, i32 1
  %13 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %13, i32 2
  %14 = load i16, i16* %__p2.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %14, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %15 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i32> %9, <4 x i32>* %__p0.addr.i.i.i, align 16
  store <4 x i16> %10, <4 x i16>* %__p1.addr.i.i.i, align 8
  store <4 x i16> %15, <4 x i16>* %__p2.addr.i.i.i, align 8
  %16 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i.i, align 16
  %17 = bitcast <4 x i32> %16 to <16 x i8>
  %18 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %19 = bitcast <4 x i16> %18 to <8 x i8>
  %20 = load <4 x i16>, <4 x i16>* %__p2.addr.i.i.i, align 8
  %21 = bitcast <4 x i16> %20 to <8 x i8>
  %vqdmlal2.i.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %18, <4 x i16> %20) #4
  %vqdmlsl_v3.i.i.i = call <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32> %16, <4 x i32> %vqdmlal2.i.i.i) #4
  store <4 x i32> %vqdmlsl_v3.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %22 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  store <4 x i32> %22, <4 x i32>* %__ret.i3.i, align 16
  %23 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %23, <4 x i32>* %__ret.i, align 16
  %24 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %24
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmlsl_high_n_s32_wrapper(<2 x i64> %a, <4 x i32> %b, i32 %c) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i64>, align 16
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__p2.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca i32, align 4
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store i32 %c, i32* %c.addr, align 4
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load i32, i32* %c.addr, align 4
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store i32 %2, i32* %__p2.addr.i, align 4
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %4, <4 x i32>* %__p0.addr.i.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %5, <4 x i32> %6, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %7 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %8 = load i32, i32* %__p2.addr.i, align 4
  store <2 x i64> %3, <2 x i64>* %__p0.addr.i2.i, align 16
  store <2 x i32> %7, <2 x i32>* %__p1.addr.i.i, align 8
  store i32 %8, i32* %__p2.addr.i.i, align 4
  %9 = load <2 x i64>, <2 x i64>* %__p0.addr.i2.i, align 16
  %10 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %11 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %11, i32 0
  %12 = load i32, i32* %__p2.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %12, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %13 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i64> %9, <2 x i64>* %__p0.addr.i.i.i, align 16
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i.i.i, align 8
  store <2 x i32> %13, <2 x i32>* %__p2.addr.i.i.i, align 8
  %14 = load <2 x i64>, <2 x i64>* %__p0.addr.i.i.i, align 16
  %15 = bitcast <2 x i64> %14 to <16 x i8>
  %16 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %17 = bitcast <2 x i32> %16 to <8 x i8>
  %18 = load <2 x i32>, <2 x i32>* %__p2.addr.i.i.i, align 8
  %19 = bitcast <2 x i32> %18 to <8 x i8>
  %vqdmlal2.i.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %16, <2 x i32> %18) #4
  %vqdmlsl_v3.i.i.i = call <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64> %14, <2 x i64> %vqdmlal2.i.i.i) #4
  store <2 x i64> %vqdmlsl_v3.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %20 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  store <2 x i64> %20, <2 x i64>* %__ret.i3.i, align 16
  %21 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %21, <2 x i64>* %__ret.i, align 16
  %22 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %22
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmul_n_s16_wrapper(<4 x i16> %a, i16 signext %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %2, %7
  store <4 x i16> %mul.i, <4 x i16>* %__ret.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmulq_n_s16_wrapper(<8 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %6, i32 3
  %7 = load i16, i16* %__p1.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %7, i32 4
  %8 = load i16, i16* %__p1.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %8, i32 5
  %9 = load i16, i16* %__p1.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %9, i32 6
  %10 = load i16, i16* %__p1.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %10, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %2, %11
  store <8 x i16> %mul.i, <8 x i16>* %__ret.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmul_n_s32_wrapper(<2 x i32> %a, i32 %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %2, %5
  store <2 x i32> %mul.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmulq_n_s32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %4, i32 1
  %5 = load i32, i32* %__p1.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %5, i32 2
  %6 = load i32, i32* %__p1.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %6, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %2, %7
  store <4 x i32> %mul.i, <4 x i32>* %__ret.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmul_n_u16_wrapper(<4 x i16> %a, i16 zeroext %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  %mul.i = mul <4 x i16> %2, %7
  store <4 x i16> %mul.i, <4 x i16>* %__ret.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmulq_n_u16_wrapper(<8 x i16> %a, i16 zeroext %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %6, i32 3
  %7 = load i16, i16* %__p1.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %7, i32 4
  %8 = load i16, i16* %__p1.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %8, i32 5
  %9 = load i16, i16* %__p1.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %9, i32 6
  %10 = load i16, i16* %__p1.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %10, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  %mul.i = mul <8 x i16> %2, %11
  store <8 x i16> %mul.i, <8 x i16>* %__ret.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %12
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmul_n_u32_wrapper(<2 x i32> %a, i32 %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  %mul.i = mul <2 x i32> %2, %5
  store <2 x i32> %mul.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmulq_n_u32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %4, i32 1
  %5 = load i32, i32* %__p1.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %5, i32 2
  %6 = load i32, i32* %__p1.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %6, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  %mul.i = mul <4 x i32> %2, %7
  store <4 x i32> %mul.i, <4 x i32>* %__ret.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %8
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_n_s16_wrapper(<4 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8, <4 x i16> %10) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_n_s32_wrapper(<2 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %10, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_high_n_s16_wrapper(<8 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load i16, i16* %__p1.addr.i, align 2
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i2.i, align 8
  store i16 %6, i16* %__p1.addr.i.i, align 2
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %8, i32 0
  %9 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %9, i32 1
  %10 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %10, i32 2
  %11 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %11, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %12, <4 x i16>* %__p1.addr.i.i.i, align 8
  %13 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %14 = bitcast <4 x i16> %13 to <8 x i8>
  %15 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %13, <4 x i16> %15) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  store <4 x i32> %17, <4 x i32>* %__ret.i3.i, align 16
  %18 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %18, <4 x i32>* %__ret.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_high_n_s32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load i32, i32* %__p1.addr.i, align 4
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i2.i, align 8
  store i32 %6, i32* %__p1.addr.i.i, align 4
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %8, i32 0
  %9 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %9, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i.i.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %12 = bitcast <2 x i32> %11 to <8 x i8>
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = bitcast <2 x i32> %13 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32> %11, <2 x i32> %13) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  store <2 x i64> %15, <2 x i64>* %__ret.i3.i, align 16
  %16 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %16, <2 x i64>* %__ret.i, align 16
  %17 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_n_u16_wrapper(<4 x i16> %a, i16 zeroext %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %vmull2.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %8, <4 x i16> %10) #4
  store <4 x i32> %vmull2.i.i, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_n_u32_wrapper(<2 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vmull2.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  store <2 x i64> %vmull2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %10, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmull_high_n_u16_wrapper(<8 x i16> %a, i16 zeroext %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load i16, i16* %__p1.addr.i, align 2
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i2.i, align 8
  store i16 %6, i16* %__p1.addr.i.i, align 2
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %8, i32 0
  %9 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %9, i32 1
  %10 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %10, i32 2
  %11 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %11, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %12, <4 x i16>* %__p1.addr.i.i.i, align 8
  %13 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %14 = bitcast <4 x i16> %13 to <8 x i8>
  %15 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %vmull2.i.i.i = call <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16> %13, <4 x i16> %15) #4
  store <4 x i32> %vmull2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  store <4 x i32> %17, <4 x i32>* %__ret.i3.i, align 16
  %18 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %18, <4 x i32>* %__ret.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmull_high_n_u32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load i32, i32* %__p1.addr.i, align 4
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i2.i, align 8
  store i32 %6, i32* %__p1.addr.i.i, align 4
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %8, i32 0
  %9 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %9, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i.i.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %12 = bitcast <2 x i32> %11 to <8 x i8>
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = bitcast <2 x i32> %13 to <8 x i8>
  %vmull2.i.i.i = call <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32> %11, <2 x i32> %13) #4
  store <2 x i64> %vmull2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  store <2 x i64> %15, <2 x i64>* %__ret.i3.i, align 16
  %16 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %16, <2 x i64>* %__ret.i, align 16
  %17 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmull_n_s16_wrapper(<4 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %vqdmull_v2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %8, <4 x i16> %10) #4
  %vqdmull_v3.i.i = bitcast <4 x i32> %vqdmull_v2.i.i to <16 x i8>
  store <4 x i32> %vqdmull_v2.i.i, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmull_n_s32_wrapper(<2 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vqdmull_v2.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %6, <2 x i32> %8) #4
  %vqdmull_v3.i.i = bitcast <2 x i64> %vqdmull_v2.i.i to <16 x i8>
  store <2 x i64> %vqdmull_v2.i.i, <2 x i64>* %__ret.i.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i.i, align 16
  store <2 x i64> %10, <2 x i64>* %__ret.i, align 16
  %11 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmull_high_n_s16_wrapper(<8 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i.i = alloca <4 x i16>, align 8
  %__ret.i.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i2.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca i16, align 2
  %__ret.i3.i = alloca <4 x i32>, align 16
  %.compoundliteral.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <8 x i16> %3, <8 x i16> %4, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i16> %shuffle.i.i, <4 x i16>* %__ret.i.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  %6 = load i16, i16* %__p1.addr.i, align 2
  store <4 x i16> %5, <4 x i16>* %__p0.addr.i2.i, align 8
  store i16 %6, i16* %__p1.addr.i.i, align 2
  %7 = load <4 x i16>, <4 x i16>* %__p0.addr.i2.i, align 8
  %8 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit.i.i = insertelement <4 x i16> undef, i16 %8, i32 0
  %9 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit1.i.i = insertelement <4 x i16> %vecinit.i.i, i16 %9, i32 1
  %10 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit2.i.i = insertelement <4 x i16> %vecinit1.i.i, i16 %10, i32 2
  %11 = load i16, i16* %__p1.addr.i.i, align 2
  %vecinit3.i.i = insertelement <4 x i16> %vecinit2.i.i, i16 %11, i32 3
  store <4 x i16> %vecinit3.i.i, <4 x i16>* %.compoundliteral.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %.compoundliteral.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p0.addr.i.i.i, align 8
  store <4 x i16> %12, <4 x i16>* %__p1.addr.i.i.i, align 8
  %13 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i.i, align 8
  %14 = bitcast <4 x i16> %13 to <8 x i8>
  %15 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i.i, align 8
  %16 = bitcast <4 x i16> %15 to <8 x i8>
  %vqdmull_v2.i.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16> %13, <4 x i16> %15) #4
  %vqdmull_v3.i.i.i = bitcast <4 x i32> %vqdmull_v2.i.i.i to <16 x i8>
  store <4 x i32> %vqdmull_v2.i.i.i, <4 x i32>* %__ret.i.i.i, align 16
  %17 = load <4 x i32>, <4 x i32>* %__ret.i.i.i, align 16
  store <4 x i32> %17, <4 x i32>* %__ret.i3.i, align 16
  %18 = load <4 x i32>, <4 x i32>* %__ret.i3.i, align 16
  store <4 x i32> %18, <4 x i32>* %__ret.i, align 16
  %19 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %19
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqdmull_high_n_s32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i.i = alloca <2 x i32>, align 8
  %__ret.i.i.i = alloca <2 x i64>, align 16
  %__p0.addr.i2.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca i32, align 4
  %__ret.i3.i = alloca <2 x i64>, align 16
  %.compoundliteral.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %shuffle.i.i = shufflevector <4 x i32> %3, <4 x i32> %4, <2 x i32> <i32 2, i32 3>
  store <2 x i32> %shuffle.i.i, <2 x i32>* %__ret.i.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  %6 = load i32, i32* %__p1.addr.i, align 4
  store <2 x i32> %5, <2 x i32>* %__p0.addr.i2.i, align 8
  store i32 %6, i32* %__p1.addr.i.i, align 4
  %7 = load <2 x i32>, <2 x i32>* %__p0.addr.i2.i, align 8
  %8 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit.i.i = insertelement <2 x i32> undef, i32 %8, i32 0
  %9 = load i32, i32* %__p1.addr.i.i, align 4
  %vecinit1.i.i = insertelement <2 x i32> %vecinit.i.i, i32 %9, i32 1
  store <2 x i32> %vecinit1.i.i, <2 x i32>* %.compoundliteral.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %.compoundliteral.i.i, align 8
  store <2 x i32> %7, <2 x i32>* %__p0.addr.i.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__p1.addr.i.i.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i.i, align 8
  %12 = bitcast <2 x i32> %11 to <8 x i8>
  %13 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i.i, align 8
  %14 = bitcast <2 x i32> %13 to <8 x i8>
  %vqdmull_v2.i.i.i = call <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32> %11, <2 x i32> %13) #4
  %vqdmull_v3.i.i.i = bitcast <2 x i64> %vqdmull_v2.i.i.i to <16 x i8>
  store <2 x i64> %vqdmull_v2.i.i.i, <2 x i64>* %__ret.i.i.i, align 16
  %15 = load <2 x i64>, <2 x i64>* %__ret.i.i.i, align 16
  store <2 x i64> %15, <2 x i64>* %__ret.i3.i, align 16
  %16 = load <2 x i64>, <2 x i64>* %__ret.i3.i, align 16
  store <2 x i64> %16, <2 x i64>* %__ret.i, align 16
  %17 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqdmulh_n_s16_wrapper(<4 x i16> %a, i16 signext %b) #0 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %vqdmulh_v2.i.i = call <4 x i16> @llvm.aarch64.neon.sqdmulh.v4i16(<4 x i16> %8, <4 x i16> %10) #4
  %vqdmulh_v3.i.i = bitcast <4 x i16> %vqdmulh_v2.i.i to <8 x i8>
  store <4 x i16> %vqdmulh_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %12, <4 x i16>* %__ret.i, align 8
  %13 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqdmulhq_n_s16_wrapper(<8 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %6, i32 3
  %7 = load i16, i16* %__p1.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %7, i32 4
  %8 = load i16, i16* %__p1.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %8, i32 5
  %9 = load i16, i16* %__p1.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %9, i32 6
  %10 = load i16, i16* %__p1.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %10, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__p1.addr.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %13 = bitcast <8 x i16> %12 to <16 x i8>
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %15 = bitcast <8 x i16> %14 to <16 x i8>
  %vqdmulhq_v2.i.i = call <8 x i16> @llvm.aarch64.neon.sqdmulh.v8i16(<8 x i16> %12, <8 x i16> %14) #4
  %vqdmulhq_v3.i.i = bitcast <8 x i16> %vqdmulhq_v2.i.i to <16 x i8>
  store <8 x i16> %vqdmulhq_v2.i.i, <8 x i16>* %__ret.i.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %16, <8 x i16>* %__ret.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqdmulh_n_s32_wrapper(<2 x i32> %a, i32 %b) #0 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vqdmulh_v2.i.i = call <2 x i32> @llvm.aarch64.neon.sqdmulh.v2i32(<2 x i32> %6, <2 x i32> %8) #4
  %vqdmulh_v3.i.i = bitcast <2 x i32> %vqdmulh_v2.i.i to <8 x i8>
  store <2 x i32> %vqdmulh_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__ret.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqdmulhq_n_s32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %4, i32 1
  %5 = load i32, i32* %__p1.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %5, i32 2
  %6 = load i32, i32* %__p1.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %6, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %7, <4 x i32>* %__p1.addr.i.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %10 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %11 = bitcast <4 x i32> %10 to <16 x i8>
  %vqdmulhq_v2.i.i = call <4 x i32> @llvm.aarch64.neon.sqdmulh.v4i32(<4 x i32> %8, <4 x i32> %10) #4
  %vqdmulhq_v3.i.i = bitcast <4 x i32> %vqdmulhq_v2.i.i to <16 x i8>
  store <4 x i32> %vqdmulhq_v2.i.i, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqrdmulh_n_s16_wrapper(<4 x i16> %a, i16 signext %b) #0 {
entry:
  %__p0.addr.i.i = alloca <4 x i16>, align 8
  %__p1.addr.i.i = alloca <4 x i16>, align 8
  %__ret.i.i = alloca <4 x i16>, align 8
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca i16, align 2
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store i16 %b, i16* %b.addr, align 2
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load i16, i16* %b.addr, align 2
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %6, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %7 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p0.addr.i.i, align 8
  store <4 x i16> %7, <4 x i16>* %__p1.addr.i.i, align 8
  %8 = load <4 x i16>, <4 x i16>* %__p0.addr.i.i, align 8
  %9 = bitcast <4 x i16> %8 to <8 x i8>
  %10 = load <4 x i16>, <4 x i16>* %__p1.addr.i.i, align 8
  %11 = bitcast <4 x i16> %10 to <8 x i8>
  %vqrdmulh_v2.i.i = call <4 x i16> @llvm.aarch64.neon.sqrdmulh.v4i16(<4 x i16> %8, <4 x i16> %10) #4
  %vqrdmulh_v3.i.i = bitcast <4 x i16> %vqrdmulh_v2.i.i to <8 x i8>
  store <4 x i16> %vqrdmulh_v2.i.i, <4 x i16>* %__ret.i.i, align 8
  %12 = load <4 x i16>, <4 x i16>* %__ret.i.i, align 8
  store <4 x i16> %12, <4 x i16>* %__ret.i, align 8
  %13 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqrdmulhq_n_s16_wrapper(<8 x i16> %a, i16 signext %b) #1 {
entry:
  %__p0.addr.i.i = alloca <8 x i16>, align 16
  %__p1.addr.i.i = alloca <8 x i16>, align 16
  %__ret.i.i = alloca <8 x i16>, align 16
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca i16, align 2
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store i16 %b, i16* %b.addr, align 2
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load i16, i16* %b.addr, align 2
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store i16 %1, i16* %__p1.addr.i, align 2
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load i16, i16* %__p1.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %3, i32 0
  %4 = load i16, i16* %__p1.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %4, i32 1
  %5 = load i16, i16* %__p1.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %5, i32 2
  %6 = load i16, i16* %__p1.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %6, i32 3
  %7 = load i16, i16* %__p1.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %7, i32 4
  %8 = load i16, i16* %__p1.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %8, i32 5
  %9 = load i16, i16* %__p1.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %9, i32 6
  %10 = load i16, i16* %__p1.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %10, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %11 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p0.addr.i.i, align 16
  store <8 x i16> %11, <8 x i16>* %__p1.addr.i.i, align 16
  %12 = load <8 x i16>, <8 x i16>* %__p0.addr.i.i, align 16
  %13 = bitcast <8 x i16> %12 to <16 x i8>
  %14 = load <8 x i16>, <8 x i16>* %__p1.addr.i.i, align 16
  %15 = bitcast <8 x i16> %14 to <16 x i8>
  %vqrdmulhq_v2.i.i = call <8 x i16> @llvm.aarch64.neon.sqrdmulh.v8i16(<8 x i16> %12, <8 x i16> %14) #4
  %vqrdmulhq_v3.i.i = bitcast <8 x i16> %vqrdmulhq_v2.i.i to <16 x i8>
  store <8 x i16> %vqrdmulhq_v2.i.i, <8 x i16>* %__ret.i.i, align 16
  %16 = load <8 x i16>, <8 x i16>* %__ret.i.i, align 16
  store <8 x i16> %16, <8 x i16>* %__ret.i, align 16
  %17 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %17
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqrdmulh_n_s32_wrapper(<2 x i32> %a, i32 %b) #0 {
entry:
  %__p0.addr.i.i = alloca <2 x i32>, align 8
  %__p1.addr.i.i = alloca <2 x i32>, align 8
  %__ret.i.i = alloca <2 x i32>, align 8
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca i32, align 4
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store i32 %b, i32* %b.addr, align 4
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load i32, i32* %b.addr, align 4
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %4, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %5 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p0.addr.i.i, align 8
  store <2 x i32> %5, <2 x i32>* %__p1.addr.i.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__p0.addr.i.i, align 8
  %7 = bitcast <2 x i32> %6 to <8 x i8>
  %8 = load <2 x i32>, <2 x i32>* %__p1.addr.i.i, align 8
  %9 = bitcast <2 x i32> %8 to <8 x i8>
  %vqrdmulh_v2.i.i = call <2 x i32> @llvm.aarch64.neon.sqrdmulh.v2i32(<2 x i32> %6, <2 x i32> %8) #4
  %vqrdmulh_v3.i.i = bitcast <2 x i32> %vqrdmulh_v2.i.i to <8 x i8>
  store <2 x i32> %vqrdmulh_v2.i.i, <2 x i32>* %__ret.i.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i.i, align 8
  store <2 x i32> %10, <2 x i32>* %__ret.i, align 8
  %11 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %11
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqrdmulhq_n_s32_wrapper(<4 x i32> %a, i32 %b) #1 {
entry:
  %__p0.addr.i.i = alloca <4 x i32>, align 16
  %__p1.addr.i.i = alloca <4 x i32>, align 16
  %__ret.i.i = alloca <4 x i32>, align 16
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca i32, align 4
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store i32 %b, i32* %b.addr, align 4
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load i32, i32* %b.addr, align 4
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store i32 %1, i32* %__p1.addr.i, align 4
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load i32, i32* %__p1.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %3, i32 0
  %4 = load i32, i32* %__p1.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %4, i32 1
  %5 = load i32, i32* %__p1.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %5, i32 2
  %6 = load i32, i32* %__p1.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %6, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %7 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p0.addr.i.i, align 16
  store <4 x i32> %7, <4 x i32>* %__p1.addr.i.i, align 16
  %8 = load <4 x i32>, <4 x i32>* %__p0.addr.i.i, align 16
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %10 = load <4 x i32>, <4 x i32>* %__p1.addr.i.i, align 16
  %11 = bitcast <4 x i32> %10 to <16 x i8>
  %vqrdmulhq_v2.i.i = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8, <4 x i32> %10) #4
  %vqrdmulhq_v3.i.i = bitcast <4 x i32> %vqrdmulhq_v2.i.i to <16 x i8>
  store <4 x i32> %vqrdmulhq_v2.i.i, <4 x i32>* %__ret.i.i, align 16
  %12 = load <4 x i32>, <4 x i32>* %__ret.i.i, align 16
  store <4 x i32> %12, <4 x i32>* %__ret.i, align 16
  %13 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %13
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vabs_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %vabs.i = call <8 x i8> @llvm.aarch64.neon.abs.v8i8(<8 x i8> %1) #4
  store <8 x i8> %vabs.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vabsq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %vabs.i = call <16 x i8> @llvm.aarch64.neon.abs.v16i8(<16 x i8> %1) #4
  store <16 x i8> %vabs.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vabs_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %vabs1.i = call <4 x i16> @llvm.aarch64.neon.abs.v4i16(<4 x i16> %1) #4
  store <4 x i16> %vabs1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vabsq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vabs1.i = call <8 x i16> @llvm.aarch64.neon.abs.v8i16(<8 x i16> %1) #4
  store <8 x i16> %vabs1.i, <8 x i16>* %__ret.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vabs_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %vabs1.i = call <2 x i32> @llvm.aarch64.neon.abs.v2i32(<2 x i32> %1) #4
  store <2 x i32> %vabs1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vabsq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vabs1.i = call <4 x i32> @llvm.aarch64.neon.abs.v4i32(<4 x i32> %1) #4
  store <4 x i32> %vabs1.i, <4 x i32>* %__ret.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vabsq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vabs1.i = call <2 x i64> @llvm.aarch64.neon.abs.v2i64(<2 x i64> %1) #4
  store <2 x i64> %vabs1.i, <2 x i64>* %__ret.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vabs_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %vabs1.i = call <1 x i64> @llvm.aarch64.neon.abs.v1i64(<1 x i64> %1) #4
  store <1 x i64> %vabs1.i, <1 x i64>* %__ret.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vabsd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vabsd_s64.i = call i64 @llvm.aarch64.neon.abs.i64(i64 %1) #4
  store i64 %vabsd_s64.i, i64* %__ret.i, align 8
  %2 = load i64, i64* %__ret.i, align 8
  ret i64 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqabs_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %vqabs_v.i = call <8 x i8> @llvm.aarch64.neon.sqabs.v8i8(<8 x i8> %1) #4
  store <8 x i8> %vqabs_v.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqabsq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %vqabsq_v.i = call <16 x i8> @llvm.aarch64.neon.sqabs.v16i8(<16 x i8> %1) #4
  store <16 x i8> %vqabsq_v.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqabs_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %vqabs_v1.i = call <4 x i16> @llvm.aarch64.neon.sqabs.v4i16(<4 x i16> %1) #4
  %vqabs_v2.i = bitcast <4 x i16> %vqabs_v1.i to <8 x i8>
  store <4 x i16> %vqabs_v1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqabsq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vqabsq_v1.i = call <8 x i16> @llvm.aarch64.neon.sqabs.v8i16(<8 x i16> %1) #4
  %vqabsq_v2.i = bitcast <8 x i16> %vqabsq_v1.i to <16 x i8>
  store <8 x i16> %vqabsq_v1.i, <8 x i16>* %__ret.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqabs_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %vqabs_v1.i = call <2 x i32> @llvm.aarch64.neon.sqabs.v2i32(<2 x i32> %1) #4
  %vqabs_v2.i = bitcast <2 x i32> %vqabs_v1.i to <8 x i8>
  store <2 x i32> %vqabs_v1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqabsq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vqabsq_v1.i = call <4 x i32> @llvm.aarch64.neon.sqabs.v4i32(<4 x i32> %1) #4
  %vqabsq_v2.i = bitcast <4 x i32> %vqabsq_v1.i to <16 x i8>
  store <4 x i32> %vqabsq_v1.i, <4 x i32>* %__ret.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqabsq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vqabsq_v1.i = call <2 x i64> @llvm.aarch64.neon.sqabs.v2i64(<2 x i64> %1) #4
  %vqabsq_v2.i = bitcast <2 x i64> %vqabsq_v1.i to <16 x i8>
  store <2 x i64> %vqabsq_v1.i, <2 x i64>* %__ret.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqabs_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %vqabs_v1.i = call <1 x i64> @llvm.aarch64.neon.sqabs.v1i64(<1 x i64> %1) #4
  %vqabs_v2.i = bitcast <1 x i64> %vqabs_v1.i to <8 x i8>
  store <1 x i64> %vqabs_v1.i, <1 x i64>* %__ret.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqabsb_s8_wrapper(i8 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %2 = insertelement <8 x i8> undef, i8 %1, i64 0
  %vqabsb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqabs.v8i8(<8 x i8> %2) #4
  %3 = extractelement <8 x i8> %vqabsb_s8.i, i64 0
  store i8 %3, i8* %__ret.i, align 1
  %4 = load i8, i8* %__ret.i, align 1
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqabsh_s16_wrapper(i16 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %2 = insertelement <4 x i16> undef, i16 %1, i64 0
  %vqabsh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqabs.v4i16(<4 x i16> %2) #4
  %3 = extractelement <4 x i16> %vqabsh_s16.i, i64 0
  store i16 %3, i16* %__ret.i, align 2
  %4 = load i16, i16* %__ret.i, align 2
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqabss_s32_wrapper(i32 %a) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vqabss_s32.i = call i32 @llvm.aarch64.neon.sqabs.i32(i32 %1) #4
  store i32 %vqabss_s32.i, i32* %__ret.i, align 4
  %2 = load i32, i32* %__ret.i, align 4
  ret i32 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqabsd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vqabsd_s64.i = call i64 @llvm.aarch64.neon.sqabs.i64(i64 %1) #4
  store i64 %vqabsd_s64.i, i64* %__ret.i, align 8
  %2 = load i64, i64* %__ret.i, align 8
  ret i64 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vneg_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %sub.i = sub <8 x i8> zeroinitializer, %1
  store <8 x i8> %sub.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vnegq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %sub.i = sub <16 x i8> zeroinitializer, %1
  store <16 x i8> %sub.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vneg_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %sub.i = sub <4 x i16> zeroinitializer, %1
  store <4 x i16> %sub.i, <4 x i16>* %__ret.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vnegq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %sub.i = sub <8 x i16> zeroinitializer, %1
  store <8 x i16> %sub.i, <8 x i16>* %__ret.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vneg_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %sub.i = sub <2 x i32> zeroinitializer, %1
  store <2 x i32> %sub.i, <2 x i32>* %__ret.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vnegq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %sub.i = sub <4 x i32> zeroinitializer, %1
  store <4 x i32> %sub.i, <4 x i32>* %__ret.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vnegq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %sub.i = sub <2 x i64> zeroinitializer, %1
  store <2 x i64> %sub.i, <2 x i64>* %__ret.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vneg_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %sub.i = sub <1 x i64> zeroinitializer, %1
  store <1 x i64> %sub.i, <1 x i64>* %__ret.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vnegd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vnegd.i = sub i64 0, %1
  store i64 %vnegd.i, i64* %__ret.i, align 8
  %2 = load i64, i64* %__ret.i, align 8
  ret i64 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vqneg_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %vqneg_v.i = call <8 x i8> @llvm.aarch64.neon.sqneg.v8i8(<8 x i8> %1) #4
  store <8 x i8> %vqneg_v.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vqnegq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %vqnegq_v.i = call <16 x i8> @llvm.aarch64.neon.sqneg.v16i8(<16 x i8> %1) #4
  store <16 x i8> %vqnegq_v.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vqneg_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %2 = bitcast <4 x i16> %1 to <8 x i8>
  %vqneg_v1.i = call <4 x i16> @llvm.aarch64.neon.sqneg.v4i16(<4 x i16> %1) #4
  %vqneg_v2.i = bitcast <4 x i16> %vqneg_v1.i to <8 x i8>
  store <4 x i16> %vqneg_v1.i, <4 x i16>* %__ret.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vqnegq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %2 = bitcast <8 x i16> %1 to <16 x i8>
  %vqnegq_v1.i = call <8 x i16> @llvm.aarch64.neon.sqneg.v8i16(<8 x i16> %1) #4
  %vqnegq_v2.i = bitcast <8 x i16> %vqnegq_v1.i to <16 x i8>
  store <8 x i16> %vqnegq_v1.i, <8 x i16>* %__ret.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vqneg_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %2 = bitcast <2 x i32> %1 to <8 x i8>
  %vqneg_v1.i = call <2 x i32> @llvm.aarch64.neon.sqneg.v2i32(<2 x i32> %1) #4
  %vqneg_v2.i = bitcast <2 x i32> %vqneg_v1.i to <8 x i8>
  store <2 x i32> %vqneg_v1.i, <2 x i32>* %__ret.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vqnegq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %2 = bitcast <4 x i32> %1 to <16 x i8>
  %vqnegq_v1.i = call <4 x i32> @llvm.aarch64.neon.sqneg.v4i32(<4 x i32> %1) #4
  %vqnegq_v2.i = bitcast <4 x i32> %vqnegq_v1.i to <16 x i8>
  store <4 x i32> %vqnegq_v1.i, <4 x i32>* %__ret.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vqnegq_s64_wrapper(<2 x i64> %a) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  %1 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %vqnegq_v1.i = call <2 x i64> @llvm.aarch64.neon.sqneg.v2i64(<2 x i64> %1) #4
  %vqnegq_v2.i = bitcast <2 x i64> %vqnegq_v1.i to <16 x i8>
  store <2 x i64> %vqnegq_v1.i, <2 x i64>* %__ret.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vqneg_s64_wrapper(<1 x i64> %a) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  %1 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %2 = bitcast <1 x i64> %1 to <8 x i8>
  %vqneg_v1.i = call <1 x i64> @llvm.aarch64.neon.sqneg.v1i64(<1 x i64> %1) #4
  %vqneg_v2.i = bitcast <1 x i64> %vqneg_v1.i to <8 x i8>
  store <1 x i64> %vqneg_v1.i, <1 x i64>* %__ret.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %3
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i8 @vqnegb_s8_wrapper(i8 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  store i8 %a, i8* %a.addr, align 1
  %0 = load i8, i8* %a.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %2 = insertelement <8 x i8> undef, i8 %1, i64 0
  %vqnegb_s8.i = call <8 x i8> @llvm.aarch64.neon.sqneg.v8i8(<8 x i8> %2) #4
  %3 = extractelement <8 x i8> %vqnegb_s8.i, i64 0
  store i8 %3, i8* %__ret.i, align 1
  %4 = load i8, i8* %__ret.i, align 1
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local signext i16 @vqnegh_s16_wrapper(i16 signext %a) #2 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  store i16 %a, i16* %a.addr, align 2
  %0 = load i16, i16* %a.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %2 = insertelement <4 x i16> undef, i16 %1, i64 0
  %vqnegh_s16.i = call <4 x i16> @llvm.aarch64.neon.sqneg.v4i16(<4 x i16> %2) #4
  %3 = extractelement <4 x i16> %vqnegh_s16.i, i64 0
  store i16 %3, i16* %__ret.i, align 2
  %4 = load i16, i16* %__ret.i, align 2
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i32 @vqnegs_s32_wrapper(i32 %a) #2 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  store i32 %a, i32* %a.addr, align 4
  %0 = load i32, i32* %a.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vqnegs_s32.i = call i32 @llvm.aarch64.neon.sqneg.i32(i32 %1) #4
  store i32 %vqnegs_s32.i, i32* %__ret.i, align 4
  %2 = load i32, i32* %__ret.i, align 4
  ret i32 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local i64 @vqnegd_s64_wrapper(i64 %a) #2 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca i64, align 8
  %a.addr = alloca i64, align 8
  store i64 %a, i64* %a.addr, align 8
  %0 = load i64, i64* %a.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vqnegd_s64.i = call i64 @llvm.aarch64.neon.sqneg.i64(i64 %1) #4
  store i64 %vqnegd_s64.i, i64* %__ret.i, align 8
  %2 = load i64, i64* %__ret.i, align 8
  ret i64 %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmvn_s8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %neg.i = xor <8 x i8> %1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  store <8 x i8> %neg.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmvnq_s8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %neg.i = xor <16 x i8> %1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  store <16 x i8> %neg.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmvn_s16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %neg.i = xor <4 x i16> %1, <i16 -1, i16 -1, i16 -1, i16 -1>
  store <4 x i16> %neg.i, <4 x i16>* %__ret.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmvnq_s16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %neg.i = xor <8 x i16> %1, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  store <8 x i16> %neg.i, <8 x i16>* %__ret.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmvn_s32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %neg.i = xor <2 x i32> %1, <i32 -1, i32 -1>
  store <2 x i32> %neg.i, <2 x i32>* %__ret.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmvnq_s32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %neg.i = xor <4 x i32> %1, <i32 -1, i32 -1, i32 -1, i32 -1>
  store <4 x i32> %neg.i, <4 x i32>* %__ret.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmvn_u8_wrapper(<8 x i8> %a) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  %1 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %neg.i = xor <8 x i8> %1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  store <8 x i8> %neg.i, <8 x i8>* %__ret.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmvnq_u8_wrapper(<16 x i8> %a) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  %1 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %neg.i = xor <16 x i8> %1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  store <16 x i8> %neg.i, <16 x i8>* %__ret.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmvn_u16_wrapper(<4 x i16> %a) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  %1 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %neg.i = xor <4 x i16> %1, <i16 -1, i16 -1, i16 -1, i16 -1>
  store <4 x i16> %neg.i, <4 x i16>* %__ret.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmvnq_u16_wrapper(<8 x i16> %a) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  %1 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %neg.i = xor <8 x i16> %1, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  store <8 x i16> %neg.i, <8 x i16>* %__ret.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmvn_u32_wrapper(<2 x i32> %a) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  %1 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %neg.i = xor <2 x i32> %1, <i32 -1, i32 -1>
  store <2 x i32> %neg.i, <2 x i32>* %__ret.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmvnq_u32_wrapper(<4 x i32> %a) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  %1 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %neg.i = xor <4 x i32> %1, <i32 -1, i32 -1, i32 -1, i32 -1>
  store <4 x i32> %neg.i, <4 x i32>* %__ret.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %2
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vand_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %and.i = and <8 x i8> %2, %3
  store <8 x i8> %and.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vandq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %and.i = and <16 x i8> %2, %3
  store <16 x i8> %and.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vand_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %and.i = and <4 x i16> %2, %3
  store <4 x i16> %and.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vandq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %and.i = and <8 x i16> %2, %3
  store <8 x i16> %and.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vand_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %and.i = and <2 x i32> %2, %3
  store <2 x i32> %and.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vandq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %and.i = and <4 x i32> %2, %3
  store <4 x i32> %and.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vand_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %and.i = and <1 x i64> %2, %3
  store <1 x i64> %and.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vandq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %and.i = and <2 x i64> %2, %3
  store <2 x i64> %and.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vand_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %and.i = and <8 x i8> %2, %3
  store <8 x i8> %and.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vandq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %and.i = and <16 x i8> %2, %3
  store <16 x i8> %and.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vand_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %and.i = and <4 x i16> %2, %3
  store <4 x i16> %and.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vandq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %and.i = and <8 x i16> %2, %3
  store <8 x i16> %and.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vand_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %and.i = and <2 x i32> %2, %3
  store <2 x i32> %and.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vandq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %and.i = and <4 x i32> %2, %3
  store <4 x i32> %and.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vand_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %and.i = and <1 x i64> %2, %3
  store <1 x i64> %and.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vandq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %and.i = and <2 x i64> %2, %3
  store <2 x i64> %and.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vorr_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %or.i = or <8 x i8> %2, %3
  store <8 x i8> %or.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vorrq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %or.i = or <16 x i8> %2, %3
  store <16 x i8> %or.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vorr_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %or.i = or <4 x i16> %2, %3
  store <4 x i16> %or.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vorrq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %or.i = or <8 x i16> %2, %3
  store <8 x i16> %or.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vorr_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %or.i = or <2 x i32> %2, %3
  store <2 x i32> %or.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vorrq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %or.i = or <4 x i32> %2, %3
  store <4 x i32> %or.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vorr_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %or.i = or <1 x i64> %2, %3
  store <1 x i64> %or.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vorrq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %or.i = or <2 x i64> %2, %3
  store <2 x i64> %or.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vorr_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %or.i = or <8 x i8> %2, %3
  store <8 x i8> %or.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vorrq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %or.i = or <16 x i8> %2, %3
  store <16 x i8> %or.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vorr_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %or.i = or <4 x i16> %2, %3
  store <4 x i16> %or.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vorrq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %or.i = or <8 x i16> %2, %3
  store <8 x i16> %or.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vorr_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %or.i = or <2 x i32> %2, %3
  store <2 x i32> %or.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vorrq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %or.i = or <4 x i32> %2, %3
  store <4 x i32> %or.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vorr_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %or.i = or <1 x i64> %2, %3
  store <1 x i64> %or.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vorrq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %or.i = or <2 x i64> %2, %3
  store <2 x i64> %or.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @veor_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %xor.i = xor <8 x i8> %2, %3
  store <8 x i8> %xor.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @veorq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %xor.i = xor <16 x i8> %2, %3
  store <16 x i8> %xor.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @veor_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %xor.i = xor <4 x i16> %2, %3
  store <4 x i16> %xor.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @veorq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %xor.i = xor <8 x i16> %2, %3
  store <8 x i16> %xor.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @veor_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %xor.i = xor <2 x i32> %2, %3
  store <2 x i32> %xor.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @veorq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %xor.i = xor <4 x i32> %2, %3
  store <4 x i32> %xor.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @veor_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %xor.i = xor <1 x i64> %2, %3
  store <1 x i64> %xor.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @veorq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %xor.i = xor <2 x i64> %2, %3
  store <2 x i64> %xor.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @veor_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %xor.i = xor <8 x i8> %2, %3
  store <8 x i8> %xor.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @veorq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %xor.i = xor <16 x i8> %2, %3
  store <16 x i8> %xor.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @veor_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %xor.i = xor <4 x i16> %2, %3
  store <4 x i16> %xor.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @veorq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %xor.i = xor <8 x i16> %2, %3
  store <8 x i16> %xor.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @veor_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %xor.i = xor <2 x i32> %2, %3
  store <2 x i32> %xor.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @veorq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %xor.i = xor <4 x i32> %2, %3
  store <4 x i32> %xor.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @veor_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %xor.i = xor <1 x i64> %2, %3
  store <1 x i64> %xor.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @veorq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %xor.i = xor <2 x i64> %2, %3
  store <2 x i64> %xor.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vbic_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %neg.i = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %and.i = and <8 x i8> %2, %neg.i
  store <8 x i8> %and.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vbicq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %neg.i = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %and.i = and <16 x i8> %2, %neg.i
  store <16 x i8> %and.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vbic_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %neg.i = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %and.i = and <4 x i16> %2, %neg.i
  store <4 x i16> %and.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vbicq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %neg.i = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %and.i = and <8 x i16> %2, %neg.i
  store <8 x i16> %and.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vbic_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %neg.i = xor <2 x i32> %3, <i32 -1, i32 -1>
  %and.i = and <2 x i32> %2, %neg.i
  store <2 x i32> %and.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vbicq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %neg.i = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %and.i = and <4 x i32> %2, %neg.i
  store <4 x i32> %and.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vbic_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %neg.i = xor <1 x i64> %3, <i64 -1>
  %and.i = and <1 x i64> %2, %neg.i
  store <1 x i64> %and.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vbicq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %neg.i = xor <2 x i64> %3, <i64 -1, i64 -1>
  %and.i = and <2 x i64> %2, %neg.i
  store <2 x i64> %and.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vbic_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %neg.i = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %and.i = and <8 x i8> %2, %neg.i
  store <8 x i8> %and.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vbicq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %neg.i = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %and.i = and <16 x i8> %2, %neg.i
  store <16 x i8> %and.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vbic_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %neg.i = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %and.i = and <4 x i16> %2, %neg.i
  store <4 x i16> %and.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vbicq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %neg.i = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %and.i = and <8 x i16> %2, %neg.i
  store <8 x i16> %and.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vbic_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %neg.i = xor <2 x i32> %3, <i32 -1, i32 -1>
  %and.i = and <2 x i32> %2, %neg.i
  store <2 x i32> %and.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vbicq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %neg.i = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %and.i = and <4 x i32> %2, %neg.i
  store <4 x i32> %and.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vbic_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %neg.i = xor <1 x i64> %3, <i64 -1>
  %and.i = and <1 x i64> %2, %neg.i
  store <1 x i64> %and.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vbicq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %neg.i = xor <2 x i64> %3, <i64 -1, i64 -1>
  %and.i = and <2 x i64> %2, %neg.i
  store <2 x i64> %and.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vorn_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %neg.i = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %or.i = or <8 x i8> %2, %neg.i
  store <8 x i8> %or.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vornq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %neg.i = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %or.i = or <16 x i8> %2, %neg.i
  store <16 x i8> %or.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vorn_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %neg.i = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %or.i = or <4 x i16> %2, %neg.i
  store <4 x i16> %or.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vornq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %neg.i = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %or.i = or <8 x i16> %2, %neg.i
  store <8 x i16> %or.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vorn_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %neg.i = xor <2 x i32> %3, <i32 -1, i32 -1>
  %or.i = or <2 x i32> %2, %neg.i
  store <2 x i32> %or.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vornq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %neg.i = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %or.i = or <4 x i32> %2, %neg.i
  store <4 x i32> %or.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vorn_s64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %neg.i = xor <1 x i64> %3, <i64 -1>
  %or.i = or <1 x i64> %2, %neg.i
  store <1 x i64> %or.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vornq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %neg.i = xor <2 x i64> %3, <i64 -1, i64 -1>
  %or.i = or <2 x i64> %2, %neg.i
  store <2 x i64> %or.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vorn_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %neg.i = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %or.i = or <8 x i8> %2, %neg.i
  store <8 x i8> %or.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vornq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %neg.i = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %or.i = or <16 x i8> %2, %neg.i
  store <16 x i8> %or.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vorn_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %neg.i = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %or.i = or <4 x i16> %2, %neg.i
  store <4 x i16> %or.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vornq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %neg.i = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %or.i = or <8 x i16> %2, %neg.i
  store <8 x i16> %or.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vorn_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %neg.i = xor <2 x i32> %3, <i32 -1, i32 -1>
  %or.i = or <2 x i32> %2, %neg.i
  store <2 x i32> %or.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vornq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %neg.i = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %or.i = or <4 x i32> %2, %neg.i
  store <4 x i32> %or.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vorn_u64_wrapper(<1 x i64> %a, <1 x i64> %b) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  %2 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %neg.i = xor <1 x i64> %3, <i64 -1>
  %or.i = or <1 x i64> %2, %neg.i
  store <1 x i64> %or.i, <1 x i64>* %__ret.i, align 8
  %4 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vornq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %neg.i = xor <2 x i64> %3, <i64 -1, i64 -1>
  %or.i = or <2 x i64> %2, %neg.i
  store <2 x i64> %or.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vbsl_s8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %vbsl.i = and <8 x i8> %3, %4
  %6 = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %vbsl1.i = and <8 x i8> %6, %5
  %vbsl2.i = or <8 x i8> %vbsl.i, %vbsl1.i
  store <8 x i8> %vbsl2.i, <8 x i8>* %__ret.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vbslq_s8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %vbsl.i = and <16 x i8> %3, %4
  %6 = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %vbsl1.i = and <16 x i8> %6, %5
  %vbsl2.i = or <16 x i8> %vbsl.i, %vbsl1.i
  store <16 x i8> %vbsl2.i, <16 x i8>* %__ret.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vbsl_s16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %5 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %6 = bitcast <4 x i16> %5 to <8 x i8>
  %7 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vbsl3.i = and <4 x i16> %3, %5
  %9 = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %vbsl4.i = and <4 x i16> %9, %7
  %vbsl5.i = or <4 x i16> %vbsl3.i, %vbsl4.i
  store <4 x i16> %vbsl5.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vbslq_s16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = bitcast <8 x i16> %3 to <16 x i8>
  %5 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %6 = bitcast <8 x i16> %5 to <16 x i8>
  %7 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %8 = bitcast <8 x i16> %7 to <16 x i8>
  %vbsl3.i = and <8 x i16> %3, %5
  %9 = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %vbsl4.i = and <8 x i16> %9, %7
  %vbsl5.i = or <8 x i16> %vbsl3.i, %vbsl4.i
  store <8 x i16> %vbsl5.i, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vbsl_s32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %5 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %6 = bitcast <2 x i32> %5 to <8 x i8>
  %7 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vbsl3.i = and <2 x i32> %3, %5
  %9 = xor <2 x i32> %3, <i32 -1, i32 -1>
  %vbsl4.i = and <2 x i32> %9, %7
  %vbsl5.i = or <2 x i32> %vbsl3.i, %vbsl4.i
  store <2 x i32> %vbsl5.i, <2 x i32>* %__ret.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vbslq_s32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = bitcast <4 x i32> %3 to <16 x i8>
  %5 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %6 = bitcast <4 x i32> %5 to <16 x i8>
  %7 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %8 = bitcast <4 x i32> %7 to <16 x i8>
  %vbsl3.i = and <4 x i32> %3, %5
  %9 = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %vbsl4.i = and <4 x i32> %9, %7
  %vbsl5.i = or <4 x i32> %vbsl3.i, %vbsl4.i
  store <4 x i32> %vbsl5.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vbsl_s64_wrapper(<1 x i64> %a, <1 x i64> %b, <1 x i64> %c) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__p2.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  %c.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  store <1 x i64> %c, <1 x i64>* %c.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  %2 = load <1 x i64>, <1 x i64>* %c.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  store <1 x i64> %2, <1 x i64>* %__p2.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %4 = bitcast <1 x i64> %3 to <8 x i8>
  %5 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %6 = bitcast <1 x i64> %5 to <8 x i8>
  %7 = load <1 x i64>, <1 x i64>* %__p2.addr.i, align 8
  %8 = bitcast <1 x i64> %7 to <8 x i8>
  %vbsl3.i = and <1 x i64> %3, %5
  %9 = xor <1 x i64> %3, <i64 -1>
  %vbsl4.i = and <1 x i64> %9, %7
  %vbsl5.i = or <1 x i64> %vbsl3.i, %vbsl4.i
  store <1 x i64> %vbsl5.i, <1 x i64>* %__ret.i, align 8
  %10 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vbslq_s64_wrapper(<2 x i64> %a, <2 x i64> %b, <2 x i64> %c) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  %c.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  store <2 x i64> %c, <2 x i64>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %6 = bitcast <2 x i64> %5 to <16 x i8>
  %7 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %vbsl3.i = and <2 x i64> %3, %5
  %9 = xor <2 x i64> %3, <i64 -1, i64 -1>
  %vbsl4.i = and <2 x i64> %9, %7
  %vbsl5.i = or <2 x i64> %vbsl3.i, %vbsl4.i
  store <2 x i64> %vbsl5.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vbsl_u8_wrapper(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__p2.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  %c.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  store <8 x i8> %c, <8 x i8>* %c.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  %2 = load <8 x i8>, <8 x i8>* %c.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  store <8 x i8> %2, <8 x i8>* %__p2.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %5 = load <8 x i8>, <8 x i8>* %__p2.addr.i, align 8
  %vbsl.i = and <8 x i8> %3, %4
  %6 = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %vbsl1.i = and <8 x i8> %6, %5
  %vbsl2.i = or <8 x i8> %vbsl.i, %vbsl1.i
  store <8 x i8> %vbsl2.i, <8 x i8>* %__ret.i, align 8
  %7 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vbslq_u8_wrapper(<16 x i8> %a, <16 x i8> %b, <16 x i8> %c) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__p2.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  %c.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  store <16 x i8> %c, <16 x i8>* %c.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  %2 = load <16 x i8>, <16 x i8>* %c.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  store <16 x i8> %2, <16 x i8>* %__p2.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %5 = load <16 x i8>, <16 x i8>* %__p2.addr.i, align 16
  %vbsl.i = and <16 x i8> %3, %4
  %6 = xor <16 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %vbsl1.i = and <16 x i8> %6, %5
  %vbsl2.i = or <16 x i8> %vbsl.i, %vbsl1.i
  store <16 x i8> %vbsl2.i, <16 x i8>* %__ret.i, align 16
  %7 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %7
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vbsl_u16_wrapper(<4 x i16> %a, <4 x i16> %b, <4 x i16> %c) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__p2.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  %c.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  store <4 x i16> %c, <4 x i16>* %c.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  %2 = load <4 x i16>, <4 x i16>* %c.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  store <4 x i16> %2, <4 x i16>* %__p2.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %4 = bitcast <4 x i16> %3 to <8 x i8>
  %5 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %6 = bitcast <4 x i16> %5 to <8 x i8>
  %7 = load <4 x i16>, <4 x i16>* %__p2.addr.i, align 8
  %8 = bitcast <4 x i16> %7 to <8 x i8>
  %vbsl3.i = and <4 x i16> %3, %5
  %9 = xor <4 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1>
  %vbsl4.i = and <4 x i16> %9, %7
  %vbsl5.i = or <4 x i16> %vbsl3.i, %vbsl4.i
  store <4 x i16> %vbsl5.i, <4 x i16>* %__ret.i, align 8
  %10 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vbslq_u16_wrapper(<8 x i16> %a, <8 x i16> %b, <8 x i16> %c) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__p2.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  %c.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  store <8 x i16> %c, <8 x i16>* %c.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  %2 = load <8 x i16>, <8 x i16>* %c.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  store <8 x i16> %2, <8 x i16>* %__p2.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %4 = bitcast <8 x i16> %3 to <16 x i8>
  %5 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %6 = bitcast <8 x i16> %5 to <16 x i8>
  %7 = load <8 x i16>, <8 x i16>* %__p2.addr.i, align 16
  %8 = bitcast <8 x i16> %7 to <16 x i8>
  %vbsl3.i = and <8 x i16> %3, %5
  %9 = xor <8 x i16> %3, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %vbsl4.i = and <8 x i16> %9, %7
  %vbsl5.i = or <8 x i16> %vbsl3.i, %vbsl4.i
  store <8 x i16> %vbsl5.i, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vbsl_u32_wrapper(<2 x i32> %a, <2 x i32> %b, <2 x i32> %c) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__p2.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  %c.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  store <2 x i32> %c, <2 x i32>* %c.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  %2 = load <2 x i32>, <2 x i32>* %c.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  store <2 x i32> %2, <2 x i32>* %__p2.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %4 = bitcast <2 x i32> %3 to <8 x i8>
  %5 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %6 = bitcast <2 x i32> %5 to <8 x i8>
  %7 = load <2 x i32>, <2 x i32>* %__p2.addr.i, align 8
  %8 = bitcast <2 x i32> %7 to <8 x i8>
  %vbsl3.i = and <2 x i32> %3, %5
  %9 = xor <2 x i32> %3, <i32 -1, i32 -1>
  %vbsl4.i = and <2 x i32> %9, %7
  %vbsl5.i = or <2 x i32> %vbsl3.i, %vbsl4.i
  store <2 x i32> %vbsl5.i, <2 x i32>* %__ret.i, align 8
  %10 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vbslq_u32_wrapper(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__p2.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  %c.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  store <4 x i32> %c, <4 x i32>* %c.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  %2 = load <4 x i32>, <4 x i32>* %c.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  store <4 x i32> %2, <4 x i32>* %__p2.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %4 = bitcast <4 x i32> %3 to <16 x i8>
  %5 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %6 = bitcast <4 x i32> %5 to <16 x i8>
  %7 = load <4 x i32>, <4 x i32>* %__p2.addr.i, align 16
  %8 = bitcast <4 x i32> %7 to <16 x i8>
  %vbsl3.i = and <4 x i32> %3, %5
  %9 = xor <4 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1>
  %vbsl4.i = and <4 x i32> %9, %7
  %vbsl5.i = or <4 x i32> %vbsl3.i, %vbsl4.i
  store <4 x i32> %vbsl5.i, <4 x i32>* %__ret.i, align 16
  %10 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <1 x i64> @vbsl_u64_wrapper(<1 x i64> %a, <1 x i64> %b, <1 x i64> %c) #0 {
entry:
  %__p0.addr.i = alloca <1 x i64>, align 8
  %__p1.addr.i = alloca <1 x i64>, align 8
  %__p2.addr.i = alloca <1 x i64>, align 8
  %__ret.i = alloca <1 x i64>, align 8
  %a.addr = alloca <1 x i64>, align 8
  %b.addr = alloca <1 x i64>, align 8
  %c.addr = alloca <1 x i64>, align 8
  store <1 x i64> %a, <1 x i64>* %a.addr, align 8
  store <1 x i64> %b, <1 x i64>* %b.addr, align 8
  store <1 x i64> %c, <1 x i64>* %c.addr, align 8
  %0 = load <1 x i64>, <1 x i64>* %a.addr, align 8
  %1 = load <1 x i64>, <1 x i64>* %b.addr, align 8
  %2 = load <1 x i64>, <1 x i64>* %c.addr, align 8
  store <1 x i64> %0, <1 x i64>* %__p0.addr.i, align 8
  store <1 x i64> %1, <1 x i64>* %__p1.addr.i, align 8
  store <1 x i64> %2, <1 x i64>* %__p2.addr.i, align 8
  %3 = load <1 x i64>, <1 x i64>* %__p0.addr.i, align 8
  %4 = bitcast <1 x i64> %3 to <8 x i8>
  %5 = load <1 x i64>, <1 x i64>* %__p1.addr.i, align 8
  %6 = bitcast <1 x i64> %5 to <8 x i8>
  %7 = load <1 x i64>, <1 x i64>* %__p2.addr.i, align 8
  %8 = bitcast <1 x i64> %7 to <8 x i8>
  %vbsl3.i = and <1 x i64> %3, %5
  %9 = xor <1 x i64> %3, <i64 -1>
  %vbsl4.i = and <1 x i64> %9, %7
  %vbsl5.i = or <1 x i64> %vbsl3.i, %vbsl4.i
  store <1 x i64> %vbsl5.i, <1 x i64>* %__ret.i, align 8
  %10 = load <1 x i64>, <1 x i64>* %__ret.i, align 8
  ret <1 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vbslq_u64_wrapper(<2 x i64> %a, <2 x i64> %b, <2 x i64> %c) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__p2.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  %c.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  store <2 x i64> %c, <2 x i64>* %c.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  %2 = load <2 x i64>, <2 x i64>* %c.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  store <2 x i64> %2, <2 x i64>* %__p2.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %6 = bitcast <2 x i64> %5 to <16 x i8>
  %7 = load <2 x i64>, <2 x i64>* %__p2.addr.i, align 16
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %vbsl3.i = and <2 x i64> %3, %5
  %9 = xor <2 x i64> %3, <i64 -1, i64 -1>
  %vbsl4.i = and <2 x i64> %9, %7
  %vbsl5.i = or <2 x i64> %vbsl3.i, %vbsl4.i
  store <2 x i64> %vbsl5.i, <2 x i64>* %__ret.i, align 16
  %10 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vdup_n_s8_wrapper(i8 signext %value) #0 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <8 x i8>, align 8
  %.compoundliteral.i = alloca <8 x i8>, align 8
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <8 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <8 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <8 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <8 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <8 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <8 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <8 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <8 x i8> %vecinit6.i, i8 %8, i32 7
  store <8 x i8> %vecinit7.i, <8 x i8>* %.compoundliteral.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %.compoundliteral.i, align 8
  store <8 x i8> %9, <8 x i8>* %__ret.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vdupq_n_s8_wrapper(i8 signext %value) #1 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <16 x i8>, align 16
  %.compoundliteral.i = alloca <16 x i8>, align 16
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <16 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <16 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <16 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <16 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <16 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <16 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <16 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <16 x i8> %vecinit6.i, i8 %8, i32 7
  %9 = load i8, i8* %__p0.addr.i, align 1
  %vecinit8.i = insertelement <16 x i8> %vecinit7.i, i8 %9, i32 8
  %10 = load i8, i8* %__p0.addr.i, align 1
  %vecinit9.i = insertelement <16 x i8> %vecinit8.i, i8 %10, i32 9
  %11 = load i8, i8* %__p0.addr.i, align 1
  %vecinit10.i = insertelement <16 x i8> %vecinit9.i, i8 %11, i32 10
  %12 = load i8, i8* %__p0.addr.i, align 1
  %vecinit11.i = insertelement <16 x i8> %vecinit10.i, i8 %12, i32 11
  %13 = load i8, i8* %__p0.addr.i, align 1
  %vecinit12.i = insertelement <16 x i8> %vecinit11.i, i8 %13, i32 12
  %14 = load i8, i8* %__p0.addr.i, align 1
  %vecinit13.i = insertelement <16 x i8> %vecinit12.i, i8 %14, i32 13
  %15 = load i8, i8* %__p0.addr.i, align 1
  %vecinit14.i = insertelement <16 x i8> %vecinit13.i, i8 %15, i32 14
  %16 = load i8, i8* %__p0.addr.i, align 1
  %vecinit15.i = insertelement <16 x i8> %vecinit14.i, i8 %16, i32 15
  store <16 x i8> %vecinit15.i, <16 x i8>* %.compoundliteral.i, align 16
  %17 = load <16 x i8>, <16 x i8>* %.compoundliteral.i, align 16
  store <16 x i8> %17, <16 x i8>* %__ret.i, align 16
  %18 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vdup_n_s16_wrapper(i16 signext %value) #0 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %4, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %5, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vdupq_n_s16_wrapper(i16 signext %value) #1 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %4, i32 3
  %5 = load i16, i16* %__p0.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %5, i32 4
  %6 = load i16, i16* %__p0.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %6, i32 5
  %7 = load i16, i16* %__p0.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %7, i32 6
  %8 = load i16, i16* %__p0.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %8, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vdup_n_s32_wrapper(i32 %value) #0 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %2, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %3, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vdupq_n_s32_wrapper(i32 %value) #1 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %2, i32 1
  %3 = load i32, i32* %__p0.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %3, i32 2
  %4 = load i32, i32* %__p0.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %4, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %5, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vdupq_n_s64_wrapper(i64 %value) #1 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i64>, align 16
  %value.addr = alloca i64, align 8
  store i64 %value, i64* %value.addr, align 8
  %0 = load i64, i64* %value.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vecinit.i = insertelement <2 x i64> undef, i64 %1, i32 0
  %2 = load i64, i64* %__p0.addr.i, align 8
  %vecinit1.i = insertelement <2 x i64> %vecinit.i, i64 %2, i32 1
  store <2 x i64> %vecinit1.i, <2 x i64>* %.compoundliteral.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %.compoundliteral.i, align 16
  store <2 x i64> %3, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vdup_n_u8_wrapper(i8 zeroext %value) #0 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <8 x i8>, align 8
  %.compoundliteral.i = alloca <8 x i8>, align 8
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <8 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <8 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <8 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <8 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <8 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <8 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <8 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <8 x i8> %vecinit6.i, i8 %8, i32 7
  store <8 x i8> %vecinit7.i, <8 x i8>* %.compoundliteral.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %.compoundliteral.i, align 8
  store <8 x i8> %9, <8 x i8>* %__ret.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vdupq_n_u8_wrapper(i8 zeroext %value) #1 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <16 x i8>, align 16
  %.compoundliteral.i = alloca <16 x i8>, align 16
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <16 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <16 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <16 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <16 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <16 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <16 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <16 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <16 x i8> %vecinit6.i, i8 %8, i32 7
  %9 = load i8, i8* %__p0.addr.i, align 1
  %vecinit8.i = insertelement <16 x i8> %vecinit7.i, i8 %9, i32 8
  %10 = load i8, i8* %__p0.addr.i, align 1
  %vecinit9.i = insertelement <16 x i8> %vecinit8.i, i8 %10, i32 9
  %11 = load i8, i8* %__p0.addr.i, align 1
  %vecinit10.i = insertelement <16 x i8> %vecinit9.i, i8 %11, i32 10
  %12 = load i8, i8* %__p0.addr.i, align 1
  %vecinit11.i = insertelement <16 x i8> %vecinit10.i, i8 %12, i32 11
  %13 = load i8, i8* %__p0.addr.i, align 1
  %vecinit12.i = insertelement <16 x i8> %vecinit11.i, i8 %13, i32 12
  %14 = load i8, i8* %__p0.addr.i, align 1
  %vecinit13.i = insertelement <16 x i8> %vecinit12.i, i8 %14, i32 13
  %15 = load i8, i8* %__p0.addr.i, align 1
  %vecinit14.i = insertelement <16 x i8> %vecinit13.i, i8 %15, i32 14
  %16 = load i8, i8* %__p0.addr.i, align 1
  %vecinit15.i = insertelement <16 x i8> %vecinit14.i, i8 %16, i32 15
  store <16 x i8> %vecinit15.i, <16 x i8>* %.compoundliteral.i, align 16
  %17 = load <16 x i8>, <16 x i8>* %.compoundliteral.i, align 16
  store <16 x i8> %17, <16 x i8>* %__ret.i, align 16
  %18 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vdup_n_u16_wrapper(i16 zeroext %value) #0 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %4, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %5, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vdupq_n_u16_wrapper(i16 zeroext %value) #1 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %4, i32 3
  %5 = load i16, i16* %__p0.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %5, i32 4
  %6 = load i16, i16* %__p0.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %6, i32 5
  %7 = load i16, i16* %__p0.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %7, i32 6
  %8 = load i16, i16* %__p0.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %8, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vdup_n_u32_wrapper(i32 %value) #0 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %2, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %3, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vdupq_n_u32_wrapper(i32 %value) #1 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %2, i32 1
  %3 = load i32, i32* %__p0.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %3, i32 2
  %4 = load i32, i32* %__p0.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %4, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %5, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vdupq_n_u64_wrapper(i64 %value) #1 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i64>, align 16
  %value.addr = alloca i64, align 8
  store i64 %value, i64* %value.addr, align 8
  %0 = load i64, i64* %value.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vecinit.i = insertelement <2 x i64> undef, i64 %1, i32 0
  %2 = load i64, i64* %__p0.addr.i, align 8
  %vecinit1.i = insertelement <2 x i64> %vecinit.i, i64 %2, i32 1
  store <2 x i64> %vecinit1.i, <2 x i64>* %.compoundliteral.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %.compoundliteral.i, align 16
  store <2 x i64> %3, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmov_n_s8_wrapper(i8 signext %value) #0 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <8 x i8>, align 8
  %.compoundliteral.i = alloca <8 x i8>, align 8
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <8 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <8 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <8 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <8 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <8 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <8 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <8 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <8 x i8> %vecinit6.i, i8 %8, i32 7
  store <8 x i8> %vecinit7.i, <8 x i8>* %.compoundliteral.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %.compoundliteral.i, align 8
  store <8 x i8> %9, <8 x i8>* %__ret.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmovq_n_s8_wrapper(i8 signext %value) #1 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <16 x i8>, align 16
  %.compoundliteral.i = alloca <16 x i8>, align 16
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <16 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <16 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <16 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <16 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <16 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <16 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <16 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <16 x i8> %vecinit6.i, i8 %8, i32 7
  %9 = load i8, i8* %__p0.addr.i, align 1
  %vecinit8.i = insertelement <16 x i8> %vecinit7.i, i8 %9, i32 8
  %10 = load i8, i8* %__p0.addr.i, align 1
  %vecinit9.i = insertelement <16 x i8> %vecinit8.i, i8 %10, i32 9
  %11 = load i8, i8* %__p0.addr.i, align 1
  %vecinit10.i = insertelement <16 x i8> %vecinit9.i, i8 %11, i32 10
  %12 = load i8, i8* %__p0.addr.i, align 1
  %vecinit11.i = insertelement <16 x i8> %vecinit10.i, i8 %12, i32 11
  %13 = load i8, i8* %__p0.addr.i, align 1
  %vecinit12.i = insertelement <16 x i8> %vecinit11.i, i8 %13, i32 12
  %14 = load i8, i8* %__p0.addr.i, align 1
  %vecinit13.i = insertelement <16 x i8> %vecinit12.i, i8 %14, i32 13
  %15 = load i8, i8* %__p0.addr.i, align 1
  %vecinit14.i = insertelement <16 x i8> %vecinit13.i, i8 %15, i32 14
  %16 = load i8, i8* %__p0.addr.i, align 1
  %vecinit15.i = insertelement <16 x i8> %vecinit14.i, i8 %16, i32 15
  store <16 x i8> %vecinit15.i, <16 x i8>* %.compoundliteral.i, align 16
  %17 = load <16 x i8>, <16 x i8>* %.compoundliteral.i, align 16
  store <16 x i8> %17, <16 x i8>* %__ret.i, align 16
  %18 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmov_n_s16_wrapper(i16 signext %value) #0 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %4, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %5, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovq_n_s16_wrapper(i16 signext %value) #1 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %4, i32 3
  %5 = load i16, i16* %__p0.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %5, i32 4
  %6 = load i16, i16* %__p0.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %6, i32 5
  %7 = load i16, i16* %__p0.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %7, i32 6
  %8 = load i16, i16* %__p0.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %8, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmov_n_s32_wrapper(i32 %value) #0 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %2, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %3, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovq_n_s32_wrapper(i32 %value) #1 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %2, i32 1
  %3 = load i32, i32* %__p0.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %3, i32 2
  %4 = load i32, i32* %__p0.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %4, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %5, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovq_n_s64_wrapper(i64 %value) #1 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i64>, align 16
  %value.addr = alloca i64, align 8
  store i64 %value, i64* %value.addr, align 8
  %0 = load i64, i64* %value.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vecinit.i = insertelement <2 x i64> undef, i64 %1, i32 0
  %2 = load i64, i64* %__p0.addr.i, align 8
  %vecinit1.i = insertelement <2 x i64> %vecinit.i, i64 %2, i32 1
  store <2 x i64> %vecinit1.i, <2 x i64>* %.compoundliteral.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %.compoundliteral.i, align 16
  store <2 x i64> %3, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vmov_n_u8_wrapper(i8 zeroext %value) #0 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <8 x i8>, align 8
  %.compoundliteral.i = alloca <8 x i8>, align 8
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <8 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <8 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <8 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <8 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <8 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <8 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <8 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <8 x i8> %vecinit6.i, i8 %8, i32 7
  store <8 x i8> %vecinit7.i, <8 x i8>* %.compoundliteral.i, align 8
  %9 = load <8 x i8>, <8 x i8>* %.compoundliteral.i, align 8
  store <8 x i8> %9, <8 x i8>* %__ret.i, align 8
  %10 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vmovq_n_u8_wrapper(i8 zeroext %value) #1 {
entry:
  %__p0.addr.i = alloca i8, align 1
  %__ret.i = alloca <16 x i8>, align 16
  %.compoundliteral.i = alloca <16 x i8>, align 16
  %value.addr = alloca i8, align 1
  store i8 %value, i8* %value.addr, align 1
  %0 = load i8, i8* %value.addr, align 1
  store i8 %0, i8* %__p0.addr.i, align 1
  %1 = load i8, i8* %__p0.addr.i, align 1
  %vecinit.i = insertelement <16 x i8> undef, i8 %1, i32 0
  %2 = load i8, i8* %__p0.addr.i, align 1
  %vecinit1.i = insertelement <16 x i8> %vecinit.i, i8 %2, i32 1
  %3 = load i8, i8* %__p0.addr.i, align 1
  %vecinit2.i = insertelement <16 x i8> %vecinit1.i, i8 %3, i32 2
  %4 = load i8, i8* %__p0.addr.i, align 1
  %vecinit3.i = insertelement <16 x i8> %vecinit2.i, i8 %4, i32 3
  %5 = load i8, i8* %__p0.addr.i, align 1
  %vecinit4.i = insertelement <16 x i8> %vecinit3.i, i8 %5, i32 4
  %6 = load i8, i8* %__p0.addr.i, align 1
  %vecinit5.i = insertelement <16 x i8> %vecinit4.i, i8 %6, i32 5
  %7 = load i8, i8* %__p0.addr.i, align 1
  %vecinit6.i = insertelement <16 x i8> %vecinit5.i, i8 %7, i32 6
  %8 = load i8, i8* %__p0.addr.i, align 1
  %vecinit7.i = insertelement <16 x i8> %vecinit6.i, i8 %8, i32 7
  %9 = load i8, i8* %__p0.addr.i, align 1
  %vecinit8.i = insertelement <16 x i8> %vecinit7.i, i8 %9, i32 8
  %10 = load i8, i8* %__p0.addr.i, align 1
  %vecinit9.i = insertelement <16 x i8> %vecinit8.i, i8 %10, i32 9
  %11 = load i8, i8* %__p0.addr.i, align 1
  %vecinit10.i = insertelement <16 x i8> %vecinit9.i, i8 %11, i32 10
  %12 = load i8, i8* %__p0.addr.i, align 1
  %vecinit11.i = insertelement <16 x i8> %vecinit10.i, i8 %12, i32 11
  %13 = load i8, i8* %__p0.addr.i, align 1
  %vecinit12.i = insertelement <16 x i8> %vecinit11.i, i8 %13, i32 12
  %14 = load i8, i8* %__p0.addr.i, align 1
  %vecinit13.i = insertelement <16 x i8> %vecinit12.i, i8 %14, i32 13
  %15 = load i8, i8* %__p0.addr.i, align 1
  %vecinit14.i = insertelement <16 x i8> %vecinit13.i, i8 %15, i32 14
  %16 = load i8, i8* %__p0.addr.i, align 1
  %vecinit15.i = insertelement <16 x i8> %vecinit14.i, i8 %16, i32 15
  store <16 x i8> %vecinit15.i, <16 x i8>* %.compoundliteral.i, align 16
  %17 = load <16 x i8>, <16 x i8>* %.compoundliteral.i, align 16
  store <16 x i8> %17, <16 x i8>* %__ret.i, align 16
  %18 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %18
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vmov_n_u16_wrapper(i16 zeroext %value) #0 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <4 x i16>, align 8
  %.compoundliteral.i = alloca <4 x i16>, align 8
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <4 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <4 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <4 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <4 x i16> %vecinit2.i, i16 %4, i32 3
  store <4 x i16> %vecinit3.i, <4 x i16>* %.compoundliteral.i, align 8
  %5 = load <4 x i16>, <4 x i16>* %.compoundliteral.i, align 8
  store <4 x i16> %5, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vmovq_n_u16_wrapper(i16 zeroext %value) #1 {
entry:
  %__p0.addr.i = alloca i16, align 2
  %__ret.i = alloca <8 x i16>, align 16
  %.compoundliteral.i = alloca <8 x i16>, align 16
  %value.addr = alloca i16, align 2
  store i16 %value, i16* %value.addr, align 2
  %0 = load i16, i16* %value.addr, align 2
  store i16 %0, i16* %__p0.addr.i, align 2
  %1 = load i16, i16* %__p0.addr.i, align 2
  %vecinit.i = insertelement <8 x i16> undef, i16 %1, i32 0
  %2 = load i16, i16* %__p0.addr.i, align 2
  %vecinit1.i = insertelement <8 x i16> %vecinit.i, i16 %2, i32 1
  %3 = load i16, i16* %__p0.addr.i, align 2
  %vecinit2.i = insertelement <8 x i16> %vecinit1.i, i16 %3, i32 2
  %4 = load i16, i16* %__p0.addr.i, align 2
  %vecinit3.i = insertelement <8 x i16> %vecinit2.i, i16 %4, i32 3
  %5 = load i16, i16* %__p0.addr.i, align 2
  %vecinit4.i = insertelement <8 x i16> %vecinit3.i, i16 %5, i32 4
  %6 = load i16, i16* %__p0.addr.i, align 2
  %vecinit5.i = insertelement <8 x i16> %vecinit4.i, i16 %6, i32 5
  %7 = load i16, i16* %__p0.addr.i, align 2
  %vecinit6.i = insertelement <8 x i16> %vecinit5.i, i16 %7, i32 6
  %8 = load i16, i16* %__p0.addr.i, align 2
  %vecinit7.i = insertelement <8 x i16> %vecinit6.i, i16 %8, i32 7
  store <8 x i16> %vecinit7.i, <8 x i16>* %.compoundliteral.i, align 16
  %9 = load <8 x i16>, <8 x i16>* %.compoundliteral.i, align 16
  store <8 x i16> %9, <8 x i16>* %__ret.i, align 16
  %10 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %10
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vmov_n_u32_wrapper(i32 %value) #0 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <2 x i32>, align 8
  %.compoundliteral.i = alloca <2 x i32>, align 8
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <2 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <2 x i32> %vecinit.i, i32 %2, i32 1
  store <2 x i32> %vecinit1.i, <2 x i32>* %.compoundliteral.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %.compoundliteral.i, align 8
  store <2 x i32> %3, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vmovq_n_u32_wrapper(i32 %value) #1 {
entry:
  %__p0.addr.i = alloca i32, align 4
  %__ret.i = alloca <4 x i32>, align 16
  %.compoundliteral.i = alloca <4 x i32>, align 16
  %value.addr = alloca i32, align 4
  store i32 %value, i32* %value.addr, align 4
  %0 = load i32, i32* %value.addr, align 4
  store i32 %0, i32* %__p0.addr.i, align 4
  %1 = load i32, i32* %__p0.addr.i, align 4
  %vecinit.i = insertelement <4 x i32> undef, i32 %1, i32 0
  %2 = load i32, i32* %__p0.addr.i, align 4
  %vecinit1.i = insertelement <4 x i32> %vecinit.i, i32 %2, i32 1
  %3 = load i32, i32* %__p0.addr.i, align 4
  %vecinit2.i = insertelement <4 x i32> %vecinit1.i, i32 %3, i32 2
  %4 = load i32, i32* %__p0.addr.i, align 4
  %vecinit3.i = insertelement <4 x i32> %vecinit2.i, i32 %4, i32 3
  store <4 x i32> %vecinit3.i, <4 x i32>* %.compoundliteral.i, align 16
  %5 = load <4 x i32>, <4 x i32>* %.compoundliteral.i, align 16
  store <4 x i32> %5, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vmovq_n_u64_wrapper(i64 %value) #1 {
entry:
  %__p0.addr.i = alloca i64, align 8
  %__ret.i = alloca <2 x i64>, align 16
  %.compoundliteral.i = alloca <2 x i64>, align 16
  %value.addr = alloca i64, align 8
  store i64 %value, i64* %value.addr, align 8
  %0 = load i64, i64* %value.addr, align 8
  store i64 %0, i64* %__p0.addr.i, align 8
  %1 = load i64, i64* %__p0.addr.i, align 8
  %vecinit.i = insertelement <2 x i64> undef, i64 %1, i32 0
  %2 = load i64, i64* %__p0.addr.i, align 8
  %vecinit1.i = insertelement <2 x i64> %vecinit.i, i64 %2, i32 1
  store <2 x i64> %vecinit1.i, <2 x i64>* %.compoundliteral.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %.compoundliteral.i, align 16
  store <2 x i64> %3, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpadd_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpadd_v.i = call <8 x i8> @llvm.aarch64.neon.addp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpadd_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpadd_v2.i = call <4 x i16> @llvm.aarch64.neon.addp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vpadd_v3.i = bitcast <4 x i16> %vpadd_v2.i to <8 x i8>
  store <4 x i16> %vpadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpadd_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpadd_v2.i = call <2 x i32> @llvm.aarch64.neon.addp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vpadd_v3.i = bitcast <2 x i32> %vpadd_v2.i to <8 x i8>
  store <2 x i32> %vpadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpadd_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpadd_v.i = call <8 x i8> @llvm.aarch64.neon.addp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpadd_v.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpadd_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpadd_v2.i = call <4 x i16> @llvm.aarch64.neon.addp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  %vpadd_v3.i = bitcast <4 x i16> %vpadd_v2.i to <8 x i8>
  store <4 x i16> %vpadd_v2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpadd_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpadd_v2.i = call <2 x i32> @llvm.aarch64.neon.addp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  %vpadd_v3.i = bitcast <2 x i32> %vpadd_v2.i to <8 x i8>
  store <2 x i32> %vpadd_v2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpaddq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpaddq_v.i = call <16 x i8> @llvm.aarch64.neon.addp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpaddq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.addp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vpaddq_v3.i = bitcast <8 x i16> %vpaddq_v2.i to <16 x i8>
  store <8 x i16> %vpaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpaddq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.addp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vpaddq_v3.i = bitcast <4 x i32> %vpaddq_v2.i to <16 x i8>
  store <4 x i32> %vpaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vpaddq_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vpaddq_v2.i = call <2 x i64> @llvm.aarch64.neon.addp.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vpaddq_v3.i = bitcast <2 x i64> %vpaddq_v2.i to <16 x i8>
  store <2 x i64> %vpaddq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpaddq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpaddq_v.i = call <16 x i8> @llvm.aarch64.neon.addp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpaddq_v.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpaddq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpaddq_v2.i = call <8 x i16> @llvm.aarch64.neon.addp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  %vpaddq_v3.i = bitcast <8 x i16> %vpaddq_v2.i to <16 x i8>
  store <8 x i16> %vpaddq_v2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpaddq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpaddq_v2.i = call <4 x i32> @llvm.aarch64.neon.addp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  %vpaddq_v3.i = bitcast <4 x i32> %vpaddq_v2.i to <16 x i8>
  store <4 x i32> %vpaddq_v2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vpaddq_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  %4 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %5 = bitcast <2 x i64> %4 to <16 x i8>
  %vpaddq_v2.i = call <2 x i64> @llvm.aarch64.neon.addp.v2i64(<2 x i64> %2, <2 x i64> %4) #4
  %vpaddq_v3.i = bitcast <2 x i64> %vpaddq_v2.i to <16 x i8>
  store <2 x i64> %vpaddq_v2.i, <2 x i64>* %__ret.i, align 16
  %6 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpmax_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpmax.i = call <8 x i8> @llvm.aarch64.neon.smaxp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpmax.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpmax_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpmax2.i = call <4 x i16> @llvm.aarch64.neon.smaxp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vpmax2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpmax_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpmax2.i = call <2 x i32> @llvm.aarch64.neon.smaxp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vpmax2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpmaxq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpmax.i = call <16 x i8> @llvm.aarch64.neon.smaxp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpmax.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpmaxq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpmax2.i = call <8 x i16> @llvm.aarch64.neon.smaxp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vpmax2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpmaxq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpmax2.i = call <4 x i32> @llvm.aarch64.neon.smaxp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vpmax2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpmax_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpmax.i = call <8 x i8> @llvm.aarch64.neon.umaxp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpmax.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpmax_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpmax2.i = call <4 x i16> @llvm.aarch64.neon.umaxp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vpmax2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpmax_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpmax2.i = call <2 x i32> @llvm.aarch64.neon.umaxp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vpmax2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpmaxq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpmax.i = call <16 x i8> @llvm.aarch64.neon.umaxp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpmax.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpmaxq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpmax2.i = call <8 x i16> @llvm.aarch64.neon.umaxp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vpmax2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpmaxq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpmax2.i = call <4 x i32> @llvm.aarch64.neon.umaxp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vpmax2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpmin_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpmin.i = call <8 x i8> @llvm.aarch64.neon.sminp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpmin.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpmin_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpmin2.i = call <4 x i16> @llvm.aarch64.neon.sminp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vpmin2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpmin_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpmin2.i = call <2 x i32> @llvm.aarch64.neon.sminp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vpmin2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpminq_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpmin.i = call <16 x i8> @llvm.aarch64.neon.sminp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpmin.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpminq_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpmin2.i = call <8 x i16> @llvm.aarch64.neon.sminp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vpmin2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpminq_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpmin2.i = call <4 x i32> @llvm.aarch64.neon.sminp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vpmin2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vpmin_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %vpmin.i = call <8 x i8> @llvm.aarch64.neon.uminp.v8i8(<8 x i8> %2, <8 x i8> %3) #4
  store <8 x i8> %vpmin.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vpmin_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = bitcast <4 x i16> %2 to <8 x i8>
  %4 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %5 = bitcast <4 x i16> %4 to <8 x i8>
  %vpmin2.i = call <4 x i16> @llvm.aarch64.neon.uminp.v4i16(<4 x i16> %2, <4 x i16> %4) #4
  store <4 x i16> %vpmin2.i, <4 x i16>* %__ret.i, align 8
  %6 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vpmin_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = bitcast <2 x i32> %2 to <8 x i8>
  %4 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %5 = bitcast <2 x i32> %4 to <8 x i8>
  %vpmin2.i = call <2 x i32> @llvm.aarch64.neon.uminp.v2i32(<2 x i32> %2, <2 x i32> %4) #4
  store <2 x i32> %vpmin2.i, <2 x i32>* %__ret.i, align 8
  %6 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vpminq_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %vpmin.i = call <16 x i8> @llvm.aarch64.neon.uminp.v16i8(<16 x i8> %2, <16 x i8> %3) #4
  store <16 x i8> %vpmin.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vpminq_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = bitcast <8 x i16> %2 to <16 x i8>
  %4 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %5 = bitcast <8 x i16> %4 to <16 x i8>
  %vpmin2.i = call <8 x i16> @llvm.aarch64.neon.uminp.v8i16(<8 x i16> %2, <8 x i16> %4) #4
  store <8 x i16> %vpmin2.i, <8 x i16>* %__ret.i, align 16
  %6 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vpminq_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = bitcast <4 x i32> %2 to <16 x i8>
  %4 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %5 = bitcast <4 x i32> %4 to <16 x i8>
  %vpmin2.i = call <4 x i32> @llvm.aarch64.neon.uminp.v4i32(<4 x i32> %2, <4 x i32> %4) #4
  store <4 x i32> %vpmin2.i, <4 x i32>* %__ret.i, align 16
  %6 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %6
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vzip1_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vzip1q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vzip1_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vzip1q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vzip1_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vzip1q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vzip1q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vzip1_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vzip1q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vzip1_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vzip1q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vzip1_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vzip1q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vzip1q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vzip2_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vzip2q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vzip2_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vzip2q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vzip2_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vzip2q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vzip2q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vzip2_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vzip2q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vzip2_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vzip2q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vzip2_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vzip2q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vzip2q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vuzp1_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vuzp1q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vuzp1_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vuzp1q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vuzp1_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vuzp1q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vuzp1q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vuzp1_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vuzp1q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vuzp1_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vuzp1q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vuzp1_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vuzp1q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vuzp1q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vuzp2_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vuzp2q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vuzp2_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vuzp2q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vuzp2_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vuzp2q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vuzp2q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vuzp2_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vuzp2q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vuzp2_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vuzp2q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vuzp2_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vuzp2q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vuzp2q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtrn1_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtrn1q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 16, i32 2, i32 18, i32 4, i32 20, i32 6, i32 22, i32 8, i32 24, i32 10, i32 26, i32 12, i32 28, i32 14, i32 30>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtrn1_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtrn1q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtrn1_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtrn1q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtrn1q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtrn1_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtrn1q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 0, i32 16, i32 2, i32 18, i32 4, i32 20, i32 6, i32 22, i32 8, i32 24, i32 10, i32 26, i32 12, i32 28, i32 14, i32 30>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtrn1_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtrn1q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtrn1_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtrn1q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtrn1q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtrn2_s8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtrn2q_s8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 1, i32 17, i32 3, i32 19, i32 5, i32 21, i32 7, i32 23, i32 9, i32 25, i32 11, i32 27, i32 13, i32 29, i32 15, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtrn2_s16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtrn2q_s16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtrn2_s32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtrn2q_s32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtrn2q_s64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i8> @vtrn2_u8_wrapper(<8 x i8> %a, <8 x i8> %b) #0 {
entry:
  %__p0.addr.i = alloca <8 x i8>, align 8
  %__p1.addr.i = alloca <8 x i8>, align 8
  %__ret.i = alloca <8 x i8>, align 8
  %a.addr = alloca <8 x i8>, align 8
  %b.addr = alloca <8 x i8>, align 8
  store <8 x i8> %a, <8 x i8>* %a.addr, align 8
  store <8 x i8> %b, <8 x i8>* %b.addr, align 8
  %0 = load <8 x i8>, <8 x i8>* %a.addr, align 8
  %1 = load <8 x i8>, <8 x i8>* %b.addr, align 8
  store <8 x i8> %0, <8 x i8>* %__p0.addr.i, align 8
  store <8 x i8> %1, <8 x i8>* %__p1.addr.i, align 8
  %2 = load <8 x i8>, <8 x i8>* %__p0.addr.i, align 8
  %3 = load <8 x i8>, <8 x i8>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <8 x i8> %2, <8 x i8> %3, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
  store <8 x i8> %shuffle.i, <8 x i8>* %__ret.i, align 8
  %4 = load <8 x i8>, <8 x i8>* %__ret.i, align 8
  ret <8 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <16 x i8> @vtrn2q_u8_wrapper(<16 x i8> %a, <16 x i8> %b) #1 {
entry:
  %__p0.addr.i = alloca <16 x i8>, align 16
  %__p1.addr.i = alloca <16 x i8>, align 16
  %__ret.i = alloca <16 x i8>, align 16
  %a.addr = alloca <16 x i8>, align 16
  %b.addr = alloca <16 x i8>, align 16
  store <16 x i8> %a, <16 x i8>* %a.addr, align 16
  store <16 x i8> %b, <16 x i8>* %b.addr, align 16
  %0 = load <16 x i8>, <16 x i8>* %a.addr, align 16
  %1 = load <16 x i8>, <16 x i8>* %b.addr, align 16
  store <16 x i8> %0, <16 x i8>* %__p0.addr.i, align 16
  store <16 x i8> %1, <16 x i8>* %__p1.addr.i, align 16
  %2 = load <16 x i8>, <16 x i8>* %__p0.addr.i, align 16
  %3 = load <16 x i8>, <16 x i8>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <16 x i8> %2, <16 x i8> %3, <16 x i32> <i32 1, i32 17, i32 3, i32 19, i32 5, i32 21, i32 7, i32 23, i32 9, i32 25, i32 11, i32 27, i32 13, i32 29, i32 15, i32 31>
  store <16 x i8> %shuffle.i, <16 x i8>* %__ret.i, align 16
  %4 = load <16 x i8>, <16 x i8>* %__ret.i, align 16
  ret <16 x i8> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i16> @vtrn2_u16_wrapper(<4 x i16> %a, <4 x i16> %b) #0 {
entry:
  %__p0.addr.i = alloca <4 x i16>, align 8
  %__p1.addr.i = alloca <4 x i16>, align 8
  %__ret.i = alloca <4 x i16>, align 8
  %a.addr = alloca <4 x i16>, align 8
  %b.addr = alloca <4 x i16>, align 8
  store <4 x i16> %a, <4 x i16>* %a.addr, align 8
  store <4 x i16> %b, <4 x i16>* %b.addr, align 8
  %0 = load <4 x i16>, <4 x i16>* %a.addr, align 8
  %1 = load <4 x i16>, <4 x i16>* %b.addr, align 8
  store <4 x i16> %0, <4 x i16>* %__p0.addr.i, align 8
  store <4 x i16> %1, <4 x i16>* %__p1.addr.i, align 8
  %2 = load <4 x i16>, <4 x i16>* %__p0.addr.i, align 8
  %3 = load <4 x i16>, <4 x i16>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <4 x i16> %2, <4 x i16> %3, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i16> %shuffle.i, <4 x i16>* %__ret.i, align 8
  %4 = load <4 x i16>, <4 x i16>* %__ret.i, align 8
  ret <4 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <8 x i16> @vtrn2q_u16_wrapper(<8 x i16> %a, <8 x i16> %b) #1 {
entry:
  %__p0.addr.i = alloca <8 x i16>, align 16
  %__p1.addr.i = alloca <8 x i16>, align 16
  %__ret.i = alloca <8 x i16>, align 16
  %a.addr = alloca <8 x i16>, align 16
  %b.addr = alloca <8 x i16>, align 16
  store <8 x i16> %a, <8 x i16>* %a.addr, align 16
  store <8 x i16> %b, <8 x i16>* %b.addr, align 16
  %0 = load <8 x i16>, <8 x i16>* %a.addr, align 16
  %1 = load <8 x i16>, <8 x i16>* %b.addr, align 16
  store <8 x i16> %0, <8 x i16>* %__p0.addr.i, align 16
  store <8 x i16> %1, <8 x i16>* %__p1.addr.i, align 16
  %2 = load <8 x i16>, <8 x i16>* %__p0.addr.i, align 16
  %3 = load <8 x i16>, <8 x i16>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <8 x i16> %2, <8 x i16> %3, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
  store <8 x i16> %shuffle.i, <8 x i16>* %__ret.i, align 16
  %4 = load <8 x i16>, <8 x i16>* %__ret.i, align 16
  ret <8 x i16> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i32> @vtrn2_u32_wrapper(<2 x i32> %a, <2 x i32> %b) #0 {
entry:
  %__p0.addr.i = alloca <2 x i32>, align 8
  %__p1.addr.i = alloca <2 x i32>, align 8
  %__ret.i = alloca <2 x i32>, align 8
  %a.addr = alloca <2 x i32>, align 8
  %b.addr = alloca <2 x i32>, align 8
  store <2 x i32> %a, <2 x i32>* %a.addr, align 8
  store <2 x i32> %b, <2 x i32>* %b.addr, align 8
  %0 = load <2 x i32>, <2 x i32>* %a.addr, align 8
  %1 = load <2 x i32>, <2 x i32>* %b.addr, align 8
  store <2 x i32> %0, <2 x i32>* %__p0.addr.i, align 8
  store <2 x i32> %1, <2 x i32>* %__p1.addr.i, align 8
  %2 = load <2 x i32>, <2 x i32>* %__p0.addr.i, align 8
  %3 = load <2 x i32>, <2 x i32>* %__p1.addr.i, align 8
  %shuffle.i = shufflevector <2 x i32> %2, <2 x i32> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i32> %shuffle.i, <2 x i32>* %__ret.i, align 8
  %4 = load <2 x i32>, <2 x i32>* %__ret.i, align 8
  ret <2 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <4 x i32> @vtrn2q_u32_wrapper(<4 x i32> %a, <4 x i32> %b) #1 {
entry:
  %__p0.addr.i = alloca <4 x i32>, align 16
  %__p1.addr.i = alloca <4 x i32>, align 16
  %__ret.i = alloca <4 x i32>, align 16
  %a.addr = alloca <4 x i32>, align 16
  %b.addr = alloca <4 x i32>, align 16
  store <4 x i32> %a, <4 x i32>* %a.addr, align 16
  store <4 x i32> %b, <4 x i32>* %b.addr, align 16
  %0 = load <4 x i32>, <4 x i32>* %a.addr, align 16
  %1 = load <4 x i32>, <4 x i32>* %b.addr, align 16
  store <4 x i32> %0, <4 x i32>* %__p0.addr.i, align 16
  store <4 x i32> %1, <4 x i32>* %__p1.addr.i, align 16
  %2 = load <4 x i32>, <4 x i32>* %__p0.addr.i, align 16
  %3 = load <4 x i32>, <4 x i32>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <4 x i32> %2, <4 x i32> %3, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i32> %shuffle.i, <4 x i32>* %__ret.i, align 16
  %4 = load <4 x i32>, <4 x i32>* %__ret.i, align 16
  ret <4 x i32> %4
}

; Function Attrs: noinline nounwind optnone ssp uwtable
define dso_local <2 x i64> @vtrn2q_u64_wrapper(<2 x i64> %a, <2 x i64> %b) #1 {
entry:
  %__p0.addr.i = alloca <2 x i64>, align 16
  %__p1.addr.i = alloca <2 x i64>, align 16
  %__ret.i = alloca <2 x i64>, align 16
  %a.addr = alloca <2 x i64>, align 16
  %b.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  store <2 x i64> %0, <2 x i64>* %__p0.addr.i, align 16
  store <2 x i64> %1, <2 x i64>* %__p1.addr.i, align 16
  %2 = load <2 x i64>, <2 x i64>* %__p0.addr.i, align 16
  %3 = load <2 x i64>, <2 x i64>* %__p1.addr.i, align 16
  %shuffle.i = shufflevector <2 x i64> %2, <2 x i64> %3, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %shuffle.i, <2 x i64>* %__ret.i, align 16
  %4 = load <2 x i64>, <2 x i64>* %__ret.i, align 16
  ret <2 x i64> %4
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.shadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.shadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.shadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.shadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.shadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.shadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uhadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uhadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uhadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uhadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uhadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uhadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.srhadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.srhadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.srhadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.srhadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.srhadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.srhadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.urhadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.urhadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.urhadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.urhadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.urhadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.urhadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqadd.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqadd.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqadd.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqadd.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uqadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uqadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uqadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uqadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uqadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uqadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.uqadd.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.uqadd.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.uqadd.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.uqadd.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.suqadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.suqadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.suqadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.suqadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.suqadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.suqadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.suqadd.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.suqadd.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.suqadd.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.suqadd.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.usqadd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.usqadd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.usqadd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.usqadd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.usqadd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.usqadd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.usqadd.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.usqadd.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.usqadd.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.usqadd.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.raddhn.v8i8(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.raddhn.v4i16(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.raddhn.v2i32(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqdmulh.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqdmulh.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqdmulh.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqdmulh.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqdmulh.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqrdmulh.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqrdmulh.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqrdmulh.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqrdmulh.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqdmull.v4i32(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqdmull.v2i64(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqdmulls.scalar(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqsub.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqsub.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqsub.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqsub.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.smull.v8i16(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.smull.v2i64(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.umull.v8i16(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.umull.v4i32(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.umull.v2i64(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.shsub.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.shsub.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.shsub.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.shsub.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.shsub.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.shsub.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uhsub.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uhsub.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uhsub.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uhsub.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uhsub.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uhsub.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqsub.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqsub.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqsub.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqsub.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqsub.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqsub.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uqsub.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uqsub.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uqsub.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uqsub.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uqsub.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uqsub.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.uqsub.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.uqsub.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.uqsub.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.uqsub.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.rsubhn.v8i8(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.rsubhn.v4i16(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.rsubhn.v2i32(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sabd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sabd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sabd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sabd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sabd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sabd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uabd.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uabd.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uabd.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uabd.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uabd.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uabd.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.smax.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.smax.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.smax.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.smax.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.smax.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.smax.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.umax.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.umax.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.umax.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.umax.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.umax.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.smin.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.smin.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.smin.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.smin.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.smin.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.smin.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.umin.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.umin.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.umin.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.umin.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.umin.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.ushl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.ushl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.ushl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.ushl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.ushl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.ushl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.ushl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.ushl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.ushl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqshl.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uqshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uqshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uqshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uqshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uqshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uqshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.uqshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.uqshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.uqshl.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.uqshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.srshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.srshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.srshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.srshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.srshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.srshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.srshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.srshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.urshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.urshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.urshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.urshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.urshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.urshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.urshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.urshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.urshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqrshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqrshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqrshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqrshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqrshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqrshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqrshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqrshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqrshl.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqrshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uqrshl.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uqrshl.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uqrshl.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uqrshl.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uqrshl.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uqrshl.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.uqrshl.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.uqrshl.v1i64(<1 x i64>, <1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.uqrshl.i32(i32, i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.uqrshl.i64(i64, i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqxtn.v8i8(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqxtn.v2i32(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uqxtn.v8i8(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uqxtn.v4i16(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uqxtn.v2i32(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.scalar.sqxtn.i32.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.scalar.uqxtn.i32.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqxtun.v4i16(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqxtun.v2i32(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.scalar.sqxtun.i32.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.abs.v8i8(<8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.abs.v16i8(<16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.abs.v4i16(<4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.abs.v8i16(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.abs.v2i32(<2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.abs.v4i32(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.abs.v2i64(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.abs.v1i64(<1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.abs.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqabs.v8i8(<8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqabs.v16i8(<16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqabs.v4i16(<4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqabs.v8i16(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqabs.v2i32(<2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqabs.v4i32(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqabs.v2i64(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqabs.v1i64(<1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqabs.i32(i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqabs.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqneg.v8i8(<8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sqneg.v16i8(<16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqneg.v4i16(<4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqneg.v8i16(<8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sqneg.v2i32(<2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqneg.v4i32(<4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.sqneg.v2i64(<2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <1 x i64> @llvm.aarch64.neon.sqneg.v1i64(<1 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i32 @llvm.aarch64.neon.sqneg.i32(i32) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.aarch64.neon.sqneg.i64(i64) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.addp.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.addp.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.addp.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.addp.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.addp.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.addp.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i64> @llvm.aarch64.neon.addp.v2i64(<2 x i64>, <2 x i64>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.smaxp.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.smaxp.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.smaxp.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.smaxp.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.smaxp.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.smaxp.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.umaxp.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.umaxp.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.umaxp.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.umaxp.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.umaxp.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.umaxp.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sminp.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sminp.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.sminp.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.sminp.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sminp.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sminp.v4i32(<4 x i32>, <4 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.uminp.v8i8(<8 x i8>, <8 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.uminp.v4i16(<4 x i16>, <4 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <2 x i32> @llvm.aarch64.neon.uminp.v2i32(<2 x i32>, <2 x i32>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.uminp.v16i8(<16 x i8>, <16 x i8>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.uminp.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.uminp.v4i32(<4 x i32>, <4 x i32>) #3

attributes #0 = { noinline nounwind optnone ssp uwtable "disable-tail-calls"="false" "frame-pointer"="non-leaf" "less-precise-fpmad"="false" "min-legal-vector-width"="64" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="apple-a12" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+ras,+rcpc,+rdm,+sha2,+v8.3a,+zcm,+zcz" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { noinline nounwind optnone ssp uwtable "disable-tail-calls"="false" "frame-pointer"="non-leaf" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="apple-a12" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+ras,+rcpc,+rdm,+sha2,+v8.3a,+zcm,+zcz" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { noinline nounwind optnone ssp uwtable "disable-tail-calls"="false" "frame-pointer"="non-leaf" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="apple-a12" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+ras,+rcpc,+rdm,+sha2,+v8.3a,+zcm,+zcz" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nosync nounwind readnone willreturn }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1, !2, !3, !4, !5}
!llvm.ident = !{!6}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"branch-target-enforcement", i32 0}
!2 = !{i32 1, !"sign-return-address", i32 0}
!3 = !{i32 1, !"sign-return-address-all", i32 0}
!4 = !{i32 1, !"sign-return-address-with-bkey", i32 0}
!5 = !{i32 7, !"PIC Level", i32 2}
!6 = !{!"clang version 12.0.1"}
