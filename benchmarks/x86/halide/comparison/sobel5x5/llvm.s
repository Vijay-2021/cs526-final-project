	.text
	.file	"posix_allocator.cpp"
	.section	.text.halide_default_malloc,"ax",@progbits
	.weak	halide_default_malloc           # -- Begin function halide_default_malloc
	.p2align	4, 0x90
	.type	halide_default_malloc,@function
halide_default_malloc:                  # @halide_default_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	64(%rsi), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_1
# %bb.2:                                # %if.end
	movq	%rax, %rcx
	addq	$71, %rax
	andq	$-64, %rax
	movq	%rcx, -8(%rax)
	popq	%rbp
	retq
.LBB0_1:
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end0:
	.size	halide_default_malloc, .Lfunc_end0-halide_default_malloc
                                        # -- End function
	.section	.text.halide_default_free,"ax",@progbits
	.weak	halide_default_free             # -- Begin function halide_default_free
	.p2align	4, 0x90
	.type	halide_default_free,@function
halide_default_free:                    # @halide_default_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	-8(%rsi), %rdi
	popq	%rbp
	jmp	free@PLT                        # TAILCALL
.Lfunc_end1:
	.size	halide_default_free, .Lfunc_end1-halide_default_free
                                        # -- End function
	.section	.text.halide_set_custom_malloc,"ax",@progbits
	.weak	halide_set_custom_malloc        # -- Begin function halide_set_custom_malloc
	.p2align	4, 0x90
	.type	halide_set_custom_malloc,@function
halide_set_custom_malloc:               # @halide_set_custom_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end2:
	.size	halide_set_custom_malloc, .Lfunc_end2-halide_set_custom_malloc
                                        # -- End function
	.section	.text.halide_set_custom_free,"ax",@progbits
	.weak	halide_set_custom_free          # -- Begin function halide_set_custom_free
	.p2align	4, 0x90
	.type	halide_set_custom_free,@function
halide_set_custom_free:                 # @halide_set_custom_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end3:
	.size	halide_set_custom_free, .Lfunc_end3-halide_set_custom_free
                                        # -- End function
	.section	.text.halide_malloc,"ax",@progbits
	.weak	halide_malloc                   # -- Begin function halide_malloc
	.p2align	4, 0x90
	.type	halide_malloc,@function
halide_malloc:                          # @halide_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end4:
	.size	halide_malloc, .Lfunc_end4-halide_malloc
                                        # -- End function
	.section	.text.halide_free,"ax",@progbits
	.weak	halide_free                     # -- Begin function halide_free
	.p2align	4, 0x90
	.type	halide_free,@function
halide_free:                            # @halide_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end5:
	.size	halide_free, .Lfunc_end5-halide_free
                                        # -- End function
	.section	.text.halide_default_error,"ax",@progbits
	.weak	halide_default_error            # -- Begin function halide_default_error
	.p2align	4, 0x90
	.type	halide_default_error,@function
halide_default_error:                   # @halide_default_error
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	subq	$4104, %rsp                     # imm = 0x1008
	movq	%rsi, %rbx
	movq	%rdi, %r15
	leaq	-26(%rbp), %rsi
	leaq	.L.str(%rip), %rdx
	leaq	-4120(%rbp), %r14
	movq	%r14, %rdi
	callq	halide_string_to_string@PLT
	leaq	4094(%rax), %rsi
	movq	%rax, %rdi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	cmpb	$10, -1(%rax)
	je	.LBB6_2
# %bb.1:                                # %if.then
	movw	$10, (%rax)
	incq	%rax
.LBB6_2:                                # %if.end
	subq	%r14, %rax
	incq	%rax
	movq	%r15, %rdi
	movq	%r14, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	addq	$4104, %rsp                     # imm = 0x1008
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end6:
	.size	halide_default_error, .Lfunc_end6-halide_default_error
                                        # -- End function
	.section	.text.halide_error,"ax",@progbits
	.weak	halide_error                    # -- Begin function halide_error
	.p2align	4, 0x90
	.type	halide_error,@function
halide_error:                           # @halide_error
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end7:
	.size	halide_error, .Lfunc_end7-halide_error
                                        # -- End function
	.section	.text.halide_set_error_handler,"ax",@progbits
	.weak	halide_set_error_handler        # -- Begin function halide_set_error_handler
	.p2align	4, 0x90
	.type	halide_set_error_handler,@function
halide_set_error_handler:               # @halide_set_error_handler
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end8:
	.size	halide_set_error_handler, .Lfunc_end8-halide_set_error_handler
                                        # -- End function
	.section	.text.halide_print,"ax",@progbits
	.weak	halide_print                    # -- Begin function halide_print
	.p2align	4, 0x90
	.type	halide_print,@function
halide_print:                           # @halide_print
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end9:
	.size	halide_print, .Lfunc_end9-halide_print
                                        # -- End function
	.section	.text.halide_set_custom_print,"ax",@progbits
	.weak	halide_set_custom_print         # -- Begin function halide_set_custom_print
	.p2align	4, 0x90
	.type	halide_set_custom_print,@function
halide_set_custom_print:                # @halide_set_custom_print
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end10:
	.size	halide_set_custom_print, .Lfunc_end10-halide_set_custom_print
                                        # -- End function
	.section	.text.halide_start_clock,"ax",@progbits
	.weak	halide_start_clock              # -- Begin function halide_start_clock
	.p2align	4, 0x90
	.type	halide_start_clock,@function
halide_start_clock:                     # @halide_start_clock
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	halide_reference_clock_inited@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	jne	.LBB11_2
# %bb.1:                                # %if.then
	movq	halide_reference_clock@GOTPCREL(%rip), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movb	$1, (%rbx)
.LBB11_2:                               # %if.end
	xorl	%eax, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end11:
	.size	halide_start_clock, .Lfunc_end11-halide_start_clock
                                        # -- End function
	.section	.text.halide_current_time_ns,"ax",@progbits
	.weak	halide_current_time_ns          # -- Begin function halide_current_time_ns
	.p2align	4, 0x90
	.type	halide_current_time_ns,@function
halide_current_time_ns:                 # @halide_current_time_ns
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	leaq	-16(%rbp), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	vmovdqa	-16(%rbp), %xmm0
	movq	halide_reference_clock@GOTPCREL(%rip), %rax
	vpsubq	(%rax), %xmm0, %xmm0
	vmovq	%xmm0, %rax
	imulq	$1000000000, %rax, %rcx         # imm = 0x3B9ACA00
	vpextrq	$1, %xmm0, %rax
	addq	%rcx, %rax
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end12:
	.size	halide_current_time_ns, .Lfunc_end12-halide_current_time_ns
                                        # -- End function
	.section	.text.halide_sleep_ms,"ax",@progbits
	.weak	halide_sleep_ms                 # -- Begin function halide_sleep_ms
	.p2align	4, 0x90
	.type	halide_sleep_ms,@function
halide_sleep_ms:                        # @halide_sleep_ms
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	imull	$1000, %esi, %edi               # imm = 0x3E8
	popq	%rbp
	jmp	usleep@PLT                      # TAILCALL
.Lfunc_end13:
	.size	halide_sleep_ms, .Lfunc_end13-halide_sleep_ms
                                        # -- End function
	.section	.text.halide_default_print,"ax",@progbits
	.weak	halide_default_print            # -- Begin function halide_default_print
	.p2align	4, 0x90
	.type	halide_default_print,@function
halide_default_print:                   # @halide_default_print
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rsi, %rdi
	callq	strlen@PLT
	movl	$1, %edi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	write@PLT                       # TAILCALL
.Lfunc_end14:
	.size	halide_default_print, .Lfunc_end14-halide_default_print
                                        # -- End function
	.section	.text.halide_host_cpu_count,"ax",@progbits
	.weak	halide_host_cpu_count           # -- Begin function halide_host_cpu_count
	.p2align	4, 0x90
	.type	halide_host_cpu_count,@function
halide_host_cpu_count:                  # @halide_host_cpu_count
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$84, %edi
	popq	%rbp
	jmp	sysconf@PLT                     # TAILCALL
.Lfunc_end15:
	.size	halide_host_cpu_count, .Lfunc_end15-halide_host_cpu_count
                                        # -- End function
	.section	.text.halide_thread_yield,"ax",@progbits
	.weak	halide_thread_yield             # -- Begin function halide_thread_yield
	.p2align	4, 0x90
	.type	halide_thread_yield,@function
halide_thread_yield:                    # @halide_thread_yield
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	sched_yield@PLT                 # TAILCALL
.Lfunc_end16:
	.size	halide_thread_yield, .Lfunc_end16-halide_thread_yield
                                        # -- End function
	.section	.text.halide_default_do_task,"ax",@progbits
	.weak	halide_default_do_task          # -- Begin function halide_default_do_task
	.p2align	4, 0x90
	.type	halide_default_do_task,@function
halide_default_do_task:                 # @halide_default_do_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rsi, %rax
	movl	%edx, %esi
	movq	%rcx, %rdx
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end17:
	.size	halide_default_do_task, .Lfunc_end17-halide_default_do_task
                                        # -- End function
	.section	.text.halide_default_do_loop_task,"ax",@progbits
	.weak	halide_default_do_loop_task     # -- Begin function halide_default_do_loop_task
	.p2align	4, 0x90
	.type	halide_default_do_loop_task,@function
halide_default_do_loop_task:            # @halide_default_do_loop_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rsi, %rax
	movl	%edx, %esi
	movl	%ecx, %edx
	movq	%r8, %rcx
	movq	%r9, %r8
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end18:
	.size	halide_default_do_loop_task, .Lfunc_end18-halide_default_do_loop_task
                                        # -- End function
	.section	.text.halide_default_do_par_for,"ax",@progbits
	.weak	halide_default_do_par_for       # -- Begin function halide_default_do_par_for
	.p2align	4, 0x90
	.type	halide_default_do_par_for,@function
halide_default_do_par_for:              # @halide_default_do_par_for
# %bb.0:                                # %entry
	testl	%ecx, %ecx
	jle	.LBB19_1
# %bb.2:                                # %if.end
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	subq	$128, %rsp
	movq	$0, -144(%rbp)
	movl	%edx, -108(%rbp)
	movl	%ecx, -104(%rbp)
	movb	$0, -96(%rbp)
	movl	$0, -112(%rbp)
	movq	%r8, -136(%rbp)
	movl	$0, -100(%rbp)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, -128(%rbp)
	movq	%rsi, -88(%rbp)
	movq	%rdi, -40(%rbp)
	movq	$0, -32(%rbp)
	movl	$0, -24(%rbp)
	movb	$0, -20(%rbp)
	leaq	-144(%rbp), %rbx
	movq	%rbx, -72(%rbp)
	movl	$0, -64(%rbp)
	movq	$0, -56(%rbp)
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movl	$1, %edi
	movq	%rbx, %rsi
	xorl	%edx, %edx
	callq	_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_@PLT
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movl	-28(%rbp), %eax
	addq	$128, %rsp
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.LBB19_1:
	xorl	%eax, %eax
	retq
.Lfunc_end19:
	.size	halide_default_do_par_for, .Lfunc_end19-halide_default_do_par_for
                                        # -- End function
	.section	.text.halide_mutex_lock,"ax",@progbits
	.weak	halide_mutex_lock               # -- Begin function halide_mutex_lock
	.p2align	4, 0x90
	.type	halide_mutex_lock,@function
halide_mutex_lock:                      # @halide_mutex_lock
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movl	$1, %ecx
	xorl	%eax, %eax
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB20_1
.LBB20_4:                               # %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex4lockEv.exit
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB20_1:                               # %if.then.i
	movq	%rdi, %rbx
	movq	(%rdi), %rax
	movl	$40, %r12d
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE@GOTPCREL(%rip), %r15
	addq	$16, %r15
	leaq	-48(%rbp), %r14
	.p2align	4, 0x90
.LBB20_2:                               # %while.cond.outer.i.i
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	jne	.LBB20_5
# %bb.3:                                # %if.then.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	%rax, %rcx
	orq	$1, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB20_2
	jmp	.LBB20_4
.LBB20_5:                               # %if.end4.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	testl	%r12d, %r12d
	jg	.LBB20_10
# %bb.6:                                #   in Loop: Header=BB20_2 Depth=1
	movl	%r12d, %ecx
.LBB20_7:                               # %if.end8.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	testb	$2, %al
	jne	.LBB20_12
# %bb.8:                                # %if.then10.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	%rax, %rdx
	orq	$2, %rdx
	lock		cmpxchgq	%rdx, (%rbx)
	jne	.LBB20_9
.LBB20_12:                              # %if.end19.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	%r15, -48(%rbp)
	movq	%rbx, -40(%rbp)
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy@PLT
	cmpq	%rbx, %rax
	je	.LBB20_4
# %bb.13:                               # %if.end24.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	(%rbx), %rax
	movl	$40, %r12d
	jmp	.LBB20_2
.LBB20_10:                              # %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	xorl	%ecx, %ecx
	cmpl	$1, %r12d
	je	.LBB20_7
# %bb.11:                               # %if.then6.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	decl	%r12d
	callq	halide_thread_yield@PLT
	movq	(%rbx), %rax
	jmp	.LBB20_2
.LBB20_9:                               # %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i
                                        #   in Loop: Header=BB20_2 Depth=1
	movl	%ecx, %r12d
	jmp	.LBB20_2
.Lfunc_end20:
	.size	halide_mutex_lock, .Lfunc_end20-halide_mutex_lock
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_ # -- Begin function _ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_,@function
_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_: # @_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rsi, %r11
	movl	%edi, %r15d
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	cmpb	$0, 2121(%rbx)
	movq	%rdx, -64(%rbp)                 # 8-byte Spill
	jne	.LBB21_10
# %bb.1:                                # %land.rhs.i.preheader
	leaq	12(%rbx), %rax
	leaq	13(%rbx), %rcx
	leaq	2128(%rbx), %rdi
	.p2align	4, 0x90
.LBB21_2:                               # %land.rhs.i
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%rax)
	jne	.LBB21_3
# %bb.4:                                # %while.body.i
                                        #   in Loop: Header=BB21_2 Depth=1
	incq	%rax
	cmpq	%rcx, %rdi
	movq	%rcx, %rsi
	cmovaq	%rdi, %rsi
	cmpq	%rax, %rsi
	jne	.LBB21_2
# %bb.5:                                # %do.body.i
	movl	$2128, %eax                     # imm = 0x850
	addq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rax
	cmpq	%rax, %rsi
	je	.LBB21_7
.LBB21_6:                               # %if.then.i
	leaq	.L.str.6(%rip), %rsi
	xorl	%edi, %edi
	movq	%r11, %r14
	callq	halide_print@PLT
	callq	abort@PLT
	movq	%r14, %r11
	movq	-64(%rbp), %rdx                 # 8-byte Reload
.LBB21_7:                               # %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit
	movl	8(%rbx), %eax
	testl	%eax, %eax
	jne	.LBB21_9
# %bb.8:                                # %if.then2
	movq	%r11, %r14
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movq	%r14, %r11
	movq	-64(%rbp), %rdx                 # 8-byte Reload
.LBB21_9:                               # %if.end
	testl	%eax, %eax
	movl	$1, %ecx
	cmovgl	%eax, %ecx
	cmpl	$256, %ecx                      # imm = 0x100
	movl	$256, %eax                      # imm = 0x100
	cmovll	%ecx, %eax
	movl	%eax, 8(%rbx)
	movb	$1, 2121(%rbx)
.LBB21_10:                              # %if.end4
	movl	%r15d, %r8d
	testl	%r15d, %r15d
	movl	%r15d, -48(%rbp)                # 4-byte Spill
	movq	%r8, -80(%rbp)                  # 8-byte Spill
	jle	.LBB21_11
# %bb.12:                               # %for.body.preheader
	cmpl	$1, %r15d
	jne	.LBB21_27
# %bb.13:
	movl	$-1, %r13d
	xorl	%eax, %eax
	xorl	%r12d, %r12d
	xorl	%r14d, %r14d
	xorl	%r10d, %r10d
	xorl	%r15d, %r15d
.LBB21_14:                              # %for.cond.cleanup.loopexit.unr-lcssa
	movq	-80(%rbp), %r8                  # 8-byte Reload
	testb	$1, %r8b
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	je	.LBB21_18
# %bb.15:                               # %for.body.epil
	shlq	$7, %rax
	movl	44(%r11,%rax), %ecx
	addl	%ecx, %r12d
	testl	%ecx, %ecx
	movzbl	%r15b, %r15d
	movl	$1, %ecx
	cmovel	%ecx, %r15d
	movzbl	%r14b, %r14d
	cmovnel	%ecx, %r14d
	cmpl	$0, 32(%r11,%rax)
	movzbl	%r10b, %r10d
	cmovnel	%ecx, %r10d
	cmpb	$0, 48(%r11,%rax)
	je	.LBB21_17
# %bb.16:                               # %if.then23.epil
	incl	%r13d
	jmp	.LBB21_18
.LBB21_11:
	xorl	%r15d, %r15d
	movl	$-1, %r13d
	xorl	%r10d, %r10d
	xorl	%r14d, %r14d
	xorl	%r12d, %r12d
	testq	%rdx, %rdx
	movl	%r10d, -52(%rbp)                # 4-byte Spill
	jne	.LBB21_33
.LBB21_20:                              # %if.then32
	movq	%r11, -72(%rbp)                 # 8-byte Spill
	movl	%r14d, %eax
	orb	%r10b, %al
	movb	%al, -41(%rbp)                  # 1-byte Spill
	movl	24(%rbx), %ecx
	cmpl	$255, %ecx
	jg	.LBB21_25
# %bb.21:                               # %land.rhs.preheader
	movb	-41(%rbp), %al                  # 1-byte Reload
	andb	$1, %al
	movzbl	%al, %eax
	addl	%eax, %r12d
	jmp	.LBB21_22
	.p2align	4, 0x90
.LBB21_24:                              # %while.body
                                        #   in Loop: Header=BB21_22 Depth=1
	incl	28(%rbx)
	movq	_ZN6Halide7Runtime8Internal13worker_threadEPv@GOTPCREL(%rip), %rdi
	xorl	%esi, %esi
	callq	halide_spawn_thread@PLT
	movslq	24(%rbx), %rdx
	leal	1(%rdx), %ecx
	movl	%ecx, 24(%rbx)
	movq	%rax, 72(%rbx,%rdx,8)
	cmpq	$255, %rdx
	jge	.LBB21_25
.LBB21_22:                              # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	movl	8(%rbx), %eax
	decl	%eax
	cmpl	%eax, %ecx
	jl	.LBB21_24
# %bb.23:                               # %lor.rhs
                                        #   in Loop: Header=BB21_22 Depth=1
	subl	2124(%rbx), %ecx
	incl	%ecx
	cmpl	%r12d, %ecx
	jl	.LBB21_24
.LBB21_25:                              # %do.end50
	testb	$1, -41(%rbp)                   # 1-byte Folded Reload
	movq	-72(%rbp), %r11                 # 8-byte Reload
	movl	-48(%rbp), %esi                 # 4-byte Reload
	movq	-80(%rbp), %r8                  # 8-byte Reload
	je	.LBB21_37
# %bb.26:                               # %if.then54
	incl	2124(%rbx)
	jmp	.LBB21_37
.LBB21_27:                              # %for.body.preheader.new
                                        # kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$-2, %r8d
	leaq	176(%r11), %rbx
	movl	$-1, %r13d
	xorl	%eax, %eax
	movl	$1, %r9d
	xorl	%r12d, %r12d
	xorl	%r14d, %r14d
	xorl	%r10d, %r10d
	xorl	%r15d, %r15d
	jmp	.LBB21_28
	.p2align	4, 0x90
.LBB21_52:                              # %if.else24.1
                                        #   in Loop: Header=BB21_28 Depth=1
	addl	-8(%rbx), %r13d
.LBB21_53:                              # %for.inc.1
                                        #   in Loop: Header=BB21_28 Depth=1
	addl	%edi, %r12d
	addq	$2, %rax
	addq	$256, %rbx                      # imm = 0x100
	cmpq	%rax, %r8
	je	.LBB21_14
.LBB21_28:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, -128(%rbx)
	je	.LBB21_30
# %bb.29:                               # %if.then23
                                        #   in Loop: Header=BB21_28 Depth=1
	incl	%r13d
	jmp	.LBB21_31
	.p2align	4, 0x90
.LBB21_30:                              # %if.else24
                                        #   in Loop: Header=BB21_28 Depth=1
	addl	-136(%rbx), %r13d
.LBB21_31:                              # %for.inc
                                        #   in Loop: Header=BB21_28 Depth=1
	movl	-132(%rbx), %ecx
	addl	%ecx, %r12d
	movl	-144(%rbx), %esi
	movl	-4(%rbx), %edi
	testl	%ecx, %ecx
	movzbl	%r15b, %r15d
	cmovel	%r9d, %r15d
	testl	%edi, %edi
	cmovel	%r9d, %r15d
	orl	%edi, %ecx
	movzbl	%r14b, %r14d
	cmovnel	%r9d, %r14d
	orl	-16(%rbx), %esi
	movzbl	%r10b, %r10d
	cmovnel	%r9d, %r10d
	cmpb	$0, (%rbx)
	je	.LBB21_52
# %bb.32:                               # %if.then23.1
                                        #   in Loop: Header=BB21_28 Depth=1
	incl	%r13d
	jmp	.LBB21_53
.LBB21_3:
	movq	%rax, %rsi
	movl	$2128, %eax                     # imm = 0x850
	addq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rax
	cmpq	%rax, %rsi
	jne	.LBB21_6
	jmp	.LBB21_7
.LBB21_17:                              # %if.else24.epil
	addl	40(%r11,%rax), %r13d
.LBB21_18:                              # %for.cond.cleanup.loopexit
	andb	$1, %r15b
	testq	%rdx, %rdx
	movl	%r10d, -52(%rbp)                # 4-byte Spill
	je	.LBB21_20
.LBB21_33:                              # %do.body61
	movl	112(%rdx), %eax
	imull	44(%rdx), %eax
	subl	96(%rdx), %eax
	cmpl	%eax, %r12d
	jle	.LBB21_35
# %bb.34:                               # %if.then66
	leaq	.L.str.3(%rip), %rsi
	xorl	%edi, %edi
	movq	%r11, -72(%rbp)                 # 8-byte Spill
	movq	%r8, %r12
	callq	halide_print@PLT
	callq	abort@PLT
	movq	%r12, %r8
	movl	-52(%rbp), %r10d                # 4-byte Reload
	movq	-72(%rbp), %r11                 # 8-byte Reload
	movq	-64(%rbp), %rdx                 # 8-byte Reload
.LBB21_35:                              # %do.end69
	movl	%r14d, %eax
	orb	%r10b, %al
	testb	$1, %al
	movl	-48(%rbp), %esi                 # 4-byte Reload
	je	.LBB21_37
# %bb.36:                               # %if.then73
	incl	96(%rdx)
.LBB21_37:                              # %if.end77
	testl	%esi, %esi
	jle	.LBB21_44
# %bb.38:                               # %for.body83.lr.ph
	movq	16(%rbx), %rax
	testb	$1, %r8b
	je	.LBB21_40
# %bb.39:                               # %for.body83.prol
	decq	%r8
	movq	%r8, %rcx
	shlq	$7, %rcx
	movq	%rax, 64(%r11,%rcx)
	leaq	(%r11,%rcx), %rax
	movq	%r11, 72(%r11,%rcx)
	movl	%esi, 80(%r11,%rcx)
	movl	$0, 96(%r11,%rcx)
.LBB21_40:                              # %for.body83.prol.loopexit
	cmpl	$1, %esi
	je	.LBB21_43
# %bb.41:                               # %for.body83.preheader
	leaq	2(%r8), %rcx
	shlq	$7, %r8
	leaq	(%r8,%r11), %rdx
	addq	$-128, %rdx
	.p2align	4, 0x90
.LBB21_42:                              # %for.body83
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, 64(%rdx)
	movq	%r11, 72(%rdx)
	movl	%esi, 80(%rdx)
	movl	$0, 96(%rdx)
	leaq	-128(%rdx), %rax
	movq	%rdx, -64(%rdx)
	movq	%r11, -56(%rdx)
	movl	%esi, -48(%rdx)
	movl	$0, -32(%rdx)
	addq	$-2, %rcx
	addq	$-256, %rdx
	cmpq	$2, %rcx
	jg	.LBB21_42
.LBB21_43:                              # %for.cond80.for.cond.cleanup82_crit_edge
	movq	%r11, 16(%rbx)
.LBB21_44:                              # %for.cond.cleanup82
	movl	24(%rbx), %eax
	movl	64(%rbx), %ecx
	cmpl	%eax, %ecx
	movl	%r13d, %edx
	cmovll	%eax, %edx
	cmpl	$0, 68(%rbx)
	cmovnel	%eax, %edx
	cmpl	%ecx, %r13d
	cmovgl	%eax, %edx
	movl	%edx, 32(%rbx)
	leaq	40(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
	movl	32(%rbx), %eax
	cmpl	28(%rbx), %eax
	jle	.LBB21_47
# %bb.45:                               # %if.then107
	leaq	48(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
	testb	%r15b, %r15b
	je	.LBB21_47
# %bb.46:                               # %if.then109
	leaq	56(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
.LBB21_47:                              # %if.end111
	orb	-52(%rbp), %r14b                # 1-byte Folded Reload
	testb	$1, %r14b
	je	.LBB21_51
# %bb.48:                               # %if.then115
	movq	-64(%rbp), %rax                 # 8-byte Reload
	testq	%rax, %rax
	je	.LBB21_50
# %bb.49:                               # %if.then117
	decl	96(%rax)
	jmp	.LBB21_51
.LBB21_50:                              # %if.else120
	decl	2124(%rbx)
.LBB21_51:                              # %if.end123
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end21:
	.size	_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_, .Lfunc_end21-_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE # -- Begin function _ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,@function
_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE: # @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdi, %r15
	xorl	%r12d, %r12d
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	leaq	16(%r13), %rax
	movq	%rax, -72(%rbp)                 # 8-byte Spill
	jmp	.LBB22_3
.LBB22_1:                               # %land.lhs.true307
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpb	$0, 124(%rbx)
	movl	$0, %r12d
	je	.LBB22_3
	.p2align	4, 0x90
.LBB22_2:                               # %if.then310
                                        #   in Loop: Header=BB22_3 Depth=1
	leaq	56(%r13), %rdi
	callq	halide_cond_broadcast@PLT
	xorl	%r12d, %r12d
.LBB22_3:                               # %while.cond
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB22_9 Depth 2
                                        #     Child Loop BB22_31 Depth 2
                                        #       Child Loop BB22_42 Depth 3
                                        #     Child Loop BB22_15 Depth 2
                                        #       Child Loop BB22_23 Depth 3
                                        #     Child Loop BB22_47 Depth 2
                                        #       Child Loop BB22_50 Depth 3
                                        #         Child Loop BB22_51 Depth 4
                                        #     Child Loop BB22_81 Depth 2
	testq	%r15, %r15
	je	.LBB22_11
# %bb.4:                                # %cond.true
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpl	$0, 40(%r15)
	jne	.LBB22_6
# %bb.5:                                # %cond.end
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpl	$0, 112(%r15)
	je	.LBB22_112
.LBB22_6:                               # %if.then
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	16(%r13), %rbx
	cmpl	$0, 116(%r15)
	je	.LBB22_25
# %bb.7:                                # %if.then3
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpl	$0, 112(%r15)
	jne	.LBB22_28
# %bb.8:                                # %while.cond6.preheader
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpq	%r15, %rbx
	je	.LBB22_90
	.p2align	4, 0x90
.LBB22_9:                               # %while.body8
                                        #   Parent Loop BB22_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rbx, %rax
	movq	64(%rbx), %rbx
	cmpq	%r15, %rbx
	jne	.LBB22_9
# %bb.10:                               # %while.end.loopexit
                                        #   in Loop: Header=BB22_3 Depth=1
	addq	$64, %rax
	jmp	.LBB22_91
	.p2align	4, 0x90
.LBB22_11:                              # %cond.false
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpb	$0, 2120(%r13)
	jne	.LBB22_112
# %bb.12:                               # %do.end.thread
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	16(%r13), %rbx
	testq	%rbx, %rbx
	je	.LBB22_98
# %bb.13:                               # %do.end27.us.preheader
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	-72(%rbp), %r14                 # 8-byte Reload
	jmp	.LBB22_15
	.p2align	4, 0x90
.LBB22_14:                              # %cleanup.us
                                        #   in Loop: Header=BB22_15 Depth=2
	movq	%rbx, %r14
	movq	64(%rbx), %rax
	addq	$64, %r14
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB22_95
.LBB22_15:                              # %do.end27.us
                                        #   Parent Loop BB22_3 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB22_23 Depth 3
	movq	88(%rbx), %rcx
	testq	%rcx, %rcx
	je	.LBB22_20
# %bb.16:                               # %if.else32.us
                                        #   in Loop: Header=BB22_15 Depth=2
	movl	44(%rcx), %eax
	movl	112(%rcx), %edx
	testl	%edx, %edx
	je	.LBB22_18
# %bb.17:                               # %if.else38.us
                                        #   in Loop: Header=BB22_15 Depth=2
	imull	%edx, %eax
.LBB22_18:                              # %if.then35.us
                                        #   in Loop: Header=BB22_15 Depth=2
	subl	96(%rcx), %eax
	movl	44(%rbx), %ecx
	cmpb	$0, 48(%rbx)
	je	.LBB22_21
.LBB22_19:                              # %lor.rhs70.us
                                        #   in Loop: Header=BB22_15 Depth=2
	cmpl	%ecx, %eax
	setge	%cl
	cmpl	$0, 112(%rbx)
	sete	%al
	andb	%cl, %al
	testb	%al, %al
	je	.LBB22_14
	jmp	.LBB22_22
	.p2align	4, 0x90
.LBB22_20:                              # %if.then31.us
                                        #   in Loop: Header=BB22_15 Depth=2
	movl	24(%r13), %eax
	subl	2124(%r13), %eax
	incl	%eax
	movl	44(%rbx), %ecx
	cmpb	$0, 48(%rbx)
	jne	.LBB22_19
.LBB22_21:                              # %if.end45.us.lor.end73.us_crit_edge
                                        #   in Loop: Header=BB22_15 Depth=2
	cmpl	%ecx, %eax
	setge	%al
	testb	%al, %al
	je	.LBB22_14
.LBB22_22:                              # %if.then86.us
                                        #   in Loop: Header=BB22_15 Depth=2
	movl	120(%rbx), %eax
	cmpl	32(%rbx), %eax
	jge	.LBB22_44
	.p2align	4, 0x90
.LBB22_23:                              # %for.body.i.us
                                        #   Parent Loop BB22_3 Depth=1
                                        #     Parent Loop BB22_15 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	24(%rbx), %rcx
	cltq
	shlq	$4, %rax
	movq	(%rcx,%rax), %rdi
	movl	8(%rcx,%rax), %esi
	callq	halide_default_semaphore_try_acquire@PLT
	testb	%al, %al
	je	.LBB22_14
# %bb.24:                               # %for.inc.i.us
                                        #   in Loop: Header=BB22_23 Depth=3
	movl	120(%rbx), %eax
	incl	%eax
	movl	%eax, 120(%rbx)
	cmpl	32(%rbx), %eax
	jl	.LBB22_23
	jmp	.LBB22_44
	.p2align	4, 0x90
.LBB22_25:                              # %if.else
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	88(%r15), %rax
	testq	%rax, %rax
	je	.LBB22_28
# %bb.26:                               # %land.lhs.true
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	116(%rax), %eax
	testl	%eax, %eax
	je	.LBB22_28
# %bb.27:                               # %if.then15
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	%eax, 116(%r15)
	leaq	56(%r13), %rdi
	callq	halide_cond_broadcast@PLT
	jmp	.LBB22_3
	.p2align	4, 0x90
.LBB22_28:                              # %do.end
                                        #   in Loop: Header=BB22_3 Depth=1
	testq	%rbx, %rbx
	je	.LBB22_96
# %bb.29:                               # %do.end27.preheader
                                        #   in Loop: Header=BB22_3 Depth=1
	leaq	16(%r13), %r14
	jmp	.LBB22_31
	.p2align	4, 0x90
.LBB22_30:                              # %cleanup
                                        #   in Loop: Header=BB22_31 Depth=2
	movq	%rbx, %r14
	movq	64(%rbx), %rax
	addq	$64, %r14
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB22_95
.LBB22_31:                              # %do.end27
                                        #   Parent Loop BB22_3 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB22_42 Depth 3
	movq	88(%rbx), %rcx
	testq	%rcx, %rcx
	je	.LBB22_36
# %bb.32:                               # %if.else32
                                        #   in Loop: Header=BB22_31 Depth=2
	movl	44(%rcx), %eax
	movl	112(%rcx), %edx
	testl	%edx, %edx
	je	.LBB22_34
# %bb.33:                               # %if.else38
                                        #   in Loop: Header=BB22_31 Depth=2
	imull	%edx, %eax
.LBB22_34:                              # %if.end45
                                        #   in Loop: Header=BB22_31 Depth=2
	subl	96(%rcx), %eax
	cmpb	$0, 48(%rbx)
	je	.LBB22_37
.LBB22_35:                              # %lor.rhs70
                                        #   in Loop: Header=BB22_31 Depth=2
	cmpl	$0, 112(%rbx)
	setne	%sil
	jmp	.LBB22_38
	.p2align	4, 0x90
.LBB22_36:                              # %if.then31
                                        #   in Loop: Header=BB22_31 Depth=2
	movl	24(%r13), %eax
	subl	2124(%r13), %eax
	incl	%eax
	cmpb	$0, 48(%rbx)
	jne	.LBB22_35
.LBB22_37:                              # %if.end45.lor.end73_crit_edge
                                        #   in Loop: Header=BB22_31 Depth=2
	xorl	%esi, %esi
.LBB22_38:                              # %lor.end73
                                        #   in Loop: Header=BB22_31 Depth=2
	movl	44(%rbx), %edi
	movq	72(%r15), %rcx
	testl	%edi, %edi
	setne	%dl
	cmpq	%rcx, 72(%rbx)
	setne	%cl
	cmpl	%edi, %eax
	jl	.LBB22_30
# %bb.39:                               # %lor.end73
                                        #   in Loop: Header=BB22_31 Depth=2
	andb	%cl, %dl
	jne	.LBB22_30
# %bb.40:                               # %lor.end73
                                        #   in Loop: Header=BB22_31 Depth=2
	testb	%sil, %sil
	jne	.LBB22_30
# %bb.41:                               # %if.then86
                                        #   in Loop: Header=BB22_31 Depth=2
	movl	120(%rbx), %eax
	cmpl	32(%rbx), %eax
	jge	.LBB22_44
	.p2align	4, 0x90
.LBB22_42:                              # %for.body.i
                                        #   Parent Loop BB22_3 Depth=1
                                        #     Parent Loop BB22_31 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	24(%rbx), %rcx
	cltq
	shlq	$4, %rax
	movq	(%rcx,%rax), %rdi
	movl	8(%rcx,%rax), %esi
	callq	halide_default_semaphore_try_acquire@PLT
	testb	%al, %al
	je	.LBB22_30
# %bb.43:                               # %for.inc.i
                                        #   in Loop: Header=BB22_42 Depth=3
	movl	120(%rbx), %eax
	incl	%eax
	movl	%eax, 120(%rbx)
	cmpl	32(%rbx), %eax
	jl	.LBB22_42
.LBB22_44:                              # %if.else127
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	$0, 120(%rbx)
	incl	112(%rbx)
	movq	88(%rbx), %rax
	movl	44(%rbx), %ecx
	testq	%rax, %rax
	je	.LBB22_61
# %bb.45:                               # %if.else143
                                        #   in Loop: Header=BB22_3 Depth=1
	addl	%ecx, 96(%rax)
	cmpb	$0, 48(%rbx)
	je	.LBB22_62
.LBB22_46:                              # %if.then156
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	64(%rbx), %rax
	movq	%rax, (%r14)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%r14d, %r14d
	movl	$1, %r13d
	.p2align	4, 0x90
.LBB22_47:                              # %while.cond161.preheader
                                        #   Parent Loop BB22_3 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB22_50 Depth 3
                                        #         Child Loop BB22_51 Depth 4
	movl	40(%rbx), %ecx
	movl	%ecx, %eax
	subl	%r14d, %eax
	cmpl	%r13d, %eax
	jle	.LBB22_54
# %bb.48:                               # %land.rhs.preheader
                                        #   in Loop: Header=BB22_47 Depth=2
	movl	32(%rbx), %eax
	movl	120(%rbx), %edx
	jmp	.LBB22_50
	.p2align	4, 0x90
.LBB22_49:                              # %while.body167
                                        #   in Loop: Header=BB22_50 Depth=3
	movl	$0, 120(%rbx)
	incl	%r13d
	movl	%ecx, %esi
	subl	%r14d, %esi
	xorl	%edx, %edx
	cmpl	%r13d, %esi
	jle	.LBB22_55
.LBB22_50:                              # %land.rhs
                                        #   Parent Loop BB22_3 Depth=1
                                        #     Parent Loop BB22_47 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB22_51 Depth 4
	cmpl	%eax, %edx
	jge	.LBB22_49
	.p2align	4, 0x90
.LBB22_51:                              # %for.body.i483
                                        #   Parent Loop BB22_3 Depth=1
                                        #     Parent Loop BB22_47 Depth=2
                                        #       Parent Loop BB22_50 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	24(%rbx), %rax
	movslq	%edx, %rcx
	shlq	$4, %rcx
	movq	(%rax,%rcx), %rdi
	movl	8(%rax,%rcx), %esi
	callq	halide_default_semaphore_try_acquire@PLT
	testb	%al, %al
	je	.LBB22_54
# %bb.52:                               # %for.inc.i486
                                        #   in Loop: Header=BB22_51 Depth=4
	movl	32(%rbx), %eax
	movl	120(%rbx), %edx
	incl	%edx
	movl	%edx, 120(%rbx)
	cmpl	%eax, %edx
	jl	.LBB22_51
# %bb.53:                               # %while.body167.loopexit
                                        #   in Loop: Header=BB22_50 Depth=3
	movl	40(%rbx), %ecx
	jmp	.LBB22_49
	.p2align	4, 0x90
.LBB22_54:                              # %while.end169
                                        #   in Loop: Header=BB22_47 Depth=2
	testl	%r13d, %r13d
	je	.LBB22_57
.LBB22_55:                              # %if.end172
                                        #   in Loop: Header=BB22_47 Depth=2
	movq	104(%rbx), %rdi
	movl	36(%rbx), %edx
	addl	%r14d, %edx
	movq	(%rbx), %rsi
	movq	8(%rbx), %r8
	movl	%r13d, %ecx
	movq	%rbx, %r9
	callq	halide_do_loop_task@PLT
	addl	%r13d, %r14d
	xorl	%r13d, %r13d
	testl	%eax, %eax
	je	.LBB22_47
# %bb.56:                               #   in Loop: Header=BB22_3 Depth=1
	movl	%eax, %r12d
	jmp	.LBB22_58
.LBB22_57:                              #   in Loop: Header=BB22_3 Depth=1
	xorl	%r12d, %r12d
	movb	$1, %r13b
.LBB22_58:                              # %while.end179
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	addl	%r14d, 36(%rbx)
	movl	40(%rbx), %eax
	subl	%r14d, %eax
	movl	%eax, 40(%rbx)
	testb	%r13b, %r13b
	je	.LBB22_65
# %bb.59:                               # %if.else190
                                        #   in Loop: Header=BB22_3 Depth=1
	testl	%eax, %eax
	jle	.LBB22_92
# %bb.60:                               # %if.then194
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	movq	16(%r13), %rax
	movq	%rax, 64(%rbx)
	movq	%rbx, 16(%r13)
	jmp	.LBB22_76
.LBB22_61:                              # %if.then136
                                        #   in Loop: Header=BB22_3 Depth=1
	addl	%ecx, 2124(%r13)
	cmpb	$0, 48(%rbx)
	jne	.LBB22_46
.LBB22_62:                              # %if.else198
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	(%rbx), %rax
	movq	%rax, -64(%rbp)                 # 8-byte Spill
	movq	8(%rbx), %rax
	movq	%rax, -56(%rbp)                 # 8-byte Spill
	movl	36(%rbx), %r13d
	movq	56(%rbx), %r12
	movq	104(%rbx), %rax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	leal	1(%r13), %eax
	movl	%eax, 36(%rbx)
	decl	40(%rbx)
	je	.LBB22_66
# %bb.63:                               # %if.end210
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	testq	%r12, %r12
	je	.LBB22_67
.LBB22_64:                              # %if.then212
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	-48(%rbp), %rdi                 # 8-byte Reload
	movq	%r12, %rsi
	movl	%r13d, %edx
	movq	-56(%rbp), %rcx                 # 8-byte Reload
	callq	halide_do_task@PLT
	jmp	.LBB22_68
.LBB22_65:                              # %if.end230.thread505
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	$0, 40(%rbx)
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	jmp	.LBB22_69
.LBB22_66:                              # %if.then208
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	64(%rbx), %rax
	movq	%rax, (%r14)
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	testq	%r12, %r12
	jne	.LBB22_64
.LBB22_67:                              # %if.else220
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	-48(%rbp), %rdi                 # 8-byte Reload
	movq	-64(%rbp), %rsi                 # 8-byte Reload
	movl	%r13d, %edx
	movl	$1, %ecx
	movq	-56(%rbp), %r8                  # 8-byte Reload
	movq	%rbx, %r9
	callq	halide_do_loop_task@PLT
.LBB22_68:                              # %if.end230
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	%eax, %r12d
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	testl	%r12d, %r12d
	je	.LBB22_76
.LBB22_69:                              # %if.then238
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	%r12d, 116(%rbx)
	movl	80(%rbx), %r9d
	testl	%r9d, %r9d
	jle	.LBB22_76
# %bb.70:                               # %do.end243.lr.ph
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	72(%rbx), %r8
	cmpl	$1, %r9d
	jne	.LBB22_77
# %bb.71:                               #   in Loop: Header=BB22_3 Depth=1
	xorl	%esi, %esi
	xorl	%eax, %eax
.LBB22_72:                              # %if.end271.loopexit.unr-lcssa
                                        #   in Loop: Header=BB22_3 Depth=1
	testb	$1, %r9b
	je	.LBB22_93
# %bb.73:                               # %do.end243.epil
                                        #   in Loop: Header=BB22_3 Depth=1
	shlq	$7, %rsi
	cmpl	$0, 116(%r8,%rsi)
	je	.LBB22_74
	.p2align	4, 0x90
.LBB22_93:                              # %if.end271
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	88(%rbx), %rcx
	movl	44(%rbx), %edx
	testq	%rcx, %rcx
	je	.LBB22_107
.LBB22_94:                              # %if.else281
                                        #   in Loop: Header=BB22_3 Depth=1
	subl	%edx, 96(%rcx)
	jmp	.LBB22_108
	.p2align	4, 0x90
.LBB22_76:                              #   in Loop: Header=BB22_3 Depth=1
	xorl	%eax, %eax
	movq	88(%rbx), %rcx
	movl	44(%rbx), %edx
	testq	%rcx, %rcx
	jne	.LBB22_94
.LBB22_107:                             # %if.then274
                                        #   in Loop: Header=BB22_3 Depth=1
	subl	%edx, 2124(%r13)
.LBB22_108:                             # %if.end290
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	112(%rbx), %ecx
	decl	%ecx
	movl	%ecx, 112(%rbx)
	testb	$1, %al
	jne	.LBB22_2
# %bb.109:                              # %lor.lhs.false297
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	$0, %r12d
	testl	%ecx, %ecx
	jne	.LBB22_3
# %bb.110:                              # %land.lhs.true300
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpl	$0, 40(%rbx)
	je	.LBB22_1
# %bb.111:                              # %lor.lhs.false304
                                        #   in Loop: Header=BB22_3 Depth=1
	cmpl	$0, 116(%rbx)
	movl	$0, %r12d
	jne	.LBB22_1
	jmp	.LBB22_3
.LBB22_77:                              # %do.end243.lr.ph.new
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	%r9d, %edi
	andl	$-2, %edi
	leaq	116(%r8), %rcx
	xorl	%esi, %esi
	xorl	%eax, %eax
	jmp	.LBB22_81
.LBB22_78:                              # %land.rhs254.1
                                        #   in Loop: Header=BB22_81 Depth=2
	movzbl	136(%rcx), %edx
.LBB22_79:                              # %land.end260.1
                                        #   in Loop: Header=BB22_81 Depth=2
	andb	$1, %al
	orb	%dl, %al
.LBB22_80:                              # %for.inc.1
                                        #   in Loop: Header=BB22_81 Depth=2
	addq	$2, %rsi
	addq	$256, %rcx                      # imm = 0x100
	cmpq	%rsi, %rdi
	je	.LBB22_72
.LBB22_81:                              # %do.end243
                                        #   Parent Loop BB22_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpl	$0, (%rcx)
	je	.LBB22_83
# %bb.82:                               # %for.inc
                                        #   in Loop: Header=BB22_81 Depth=2
	cmpl	$0, 128(%rcx)
	jne	.LBB22_80
	jmp	.LBB22_87
	.p2align	4, 0x90
.LBB22_83:                              # %if.then247
                                        #   in Loop: Header=BB22_81 Depth=2
	movl	%r12d, (%rcx)
	cmpl	$0, 112(%rbx)
	je	.LBB22_85
# %bb.84:                               #   in Loop: Header=BB22_81 Depth=2
	xorl	%edx, %edx
	andb	$1, %al
	orb	%dl, %al
	cmpl	$0, 128(%rcx)
	jne	.LBB22_80
	jmp	.LBB22_87
.LBB22_85:                              # %land.rhs254
                                        #   in Loop: Header=BB22_81 Depth=2
	movzbl	8(%rcx), %edx
	andb	$1, %al
	orb	%dl, %al
	cmpl	$0, 128(%rcx)
	jne	.LBB22_80
	.p2align	4, 0x90
.LBB22_87:                              # %if.then247.1
                                        #   in Loop: Header=BB22_81 Depth=2
	movl	%r12d, 128(%rcx)
	cmpl	$0, 112(%rbx)
	je	.LBB22_78
# %bb.88:                               #   in Loop: Header=BB22_81 Depth=2
	xorl	%edx, %edx
	jmp	.LBB22_79
.LBB22_90:                              #   in Loop: Header=BB22_3 Depth=1
	leaq	16(%r13), %rax
.LBB22_91:                              # %while.end
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	64(%r15), %rcx
	movq	%rcx, (%rax)
	movl	$0, 40(%r15)
	jmp	.LBB22_3
.LBB22_92:                              #   in Loop: Header=BB22_3 Depth=1
	xorl	%eax, %eax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	movq	88(%rbx), %rcx
	movl	44(%rbx), %edx
	testq	%rcx, %rcx
	jne	.LBB22_94
	jmp	.LBB22_107
.LBB22_95:                              # %if.then103
                                        #   in Loop: Header=BB22_3 Depth=1
	testq	%r15, %r15
	je	.LBB22_98
.LBB22_96:                              # %if.then105
                                        #   in Loop: Header=BB22_3 Depth=1
	leal	1(%r12), %ebx
	cmpl	$39, %r12d
	jg	.LBB22_100
# %bb.97:                               # %if.then107
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	callq	halide_thread_yield@PLT
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movl	%ebx, %r12d
	jmp	.LBB22_3
.LBB22_98:                              # %if.else112
                                        #   in Loop: Header=BB22_3 Depth=1
	incl	64(%r13)
	movl	28(%r13), %eax
	cmpl	32(%r13), %eax
	jle	.LBB22_101
# %bb.99:                               # %if.then115
                                        #   in Loop: Header=BB22_3 Depth=1
	decl	%eax
	movl	%eax, 28(%r13)
	leaq	48(%r13), %rdi
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
	incl	28(%r13)
	decl	64(%r13)
	jmp	.LBB22_3
.LBB22_100:                             # %if.else108
                                        #   in Loop: Header=BB22_3 Depth=1
	incl	68(%r13)
	movb	$1, 124(%r15)
	leaq	56(%r13), %rdi
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
	movb	$0, 124(%r15)
	decl	68(%r13)
	movl	%ebx, %r12d
	jmp	.LBB22_3
.LBB22_101:                             # %if.else118
                                        #   in Loop: Header=BB22_3 Depth=1
	leal	1(%r12), %ebx
	cmpl	$39, %r12d
	jg	.LBB22_103
# %bb.102:                              # %if.then121
                                        #   in Loop: Header=BB22_3 Depth=1
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	callq	halide_thread_yield@PLT
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	jmp	.LBB22_104
.LBB22_103:                             # %if.else122
                                        #   in Loop: Header=BB22_3 Depth=1
	leaq	40(%r13), %rdi
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
.LBB22_104:                             # %if.end124
                                        #   in Loop: Header=BB22_3 Depth=1
	movl	%ebx, %r12d
	decl	64(%r13)
	jmp	.LBB22_3
.LBB22_74:                              # %if.then247.epil
                                        #   in Loop: Header=BB22_3 Depth=1
	leaq	(%r8,%rsi), %rcx
	addq	$116, %rcx
	movl	%r12d, (%rcx)
	cmpl	$0, 112(%rbx)
	je	.LBB22_105
# %bb.75:                               #   in Loop: Header=BB22_3 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB22_106
.LBB22_105:                             # %land.rhs254.epil
                                        #   in Loop: Header=BB22_3 Depth=1
	movb	124(%r8,%rsi), %cl
.LBB22_106:                             # %land.end260.epil
                                        #   in Loop: Header=BB22_3 Depth=1
	andb	$1, %al
	orb	%cl, %al
	movq	88(%rbx), %rcx
	movl	44(%rbx), %edx
	testq	%rcx, %rcx
	jne	.LBB22_94
	jmp	.LBB22_107
.LBB22_112:                             # %while.end316
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end22:
	.size	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE, .Lfunc_end22-_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
                                        # -- End function
	.section	.text.halide_mutex_unlock,"ax",@progbits
	.weak	halide_mutex_unlock             # -- Begin function halide_mutex_unlock
	.p2align	4, 0x90
	.type	halide_mutex_unlock,@function
halide_mutex_unlock:                    # @halide_mutex_unlock
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	xorl	%ecx, %ecx
	movl	$1, %eax
	lock		cmpxchgq	%rcx, (%rdi)
	je	.LBB23_3
# %bb.1:                                # %if.then.i
	movq	%rdi, %rsi
	xorl	%ecx, %ecx
	movl	$1, %eax
	lock		cmpxchgq	%rcx, (%rdi)
	je	.LBB23_3
# %bb.2:                                # %if.end.i.i
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE@GOTPCREL(%rip), %rax
	addq	$16, %rax
	movq	%rax, -16(%rbp)
	movq	%rsi, -8(%rbp)
	leaq	-16(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy@PLT
.LBB23_3:                               # %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end23:
	.size	halide_mutex_unlock, .Lfunc_end23-halide_mutex_unlock
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rsi, %r15
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movq	%rsi, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy@PLT
	movq	%rax, %rbx
	movq	8(%rax), %r12
	movq	%rax, %r13
	addq	$8, %r13
	xorl	%eax, %eax
	movq	%rax, -64(%rbp)                 # 8-byte Spill
                                        # implicit-def: $rax
                                        # kill: killed $rax
	jmp	.LBB24_2
	.p2align	4, 0x90
.LBB24_1:                               #   in Loop: Header=BB24_2 Depth=1
	leaq	144(%r12), %r13
	movq	%r12, -64(%rbp)                 # 8-byte Spill
	movq	%rax, %r12
	cmpq	%r15, %r14
	je	.LBB24_22
.LBB24_2:                               # %while.cond
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB24_6 Depth 2
                                        #     Child Loop BB24_12 Depth 2
	testq	%r12, %r12
	je	.LBB24_17
# %bb.3:                                # %while.body
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	136(%r12), %r14
	movq	144(%r12), %rax
	cmpq	%r15, %r14
	jne	.LBB24_1
# %bb.4:                                # %if.then
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	%rax, (%r13)
	cmpq	%r12, 16(%rbx)
	je	.LBB24_9
# %bb.5:                                # %while.cond7.preheader
                                        #   in Loop: Header=BB24_2 Depth=1
	testq	%rax, %rax
	je	.LBB24_10
	.p2align	4, 0x90
.LBB24_6:                               # %while.body9
                                        #   Parent Loop BB24_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	136(%rax), %rcx
	cmpq	%r15, %rcx
	je	.LBB24_8
# %bb.7:                                # %while.body9
                                        #   in Loop: Header=BB24_6 Depth=2
	movq	144(%rax), %rax
	testq	%rax, %rax
	jne	.LBB24_6
.LBB24_8:                               # %if.end.loopexit
                                        #   in Loop: Header=BB24_2 Depth=1
	cmpq	%r15, %rcx
	sete	%al
	jmp	.LBB24_11
.LBB24_9:                               # %if.then5
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	-64(%rbp), %rax                 # 8-byte Reload
	movq	%rax, 16(%rbx)
.LBB24_10:                              #   in Loop: Header=BB24_2 Depth=1
	xorl	%eax, %eax
.LBB24_11:                              # %if.end
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	movq	(%rdi), %rcx
	movzbl	%al, %edx
	movl	$1, %esi
	movq	%rdx, -48(%rbp)                 # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx
	callq	*16(%rcx)
	movq	%rax, 152(%r12)
	movq	%r12, %rdi
	callq	pthread_mutex_lock@PLT
	movq	(%rbx), %rax
	.p2align	4, 0x90
.LBB24_12:                              # %atomicrmw.start
                                        #   Parent Loop BB24_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB24_12
# %bb.13:                               # %atomicrmw.end
                                        #   in Loop: Header=BB24_2 Depth=1
	cmpq	$4, %rax
	jb	.LBB24_16
# %bb.14:                               # %atomicrmw.end
                                        #   in Loop: Header=BB24_2 Depth=1
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB24_16
# %bb.15:                               # %if.then.i65
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
.LBB24_16:                              # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66
                                        #   in Loop: Header=BB24_2 Depth=1
	movb	$0, 128(%r12)
	leaq	64(%r12), %rdi
	callq	pthread_cond_signal@PLT
	movq	%r12, %rdi
	callq	pthread_mutex_unlock@PLT
	cmpq	%r15, %r14
	jne	.LBB24_2
	jmp	.LBB24_22
.LBB24_17:                              # %while.end22
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	movq	(%rdi), %rax
	xorl	%esi, %esi
	xorl	%edx, %edx
	callq	*16(%rax)
	movq	(%rbx), %rax
	.p2align	4, 0x90
.LBB24_18:                              # %atomicrmw.start2
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB24_18
# %bb.19:                               # %atomicrmw.end1
	xorl	%ecx, %ecx
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	cmpq	$4, %rax
	jb	.LBB24_22
# %bb.20:                               # %atomicrmw.end1
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB24_22
# %bb.21:                               # %if.then.i
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
	xorl	%eax, %eax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
.LBB24_22:                              # %cleanup27
	movq	-48(%rbp), %rax                 # 8-byte Reload
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end24:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy, .Lfunc_end24-_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy,@function
_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy: # @_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movabsq	$-7046029254386353131, %rax     # imm = 0x9E3779B97F4A7C15
	imulq	%rdi, %rax
	shrq	$54, %rax
	leaq	(%rax,%rax,2), %rcx
	movq	_ZN6Halide7Runtime8Internal15Synchronization5tableE@GOTPCREL(%rip), %rdx
	leaq	(%rdx,%rcx,8), %rbx
	movl	$1, %esi
	xorl	%eax, %eax
	lock		cmpxchgq	%rsi, (%rdx,%rcx,8)
	je	.LBB25_2
# %bb.1:                                # %if.then.i
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv@PLT
.LBB25_2:                               # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end25:
	.size	_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy, .Lfunc_end25-_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv,@function
_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv: # @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r14
	movq	(%rdi), %rbx
	.p2align	4, 0x90
.LBB26_1:                               # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	cmpq	$4, %rbx
	jb	.LBB26_18
# %bb.2:                                # %while.cond
                                        #   in Loop: Header=BB26_1 Depth=1
	movl	%ebx, %eax
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB26_18
# %bb.3:                                # %if.end
                                        #   in Loop: Header=BB26_1 Depth=1
	movq	%rbx, %rcx
	orq	$2, %rcx
	movq	%rbx, %rax
	lock		cmpxchgq	%rcx, (%r14)
	movq	%rax, %rbx
	jne	.LBB26_1
	jmp	.LBB26_4
	.p2align	4, 0x90
.LBB26_11:                              #   in Loop: Header=BB26_4 Depth=1
	movq	%rax, %rbx
	#MEMBARRIER
	jmp	.LBB26_4
	.p2align	4, 0x90
.LBB26_5:                               # %while.body17.preheader
                                        #   in Loop: Header=BB26_4 Depth=1
	movq	-48(%rbp), %r15                 # 8-byte Reload
	jmp	.LBB26_6
	.p2align	4, 0x90
.LBB26_8:                               # %do.end
                                        #   in Loop: Header=BB26_6 Depth=2
	movq	%r15, 144(%r13)
	movq	152(%r13), %r12
	movq	%r13, %r15
	testq	%r12, %r12
	jne	.LBB26_9
.LBB26_6:                               # %while.body17
                                        #   Parent Loop BB26_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	136(%r15), %r13
	testq	%r13, %r13
	jne	.LBB26_8
# %bb.7:                                # %if.then20
                                        #   in Loop: Header=BB26_6 Depth=2
	xorl	%edi, %edi
	leaq	.L.str.5(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB26_8
	.p2align	4, 0x90
.LBB26_4:                               # %while.cond11
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_6 Depth 2
                                        #     Child Loop BB26_13 Depth 2
	movq	%rbx, %rax
	andq	$-4, %rax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	movq	152(%rax), %r12
	testq	%r12, %r12
	je	.LBB26_5
.LBB26_9:                               # %while.end23
                                        #   in Loop: Header=BB26_4 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	%r12, 152(%rax)
	testb	$1, %bl
	jne	.LBB26_10
# %bb.12:                               # %if.end35
                                        #   in Loop: Header=BB26_4 Depth=1
	movq	144(%r12), %rax
	testq	%rax, %rax
	jne	.LBB26_16
	.p2align	4, 0x90
.LBB26_13:                              # %while.body41
                                        #   Parent Loop BB26_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%ebx, %ecx
	andl	$1, %ecx
	movq	%rbx, %rax
	lock		cmpxchgq	%rcx, (%r14)
	je	.LBB26_17
# %bb.14:                               # %if.end47
                                        #   in Loop: Header=BB26_13 Depth=2
	movq	%rax, %rbx
	cmpq	$4, %rax
	jb	.LBB26_13
# %bb.15:                               # %cleanup70
                                        #   in Loop: Header=BB26_4 Depth=1
	#MEMBARRIER
	jmp	.LBB26_4
	.p2align	4, 0x90
.LBB26_10:                              # %if.then27
                                        #   in Loop: Header=BB26_4 Depth=1
	movq	%rbx, %rcx
	andq	$-3, %rcx
	movq	%rbx, %rax
	lock		cmpxchgq	%rcx, (%r14)
	jne	.LBB26_11
.LBB26_18:                              # %cleanup75
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB26_16:                              # %if.else62
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	movq	%rax, 152(%rcx)
	lock		andq	$-3, (%r14)
.LBB26_17:                              # %if.end66
	movq	%r12, %rdi
	callq	pthread_mutex_lock@PLT
	movb	$0, 128(%r12)
	leaq	64(%r12), %rdi
	callq	pthread_cond_signal@PLT
	movq	%r12, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	pthread_mutex_unlock@PLT        # TAILCALL
.Lfunc_end26:
	.size	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv, .Lfunc_end26-_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv
.LCPI27_0:
	.zero	16
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv,@function
_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv: # @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	movq	%rdi, %r12
	movq	(%rdi), %rbx
	movl	$40, %r13d
	leaq	-144(%rbp), %r14
	leaq	-208(%rbp), %r15
	jmp	.LBB27_1
.LBB27_17:                              # %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%r15, %rdi
	callq	pthread_mutex_unlock@PLT
	movq	(%r12), %rbx
	movl	$40, %r13d
.LBB27_18:                              # %if.end22
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%r14, %rdi
	callq	pthread_cond_destroy@PLT
	movq	%r15, %rdi
	callq	pthread_mutex_destroy@PLT
	.p2align	4, 0x90
.LBB27_1:                               # %while.cond.outer
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB27_15 Depth 2
	testb	$1, %bl
	jne	.LBB27_4
# %bb.2:                                # %if.then
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%rbx, %rcx
	orq	$1, %rcx
	movq	%rbx, %rax
	lock		cmpxchgq	%rcx, (%r12)
	je	.LBB27_19
# %bb.3:                                # %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%rax, %rbx
	jmp	.LBB27_1
.LBB27_4:                               # %if.end4
                                        #   in Loop: Header=BB27_1 Depth=1
	testl	%r13d, %r13d
	jle	.LBB27_5
# %bb.8:                                # %if.end4
                                        #   in Loop: Header=BB27_1 Depth=1
	cmpq	$4, %rbx
	jb	.LBB27_5
# %bb.9:                                # %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit
                                        #   in Loop: Header=BB27_1 Depth=1
	movl	$0, -44(%rbp)                   # 4-byte Folded Spill
	cmpl	$2, %r13d
	jl	.LBB27_6
# %bb.10:                               # %if.then7
                                        #   in Loop: Header=BB27_1 Depth=1
	decl	%r13d
	callq	halide_thread_yield@PLT
	movq	(%r12), %rbx
	jmp	.LBB27_1
.LBB27_5:                               #   in Loop: Header=BB27_1 Depth=1
	movl	%r13d, -44(%rbp)                # 4-byte Spill
.LBB27_6:                               # %if.end9
                                        #   in Loop: Header=BB27_1 Depth=1
	movb	$0, -80(%rbp)
	movq	%r15, %rdi
	xorl	%esi, %esi
	callq	pthread_mutex_init@PLT
	movq	%r14, %rdi
	xorl	%esi, %esi
	callq	pthread_cond_init@PLT
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, -72(%rbp)
	movq	$0, -56(%rbp)
	movb	$1, -80(%rbp)
	movq	%rbx, %rax
	andq	$-4, %rax
	je	.LBB27_7
# %bb.11:                               # %if.else
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%rax, -72(%rbp)
	jmp	.LBB27_12
.LBB27_7:                               # %if.then12
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%r15, -56(%rbp)
.LBB27_12:                              # %if.end13
                                        #   in Loop: Header=BB27_1 Depth=1
	movl	-44(%rbp), %r13d                # 4-byte Reload
	movl	%ebx, %ecx
	andl	$3, %ecx
	orq	%r15, %rcx
	movq	%rbx, %rax
	lock		cmpxchgq	%rcx, (%r12)
	jne	.LBB27_13
# %bb.14:                               # %if.then19
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%r15, %rdi
	callq	pthread_mutex_lock@PLT
	cmpb	$0, -80(%rbp)
	je	.LBB27_17
	.p2align	4, 0x90
.LBB27_15:                              # %while.body.i
                                        #   Parent Loop BB27_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	pthread_cond_wait@PLT
	cmpb	$0, -80(%rbp)
	jne	.LBB27_15
	jmp	.LBB27_17
.LBB27_13:                              # %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit
                                        #   in Loop: Header=BB27_1 Depth=1
	movq	%rax, %rbx
	jmp	.LBB27_18
.LBB27_19:                              # %cleanup23
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end27:
	.size	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv, .Lfunc_end27-_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE,@function
_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE: # @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	8(%rdi), %rax
	movq	(%rax), %rax
	cmpq	$3, %rax
	sete	%al
	popq	%rbp
	retq
.Lfunc_end28:
	.size	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE, .Lfunc_end28-_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end29:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv, .Lfunc_end29-_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib,@function
_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib: # @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%edx, %eax
	addq	%rax, %rax
	movq	8(%rdi), %rcx
	movq	%rax, (%rcx)
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end30:
	.size	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib, .Lfunc_end30-_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end31:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb, .Lfunc_end31-_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
                                        # -- End function
	.section	.text.halide_cond_broadcast,"ax",@progbits
	.weak	halide_cond_broadcast           # -- Begin function halide_cond_broadcast
	.p2align	4, 0x90
	.type	halide_cond_broadcast,@function
halide_cond_broadcast:                  # @halide_cond_broadcast
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	movq	(%rdi), %rdx
	testq	%rdx, %rdx
	je	.LBB32_2
# %bb.1:                                # %if.end.i
	movq	%rdi, %rsi
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE@GOTPCREL(%rip), %rax
	addq	$16, %rax
	movq	%rax, -24(%rbp)
	movq	%rdi, -16(%rbp)
	movq	%rdx, -8(%rbp)
	leaq	-24(%rbp), %rdi
	xorl	%ecx, %ecx
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy@PLT
.LBB32_2:                               # %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond9broadcastEv.exit
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end32:
	.size	halide_cond_broadcast, .Lfunc_end32-halide_cond_broadcast
                                        # -- End function
	.section	.text.halide_default_semaphore_try_acquire,"ax",@progbits
	.weak	halide_default_semaphore_try_acquire # -- Begin function halide_default_semaphore_try_acquire
	.p2align	4, 0x90
	.type	halide_default_semaphore_try_acquire,@function
halide_default_semaphore_try_acquire:   # @halide_default_semaphore_try_acquire
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	testl	%esi, %esi
	je	.LBB33_1
# %bb.2:                                # %if.end
	movl	(%rdi), %eax
	movl	%eax, %edx
	subl	%esi, %edx
	js	.LBB33_3
	.p2align	4, 0x90
.LBB33_4:                               # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	lock		cmpxchgl	%edx, (%rdi)
	sete	%cl
	je	.LBB33_6
# %bb.5:                                # %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit
                                        #   in Loop: Header=BB33_4 Depth=1
	movl	%eax, %edx
	subl	%esi, %edx
	jns	.LBB33_4
.LBB33_6:                               # %return
	movl	%ecx, %eax
	popq	%rbp
	retq
.LBB33_1:
	movb	$1, %cl
	movl	%ecx, %eax
	popq	%rbp
	retq
.LBB33_3:
	xorl	%ecx, %ecx
	movl	%ecx, %eax
	popq	%rbp
	retq
.Lfunc_end33:
	.size	halide_default_semaphore_try_acquire, .Lfunc_end33-halide_default_semaphore_try_acquire
                                        # -- End function
	.section	.text.halide_cond_wait,"ax",@progbits
	.weak	halide_cond_wait                # -- Begin function halide_cond_wait
	.p2align	4, 0x90
	.type	halide_cond_wait,@function
halide_cond_wait:                       # @halide_cond_wait
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$48, %rsp
	movq	%rsi, %rbx
	movq	%rdi, %rsi
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE@GOTPCREL(%rip), %rax
	addq	$16, %rax
	movq	%rax, -72(%rbp)
	movq	%rdi, -64(%rbp)
	movq	%rbx, -56(%rbp)
	leaq	-72(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy@PLT
	cmpq	%rbx, %rax
	jne	.LBB34_1
# %bb.14:                               # %if.else.i
	movq	(%rbx), %rax
	testb	$1, %al
	jne	.LBB34_16
# %bb.15:                               # %if.then2.i
	leaq	.L.str.5.6(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB34_16
.LBB34_1:                               # %if.then.i
	movl	$1, %ecx
	xorl	%eax, %eax
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB34_2
.LBB34_16:                              # %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit
	addq	$48, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB34_2:                               # %if.then.i.i
	movq	(%rbx), %rax
	movl	$40, %r12d
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE@GOTPCREL(%rip), %r15
	addq	$16, %r15
	leaq	-48(%rbp), %r14
	.p2align	4, 0x90
.LBB34_3:                               # %while.cond.outer.i.i.i
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	jne	.LBB34_5
# %bb.4:                                # %if.then.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	movq	%rax, %rcx
	orq	$1, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB34_3
	jmp	.LBB34_16
.LBB34_5:                               # %if.end4.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	testl	%r12d, %r12d
	jg	.LBB34_10
# %bb.6:                                #   in Loop: Header=BB34_3 Depth=1
	movl	%r12d, %ecx
.LBB34_7:                               # %if.end8.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	testb	$2, %al
	jne	.LBB34_12
# %bb.8:                                # %if.then10.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	movq	%rax, %rdx
	orq	$2, %rdx
	lock		cmpxchgq	%rdx, (%rbx)
	jne	.LBB34_9
.LBB34_12:                              # %if.end19.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	movq	%r15, -48(%rbp)
	movq	%rbx, -40(%rbp)
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy@PLT
	cmpq	%rbx, %rax
	je	.LBB34_16
# %bb.13:                               # %if.end24.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	movq	(%rbx), %rax
	movl	$40, %r12d
	jmp	.LBB34_3
.LBB34_10:                              # %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	xorl	%ecx, %ecx
	cmpl	$1, %r12d
	je	.LBB34_7
# %bb.11:                               # %if.then6.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	decl	%r12d
	callq	halide_thread_yield@PLT
	movq	(%rbx), %rax
	jmp	.LBB34_3
.LBB34_9:                               # %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i
                                        #   in Loop: Header=BB34_3 Depth=1
	movl	%ecx, %r12d
	jmp	.LBB34_3
.Lfunc_end34:
	.size	halide_cond_wait, .Lfunc_end34-halide_cond_wait
                                        # -- End function
	.section	.text.halide_do_loop_task,"ax",@progbits
	.weak	halide_do_loop_task             # -- Begin function halide_do_loop_task
	.p2align	4, 0x90
	.type	halide_do_loop_task,@function
halide_do_loop_task:                    # @halide_do_loop_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end35:
	.size	halide_do_loop_task, .Lfunc_end35-halide_do_loop_task
                                        # -- End function
	.section	.text.halide_do_task,"ax",@progbits
	.weak	halide_do_task                  # -- Begin function halide_do_task
	.p2align	4, 0x90
	.type	halide_do_task,@function
halide_do_task:                         # @halide_do_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal14custom_do_taskE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end36:
	.size	halide_do_task, .Lfunc_end36-halide_do_task
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	movq	%rsi, %r13
	movq	%rdi, %r14
	movb	$0, -88(%rbp)
	leaq	-216(%rbp), %r12
	movq	%r12, %rdi
	xorl	%esi, %esi
	callq	pthread_mutex_init@PLT
	leaq	-152(%rbp), %r15
	movq	%r15, %rdi
	xorl	%esi, %esi
	callq	pthread_cond_init@PLT
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, -80(%rbp)
	movq	$0, -64(%rbp)
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy@PLT
	movq	%rax, %rbx
	movb	$0, -56(%rbp)
	movq	$0, -48(%rbp)
	movq	(%r14), %rax
	leaq	-56(%rbp), %rsi
	movq	%r14, %rdi
	callq	*(%rax)
	testb	%al, %al
	je	.LBB37_1
# %bb.6:                                # %if.end
	movq	$0, -72(%rbp)
	movq	%r13, -80(%rbp)
	movb	$1, -88(%rbp)
	movq	%rbx, %rax
	addq	$8, %rax
	movl	$144, %ecx
	addq	16(%rbx), %rcx
	cmpq	$0, 8(%rbx)
	cmoveq	%rax, %rcx
	movq	%r12, (%rcx)
	movq	%r12, 16(%rbx)
	movq	(%rbx), %rax
	.p2align	4, 0x90
.LBB37_7:                               # %atomicrmw.start2
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB37_7
# %bb.8:                                # %atomicrmw.end1
	cmpq	$4, %rax
	jb	.LBB37_11
# %bb.9:                                # %atomicrmw.end1
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB37_11
# %bb.10:                               # %if.then.i28
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
.LBB37_11:                              # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29
	movq	(%r14), %rax
	movq	%r14, %rdi
	callq	*8(%rax)
	leaq	-216(%rbp), %rdi
	callq	pthread_mutex_lock@PLT
	cmpb	$0, -88(%rbp)
	je	.LBB37_14
# %bb.12:                               # %while.body.i.preheader
	leaq	-216(%rbp), %rbx
	.p2align	4, 0x90
.LBB37_13:                              # %while.body.i
                                        # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	pthread_cond_wait@PLT
	cmpb	$0, -88(%rbp)
	jne	.LBB37_13
.LBB37_14:                              # %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit
	leaq	-216(%rbp), %rdi
	callq	pthread_mutex_unlock@PLT
	leaq	-64(%rbp), %r14
	jmp	.LBB37_15
.LBB37_1:                               # %if.then
	leaq	-48(%rbp), %r14
	movq	(%rbx), %rax
	.p2align	4, 0x90
.LBB37_2:                               # %atomicrmw.start
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rbx)
	jne	.LBB37_2
# %bb.3:                                # %atomicrmw.end
	cmpq	$4, %rax
	jb	.LBB37_15
# %bb.4:                                # %atomicrmw.end
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB37_15
# %bb.5:                                # %if.then.i
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
.LBB37_15:                              # %cleanup
	movq	(%r14), %rbx
	movq	%r15, %rdi
	callq	pthread_cond_destroy@PLT
	leaq	-216(%rbp), %rdi
	callq	pthread_mutex_destroy@PLT
	movq	%rbx, %rax
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end37:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy, .Lfunc_end37-_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE,@function
_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE: # @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	8(%rdi), %rax
	movq	(%rax), %rdx
	movq	16(%rdi), %rcx
	testq	%rdx, %rdx
	je	.LBB38_1
# %bb.2:                                # %if.else
	movb	$1, %al
	cmpq	%rcx, %rdx
	je	.LBB38_4
# %bb.3:                                # %if.then5
	movq	%rcx, 8(%rsi)
	xorl	%eax, %eax
.LBB38_4:                               # %cleanup
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.LBB38_1:                               # %if.then
	movq	%rcx, (%rax)
	movb	$1, %al
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.Lfunc_end38:
	.size	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE, .Lfunc_end38-_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv,@function
_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv: # @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	movq	16(%rdi), %rsi
	xorl	%ecx, %ecx
	movl	$1, %eax
	lock		cmpxchgq	%rcx, (%rsi)
	je	.LBB39_3
# %bb.1:                                # %if.then.i
	xorl	%ecx, %ecx
	movl	$1, %eax
	lock		cmpxchgq	%rcx, (%rsi)
	je	.LBB39_3
# %bb.2:                                # %if.end.i.i
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE@GOTPCREL(%rip), %rax
	addq	$16, %rax
	movq	%rax, -16(%rbp)
	movq	%rsi, -8(%rbp)
	leaq	-16(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy@PLT
.LBB39_3:                               # %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end39:
	.size	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv, .Lfunc_end39-_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib,@function
_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib: # @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib
# %bb.0:                                # %entry
	testl	%edx, %edx
	jne	.LBB40_2
# %bb.1:                                # %if.then
	pushq	%rbp
	movq	%rsp, %rbp
	movq	8(%rdi), %rax
	movq	$0, (%rax)
	popq	%rbp
.LBB40_2:                               # %if.end
	xorl	%eax, %eax
	retq
.Lfunc_end40:
	.size	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib, .Lfunc_end40-_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rcx, %r14
	movq	%rdx, %r12
	movq	%rsi, %rbx
	movq	%rdi, %r15
	leaq	-56(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy@PLT
	movb	$0, -72(%rbp)
	movq	$0, -64(%rbp)
	movq	(%r15), %rax
	leaq	-72(%rbp), %rsi
	movq	%r15, %rdi
	callq	*(%rax)
	testb	%al, %al
	je	.LBB41_1
# %bb.2:                                # %if.end
	movq	-56(%rbp), %rdx
	movq	8(%rdx), %rcx
	testq	%rcx, %rcx
	je	.LBB41_3
# %bb.4:                                # %while.body.preheader
	addq	$8, %rdx
	xorl	%r13d, %r13d
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%edi, %edi
	jmp	.LBB41_5
	.p2align	4, 0x90
.LBB41_6:                               #   in Loop: Header=BB41_5 Depth=1
	leaq	144(%rsi), %rdx
	movq	%rsi, %rdi
.LBB41_15:                              # %if.end22
                                        #   in Loop: Header=BB41_5 Depth=1
	testq	%rcx, %rcx
	je	.LBB41_16
.LBB41_5:                               # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rcx, %rsi
	movq	136(%rcx), %rax
	movq	144(%rcx), %rcx
	cmpq	%rbx, %rax
	jne	.LBB41_6
# %bb.7:                                # %if.then4
                                        #   in Loop: Header=BB41_5 Depth=1
	movq	%rcx, (%rdx)
	movq	-56(%rbp), %rax
	cmpq	%rsi, 16(%rax)
	je	.LBB41_8
# %bb.9:                                # %if.end10
                                        #   in Loop: Header=BB41_5 Depth=1
	testq	%r13, %r13
	je	.LBB41_10
	jmp	.LBB41_12
.LBB41_8:                               # %if.then7
                                        #   in Loop: Header=BB41_5 Depth=1
	movq	%rdi, 16(%rax)
	testq	%r13, %r13
	jne	.LBB41_12
.LBB41_10:                              # %if.end10
                                        #   in Loop: Header=BB41_5 Depth=1
	cmpb	$0, -72(%rbp)
	je	.LBB41_12
# %bb.11:                               #   in Loop: Header=BB41_5 Depth=1
	movq	%rsi, %r13
	jmp	.LBB41_15
	.p2align	4, 0x90
.LBB41_12:                              # %if.else
                                        #   in Loop: Header=BB41_5 Depth=1
	movq	%rsi, %rax
	testq	%r9, %r9
	je	.LBB41_14
# %bb.13:                               # %if.else15
                                        #   in Loop: Header=BB41_5 Depth=1
	movq	%rsi, 144(%r8)
	movq	%r9, %rax
.LBB41_14:                              # %if.end17
                                        #   in Loop: Header=BB41_5 Depth=1
	movq	%r12, 136(%rsi)
	movq	%rax, %r9
	movq	%rsi, %r8
	jmp	.LBB41_15
.LBB41_1:                               # %if.then
	leaq	-56(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE@PLT
	xorl	%eax, %eax
	jmp	.LBB41_26
.LBB41_16:                              # %while.end
	testq	%r9, %r9
	je	.LBB41_17
# %bb.18:                               # %if.then24
	movq	$0, 144(%r8)
	movq	-48(%rbp), %rcx
	cmpq	$0, 8(%rcx)
	je	.LBB41_19
# %bb.20:                               # %if.else31
	movl	$144, %eax
	addq	16(%rcx), %rax
	jmp	.LBB41_21
.LBB41_3:
	xorl	%eax, %eax
	xorl	%r13d, %r13d
	jmp	.LBB41_22
.LBB41_17:
	xorl	%eax, %eax
	jmp	.LBB41_22
.LBB41_19:
	leaq	8(%rcx), %rax
.LBB41_21:                              # %if.end35
	movq	%r9, (%rax)
	movq	%r8, 16(%rcx)
	movb	$1, %al
.LBB41_22:                              # %if.end38
	xorl	%edx, %edx
	testq	%r13, %r13
	setne	%dl
	movq	(%r15), %rbx
	movzbl	%al, %ecx
	leaq	-72(%rbp), %rsi
	movq	%r15, %rdi
	callq	*24(%rbx)
	testq	%r13, %r13
	je	.LBB41_24
# %bb.23:                               # %if.then44
	movq	%r14, 152(%r13)
	movq	%r13, %rdi
	callq	pthread_mutex_lock@PLT
	leaq	-56(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE@PLT
	movb	$0, 128(%r13)
	leaq	64(%r13), %rdi
	callq	pthread_cond_signal@PLT
	movq	%r13, %rdi
	callq	pthread_mutex_unlock@PLT
	jmp	.LBB41_25
.LBB41_24:                              # %if.else48
	leaq	-56(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE@PLT
.LBB41_25:                              # %if.end49
	testq	%r13, %r13
	setne	%al
	andb	-72(%rbp), %al
	movzbl	%al, %eax
.LBB41_26:                              # %cleanup
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end41:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy, .Lfunc_end41-_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy,@function
_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy: # @_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %r14
	movabsq	$-7046029254386353131, %rax     # imm = 0x9E3779B97F4A7C15
	imulq	%rax, %rsi
	shrq	$54, %rsi
	imulq	%rax, %rdx
	shrq	$54, %rdx
	cmpq	%rdx, %rsi
	jne	.LBB42_4
# %bb.1:                                # %if.then
	leaq	(%rsi,%rsi,2), %rcx
	movq	_ZN6Halide7Runtime8Internal15Synchronization5tableE@GOTPCREL(%rip), %rdx
	leaq	(%rdx,%rcx,8), %rbx
	movl	$1, %esi
	xorl	%eax, %eax
	lock		cmpxchgq	%rsi, (%rdx,%rcx,8)
	jne	.LBB42_3
# %bb.2:
	movq	%rbx, %r15
	jmp	.LBB42_14
.LBB42_4:                               # %if.else
	jae	.LBB42_9
# %bb.5:                                # %if.then3
	leaq	(%rsi,%rsi,2), %rcx
	movq	_ZN6Halide7Runtime8Internal15Synchronization5tableE@GOTPCREL(%rip), %rsi
	leaq	(%rsi,%rcx,8), %rbx
	leaq	(%rdx,%rdx,2), %rax
	leaq	(%rsi,%rax,8), %r15
	movl	$1, %r12d
	xorl	%eax, %eax
	lock		cmpxchgq	%r12, (%rsi,%rcx,8)
	je	.LBB42_7
# %bb.6:                                # %if.then.i53
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv@PLT
.LBB42_7:                               # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54
	xorl	%eax, %eax
	lock		cmpxchgq	%r12, (%r15)
	je	.LBB42_14
# %bb.8:                                # %if.then.i50
	movq	%r15, %rdi
	jmp	.LBB42_13
.LBB42_3:                               # %if.then.i43
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv@PLT
	movq	%rbx, %r15
	jmp	.LBB42_14
.LBB42_9:                               # %if.else9
	leaq	(%rdx,%rdx,2), %rcx
	movq	_ZN6Halide7Runtime8Internal15Synchronization5tableE@GOTPCREL(%rip), %rdx
	leaq	(%rdx,%rcx,8), %r15
	leaq	(%rsi,%rsi,2), %rax
	leaq	(%rdx,%rax,8), %rbx
	movl	$1, %r12d
	xorl	%eax, %eax
	lock		cmpxchgq	%r12, (%rdx,%rcx,8)
	je	.LBB42_11
# %bb.10:                               # %if.then.i40
	movq	%r15, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv@PLT
.LBB42_11:                              # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41
	xorl	%eax, %eax
	lock		cmpxchgq	%r12, (%rbx)
	je	.LBB42_14
# %bb.12:                               # %if.then.i
	movq	%rbx, %rdi
.LBB42_13:                              # %cleanup
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv@PLT
.LBB42_14:                              # %cleanup
	movq	%rbx, (%r14)
	movq	%r15, 8(%r14)
	movq	%r14, %rax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end42:
	.size	_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy, .Lfunc_end42-_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE,@function
_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE: # @_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	(%rdi), %rdi
	movq	8(%rbx), %rcx
	cmpq	%rcx, %rdi
	je	.LBB43_1
# %bb.6:                                # %if.else
	jbe	.LBB43_14
# %bb.7:                                # %if.then5
	movq	(%rdi), %rax
	.p2align	4, 0x90
.LBB43_8:                               # %atomicrmw.start2
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB43_8
# %bb.9:                                # %atomicrmw.end1
	cmpq	$4, %rax
	jb	.LBB43_12
# %bb.10:                               # %atomicrmw.end1
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB43_12
# %bb.11:                               # %if.then.i30
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
.LBB43_12:                              # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31
	movq	8(%rbx), %rdi
	movq	(%rdi), %rax
	.p2align	4, 0x90
.LBB43_13:                              # %atomicrmw.start8
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB43_13
	jmp	.LBB43_3
.LBB43_1:                               # %if.then
	movq	(%rdi), %rax
	.p2align	4, 0x90
.LBB43_2:                               # %atomicrmw.start
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB43_2
	jmp	.LBB43_3
.LBB43_14:                              # %if.else10
	movq	(%rcx), %rax
	.p2align	4, 0x90
.LBB43_15:                              # %atomicrmw.start14
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rdx
	andq	$-2, %rdx
	lock		cmpxchgq	%rdx, (%rcx)
	jne	.LBB43_15
# %bb.16:                               # %atomicrmw.end13
	cmpq	$4, %rax
	jb	.LBB43_19
# %bb.17:                               # %atomicrmw.end13
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB43_19
# %bb.18:                               # %if.then.i44
	movq	%rcx, %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT
.LBB43_19:                              # %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45
	movq	(%rbx), %rdi
	movq	(%rdi), %rax
	.p2align	4, 0x90
.LBB43_20:                              # %atomicrmw.start20
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	andq	$-2, %rcx
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB43_20
.LBB43_3:                               # %atomicrmw.end
	cmpq	$4, %rax
	jb	.LBB43_21
# %bb.4:                                # %atomicrmw.end
	andl	$2, %eax
	testq	%rax, %rax
	jne	.LBB43_21
# %bb.5:                                # %if.then.i
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv@PLT # TAILCALL
.LBB43_21:                              # %if.end15
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end43:
	.size	_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE, .Lfunc_end43-_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE,@function
_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE: # @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	8(%rdi), %rax
	movq	(%rax), %r9
	movq	16(%rdi), %rdx
	cmpq	%rdx, %r9
	jne	.LBB44_6
# %bb.1:                                # %if.end
	movq	$0, (%rax)
	movq	16(%rdi), %rdi
	movq	(%rdi), %rax
	movb	$1, %r8b
	.p2align	4, 0x90
.LBB44_2:                               # %if.end
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	je	.LBB44_5
# %bb.3:                                # %if.end.i
                                        #   in Loop: Header=BB44_2 Depth=1
	movq	%rax, %rcx
	orq	$2, %rcx
	lock		cmpxchgq	%rcx, (%rdi)
	jne	.LBB44_2
# %bb.4:
	xorl	%r8d, %r8d
.LBB44_5:                               # %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit
	movb	%r8b, (%rsi)
.LBB44_6:                               # %cleanup
	cmpq	%rdx, %r9
	sete	%al
	popq	%rbp
	retq
.Lfunc_end44:
	.size	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE, .Lfunc_end44-_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end45:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib, .Lfunc_end45-_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb,@function
_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb: # @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpb	$0, (%rsi)
	je	.LBB46_3
# %bb.1:                                # %entry
	testb	%cl, %cl
	je	.LBB46_3
# %bb.2:                                # %if.then
	movq	16(%rdi), %rax
	lock		orq	$2, (%rax)
.LBB46_3:                               # %if.end
	popq	%rbp
	retq
.Lfunc_end46:
	.size	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb, .Lfunc_end46-_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv # -- Begin function _ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,@function
_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv: # @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.1(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	jne	.LBB47_2
# %bb.1:                                # %if.end
	leaq	.L.str.2(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB47_3
.LBB47_2:                               # %cond.true
	movq	%rax, %rdi
	popq	%rbp
	jmp	atoi@PLT                        # TAILCALL
.LBB47_3:                               # %cond.false
	popq	%rbp
	jmp	halide_host_cpu_count@PLT       # TAILCALL
.Lfunc_end47:
	.size	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv, .Lfunc_end47-_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal13worker_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13worker_threadEPv # -- Begin function _ZN6Halide7Runtime8Internal13worker_threadEPv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal13worker_threadEPv,@function
_ZN6Halide7Runtime8Internal13worker_threadEPv: # @_ZN6Halide7Runtime8Internal13worker_threadEPv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end48:
	.size	_ZN6Halide7Runtime8Internal13worker_threadEPv, .Lfunc_end48-_ZN6Halide7Runtime8Internal13worker_threadEPv
                                        # -- End function
	.section	.text.halide_spawn_thread,"ax",@progbits
	.weak	halide_spawn_thread             # -- Begin function halide_spawn_thread
	.p2align	4, 0x90
	.type	halide_spawn_thread,@function
halide_spawn_thread:                    # @halide_spawn_thread
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %r15
	movl	$24, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	movq	%r15, (%rax)
	movq	%r14, 8(%rax)
	leaq	16(%rax), %rdi
	movq	$0, 16(%rax)
	movq	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv@GOTPCREL(%rip), %rdx
	xorl	%esi, %esi
	movq	%rax, %rcx
	callq	pthread_create@PLT
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end49:
	.size	halide_spawn_thread, .Lfunc_end49-halide_spawn_thread
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv # -- Begin function _ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,@function
_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv: # @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	movq	8(%rdi), %rdi
	callq	*(%rax)
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end50:
	.size	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv, .Lfunc_end50-_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
                                        # -- End function
	.section	.text.halide_default_do_parallel_tasks,"ax",@progbits
	.weak	halide_default_do_parallel_tasks # -- Begin function halide_default_do_parallel_tasks
	.p2align	4, 0x90
	.type	halide_default_do_parallel_tasks,@function
halide_default_do_parallel_tasks:       # @halide_default_do_parallel_tasks
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r15
	movl	%esi, %r14d
	movslq	%esi, %rax
	movq	%rsp, %r12
	movq	%rax, %rcx
	shlq	$7, %rcx
	subq	%rcx, %r12
	movq	%r12, %rsp
	testl	%eax, %eax
	jle	.LBB51_4
# %bb.1:                                # %for.body.preheader
	leaq	124(%r12), %rax
	xorl	%ecx, %ecx
	jmp	.LBB51_2
	.p2align	4, 0x90
.LBB51_6:                               # %if.end
                                        #   in Loop: Header=BB51_2 Depth=1
	vmovups	(%rdx), %ymm0
	vmovups	24(%rdx), %ymm1
	addq	$56, %rdx
	vmovups	%ymm1, -100(%rax)
	vmovups	%ymm0, -124(%rax)
	movq	$0, -68(%rax)
	movq	%rdi, -20(%rax)
	movq	$0, -12(%rax)
	movl	$0, -4(%rax)
	movb	$0, (%rax)
	movq	%r15, -36(%rax)
.LBB51_7:                               # %for.inc
                                        #   in Loop: Header=BB51_2 Depth=1
	incq	%rcx
	movslq	%r14d, %rsi
	subq	$-128, %rax
	cmpq	%rsi, %rcx
	jge	.LBB51_4
.LBB51_2:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	$0, 40(%rdx)
	jg	.LBB51_6
# %bb.3:                                # %if.then
                                        #   in Loop: Header=BB51_2 Depth=1
	decl	%r14d
	jmp	.LBB51_7
.LBB51_4:                               # %for.cond.cleanup
	testl	%r14d, %r14d
	je	.LBB51_5
# %bb.8:                                # %if.end19
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	vzeroupper
	callq	halide_mutex_lock@PLT
	movl	%r14d, %edi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_@PLT
	testl	%r14d, %r14d
	jle	.LBB51_9
# %bb.12:                               # %for.body25.preheader
	movl	%r14d, %ebx
	xorl	%r14d, %r14d
	.p2align	4, 0x90
.LBB51_13:                              # %for.body25
                                        # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movl	116(%r12), %eax
	testl	%eax, %eax
	cmovnel	%eax, %r14d
	subq	$-128, %r12
	decq	%rbx
	jne	.LBB51_13
	jmp	.LBB51_10
.LBB51_5:
	xorl	%r14d, %r14d
	jmp	.LBB51_11
.LBB51_9:
	xorl	%r14d, %r14d
.LBB51_10:                              # %for.cond.cleanup24
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
.LBB51_11:                              # %cleanup
	movl	%r14d, %eax
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end51:
	.size	halide_default_do_parallel_tasks, .Lfunc_end51-halide_default_do_parallel_tasks
                                        # -- End function
	.section	.text.halide_default_semaphore_init,"ax",@progbits
	.weak	halide_default_semaphore_init   # -- Begin function halide_default_semaphore_init
	.p2align	4, 0x90
	.type	halide_default_semaphore_init,@function
halide_default_semaphore_init:          # @halide_default_semaphore_init
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%esi, %eax
	movl	%esi, (%rdi)
	popq	%rbp
	retq
.Lfunc_end52:
	.size	halide_default_semaphore_init, .Lfunc_end52-halide_default_semaphore_init
                                        # -- End function
	.section	.text.halide_default_semaphore_release,"ax",@progbits
	.weak	halide_default_semaphore_release # -- Begin function halide_default_semaphore_release
	.p2align	4, 0x90
	.type	halide_default_semaphore_release,@function
halide_default_semaphore_release:       # @halide_default_semaphore_release
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movl	%esi, %r14d
	movl	%esi, %ebx
	lock		xaddl	%ebx, (%rdi)
	testl	%esi, %esi
	je	.LBB53_3
# %bb.1:                                # %entry
	testl	%ebx, %ebx
	jne	.LBB53_3
# %bb.2:                                # %if.then
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r15
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	leaq	40(%r15), %rdi
	callq	halide_cond_broadcast@PLT
	leaq	56(%r15), %rdi
	callq	halide_cond_broadcast@PLT
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
.LBB53_3:                               # %if.end
	addl	%r14d, %ebx
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end53:
	.size	halide_default_semaphore_release, .Lfunc_end53-halide_default_semaphore_release
                                        # -- End function
	.section	.text.halide_thread_pool_cleanup,"ax",@progbits
	.weak	halide_thread_pool_cleanup      # -- Begin function halide_thread_pool_cleanup
	.p2align	4, 0x90
	.type	halide_thread_pool_cleanup,@function
halide_thread_pool_cleanup:             # @halide_thread_pool_cleanup
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_thread_pool@PLT # TAILCALL
.Lfunc_end54:
	.size	halide_thread_pool_cleanup, .Lfunc_end54-halide_thread_pool_cleanup
                                        # -- End function
	.section	.text.halide_shutdown_thread_pool,"ax",@progbits
	.weak	halide_shutdown_thread_pool     # -- Begin function halide_shutdown_thread_pool
	.p2align	4, 0x90
	.type	halide_shutdown_thread_pool,@function
halide_shutdown_thread_pool:            # @halide_shutdown_thread_pool
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	cmpb	$0, 2121(%r14)
	je	.LBB55_5
# %bb.1:                                # %if.then
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movb	$1, 2120(%r14)
	leaq	56(%r14), %rdi
	callq	halide_cond_broadcast@PLT
	leaq	40(%r14), %rdi
	callq	halide_cond_broadcast@PLT
	leaq	48(%r14), %rdi
	callq	halide_cond_broadcast@PLT
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	cmpl	$0, 24(%r14)
	jle	.LBB55_4
# %bb.2:                                # %for.body.preheader
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB55_3:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	72(%r14,%rbx,8), %rdi
	callq	halide_join_thread@PLT
	incq	%rbx
	movslq	24(%r14), %rax
	cmpq	%rax, %rbx
	jl	.LBB55_3
.LBB55_4:                               # %for.cond.cleanup
	addq	$12, %r14
	movl	$2116, %edx                     # imm = 0x844
	movq	%r14, %rdi
	xorl	%esi, %esi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	memset@PLT                      # TAILCALL
.LBB55_5:                               # %if.end
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end55:
	.size	halide_shutdown_thread_pool, .Lfunc_end55-halide_shutdown_thread_pool
                                        # -- End function
	.section	.text.halide_join_thread,"ax",@progbits
	.weak	halide_join_thread              # -- Begin function halide_join_thread
	.p2align	4, 0x90
	.type	halide_join_thread,@function
halide_join_thread:                     # @halide_join_thread
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	$0, -16(%rbp)
	movq	16(%rdi), %rdi
	leaq	-16(%rbp), %rsi
	callq	pthread_join@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end56:
	.size	halide_join_thread, .Lfunc_end56-halide_join_thread
                                        # -- End function
	.section	.text.halide_cond_signal,"ax",@progbits
	.weak	halide_cond_signal              # -- Begin function halide_cond_signal
	.p2align	4, 0x90
	.type	halide_cond_signal,@function
halide_cond_signal:                     # @halide_cond_signal
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	movq	(%rdi), %rax
	testq	%rax, %rax
	je	.LBB57_2
# %bb.1:                                # %if.end.i
	movq	%rdi, %rsi
	movq	_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE@GOTPCREL(%rip), %rcx
	addq	$16, %rcx
	movq	%rcx, -24(%rbp)
	movq	%rdi, -16(%rbp)
	movq	%rax, -8(%rbp)
	leaq	-24(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy@PLT
.LBB57_2:                               # %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond6signalEv.exit
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end57:
	.size	halide_cond_signal, .Lfunc_end57-halide_cond_signal
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE,@function
_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE: # @_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movb	$1, %al
	popq	%rbp
	retq
.Lfunc_end58:
	.size	_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE, .Lfunc_end58-_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib,"axG",@progbits,_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib,comdat
	.weak	_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib # -- Begin function _ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib,@function
_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib: # @_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib
# %bb.0:                                # %entry
	testl	%edx, %edx
	jne	.LBB59_2
# %bb.1:                                # %if.then
	pushq	%rbp
	movq	%rsp, %rbp
	movq	8(%rdi), %rax
	movq	$0, (%rax)
	popq	%rbp
.LBB59_2:                               # %if.end
	xorl	%eax, %eax
	retq
.Lfunc_end59:
	.size	_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib, .Lfunc_end59-_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib
                                        # -- End function
	.section	.text.halide_mutex_array_create,"ax",@progbits
	.weak	halide_mutex_array_create       # -- Begin function halide_mutex_array_create
	.p2align	4, 0x90
	.type	halide_mutex_array_create,@function
halide_mutex_array_create:              # @halide_mutex_array_create
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movl	%edi, %r15d
	xorl	%ebx, %ebx
	movl	$8, %esi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB60_4
# %bb.1:                                # %if.end
	movq	%rax, %r14
	movslq	%r15d, %rbx
	shlq	$3, %rbx
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, (%r14)
	testq	%rax, %rax
	je	.LBB60_2
# %bb.3:                                # %if.end6
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%rbx, %rdx
	callq	memset@PLT
	movq	%r14, %rbx
	jmp	.LBB60_4
.LBB60_2:                               # %if.then5
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
.LBB60_4:                               # %cleanup
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end60:
	.size	halide_mutex_array_create, .Lfunc_end60-halide_mutex_array_create
                                        # -- End function
	.section	.text.halide_mutex_array_destroy,"ax",@progbits
	.weak	halide_mutex_array_destroy      # -- Begin function halide_mutex_array_destroy
	.p2align	4, 0x90
	.type	halide_mutex_array_destroy,@function
halide_mutex_array_destroy:             # @halide_mutex_array_destroy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	(%rsi), %rsi
	callq	halide_free@PLT
	movq	%rbx, %rdi
	movq	%r14, %rsi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_free@PLT                 # TAILCALL
.Lfunc_end61:
	.size	halide_mutex_array_destroy, .Lfunc_end61-halide_mutex_array_destroy
                                        # -- End function
	.section	.text.halide_mutex_array_lock,"ax",@progbits
	.weak	halide_mutex_array_lock         # -- Begin function halide_mutex_array_lock
	.p2align	4, 0x90
	.type	halide_mutex_array_lock,@function
halide_mutex_array_lock:                # @halide_mutex_array_lock
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	%esi, %rax
	shlq	$3, %rax
	addq	(%rdi), %rax
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end62:
	.size	halide_mutex_array_lock, .Lfunc_end62-halide_mutex_array_lock
                                        # -- End function
	.section	.text.halide_mutex_array_unlock,"ax",@progbits
	.weak	halide_mutex_array_unlock       # -- Begin function halide_mutex_array_unlock
	.p2align	4, 0x90
	.type	halide_mutex_array_unlock,@function
halide_mutex_array_unlock:              # @halide_mutex_array_unlock
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	%esi, %rax
	shlq	$3, %rax
	addq	(%rdi), %rax
	movq	%rax, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end63:
	.size	halide_mutex_array_unlock, .Lfunc_end63-halide_mutex_array_unlock
                                        # -- End function
	.section	.text.halide_set_num_threads,"ax",@progbits
	.weak	halide_set_num_threads          # -- Begin function halide_set_num_threads
	.p2align	4, 0x90
	.type	halide_set_num_threads,@function
halide_set_num_threads:                 # @halide_set_num_threads
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movl	%edi, %ebx
	testl	%edi, %edi
	js	.LBB64_1
# %bb.2:                                # %if.end
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	je	.LBB64_3
# %bb.4:                                # %if.end3
	movl	8(%r14), %r14d
	movl	$256, %eax                      # imm = 0x100
	cmpl	$256, %ebx                      # imm = 0x100
	jle	.LBB64_5
	jmp	.LBB64_6
.LBB64_1:                               # %if.end3.thread
	leaq	.L.str.4(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movl	8(%r14), %r14d
	jmp	.LBB64_5
.LBB64_3:                               # %if.then2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %ebx
	movl	8(%r14), %r14d
	movl	$256, %eax                      # imm = 0x100
	cmpl	$256, %ebx                      # imm = 0x100
	jg	.LBB64_6
.LBB64_5:                               # %if.else.i
	testl	%ebx, %ebx
	movl	$1, %eax
	cmovgl	%ebx, %eax
.LBB64_6:                               # %_ZN6Halide7Runtime8Internal17clamp_num_threadsEi.exit
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	movl	%eax, 8(%rdi)
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end64:
	.size	halide_set_num_threads, .Lfunc_end64-halide_set_num_threads
                                        # -- End function
	.section	.text.halide_set_custom_do_task,"ax",@progbits
	.weak	halide_set_custom_do_task       # -- Begin function halide_set_custom_do_task
	.p2align	4, 0x90
	.type	halide_set_custom_do_task,@function
halide_set_custom_do_task:              # @halide_set_custom_do_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal14custom_do_taskE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end65:
	.size	halide_set_custom_do_task, .Lfunc_end65-halide_set_custom_do_task
                                        # -- End function
	.section	.text.halide_set_custom_do_loop_task,"ax",@progbits
	.weak	halide_set_custom_do_loop_task  # -- Begin function halide_set_custom_do_loop_task
	.p2align	4, 0x90
	.type	halide_set_custom_do_loop_task,@function
halide_set_custom_do_loop_task:         # @halide_set_custom_do_loop_task
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end66:
	.size	halide_set_custom_do_loop_task, .Lfunc_end66-halide_set_custom_do_loop_task
                                        # -- End function
	.section	.text.halide_set_custom_do_par_for,"ax",@progbits
	.weak	halide_set_custom_do_par_for    # -- Begin function halide_set_custom_do_par_for
	.p2align	4, 0x90
	.type	halide_set_custom_do_par_for,@function
halide_set_custom_do_par_for:           # @halide_set_custom_do_par_for
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_do_par_forE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end67:
	.size	halide_set_custom_do_par_for, .Lfunc_end67-halide_set_custom_do_par_for
                                        # -- End function
	.section	.text.halide_set_custom_parallel_runtime,"ax",@progbits
	.weak	halide_set_custom_parallel_runtime # -- Begin function halide_set_custom_parallel_runtime
	.p2align	4, 0x90
	.type	halide_set_custom_parallel_runtime,@function
halide_set_custom_parallel_runtime:     # @halide_set_custom_parallel_runtime
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	16(%rbp), %r10
	movq	_ZN6Halide7Runtime8Internal17custom_do_par_forE@GOTPCREL(%rip), %rax
	movq	%rdi, (%rax)
	movq	_ZN6Halide7Runtime8Internal14custom_do_taskE@GOTPCREL(%rip), %rax
	movq	%rsi, (%rax)
	movq	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE@GOTPCREL(%rip), %rax
	movq	%rdx, (%rax)
	movq	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE@GOTPCREL(%rip), %rax
	movq	%rcx, (%rax)
	movq	_ZN6Halide7Runtime8Internal21custom_semaphore_initE@GOTPCREL(%rip), %rax
	movq	%r8, (%rax)
	movq	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE@GOTPCREL(%rip), %rax
	movq	%r9, (%rax)
	movq	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE@GOTPCREL(%rip), %rax
	movq	%r10, (%rax)
	popq	%rbp
	retq
.Lfunc_end68:
	.size	halide_set_custom_parallel_runtime, .Lfunc_end68-halide_set_custom_parallel_runtime
                                        # -- End function
	.section	.text.halide_do_par_for,"ax",@progbits
	.weak	halide_do_par_for               # -- Begin function halide_do_par_for
	.p2align	4, 0x90
	.type	halide_do_par_for,@function
halide_do_par_for:                      # @halide_do_par_for
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_do_par_forE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end69:
	.size	halide_do_par_for, .Lfunc_end69-halide_do_par_for
                                        # -- End function
	.section	.text.halide_do_parallel_tasks,"ax",@progbits
	.weak	halide_do_parallel_tasks        # -- Begin function halide_do_parallel_tasks
	.p2align	4, 0x90
	.type	halide_do_parallel_tasks,@function
halide_do_parallel_tasks:               # @halide_do_parallel_tasks
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end70:
	.size	halide_do_parallel_tasks, .Lfunc_end70-halide_do_parallel_tasks
                                        # -- End function
	.section	.text.halide_semaphore_init,"ax",@progbits
	.weak	halide_semaphore_init           # -- Begin function halide_semaphore_init
	.p2align	4, 0x90
	.type	halide_semaphore_init,@function
halide_semaphore_init:                  # @halide_semaphore_init
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal21custom_semaphore_initE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end71:
	.size	halide_semaphore_init, .Lfunc_end71-halide_semaphore_init
                                        # -- End function
	.section	.text.halide_semaphore_release,"ax",@progbits
	.weak	halide_semaphore_release        # -- Begin function halide_semaphore_release
	.p2align	4, 0x90
	.type	halide_semaphore_release,@function
halide_semaphore_release:               # @halide_semaphore_release
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end72:
	.size	halide_semaphore_release, .Lfunc_end72-halide_semaphore_release
                                        # -- End function
	.section	.text.halide_semaphore_try_acquire,"ax",@progbits
	.weak	halide_semaphore_try_acquire    # -- Begin function halide_semaphore_try_acquire
	.p2align	4, 0x90
	.type	halide_semaphore_try_acquire,@function
halide_semaphore_try_acquire:           # @halide_semaphore_try_acquire
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end73:
	.size	halide_semaphore_try_acquire, .Lfunc_end73-halide_semaphore_try_acquire
                                        # -- End function
	.section	.text.halide_default_get_symbol,"ax",@progbits
	.weak	halide_default_get_symbol       # -- Begin function halide_default_get_symbol
	.p2align	4, 0x90
	.type	halide_default_get_symbol,@function
halide_default_get_symbol:              # @halide_default_get_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rsi
	xorl	%edi, %edi
	popq	%rbp
	jmp	dlsym@PLT                       # TAILCALL
.Lfunc_end74:
	.size	halide_default_get_symbol, .Lfunc_end74-halide_default_get_symbol
                                        # -- End function
	.section	.text.halide_default_load_library,"ax",@progbits
	.weak	halide_default_load_library     # -- Begin function halide_default_load_library
	.p2align	4, 0x90
	.type	halide_default_load_library,@function
halide_default_load_library:            # @halide_default_load_library
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movl	$1, %esi
	callq	dlopen@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	jne	.LBB75_2
# %bb.1:                                # %if.then
	callq	dlerror@PLT
.LBB75_2:                               # %if.end
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end75:
	.size	halide_default_load_library, .Lfunc_end75-halide_default_load_library
                                        # -- End function
	.section	.text.halide_default_get_library_symbol,"ax",@progbits
	.weak	halide_default_get_library_symbol # -- Begin function halide_default_get_library_symbol
	.p2align	4, 0x90
	.type	halide_default_get_library_symbol,@function
halide_default_get_library_symbol:      # @halide_default_get_library_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	dlsym@PLT                       # TAILCALL
.Lfunc_end76:
	.size	halide_default_get_library_symbol, .Lfunc_end76-halide_default_get_library_symbol
                                        # -- End function
	.section	.text.halide_set_custom_get_symbol,"ax",@progbits
	.weak	halide_set_custom_get_symbol    # -- Begin function halide_set_custom_get_symbol
	.p2align	4, 0x90
	.type	halide_set_custom_get_symbol,@function
halide_set_custom_get_symbol:           # @halide_set_custom_get_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end77:
	.size	halide_set_custom_get_symbol, .Lfunc_end77-halide_set_custom_get_symbol
                                        # -- End function
	.section	.text.halide_set_custom_load_library,"ax",@progbits
	.weak	halide_set_custom_load_library  # -- Begin function halide_set_custom_load_library
	.p2align	4, 0x90
	.type	halide_set_custom_load_library,@function
halide_set_custom_load_library:         # @halide_set_custom_load_library
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end78:
	.size	halide_set_custom_load_library, .Lfunc_end78-halide_set_custom_load_library
                                        # -- End function
	.section	.text.halide_set_custom_get_library_symbol,"ax",@progbits
	.weak	halide_set_custom_get_library_symbol # -- Begin function halide_set_custom_get_library_symbol
	.p2align	4, 0x90
	.type	halide_set_custom_get_library_symbol,@function
halide_set_custom_get_library_symbol:   # @halide_set_custom_get_library_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end79:
	.size	halide_set_custom_get_library_symbol, .Lfunc_end79-halide_set_custom_get_library_symbol
                                        # -- End function
	.section	.text.halide_get_symbol,"ax",@progbits
	.weak	halide_get_symbol               # -- Begin function halide_get_symbol
	.p2align	4, 0x90
	.type	halide_get_symbol,@function
halide_get_symbol:                      # @halide_get_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                         # TAILCALL
.Lfunc_end80:
	.size	halide_get_symbol, .Lfunc_end80-halide_get_symbol
                                        # -- End function
	.section	.text.halide_load_library,"ax",@progbits
	.weak	halide_load_library             # -- Begin function halide_load_library
	.p2align	4, 0x90
	.type	halide_load_library,@function
halide_load_library:                    # @halide_load_library
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                         # TAILCALL
.Lfunc_end81:
	.size	halide_load_library, .Lfunc_end81-halide_load_library
                                        # -- End function
	.section	.text.halide_get_library_symbol,"ax",@progbits
	.weak	halide_get_library_symbol       # -- Begin function halide_get_library_symbol
	.p2align	4, 0x90
	.type	halide_get_library_symbol,@function
halide_get_library_symbol:              # @halide_get_library_symbol
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end82:
	.size	halide_get_library_symbol, .Lfunc_end82-halide_get_library_symbol
                                        # -- End function
	.section	.text.halide_set_gpu_device,"ax",@progbits
	.weak	halide_set_gpu_device           # -- Begin function halide_set_gpu_device
	.p2align	4, 0x90
	.type	halide_set_gpu_device,@function
halide_set_gpu_device:                  # @halide_set_gpu_device
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end83:
	.size	halide_set_gpu_device, .Lfunc_end83-halide_set_gpu_device
                                        # -- End function
	.section	.text.halide_get_gpu_device,"ax",@progbits
	.weak	halide_get_gpu_device           # -- Begin function halide_get_gpu_device
	.p2align	4, 0x90
	.type	halide_get_gpu_device,@function
halide_get_gpu_device:                  # @halide_get_gpu_device
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE@GOTPCREL(%rip), %rbx
	.p2align	4, 0x90
.LBB84_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movb	$1, %al
	xchgb	%al, (%rbx)
	testb	%al, %al
	jne	.LBB84_1
# %bb.2:                                # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %r14
	cmpb	$0, (%r14)
	je	.LBB84_4
# %bb.3:                                # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	jmp	.LBB84_8
.LBB84_4:                               # %if.then
	leaq	.L.str.8(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB84_5
# %bb.6:                                # %if.then2
	movq	%rax, %rdi
	callq	atoi@PLT
	jmp	.LBB84_7
.LBB84_5:
	movl	$-1, %eax
.LBB84_7:                               # %if.end
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rcx
	movl	%eax, (%rcx)
	movb	$1, (%r14)
.LBB84_8:                               # %if.end4
	movb	$0, (%rbx)
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end84:
	.size	halide_get_gpu_device, .Lfunc_end84-halide_get_gpu_device
                                        # -- End function
	.section	.text.halide_default_trace,"ax",@progbits
	.weak	halide_default_trace            # -- Begin function halide_default_trace
	.p2align	4, 0x90
	.type	halide_default_trace,@function
halide_default_trace:                   # @halide_default_trace
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%rsi, %r14
	movl	$1, %r15d
	movl	$1, %r12d
	lock		xaddl	%r12d, _ZZ20halide_default_traceE3ids(%rip)
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	callq	halide_get_trace_file@PLT
	movl	%eax, -48(%rbp)                 # 4-byte Spill
	testl	%eax, %eax
	movq	%r14, -64(%rbp)                 # 8-byte Spill
	movl	%r12d, -68(%rbp)                # 4-byte Spill
	jle	.LBB85_40
# %bb.1:                                # %if.then
	movzwl	34(%r14), %eax
	movzbl	33(%r14), %ebx
	addq	$7, %rbx
	shrq	$3, %rbx
	imulq	%rax, %rbx
	movl	48(%r14), %r12d
	shll	$2, %r12d
	movq	(%r14), %rdi
	callq	strlen@PLT
	movq	%rax, %r13
	incl	%r13d
	movq	24(%r14), %rdi
	testq	%rdi, %rdi
	je	.LBB85_3
# %bb.2:                                # %cond.true
	callq	strlen@PLT
	movq	%rax, %r15
	incl	%r15d
.LBB85_3:                               # %cond.end
	movq	%r12, -88(%rbp)                 # 8-byte Spill
	leal	(%r12,%rbx), %eax
	addl	%r13d, %eax
	leal	(%r15,%rax), %r12d
	addl	$31, %r12d
	andl	$-4, %r12d
	movq	_ZN6Halide7Runtime8Internal19halide_trace_bufferE@GOTPCREL(%rip), %rax
	movq	(%rax), %r14
	leaq	12(%r14), %rax
	movq	%rax, -80(%rbp)                 # 8-byte Spill
	cmpl	$1048577, %r12d                 # imm = 0x100001
	movq	%r15, -112(%rbp)                # 8-byte Spill
	movq	%r13, -104(%rbp)                # 8-byte Spill
	movq	%rbx, -96(%rbp)                 # 8-byte Spill
	jae	.LBB85_4
# %bb.12:                               # %while.body.i.i.us.i.preheader
	movl	$1073741823, %r13d              # imm = 0x3FFFFFFF
	movl	$-2147483648, %r15d             # imm = 0x80000000
	jmp	.LBB85_13
.LBB85_20:                              # %do.end.critedge.i.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	lock		andl	$2147483647, (%r14)     # imm = 0x7FFFFFFF
	.p2align	4, 0x90
.LBB85_13:                              # %while.body.i.i.us.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB85_16 Depth 2
	movl	(%r14), %eax
	andl	%r13d, %eax
	leal	1(%rax), %ecx
                                        # kill: def $eax killed $eax killed $rax
	lock		cmpxchgl	%ecx, (%r14)
	jne	.LBB85_13
# %bb.14:                               # %do.end.i.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	movl	%r12d, %eax
	lock		xaddl	%eax, 4(%r14)
	leal	(%rax,%r12), %ecx
	cmpl	$1048577, %ecx                  # imm = 0x100001
	jb	.LBB85_22
# %bb.15:                               # %while.body.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	lock		addl	%r12d, 8(%r14)
	lock		decl	(%r14)
	.p2align	4, 0x90
.LBB85_16:                              # %while.body.i.i5.us.i
                                        #   Parent Loop BB85_13 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	lock		orl	$1073741824, (%r14)     # imm = 0x40000000
	movl	$1073741824, %eax               # imm = 0x40000000
	lock		cmpxchgl	%r15d, (%r14)
	jne	.LBB85_16
# %bb.17:                               # %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	movl	4(%r14), %ebx
	testl	%ebx, %ebx
	je	.LBB85_20
# %bb.18:                               # %if.then.i9.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	subl	8(%r14), %ebx
	movl	%ebx, 4(%r14)
	movl	-48(%rbp), %edi                 # 4-byte Reload
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	movq	%rbx, %rdx
	callq	write@PLT
	movq	$0, 4(%r14)
	lock		andl	$2147483647, (%r14)     # imm = 0x7FFFFFFF
	cmpl	%eax, %ebx
	je	.LBB85_13
# %bb.19:                               # %if.then10.i.us.i
                                        #   in Loop: Header=BB85_13 Depth=1
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	leaq	.L.str.32(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB85_13
.LBB85_40:                              # %if.else
	movl	$4096, %edi                     # imm = 0x1000
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB85_41
# %bb.42:                               # %if.then6.i451
	leaq	4095(%rbx), %r13
	movb	$0, 4095(%rbx)
	jmp	.LBB85_43
.LBB85_4:
	movl	$1073741823, %r15d              # imm = 0x3FFFFFFF
	movl	$-2147483648, %r13d             # imm = 0x80000000
	jmp	.LBB85_5
.LBB85_21:                              # %do.end.critedge.i.i
                                        #   in Loop: Header=BB85_5 Depth=1
	lock		andl	$2147483647, (%r14)     # imm = 0x7FFFFFFF
	.p2align	4, 0x90
.LBB85_5:                               # %while.body.i.i.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB85_8 Depth 2
	movl	(%r14), %eax
	andl	%r15d, %eax
	leal	1(%rax), %ecx
                                        # kill: def $eax killed $eax killed $rax
	lock		cmpxchgl	%ecx, (%r14)
	jne	.LBB85_5
# %bb.6:                                # %if.then.i.i
                                        #   in Loop: Header=BB85_5 Depth=1
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	leaq	.L.str.31(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	%r12d, %eax
	lock		xaddl	%eax, 4(%r14)
	leal	(%rax,%r12), %ecx
	cmpl	$1048577, %ecx                  # imm = 0x100001
	jb	.LBB85_22
# %bb.7:                                # %while.body.i
                                        #   in Loop: Header=BB85_5 Depth=1
	lock		addl	%r12d, 8(%r14)
	lock		decl	(%r14)
	.p2align	4, 0x90
.LBB85_8:                               # %while.body.i.i5.i
                                        #   Parent Loop BB85_5 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	lock		orl	$1073741824, (%r14)     # imm = 0x40000000
	movl	$1073741824, %eax               # imm = 0x40000000
	lock		cmpxchgl	%r13d, (%r14)
	jne	.LBB85_8
# %bb.9:                                # %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.i
                                        #   in Loop: Header=BB85_5 Depth=1
	movl	4(%r14), %ebx
	testl	%ebx, %ebx
	je	.LBB85_21
# %bb.10:                               # %if.then.i9.i
                                        #   in Loop: Header=BB85_5 Depth=1
	subl	8(%r14), %ebx
	movl	%ebx, 4(%r14)
	movl	-48(%rbp), %edi                 # 4-byte Reload
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	movq	%rbx, %rdx
	callq	write@PLT
	movq	$0, 4(%r14)
	lock		andl	$2147483647, (%r14)     # imm = 0x7FFFFFFF
	cmpl	%eax, %ebx
	je	.LBB85_5
# %bb.11:                               # %if.then10.i.i
                                        #   in Loop: Header=BB85_5 Depth=1
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	leaq	.L.str.32(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB85_5
.LBB85_22:                              # %_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit
	movl	%eax, %eax
	leaq	(%r14,%rax), %rbx
	addq	$12, %rbx
	cmpl	$4097, %r12d                    # imm = 0x1001
	movq	-96(%rbp), %r13                 # 8-byte Reload
	jb	.LBB85_27
# %bb.23:                               # %if.then17
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r14
	testq	%rax, %rax
	je	.LBB85_24
# %bb.25:                               # %if.else.i421
	leaq	1023(%r14), %r15
	movb	$0, 1023(%r14)
	movl	%r12d, %edx
	movq	%r14, %rdi
	movq	%r15, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.7.164(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	subq	%r14, %rax
	leaq	1(%rax), %rdx
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_print@PLT
	jmp	.LBB85_26
.LBB85_41:
	xorl	%r13d, %r13d
.LBB85_43:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit
	movzbl	33(%r14), %eax
	movl	$8, %ecx
	.p2align	4, 0x90
.LBB85_44:                              # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movl	%ecx, %r15d
	leal	(%r15,%r15), %ecx
	cmpl	%eax, %r15d
	jl	.LBB85_44
# %bb.45:                               # %do.body
	cmpl	$65, %r15d
	jl	.LBB85_47
# %bb.46:                               # %if.then63
	leaq	.L.str.2.11(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB85_47:                              # %do.end
	movl	36(%r14), %ecx
	leaq	.L__const.halide_default_trace.event_types(%rip), %rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	(%rax,%rcx,8), %rdx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.177(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	(%r14), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movslq	44(%r14), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.22.179(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	cmpw	$2, 34(%r14)
	jb	.LBB85_49
# %bb.48:                               # %if.then80
	leaq	.L.str.17(%rip), %rdx
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB85_49:                              # %if.end82
	cmpl	$0, 48(%r14)
	movq	%rbx, -80(%rbp)                 # 8-byte Spill
	jle	.LBB85_56
# %bb.50:                               # %if.end100.peel
	movq	16(%r14), %rax
	movslq	(%rax), %rdx
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	cmpl	$2, 48(%r14)
	jl	.LBB85_56
# %bb.51:                               # %if.then86.preheader
	movl	$1, %ebx
	leaq	.L.str.55(%rip), %r12
	jmp	.LBB85_52
	.p2align	4, 0x90
.LBB85_54:                              # %if.else97.split
                                        #   in Loop: Header=BB85_52 Depth=1
	movq	%r12, %rdx
.LBB85_55:                              # %if.end100
                                        #   in Loop: Header=BB85_52 Depth=1
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	-64(%rbp), %r14                 # 8-byte Reload
	movq	16(%r14), %rcx
	movslq	(%rcx,%rbx,4), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	incq	%rbx
	movslq	48(%r14), %rax
	cmpq	%rax, %rbx
	jge	.LBB85_56
.LBB85_52:                              # %if.then86
                                        # =>This Inner Loop Header: Depth=1
	movzwl	34(%r14), %ecx
	cmpl	$2, %ecx
	jb	.LBB85_54
# %bb.53:                               # %land.lhs.true
                                        #   in Loop: Header=BB85_52 Depth=1
	movl	%ebx, %eax
	xorl	%edx, %edx
	divl	%ecx
	movl	%edx, %eax
	leaq	.L.str.18(%rip), %rdx
	testl	%eax, %eax
	jne	.LBB85_54
	jmp	.LBB85_55
.LBB85_56:                              # %for.cond.cleanup
	cmpw	$1, 34(%r14)
	leaq	.L.str.20(%rip), %rax
	leaq	.L.str.8.119(%rip), %rdx
	cmovaq	%rax, %rdx
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
	cmpl	$2, -48(%rbp)                   # 4-byte Folded Reload
	jge	.LBB85_93
# %bb.57:                               # %if.then115
	cmpw	$1, 34(%r14)
	leaq	.L.str.22(%rip), %rax
	leaq	.L.str.23(%rip), %rdx
	cmovaq	%rax, %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
	cmpw	$0, 34(%r14)
	je	.LBB85_93
# %bb.58:                               # %if.end136.peel
	leaq	8(%r14), %rax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	movzbl	32(%r14), %eax
	cmpq	$3, %rax
	ja	.LBB85_87
# %bb.59:                               # %if.end136.peel
	leaq	.LJTI85_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB85_78:                              # %if.then140.peel
	cmpl	$8, %r15d
	je	.LBB85_83
# %bb.79:                               # %if.then140.peel
	cmpl	$16, %r15d
	je	.LBB85_82
# %bb.80:                               # %if.then140.peel
	cmpl	$32, %r15d
	jne	.LBB85_84
# %bb.81:                               # %if.then158.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movslq	(%rax), %rdx
	jmp	.LBB85_85
.LBB85_24:                              # %if.then.i415
	movl	%r12d, %edx
	xorl	%edi, %edi
	xorl	%esi, %esi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.7.164(%rip), %rdx
	movq	%rax, %rdi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
.LBB85_26:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE0ELy1024EED2Ev.exit
	movq	%r14, %rdi
	callq	free@PLT
.LBB85_27:                              # %if.end
	movl	%r12d, (%rbx)
	movl	-68(%rbp), %r15d                # 4-byte Reload
	movl	%r15d, 4(%rbx)
	movq	-64(%rbp), %r14                 # 8-byte Reload
	movl	32(%r14), %eax
	movl	%eax, 8(%rbx)
	vmovups	36(%r14), %xmm0
	vmovups	%xmm0, 12(%rbx)
	movq	16(%r14), %rsi
	testq	%rsi, %rsi
	je	.LBB85_29
# %bb.28:                               # %if.then28
	leaq	28(%rbx), %rdi
	movl	-88(%rbp), %edx                 # 4-byte Reload
	callq	memcpy@PLT
.LBB85_29:                              # %if.end33
	movq	8(%r14), %rsi
	testq	%rsi, %rsi
	je	.LBB85_31
# %bb.30:                               # %if.then35
	movslq	24(%rbx), %rax
	leaq	(%rbx,%rax,4), %rdi
	addq	$28, %rdi
	movq	%r13, %rdx
	callq	memcpy@PLT
.LBB85_31:                              # %if.end40
	movslq	24(%rbx), %rax
	leaq	(%rbx,%rax,4), %rax
	addq	$28, %rax
	movzwl	10(%rbx), %ecx
	movzbl	9(%rbx), %edi
	addq	$7, %rdi
	shrq	$3, %rdi
	imulq	%rcx, %rdi
	addq	%rax, %rdi
	movq	(%r14), %rsi
	movl	-104(%rbp), %edx                # 4-byte Reload
	callq	memcpy@PLT
	movslq	24(%rbx), %rax
	leaq	(%rbx,%rax,4), %rax
	addq	$28, %rax
	movzwl	10(%rbx), %ecx
	movzbl	9(%rbx), %edi
	addq	$7, %rdi
	shrq	$3, %rdi
	imulq	%rcx, %rdi
	addq	%rax, %rdi
	.p2align	4, 0x90
.LBB85_32:                              # %while.cond.i437
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%rdi)
	leaq	1(%rdi), %rdi
	jne	.LBB85_32
# %bb.33:                               # %_ZN21halide_trace_packet_t9trace_tagEv.exit
	movq	24(%r14), %rax
	testq	%rax, %rax
	leaq	.L.str.1.10(%rip), %rsi
	cmovneq	%rax, %rsi
	movl	-112(%rbp), %edx                # 4-byte Reload
	callq	memcpy@PLT
	movq	_ZN6Halide7Runtime8Internal19halide_trace_bufferE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	mfence
	lock		decl	(%rax)
	cmpl	$9, 36(%r14)
	jne	.LBB85_130
# %bb.34:                               # %if.then57
	movq	(%rcx), %rbx
	movl	$-2147483648, %ecx              # imm = 0x80000000
	.p2align	4, 0x90
.LBB85_35:                              # %while.body.i.i
                                        # =>This Inner Loop Header: Depth=1
	lock		orl	$1073741824, (%rbx)     # imm = 0x40000000
	movl	$1073741824, %eax               # imm = 0x40000000
	lock		cmpxchgl	%ecx, (%rbx)
	jne	.LBB85_35
# %bb.36:                               # %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i
	movl	4(%rbx), %r14d
	testl	%r14d, %r14d
	je	.LBB85_39
# %bb.37:                               # %if.then.i442
	subl	8(%rbx), %r14d
	movl	%r14d, 4(%rbx)
	leaq	12(%rbx), %rsi
	movl	-48(%rbp), %edi                 # 4-byte Reload
	movq	%r14, %rdx
	callq	write@PLT
	movq	$0, 4(%rbx)
	lock		andl	$2147483647, (%rbx)     # imm = 0x7FFFFFFF
	cmpl	%eax, %r14d
	je	.LBB85_130
# %bb.38:                               # %if.then10.i
	leaq	.L.str.32(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB85_130
.LBB85_39:                              # %do.end.critedge.i
	lock		andl	$2147483647, (%rbx)     # imm = 0x7FFFFFFF
	jmp	.LBB85_130
.LBB85_70:                              # %if.then176.peel
	cmpl	$8, %r15d
	je	.LBB85_76
# %bb.71:                               # %if.then176.peel
	cmpl	$16, %r15d
	je	.LBB85_75
# %bb.72:                               # %if.then176.peel
	cmpl	$32, %r15d
	jne	.LBB85_77
# %bb.73:                               # %if.then194.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movl	(%rax), %edx
	jmp	.LBB85_74
.LBB85_61:                              # %do.body213.peel
	cmpl	$15, %r15d
	jg	.LBB85_63
# %bb.62:                               # %if.then215.peel
	leaq	.L.str.24(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB85_63:                              # %do.end218.peel
	cmpl	$32, %r15d
	je	.LBB85_66
# %bb.64:                               # %do.end218.peel
	cmpl	$16, %r15d
	jne	.LBB85_67
# %bb.65:                               # %if.then227.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzwl	(%rax), %edi
	callq	halide_float16_bits_to_double@PLT
	jmp	.LBB85_68
.LBB85_60:                              # %if.then244.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax), %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_pointer_to_string@PLT
	jmp	.LBB85_86
.LBB85_66:                              # %if.then220.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	vmovss	(%rax), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%r12, %rdi
	movq	%r13, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	jmp	.LBB85_86
.LBB85_67:                              # %if.else232.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	vmovsd	(%rax), %xmm0                   # xmm0 = mem[0],zero
.LBB85_68:                              # %for.inc253.peel
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	jmp	.LBB85_86
.LBB85_83:                              # %if.then142.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movsbq	(%rax), %rdx
	jmp	.LBB85_85
.LBB85_82:                              # %if.then150.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movswq	(%rax), %rdx
	jmp	.LBB85_85
.LBB85_84:                              # %if.else163.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax), %rdx
	jmp	.LBB85_85
.LBB85_76:                              # %if.then178.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzbl	(%rax), %edx
	jmp	.LBB85_85
.LBB85_75:                              # %if.then186.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzwl	(%rax), %edx
.LBB85_85:                              # %for.inc253.peel
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	jmp	.LBB85_86
.LBB85_77:                              # %if.else199.peel
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax), %rdx
.LBB85_74:                              # %for.inc253.peel
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
.LBB85_86:                              # %for.inc253.peel
	movq	%rax, %r12
.LBB85_87:                              # %for.inc253.peel
	movq	-64(%rbp), %rax                 # 8-byte Reload
	cmpw	$2, 34(%rax)
	jb	.LBB85_93
# %bb.88:                               # %if.end136.preheader
	movl	$1, %r14d
	movq	-64(%rbp), %rbx                 # 8-byte Reload
	jmp	.LBB85_89
.LBB85_112:                             # %if.then186
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzwl	(%rax,%r14,2), %edx
	.p2align	4, 0x90
.LBB85_104:                             # %for.inc253
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
.LBB85_126:                             # %for.inc253
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	%rax, %r12
.LBB85_127:                             # %for.inc253
                                        #   in Loop: Header=BB85_89 Depth=1
	incq	%r14
	movq	-64(%rbp), %rbx                 # 8-byte Reload
	movzwl	34(%rbx), %eax
	cmpq	%rax, %r14
	jae	.LBB85_91
.LBB85_89:                              # %if.end136
                                        # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	movq	%r13, %rsi
	leaq	.L.str.55(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
	movzbl	32(%rbx), %eax
	cmpq	$3, %rax
	ja	.LBB85_127
# %bb.90:                               # %if.end136
                                        #   in Loop: Header=BB85_89 Depth=1
	leaq	.LJTI85_1(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB85_100:                             # %if.then140
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$32, %r15d
	je	.LBB85_106
# %bb.101:                              # %if.then140
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$16, %r15d
	je	.LBB85_105
# %bb.102:                              # %if.then140
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$8, %r15d
	jne	.LBB85_107
# %bb.103:                              # %if.then142
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movsbq	(%rax,%r14), %rdx
	jmp	.LBB85_104
	.p2align	4, 0x90
.LBB85_108:                             # %if.then176
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$32, %r15d
	je	.LBB85_113
# %bb.109:                              # %if.then176
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$16, %r15d
	je	.LBB85_112
# %bb.110:                              # %if.then176
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$8, %r15d
	jne	.LBB85_115
# %bb.111:                              # %if.then178
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzbl	(%rax,%r14), %edx
	jmp	.LBB85_104
	.p2align	4, 0x90
.LBB85_116:                             # %do.body213
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$15, %r15d
	jg	.LBB85_118
# %bb.117:                              # %if.then215
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	leaq	.L.str.24(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB85_118:                             # %do.end218
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$16, %r15d
	je	.LBB85_121
# %bb.119:                              # %do.end218
                                        #   in Loop: Header=BB85_89 Depth=1
	cmpl	$32, %r15d
	jne	.LBB85_122
# %bb.120:                              # %if.then220
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	vmovss	(%rax,%r14,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%r12, %rdi
	movq	%r13, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	jmp	.LBB85_126
	.p2align	4, 0x90
.LBB85_125:                             # %if.then244
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax,%r14,8), %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_pointer_to_string@PLT
	jmp	.LBB85_126
.LBB85_121:                             # %if.then227
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movzwl	(%rax,%r14,2), %edi
	callq	halide_float16_bits_to_double@PLT
	jmp	.LBB85_123
.LBB85_122:                             # %if.else232
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	vmovsd	(%rax,%r14,8), %xmm0            # xmm0 = mem[0],zero
.LBB85_123:                             # %for.inc253
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	jmp	.LBB85_126
.LBB85_106:                             # %if.then158
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movslq	(%rax,%r14,4), %rdx
	jmp	.LBB85_104
.LBB85_105:                             # %if.then150
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movswq	(%rax,%r14,2), %rdx
	jmp	.LBB85_104
.LBB85_113:                             # %if.then194
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movl	(%rax,%r14,4), %edx
	jmp	.LBB85_114
.LBB85_107:                             # %if.else163
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax,%r14,8), %rdx
	jmp	.LBB85_104
.LBB85_115:                             # %if.else199
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	movq	(%rax,%r14,8), %rdx
.LBB85_114:                             # %for.inc253
                                        #   in Loop: Header=BB85_89 Depth=1
	movq	%r12, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	jmp	.LBB85_126
.LBB85_91:                              # %for.cond.cleanup131
	cmpw	$1, %ax
	jbe	.LBB85_93
# %bb.92:                               # %if.then260
	leaq	.L.str.25(%rip), %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
.LBB85_93:                              # %if.end263
	movq	-64(%rbp), %rax                 # 8-byte Reload
	movq	24(%rax), %rax
	testq	%rax, %rax
	je	.LBB85_96
# %bb.94:                               # %land.lhs.true266
	cmpb	$0, (%rax)
	je	.LBB85_96
# %bb.95:                               # %if.then269
	leaq	.L.str.26(%rip), %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	movq	24(%rcx), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.27(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
.LBB85_96:                              # %if.end274
	leaq	.L.str.7.164(%rip), %rdx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r14
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.p2align	4, 0x90
.LBB85_97:                              # %while.cond.i560
                                        # =>This Inner Loop Header: Depth=1
	movb	$1, %al
	xchgb	%al, (%rbx)
	testb	%al, %al
	jne	.LBB85_97
# %bb.98:                               # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
	movq	-80(%rbp), %r12                 # 8-byte Reload
	testq	%r12, %r12
	je	.LBB85_99
# %bb.128:                              # %if.else.i
	subq	%r12, %r14
	incq	%r14
	movq	-56(%rbp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%r12, %rsi
	movq	%r14, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_print@PLT
	movb	$0, (%rbx)
	movq	%r15, %rdi
	movq	%r12, %rsi
	movq	%r14, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	jmp	.LBB85_129
.LBB85_99:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %r14
	movq	-56(%rbp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	movb	$0, (%rbx)
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_error@PLT
.LBB85_129:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EED2Ev.exit
	movq	%r12, %rdi
	callq	free@PLT
	movl	-68(%rbp), %r15d                # 4-byte Reload
.LBB85_130:                             # %if.end277
	movl	%r15d, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end85:
	.size	halide_default_trace, .Lfunc_end85-halide_default_trace
	.section	.rodata.halide_default_trace,"a",@progbits
	.p2align	2
.LJTI85_0:
	.long	.LBB85_78-.LJTI85_0
	.long	.LBB85_70-.LJTI85_0
	.long	.LBB85_61-.LJTI85_0
	.long	.LBB85_60-.LJTI85_0
.LJTI85_1:
	.long	.LBB85_100-.LJTI85_1
	.long	.LBB85_108-.LJTI85_1
	.long	.LBB85_116-.LJTI85_1
	.long	.LBB85_125-.LJTI85_1
                                        # -- End function
	.section	.text.halide_get_trace_file,"ax",@progbits
	.weak	halide_get_trace_file           # -- Begin function halide_get_trace_file
	.p2align	4, 0x90
	.type	halide_get_trace_file,@function
halide_get_trace_file:                  # @halide_get_trace_file
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.p2align	4, 0x90
.LBB86_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movb	$1, %al
	xchgb	%al, (%rbx)
	testb	%al, %al
	jne	.LBB86_1
# %bb.2:                                # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %r12
	cmpl	$0, (%r12)
	js	.LBB86_3
.LBB86_9:                               # %if.end11
	movl	(%r12), %eax
	movb	$0, (%rbx)
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB86_3:                               # %if.then
	leaq	.L.str.28(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB86_8
# %bb.4:                                # %if.then1
	leaq	.L.str.29(%rip), %rsi
	movq	%rax, %rdi
	callq	fopen@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	jne	.LBB86_6
# %bb.5:                                # %if.then4
	leaq	.L.str.30(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB86_6:                               # %do.end
	movq	%r15, %rdi
	callq	fileno@PLT
	movl	%eax, %edi
	callq	halide_set_trace_file@PLT
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rax
	movq	%r15, (%rax)
	movq	_ZN6Halide7Runtime8Internal19halide_trace_bufferE@GOTPCREL(%rip), %r14
	cmpq	$0, (%r14)
	jne	.LBB86_9
# %bb.7:                                # %if.then7
	movl	$1048588, %edi                  # imm = 0x10000C
	callq	malloc@PLT
	movq	%rax, (%r14)
	movq	$0, 4(%rax)
	movl	$0, (%rax)
	jmp	.LBB86_9
.LBB86_8:                               # %if.else
	xorl	%edi, %edi
	callq	halide_set_trace_file@PLT
	jmp	.LBB86_9
.Lfunc_end86:
	.size	halide_get_trace_file, .Lfunc_end86-halide_get_trace_file
                                        # -- End function
	.section	.text.halide_set_trace_file,"ax",@progbits
	.weak	halide_set_trace_file           # -- Begin function halide_set_trace_file
	.p2align	4, 0x90
	.type	halide_set_trace_file,@function
halide_set_trace_file:                  # @halide_set_trace_file
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	popq	%rbp
	retq
.Lfunc_end87:
	.size	halide_set_trace_file, .Lfunc_end87-halide_set_trace_file
                                        # -- End function
	.section	.text.halide_trace_cleanup,"ax",@progbits
	.weak	halide_trace_cleanup            # -- Begin function halide_trace_cleanup
	.p2align	4, 0x90
	.type	halide_trace_cleanup,@function
halide_trace_cleanup:                   # @halide_trace_cleanup
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_trace@PLT       # TAILCALL
.Lfunc_end88:
	.size	halide_trace_cleanup, .Lfunc_end88-halide_trace_cleanup
                                        # -- End function
	.section	.text.halide_shutdown_trace,"ax",@progbits
	.weak	halide_shutdown_trace           # -- Begin function halide_shutdown_trace
	.p2align	4, 0x90
	.type	halide_shutdown_trace,@function
halide_shutdown_trace:                  # @halide_shutdown_trace
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %r14
	movq	(%r14), %rdi
	testq	%rdi, %rdi
	je	.LBB89_1
# %bb.2:                                # %if.then
	callq	fclose@PLT
	movl	%eax, %ebx
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	movb	$0, (%rax)
	movq	$0, (%r14)
	movq	_ZN6Halide7Runtime8Internal19halide_trace_bufferE@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	testq	%rdi, %rdi
	je	.LBB89_4
# %bb.3:                                # %if.then2
	callq	free@PLT
	jmp	.LBB89_4
.LBB89_1:
	xorl	%ebx, %ebx
.LBB89_4:                               # %return
	movl	%ebx, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end89:
	.size	halide_shutdown_trace, .Lfunc_end89-halide_shutdown_trace
                                        # -- End function
	.section	.text.halide_set_custom_trace,"ax",@progbits
	.weak	halide_set_custom_trace         # -- Begin function halide_set_custom_trace
	.p2align	4, 0x90
	.type	halide_set_custom_trace,@function
halide_set_custom_trace:                # @halide_set_custom_trace
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end90:
	.size	halide_set_custom_trace, .Lfunc_end90-halide_set_custom_trace
                                        # -- End function
	.section	.text.halide_trace,"ax",@progbits
	.weak	halide_trace                    # -- Begin function halide_trace
	.p2align	4, 0x90
	.type	halide_trace,@function
halide_trace:                           # @halide_trace
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end91:
	.size	halide_trace, .Lfunc_end91-halide_trace
                                        # -- End function
	.section	.text.halide_trace_helper,"ax",@progbits
	.weak	halide_trace_helper             # -- Begin function halide_trace_helper
	.p2align	4, 0x90
	.type	halide_trace_helper,@function
halide_trace_helper:                    # @halide_trace_helper
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	movl	%r9d, %ebx
	movq	%rcx, %r14
	movq	%rdx, %r12
	movq	%rdi, %r15
	movslq	48(%rbp), %r13
	movl	40(%rbp), %r9d
	movl	32(%rbp), %ecx
	movl	24(%rbp), %edx
	movl	16(%rbp), %eax
	movq	56(%rbp), %rdi
	movq	%rsi, -96(%rbp)
	movq	%r12, -88(%rbp)
	movq	%r14, -80(%rbp)
	movq	%rdi, -72(%rbp)
	movb	%r8b, -64(%rbp)
	movb	%bl, -63(%rbp)
	movw	%ax, -62(%rbp)
	movl	%edx, -60(%rbp)
	movl	%ecx, -56(%rbp)
	movl	%r9d, -52(%rbp)
	movl	%r13d, -48(%rbp)
	leaq	-96(%rbp), %rsi
	movl	$56, %edx
	movq	%r15, %rdi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	leal	7(%rbx), %eax
	addl	$14, %ebx
	testl	%eax, %eax
	cmovnsl	%eax, %ebx
	sarl	$3, %ebx
	imull	16(%rbp), %ebx
	movslq	%ebx, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	shlq	$2, %r13
	movq	%r15, %rdi
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	leaq	-96(%rbp), %rsi
	callq	halide_trace@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end92:
	.size	halide_trace_helper, .Lfunc_end92-halide_trace_helper
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal9ends_withEPKcS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_ # -- Begin function _ZN6Halide7Runtime8Internal9ends_withEPKcS3_
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_,@function
_ZN6Halide7Runtime8Internal9ends_withEPKcS3_: # @_ZN6Halide7Runtime8Internal9ends_withEPKcS3_
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	$-2, %rax
	.p2align	4, 0x90
.LBB93_1:                               # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, 2(%rdi,%rax)
	leaq	1(%rax), %rax
	jne	.LBB93_1
# %bb.2:                                # %while.cond1.preheader
	movq	$-2, %rcx
	.p2align	4, 0x90
.LBB93_3:                               # %while.cond1
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, 2(%rsi,%rcx)
	leaq	1(%rcx), %rcx
	jne	.LBB93_3
# %bb.4:                                # %while.cond6.preheader
	xorl	%r8d, %r8d
	cmpq	$-1, %rax
	je	.LBB93_5
# %bb.7:                                # %while.cond6.preheader
	movl	$0, %edx
	cmpq	$-1, %rcx
	je	.LBB93_6
	.p2align	4, 0x90
.LBB93_8:                               # %if.end
                                        # =>This Inner Loop Header: Depth=1
	movzbl	(%rdi,%rax), %edx
	movzbl	(%rsi,%rcx), %r8d
	testq	%rax, %rax
	je	.LBB93_6
# %bb.9:                                # %if.end
                                        #   in Loop: Header=BB93_8 Depth=1
	testq	%rcx, %rcx
	je	.LBB93_6
# %bb.10:                               # %if.end.while.body8_crit_edge
                                        #   in Loop: Header=BB93_8 Depth=1
	decq	%rax
	decq	%rcx
	cmpb	%r8b, %dl
	je	.LBB93_8
# %bb.11:
	xorl	%eax, %eax
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.LBB93_5:
	xorl	%edx, %edx
.LBB93_6:                               # %while.end13
	cmpb	%r8b, %dl
	sete	%al
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.Lfunc_end93:
	.size	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_, .Lfunc_end93-_ZN6Halide7Runtime8Internal9ends_withEPKcS3_
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function halide_debug_to_file
.LCPI94_0:
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	7                               # 0x7
.LCPI94_1:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
.LCPI94_14:
	.long	327962                          # 0x5011a
	.long	1                               # 0x1
	.long	194                             # 0xc2
	.long	327963                          # 0x5011b
	.long	1                               # 0x1
	.long	202                             # 0xca
	.long	196892                          # 0x3011c
	.long	1                               # 0x1
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI94_2:
	.quad	4                               # 0x4
.LCPI94_4:
	.quad	8                               # 0x8
.LCPI94_5:
	.quad	128                             # 0x80
.LCPI94_6:
	.quad	256                             # 0x100
.LCPI94_7:
	.quad	384                             # 0x180
.LCPI94_8:
	.quad	32                              # 0x20
.LCPI94_10:
	.quad	68                              # 0x44
.LCPI94_11:
	.quad	132                             # 0x84
.LCPI94_12:
	.quad	196                             # 0xc4
.LCPI94_13:
	.quad	16                              # 0x10
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI94_3:
	.long	1                               # 0x1
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI94_9:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
.LCPI94_15:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	1                               # 0x1
	.long	1                               # 0x1
.LCPI94_16:
	.long	2                               # 0x2
	.long	6                               # 0x6
	.long	10                              # 0xa
	.long	14                              # 0xe
	.section	.text.halide_debug_to_file,"ax",@progbits
	.weak	halide_debug_to_file
	.p2align	4, 0x90
	.type	halide_debug_to_file,@function
halide_debug_to_file:                   # @halide_debug_to_file
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4424, %rsp                     # imm = 0x1148
	movq	%rcx, %r13
	movl	%edx, %ebx
	movq	%rsi, %r12
	movq	%rdi, %r15
	cmpq	$0, 16(%rcx)
	jne	.LBB94_4
# %bb.1:                                # %_ZNK15halide_buffer_t15is_bounds_queryEv.exit
	cmpq	$0, (%r13)
	je	.LBB94_2
.LBB94_4:                               # %if.end
	cmpl	$5, 36(%r13)
	jl	.LBB94_6
# %bb.5:                                # %if.then1
	leaq	.L.str.1.35(%rip), %rsi
	jmp	.LBB94_3
.LBB94_6:                               # %if.end2
	movq	%r15, %rdi
	movq	%r13, %rsi
	callq	halide_copy_to_host@PLT
	movl	%eax, %r14d
	testl	%eax, %eax
	jne	.LBB94_163
# %bb.7:                                # %if.end6
	leaq	.L.str.2.36(%rip), %rsi
	movq	%r12, %rdi
	callq	fopen@PLT
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	testq	%rax, %rax
	je	.LBB94_8
# %bb.9:                                # %if.end9
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, -144(%rbp)
	vmovdqu	%ymm0, -112(%rbp)
	movslq	36(%r13), %rax
	movl	%eax, %edx
	testq	%rax, %rax
	jle	.LBB94_10
# %bb.11:                               # %for.body.lr.ph
	movq	40(%r13), %rcx
	leaq	-1(%rdx), %rsi
	cmpq	$3, %rsi
	movl	$4, %esi
	cmovbq	%rdx, %rsi
	vmovdqu	(%rcx), %xmm0
	vmovdqa	%xmm0, -144(%rbp)
	movl	-140(%rbp), %r9d
	cmpq	$1, %rsi
	je	.LBB94_12
# %bb.164:                              # %for.body.1
	vmovdqu	16(%rcx), %xmm0
	vmovdqa	%xmm0, -128(%rbp)
	movl	-124(%rbp), %edi
	imulq	%rdi, %r9
	cmpl	$2, %esi
	je	.LBB94_12
# %bb.165:                              # %for.body.2
	vmovdqu	32(%rcx), %xmm0
	vmovdqa	%xmm0, -112(%rbp)
	movl	-108(%rbp), %edi
	imulq	%rdi, %r9
	cmpl	$3, %esi
	je	.LBB94_12
# %bb.166:                              # %for.body.3
	vmovdqu	48(%rcx), %xmm0
	vmovdqa	%xmm0, -96(%rbp)
	movl	-92(%rbp), %ecx
	imulq	%rcx, %r9
.LBB94_12:                              # %for.cond19.preheader
	cmpl	$3, %eax
	jg	.LBB94_24
# %bb.13:                               # %for.body22.preheader
	movl	$3, %ecx
	subl	%edx, %ecx
	cmpl	$7, %ecx
	jae	.LBB94_14
	jmp	.LBB94_22
.LBB94_2:                               # %if.then
	leaq	.L.str.34(%rip), %rsi
.LBB94_3:                               # %return
	movq	%r15, %rdi
	callq	halide_error@PLT
	movl	$-1, %r14d
.LBB94_163:                             # %return
	movl	%r14d, %eax
	addq	$4424, %rsp                     # imm = 0x1148
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB94_8:
	movl	$-2, %r14d
	jmp	.LBB94_163
.LBB94_10:
	movl	$1, %r9d
	movl	$3, %ecx
	subl	%edx, %ecx
	cmpl	$7, %ecx
	jb	.LBB94_22
.LBB94_14:                              # %vector.ph
	incq	%rcx
	movq	%rcx, %r8
	vpbroadcastq	%rax, %ymm1
	vpaddq	.LCPI94_0(%rip), %ymm1, %ymm0
	vpaddq	.LCPI94_1(%rip), %ymm1, %ymm1
	andq	$-8, %r8
	leaq	-8(%r8), %rdx
	movq	%rdx, %rdi
	shrq	$3, %rdi
	incq	%rdi
	movl	%edi, %esi
	andl	$3, %esi
	cmpq	$24, %rdx
	jb	.LBB94_17
# %bb.15:                               # %vector.ph.new
	andq	$-4, %rdi
	negq	%rdi
	leaq	-144(%rbp), %rdx
	vpbroadcastq	%rdx, %ymm2
	vpbroadcastq	.LCPI94_2(%rip), %ymm3  # ymm3 = [4,4,4,4]
	vpxor	%xmm4, %xmm4, %xmm4
	vpbroadcastd	.LCPI94_3(%rip), %xmm5  # xmm5 = [1,1,1,1]
	vpbroadcastq	.LCPI94_4(%rip), %ymm6  # ymm6 = [8,8,8,8]
	vpbroadcastq	.LCPI94_5(%rip), %ymm7  # ymm7 = [128,128,128,128]
	vpbroadcastq	.LCPI94_6(%rip), %ymm8  # ymm8 = [256,256,256,256]
	vpbroadcastq	.LCPI94_7(%rip), %ymm9  # ymm9 = [384,384,384,384]
	vpbroadcastq	.LCPI94_8(%rip), %ymm10 # ymm10 = [32,32,32,32]
	.p2align	4, 0x90
.LBB94_16:                              # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vpsllq	$4, %ymm1, %ymm11
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm11) {%k1}
	vpsllq	$4, %ymm0, %ymm12
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm12) {%k1}
	vpaddq	%ymm2, %ymm11, %ymm13
	vpor	%ymm3, %ymm13, %ymm14
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm14) {%k1}
	vpaddq	%ymm2, %ymm12, %ymm14
	vpor	%ymm3, %ymm14, %ymm15
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm15) {%k1}
	vpor	%ymm6, %ymm13, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpor	%ymm6, %ymm14, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpaddq	%ymm7, %ymm11, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm13) {%k1}
	vpaddq	%ymm7, %ymm12, %ymm14
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm14) {%k1}
	vpaddq	%ymm2, %ymm13, %ymm13
	vpor	%ymm3, %ymm13, %ymm15
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm15) {%k1}
	vpaddq	%ymm2, %ymm14, %ymm14
	vpor	%ymm3, %ymm14, %ymm15
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm15) {%k1}
	vpor	%ymm6, %ymm13, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpor	%ymm6, %ymm14, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpaddq	%ymm8, %ymm11, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm13) {%k1}
	vpaddq	%ymm8, %ymm12, %ymm14
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm14) {%k1}
	vpaddq	%ymm2, %ymm13, %ymm13
	vpor	%ymm3, %ymm13, %ymm15
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm15) {%k1}
	vpaddq	%ymm2, %ymm14, %ymm14
	vpor	%ymm3, %ymm14, %ymm15
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm15) {%k1}
	vpor	%ymm6, %ymm13, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpor	%ymm6, %ymm14, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm13) {%k1}
	vpaddq	%ymm9, %ymm11, %ymm11
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm11) {%k1}
	vpaddq	%ymm9, %ymm12, %ymm12
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm12) {%k1}
	vpaddq	%ymm2, %ymm11, %ymm11
	vpor	%ymm3, %ymm11, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm13) {%k1}
	vpaddq	%ymm2, %ymm12, %ymm12
	vpor	%ymm3, %ymm12, %ymm13
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm13) {%k1}
	vpor	%ymm6, %ymm11, %ymm11
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm11) {%k1}
	vpor	%ymm6, %ymm12, %ymm11
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm11) {%k1}
	vpaddq	%ymm1, %ymm10, %ymm1
	vpaddq	%ymm0, %ymm10, %ymm0
	addq	$4, %rdi
	jne	.LBB94_16
.LBB94_17:                              # %middle.block.unr-lcssa
	testq	%rsi, %rsi
	je	.LBB94_20
# %bb.18:                               # %vector.body.epil.preheader
	negq	%rsi
	leaq	-144(%rbp), %rdx
	vpbroadcastq	.LCPI94_2(%rip), %ymm2  # ymm2 = [4,4,4,4]
	vpbroadcastq	%rdx, %ymm3
	vpxor	%xmm4, %xmm4, %xmm4
	vpbroadcastd	.LCPI94_3(%rip), %xmm5  # xmm5 = [1,1,1,1]
	vpbroadcastq	.LCPI94_4(%rip), %ymm6  # ymm6 = [8,8,8,8]
	.p2align	4, 0x90
.LBB94_19:                              # %vector.body.epil
                                        # =>This Inner Loop Header: Depth=1
	vpsllq	$4, %ymm1, %ymm7
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm7) {%k1}
	vpsllq	$4, %ymm0, %ymm8
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (%rdx,%ymm8) {%k1}
	vpaddq	%ymm7, %ymm3, %ymm7
	vpor	%ymm2, %ymm7, %ymm9
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm9) {%k1}
	vpaddq	%ymm3, %ymm8, %ymm8
	vpor	%ymm2, %ymm8, %ymm9
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm5, (,%ymm9) {%k1}
	vpor	%ymm6, %ymm7, %ymm7
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm7) {%k1}
	vpor	%ymm6, %ymm8, %ymm7
	kxnorw	%k0, %k0, %k1
	vpscatterqd	%xmm4, (,%ymm7) {%k1}
	vpaddq	%ymm6, %ymm1, %ymm1
	vpaddq	%ymm6, %ymm0, %ymm0
	incq	%rsi
	jne	.LBB94_19
.LBB94_20:                              # %middle.block
	cmpq	%r8, %rcx
	je	.LBB94_24
# %bb.21:
	addq	%r8, %rax
.LBB94_22:                              # %for.body22.preheader214
	leal	-4(%rax), %ecx
	shlq	$4, %rax
	addq	%rbp, %rax
	addq	$-136, %rax
	movabsq	$4294967296, %rdx               # imm = 0x100000000
	.p2align	4, 0x90
.LBB94_23:                              # %for.body22
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdx, -8(%rax)
	movl	$0, (%rax)
	addq	$16, %rax
	incl	%ecx
	jne	.LBB94_23
.LBB94_24:                              # %for.cond.cleanup21
	movq	%r9, -72(%rbp)                  # 8-byte Spill
	movzbl	33(%r13), %r14d
	addq	$7, %r14
	movq	%r14, %rax
	shrq	$3, %rax
	movq	%rax, -80(%rbp)                 # 8-byte Spill
	leaq	.L.str.3.37(%rip), %rsi
	movq	%r12, %rdi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_@PLT
	testb	%al, %al
	jne	.LBB94_26
# %bb.25:                               # %lor.lhs.false
	leaq	.L.str.4.38(%rip), %rsi
	movq	%r12, %rdi
	callq	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_@PLT
	testb	%al, %al
	je	.LBB94_37
.LBB94_26:                              # %if.then36
	movl	-140(%rbp), %ecx
	movl	-124(%rbp), %eax
	movl	-92(%rbp), %r12d
	cmpl	$2, %r12d
	setb	%dl
	movl	-108(%rbp), %esi
	cmpl	$5, %esi
	movl	%ebx, %edi
	setl	%bl
	testb	%bl, %dl
	movl	$1, %r15d
	cmovel	%esi, %r15d
	cmovnel	%esi, %r12d
	movabsq	$34362509641, %rdx              # imm = 0x8002A4949
	movq	%rdx, -4464(%rbp)
	movl	$16777231, -4456(%rbp)          # imm = 0x100000F
	movw	$4, -4452(%rbp)
	movl	$1, -4450(%rbp)
	movl	%ecx, -4446(%rbp)
	movabsq	$4295229697, %rcx               # imm = 0x100040101
	movq	%rcx, -4442(%rbp)
	movl	%eax, -4434(%rbp)
	andl	$-8, %r14d
	movabsq	$4295164162, %rcx               # imm = 0x100030102
	movq	%rcx, -4430(%rbp)
	movw	%r14w, -4422(%rbp)
	movabsq	$4295164163, %rcx               # imm = 0x100030103
	movq	%rcx, -4418(%rbp)
	movw	$1, -4410(%rbp)
	xorl	%ecx, %ecx
	cmpl	$2, %r12d
	setg	%cl
	incl	%ecx
	movabsq	$4295164166, %rdx               # imm = 0x100030106
	movq	%rdx, -4406(%rbp)
	movw	%cx, -4398(%rbp)
	movl	$262417, -4394(%rbp)            # imm = 0x40111
	movl	%r12d, -4390(%rbp)
	movabsq	$845614636073170, %rcx          # imm = 0x30115000000D2
	movq	%rcx, -4386(%rbp)
	movl	$1, -4378(%rbp)
	movw	%r12w, -4374(%rbp)
	movabsq	$4295229718, %rcx               # imm = 0x100040116
	movq	%rcx, -4370(%rbp)
	movl	%eax, -4362(%rbp)
	movq	-72(%rbp), %rcx                 # 8-byte Reload
	imull	-80(%rbp), %ecx                 # 4-byte Folded Reload
	cmpl	$1, %r12d
	leal	210(,%r12,4), %eax
	cmovel	%ecx, %eax
	movl	$262423, -4358(%rbp)            # imm = 0x40117
	movl	%r12d, -4354(%rbp)
	movl	%eax, -4350(%rbp)
	vmovaps	.LCPI94_14(%rip), %ymm0         # ymm0 = [327962,1,194,327963,1,202,196892,1]
	vmovups	%ymm0, -4346(%rbp)
	movw	$2, -4314(%rbp)
	movabsq	$4295164200, %rax               # imm = 0x100030128
	movq	%rax, -4310(%rbp)
	movw	$1, -4302(%rbp)
	movslq	%edi, %rax
	movq	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE@GOTPCREL(%rip), %rcx
	movzwl	(%rcx,%rax,2), %eax
	movabsq	$4295164243, %rcx               # imm = 0x100030153
	movq	%rcx, -4298(%rbp)
	movw	%ax, -4290(%rbp)
	movabsq	$4295262437, %rax               # imm = 0x1000480E5
	movq	%rax, -4286(%rbp)
	movl	%r15d, -4278(%rbp)
	vmovdqa	.LCPI94_15(%rip), %xmm0         # xmm0 = [0,1,1,1]
	vmovdqu	%xmm0, -4274(%rbp)
	movl	$1, -4258(%rbp)
	leaq	-4464(%rbp), %rdi
	movl	$210, %esi
	movl	$1, %edx
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	vzeroupper
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_27
# %bb.28:                               # %if.end103
	cmpl	$2, %r12d
	jl	.LBB94_124
# %bb.29:                               # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731.lr.ph
	leal	210(,%r12,8), %eax
	movl	%eax, -368(%rbp)
	imull	-80(%rbp), %r15d                # 4-byte Folded Reload
	leaq	-368(%rbp), %r14
	movl	%r12d, %ebx
	.p2align	4, 0x90
.LBB94_30:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r14, %rdi
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_31
# %bb.32:                               # %if.end118
                                        #   in Loop: Header=BB94_30 Depth=1
	movl	-140(%rbp), %eax
	imull	%r15d, %eax
	imull	-124(%rbp), %eax
	addl	%eax, -368(%rbp)
	decl	%ebx
	jne	.LBB94_30
# %bb.33:                               # %for.end129
	movl	%eax, -212(%rbp)
	leaq	-212(%rbp), %r14
	movq	-48(%rbp), %rbx                 # 8-byte Reload
.LBB94_35:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r14, %rdi
	movq	%rbx, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_36
# %bb.34:                               # %for.cond138
                                        #   in Loop: Header=BB94_35 Depth=1
	decl	%r12d
	jne	.LBB94_35
.LBB94_124:                             # %cleanup154.thread
	xorl	%ecx, %ecx
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	-80(%rbp), %r11                 # 8-byte Reload
	jmp	.LBB94_125
.LBB94_27:
	movl	$-3, %r14d
	jmp	.LBB94_161
.LBB94_37:                              # %if.else164
	leaq	.L.str.5.39(%rip), %rsi
	movq	%r12, %rdi
	callq	_ZN6Halide7Runtime8Internal9ends_withEPKcS3_@PLT
	testb	%al, %al
	je	.LBB94_122
# %bb.38:                               # %while.cond.preheader
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB94_39:                              # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%r12,%rdx)
	leaq	1(%rdx), %rdx
	jne	.LBB94_39
	.p2align	4, 0x90
.LBB94_40:                              # %while.body171
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$46, -2(%r12,%rdx)
	leaq	-1(%rdx), %rdx
	jne	.LBB94_40
# %bb.41:                               # %while.cond174.preheader
	movq	%rdx, %rax
	negq	%rax
	movl	$1, %ecx
	subq	%rdx, %rcx
	xorl	%r14d, %r14d
	.p2align	4, 0x90
.LBB94_42:                              # %while.cond174
                                        # =>This Inner Loop Header: Depth=1
	testq	%rcx, %rcx
	je	.LBB94_43
# %bb.44:                               # %land.rhs176
                                        #   in Loop: Header=BB94_42 Depth=1
	leaq	(%r12,%r14), %rsi
	decq	%r14
	incq	%rcx
	cmpb	$47, -2(%rdx,%rsi)
	jne	.LBB94_42
	jmp	.LBB94_45
.LBB94_122:                             # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631
	movl	-140(%rbp), %eax
	movl	-124(%rbp), %ecx
	movl	%eax, -4464(%rbp)
	movl	%ecx, -4460(%rbp)
	movl	-108(%rbp), %eax
	movl	%eax, -4456(%rbp)
	movl	-92(%rbp), %eax
	movl	%eax, -4452(%rbp)
	movl	%ebx, -4448(%rbp)
	leaq	-4464(%rbp), %rdi
	movl	$20, %esi
	movl	$1, %edx
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rcx
	callq	fwrite@PLT
	xorl	%ecx, %ecx
	testq	%rax, %rax
	movq	-80(%rbp), %r11                 # 8-byte Reload
	jne	.LBB94_125
# %bb.123:
	movl	$-12, %r14d
	jmp	.LBB94_162
.LBB94_31:
	movl	$-4, %r14d
	jmp	.LBB94_161
.LBB94_43:
	movq	%rax, %r14
.LBB94_45:                              # %while.end183
	leaq	-4464(%rbp), %r9
	movq	%r9, %rcx
	cmpq	$-1, %r14
	je	.LBB94_67
# %bb.46:                               # %while.body187.preheader
	cmpq	$-129, %r14
	jbe	.LBB94_48
# %bb.47:
	leaq	-4464(%rbp), %rcx
	movq	%r14, %rsi
	jmp	.LBB94_59
.LBB94_36:                              # %select.unfold
	movl	$-5, %r14d
	jmp	.LBB94_162
.LBB94_48:                              # %vector.memcheck
	movq	%r14, %r8
	notq	%r8
	leaq	-4464(%rbp), %rcx
	leaq	(%r12,%r14), %r10
	leaq	(%r12,%rdx), %rax
	decq	%rax
	cmpq	%rax, %rcx
	jae	.LBB94_50
# %bb.49:                               # %vector.memcheck
	leaq	(%r8,%rbp), %rax
	addq	$-4464, %rax                    # imm = 0xEE90
	leaq	(%r10,%rdx), %rdi
	movq	%r14, %rsi
	cmpq	%rax, %rdi
	jb	.LBB94_59
.LBB94_50:                              # %vector.ph41
	movq	%r8, %rsi
	andq	$-128, %rsi
	leaq	-128(%rsi), %rax
	movq	%rax, %rcx
	shrq	$7, %rcx
	incq	%rcx
	movl	%ecx, %r11d
	andl	$3, %r11d
	cmpq	$384, %rax                      # imm = 0x180
	jae	.LBB94_52
# %bb.51:
	xorl	%edi, %edi
	jmp	.LBB94_54
.LBB94_52:                              # %vector.ph41.new
	andq	$-4, %rcx
	negq	%rcx
	xorl	%edi, %edi
.LBB94_53:                              # %vector.body38
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r10,%rdi), %rax
	vmovups	(%rdx,%rax), %ymm0
	vmovups	32(%rdx,%rax), %ymm1
	vmovups	64(%rdx,%rax), %ymm2
	vmovups	96(%rdx,%rax), %ymm3
	vmovups	%ymm0, -4464(%rbp,%rdi)
	vmovups	%ymm1, -4432(%rbp,%rdi)
	vmovups	%ymm2, -4400(%rbp,%rdi)
	vmovups	%ymm3, -4368(%rbp,%rdi)
	vmovups	128(%rdx,%rax), %ymm0
	vmovups	160(%rdx,%rax), %ymm1
	vmovups	192(%rdx,%rax), %ymm2
	vmovups	224(%rdx,%rax), %ymm3
	vmovups	%ymm0, -4336(%rbp,%rdi)
	vmovups	%ymm1, -4304(%rbp,%rdi)
	vmovups	%ymm2, -4272(%rbp,%rdi)
	vmovups	%ymm3, -4240(%rbp,%rdi)
	vmovups	256(%rdx,%rax), %ymm0
	vmovups	288(%rdx,%rax), %ymm1
	vmovups	320(%rdx,%rax), %ymm2
	vmovups	352(%rdx,%rax), %ymm3
	vmovups	%ymm0, -4208(%rbp,%rdi)
	vmovups	%ymm1, -4176(%rbp,%rdi)
	vmovups	%ymm2, -4144(%rbp,%rdi)
	vmovups	%ymm3, -4112(%rbp,%rdi)
	vmovdqu	384(%rdx,%rax), %ymm0
	vmovdqu	416(%rdx,%rax), %ymm1
	vmovdqu	448(%rdx,%rax), %ymm2
	vmovdqu	480(%rdx,%rax), %ymm3
	vmovdqu	%ymm0, -4080(%rbp,%rdi)
	vmovdqu	%ymm1, -4048(%rbp,%rdi)
	vmovdqu	%ymm2, -4016(%rbp,%rdi)
	vmovdqu	%ymm3, -3984(%rbp,%rdi)
	addq	$512, %rdi                      # imm = 0x200
	addq	$4, %rcx
	jne	.LBB94_53
.LBB94_54:                              # %middle.block36.unr-lcssa
	testq	%r11, %r11
	je	.LBB94_57
# %bb.55:                               # %vector.body38.epil.preheader
	leaq	(%r14,%rdi), %rax
	leaq	-4464(%rbp), %rcx
	addq	%rdi, %rcx
	addq	$96, %rcx
	shlq	$7, %r11
	leaq	(%r12,%rax), %r10
	addq	$96, %r10
	xorl	%eax, %eax
.LBB94_56:                              # %vector.body38.epil
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r10,%rax), %rdi
	vmovdqu	-96(%rdx,%rdi), %ymm0
	vmovdqu	-64(%rdx,%rdi), %ymm1
	vmovdqu	-32(%rdx,%rdi), %ymm2
	vmovdqu	(%rdx,%rdi), %ymm3
	vmovdqu	%ymm0, -96(%rcx,%rax)
	vmovdqu	%ymm1, -64(%rcx,%rax)
	vmovdqu	%ymm2, -32(%rcx,%rax)
	vmovdqu	%ymm3, (%rcx,%rax)
	subq	$-128, %rax
	cmpq	%rax, %r11
	jne	.LBB94_56
.LBB94_57:                              # %middle.block36
	leaq	(%rsi,%rbp), %rcx
	addq	$-4464, %rcx                    # imm = 0xEE90
	cmpq	%r8, %rsi
	je	.LBB94_66
# %bb.58:
	addq	%r14, %rsi
.LBB94_59:                              # %while.body187.preheader212
	movq	$-2, %r8
	subq	%rsi, %r8
	movl	%esi, %edi
	andl	$7, %edi
	cmpq	$7, %rdi
	je	.LBB94_62
# %bb.60:                               # %while.body187.prol.preheader
	xorq	$7, %rdi
.LBB94_61:                              # %while.body187.prol
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%rsi), %rax
	incq	%rsi
	movzbl	(%rdx,%rax), %eax
	movb	%al, (%rcx)
	incq	%rcx
	decq	%rdi
	jne	.LBB94_61
.LBB94_62:                              # %while.body187.prol.loopexit
	cmpq	$7, %r8
	jb	.LBB94_66
# %bb.63:
	movl	%ebx, %edi
.LBB94_64:                              # %while.body187
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%rsi), %rax
	movzbl	(%rdx,%rax), %ebx
	movb	%bl, (%rcx)
	movzbl	1(%rdx,%rax), %ebx
	movb	%bl, 1(%rcx)
	movzbl	2(%rdx,%rax), %ebx
	movb	%bl, 2(%rcx)
	movzbl	3(%rdx,%rax), %ebx
	movb	%bl, 3(%rcx)
	movzbl	4(%rdx,%rax), %ebx
	movb	%bl, 4(%rcx)
	movzbl	5(%rdx,%rax), %ebx
	movb	%bl, 5(%rcx)
	movzbl	6(%rdx,%rax), %ebx
	movb	%bl, 6(%rcx)
	addq	$8, %rsi
	movzbl	7(%rdx,%rax), %eax
	movb	%al, 7(%rcx)
	addq	$8, %rcx
	cmpq	$-1, %rsi
	jne	.LBB94_64
# %bb.65:
	movl	%edi, %ebx
.LBB94_66:                              # %while.cond191.preheader
	leaq	-4208(%rbp), %rax
	cmpq	%rax, %rcx
	jae	.LBB94_86
.LBB94_67:                              # %iter.check
	subq	%rcx, %r9
	addq	$256, %r9                       # imm = 0x100
	cmpq	$4, %r9
	jae	.LBB94_68
.LBB94_83:                              # %while.body194.preheader
	movq	%rcx, %rax
	jmp	.LBB94_84
.LBB94_68:                              # %vector.main.loop.iter.check
	cmpq	$128, %r9
	jae	.LBB94_73
# %bb.69:
	xorl	%edx, %edx
	jmp	.LBB94_70
.LBB94_73:                              # %vector.ph70
	movq	%r9, %rdx
	andq	$-128, %rdx
	leaq	-128(%rdx), %rax
	movq	%rax, %rdi
	shrq	$7, %rdi
	incq	%rdi
	movl	%edi, %r8d
	andl	$7, %r8d
	cmpq	$896, %rax                      # imm = 0x380
	jae	.LBB94_75
# %bb.74:
	xorl	%eax, %eax
	jmp	.LBB94_77
.LBB94_75:                              # %vector.ph70.new
	leaq	992(%rcx), %rsi
	andq	$-8, %rdi
	negq	%rdi
	xorl	%eax, %eax
	vpxor	%xmm0, %xmm0, %xmm0
.LBB94_76:                              # %vector.body63
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	%ymm0, -992(%rsi,%rax)
	vmovdqu	%ymm0, -960(%rsi,%rax)
	vmovdqu	%ymm0, -928(%rsi,%rax)
	vmovdqu	%ymm0, -896(%rsi,%rax)
	vmovdqu	%ymm0, -864(%rsi,%rax)
	vmovdqu	%ymm0, -832(%rsi,%rax)
	vmovdqu	%ymm0, -800(%rsi,%rax)
	vmovdqu	%ymm0, -768(%rsi,%rax)
	vmovdqu	%ymm0, -736(%rsi,%rax)
	vmovdqu	%ymm0, -704(%rsi,%rax)
	vmovdqu	%ymm0, -672(%rsi,%rax)
	vmovdqu	%ymm0, -640(%rsi,%rax)
	vmovdqu	%ymm0, -608(%rsi,%rax)
	vmovdqu	%ymm0, -576(%rsi,%rax)
	vmovdqu	%ymm0, -544(%rsi,%rax)
	vmovdqu	%ymm0, -512(%rsi,%rax)
	vmovdqu	%ymm0, -480(%rsi,%rax)
	vmovdqu	%ymm0, -448(%rsi,%rax)
	vmovdqu	%ymm0, -416(%rsi,%rax)
	vmovdqu	%ymm0, -384(%rsi,%rax)
	vmovdqu	%ymm0, -352(%rsi,%rax)
	vmovdqu	%ymm0, -320(%rsi,%rax)
	vmovdqu	%ymm0, -288(%rsi,%rax)
	vmovdqu	%ymm0, -256(%rsi,%rax)
	vmovdqu	%ymm0, -224(%rsi,%rax)
	vmovdqu	%ymm0, -192(%rsi,%rax)
	vmovdqu	%ymm0, -160(%rsi,%rax)
	vmovdqu	%ymm0, -128(%rsi,%rax)
	vmovdqu	%ymm0, -96(%rsi,%rax)
	vmovdqu	%ymm0, -64(%rsi,%rax)
	vmovdqu	%ymm0, -32(%rsi,%rax)
	vmovdqu	%ymm0, (%rsi,%rax)
	addq	$1024, %rax                     # imm = 0x400
	addq	$8, %rdi
	jne	.LBB94_76
.LBB94_77:                              # %middle.block61.unr-lcssa
	testq	%r8, %r8
	je	.LBB94_80
# %bb.78:                               # %vector.body63.epil.preheader
	addq	%rcx, %rax
	addq	$96, %rax
	shlq	$7, %r8
	xorl	%esi, %esi
	vpxor	%xmm0, %xmm0, %xmm0
.LBB94_79:                              # %vector.body63.epil
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	%ymm0, -96(%rax,%rsi)
	vmovdqu	%ymm0, -64(%rax,%rsi)
	vmovdqu	%ymm0, -32(%rax,%rsi)
	vmovdqu	%ymm0, (%rax,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %r8
	jne	.LBB94_79
.LBB94_80:                              # %middle.block61
	cmpq	%r9, %rdx
	je	.LBB94_86
# %bb.81:                               # %vec.epilog.iter.check
	testb	$124, %r9b
	je	.LBB94_82
.LBB94_70:                              # %vec.epilog.ph
	leaq	-4464(%rbp), %rsi
	subq	%rcx, %rsi
	addq	$256, %rsi                      # imm = 0x100
	movq	%rsi, %rdi
	andq	$-4, %rdi
	leaq	(%rcx,%rdi), %rax
.LBB94_71:                              # %vec.epilog.vector.body
                                        # =>This Inner Loop Header: Depth=1
	movl	$0, (%rcx,%rdx)
	addq	$4, %rdx
	cmpq	%rdx, %rdi
	jne	.LBB94_71
# %bb.72:                               # %vec.epilog.middle.block
	cmpq	%rsi, %rdi
	je	.LBB94_86
.LBB94_84:                              # %while.body194.preheader
	leaq	-4208(%rbp), %rcx
.LBB94_85:                              # %while.body194
                                        # =>This Inner Loop Header: Depth=1
	movb	$0, (%rax)
	incq	%rax
	cmpq	%rax, %rcx
	jne	.LBB94_85
.LBB94_86:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705
	movl	%ebx, -52(%rbp)                 # 4-byte Spill
	vmovups	.L__const.halide_debug_to_file.header+96(%rip), %ymm0
	vmovups	%ymm0, -272(%rbp)
	vmovups	.L__const.halide_debug_to_file.header+64(%rip), %ymm0
	vmovups	%ymm0, -304(%rbp)
	vmovups	.L__const.halide_debug_to_file.header+32(%rip), %ymm0
	vmovups	%ymm0, -336(%rbp)
	vmovdqu	.L__const.halide_debug_to_file.header(%rip), %ymm0
	vmovdqu	%ymm0, -368(%rbp)
	movb	$0, -240(%rbp)
	leaq	-368(%rbp), %rdi
	movl	$1, %ebx
	movl	$128, %esi
	movl	$1, %edx
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	vzeroupper
	callq	fwrite@PLT
	movl	36(%r13), %r9d
	testl	%r9d, %r9d
	jle	.LBB94_87
# %bb.88:                               # %for.body.lr.ph.i.i
	movq	40(%r13), %r8
	cmpl	$17, %r9d
	jae	.LBB94_90
# %bb.89:
	xorl	%edx, %edx
	xorl	%ebx, %ebx
	jmp	.LBB94_93
.LBB94_87:
	xorl	%esi, %esi
	jmp	.LBB94_107
.LBB94_90:                              # %vector.ph96
	movl	%r9d, %edx
	andl	$15, %edx
	testq	%rdx, %rdx
	movl	$16, %esi
	cmovneq	%rdx, %rsi
	movq	%r9, %rdx
	subq	%rsi, %rdx
	leaq	200(%r8), %rsi
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	.LCPI94_1(%rip), %ymm1          # ymm1 = [0,1,2,3]
	vmovdqa64	.LCPI94_9(%rip), %xmm24 # xmm24 = [0,4,8,12]
	vpxor	%xmm3, %xmm3, %xmm3
	vpbroadcastq	%r8, %ymm9
	vpbroadcastq	.LCPI94_2(%rip), %ymm21 # ymm21 = [4,4,4,4]
	vpbroadcastq	.LCPI94_10(%rip), %ymm22 # ymm22 = [68,68,68,68]
	vpbroadcastq	.LCPI94_11(%rip), %ymm23 # ymm23 = [132,132,132,132]
	vpbroadcastq	.LCPI94_12(%rip), %ymm8 # ymm8 = [196,196,196,196]
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpbroadcastq	.LCPI94_13(%rip), %ymm10 # ymm10 = [16,16,16,16]
	movq	%rdx, %rdi
	vpxor	%xmm11, %xmm11, %xmm11
	vpxor	%xmm12, %xmm12, %xmm12
	vpxor	%xmm13, %xmm13, %xmm13
.LBB94_91:                              # %vector.body94
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	-192(%rsi), %ymm14
	vmovdqu	-128(%rsi), %ymm15
	vmovdqu64	-64(%rsi), %ymm16
	vmovdqu64	(%rsi), %ymm17
	vpermt2d	-160(%rsi), %ymm24, %ymm14
	vpermt2d	-96(%rsi), %ymm24, %ymm15
	vpermt2d	-32(%rsi), %ymm24, %ymm16
	vpermt2d	32(%rsi), %ymm24, %ymm17
	vpcmpgtd	%xmm3, %xmm14, %k4
	vpcmpgtd	%xmm3, %xmm15, %k3
	vpcmpgtd	%xmm3, %xmm16, %k2
	vpcmpgtd	%xmm3, %xmm17, %k1
	vpsllq	$4, %ymm1, %ymm18
	vpaddq	%ymm18, %ymm9, %ymm18
	vpaddq	%ymm21, %ymm18, %ymm19
	vpaddq	%ymm22, %ymm18, %ymm20
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm19), %xmm5 {%k5}
	vpaddq	%ymm23, %ymm18, %ymm19
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm20), %xmm6 {%k5}
	vpaddq	%ymm8, %ymm18, %ymm18
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm19), %xmm7 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm18), %xmm2 {%k5}
	vpmovzxdq	%xmm14, %ymm14          # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero
	vpmovzxdq	%xmm15, %ymm15          # ymm15 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpmovzxdq	%xmm16, %ymm16          # ymm16 = xmm16[0],zero,xmm16[1],zero,xmm16[2],zero,xmm16[3],zero
	vpmovzxdq	%xmm17, %ymm17          # ymm17 = xmm17[0],zero,xmm17[1],zero,xmm17[2],zero,xmm17[3],zero
	vpaddd	%xmm4, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm7, %xmm7
	vpmovsxdq	%xmm5, %ymm5
	vpmovsxdq	%xmm6, %ymm6
	vpmovsxdq	%xmm7, %ymm7
	vpmullq	%ymm14, %ymm5, %ymm5 {%k4} {z}
	vpmullq	%ymm15, %ymm6, %ymm6 {%k3} {z}
	vpmullq	%ymm16, %ymm7, %ymm7 {%k2} {z}
	vpaddd	%xmm4, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpmullq	%ymm17, %ymm2, %ymm2 {%k1} {z}
	vpaddq	%ymm5, %ymm0, %ymm0
	vpaddq	%ymm6, %ymm11, %ymm11
	vpaddq	%ymm7, %ymm12, %ymm12
	vpaddq	%ymm2, %ymm13, %ymm13
	vpaddq	%ymm1, %ymm10, %ymm1
	addq	$256, %rsi                      # imm = 0x100
	addq	$-16, %rdi
	jne	.LBB94_91
# %bb.92:                               # %middle.block92
	vpaddq	%ymm0, %ymm11, %ymm0
	vpaddq	%ymm0, %ymm12, %ymm0
	vpaddq	%ymm0, %ymm13, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rbx
.LBB94_93:                              # %for.body.i.i.preheader
	movq	%r9, %rsi
	subq	%rdx, %rsi
	shlq	$4, %rdx
	addq	%r8, %rdx
	addq	$8, %rdx
	jmp	.LBB94_94
	.p2align	4, 0x90
.LBB94_96:                              # %if.end.i.i
                                        #   in Loop: Header=BB94_94 Depth=1
	addq	$16, %rdx
	decq	%rsi
	je	.LBB94_97
.LBB94_94:                              # %for.body.i.i
                                        # =>This Inner Loop Header: Depth=1
	movl	(%rdx), %edi
	testl	%edi, %edi
	jle	.LBB94_96
# %bb.95:                               # %if.then.i.i
                                        #   in Loop: Header=BB94_94 Depth=1
	movslq	-4(%rdx), %rcx
	decq	%rcx
	imulq	%rdi, %rcx
	addq	%rcx, %rbx
	jmp	.LBB94_96
.LBB94_97:                              # %for.body.i13.i.preheader
	cmpl	$17, %r9d
	jae	.LBB94_99
# %bb.98:
	xorl	%edx, %edx
	xorl	%esi, %esi
	jmp	.LBB94_102
.LBB94_99:                              # %vector.ph129
	movl	%r9d, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	movl	$16, %esi
	cmovneq	%rcx, %rsi
	movq	%r9, %rdx
	subq	%rsi, %rdx
	leaq	200(%r8), %rsi
	vmovdqa	.LCPI94_1(%rip), %ymm1          # ymm1 = [0,1,2,3]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	.LCPI94_9(%rip), %xmm2          # xmm2 = [0,4,8,12]
	vpbroadcastq	%r8, %ymm8
	vpbroadcastq	.LCPI94_2(%rip), %ymm20 # ymm20 = [4,4,4,4]
	vpbroadcastq	.LCPI94_10(%rip), %ymm21 # ymm21 = [68,68,68,68]
	vpbroadcastq	.LCPI94_11(%rip), %ymm22 # ymm22 = [132,132,132,132]
	vpbroadcastq	.LCPI94_12(%rip), %ymm23 # ymm23 = [196,196,196,196]
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpbroadcastq	.LCPI94_13(%rip), %ymm9 # ymm9 = [16,16,16,16]
	movq	%rdx, %rdi
	vpxor	%xmm10, %xmm10, %xmm10
	vpxor	%xmm11, %xmm11, %xmm11
	vpxor	%xmm12, %xmm12, %xmm12
.LBB94_100:                             # %vector.body127
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	-192(%rsi), %ymm13
	vmovdqu	-128(%rsi), %ymm14
	vmovdqu	-64(%rsi), %ymm15
	vmovdqu64	(%rsi), %ymm16
	vpermt2d	-160(%rsi), %ymm2, %ymm13
	vpermt2d	-96(%rsi), %ymm2, %ymm14
	vpermt2d	-32(%rsi), %ymm2, %ymm15
	vpermt2d	32(%rsi), %ymm2, %ymm16
	vpmovd2m	%xmm13, %k4
	vpmovd2m	%xmm14, %k3
	vpmovd2m	%xmm15, %k2
	vpmovd2m	%xmm16, %k1
	vpmovzxdq	%xmm13, %ymm13          # ymm13 = xmm13[0],zero,xmm13[1],zero,xmm13[2],zero,xmm13[3],zero
	vpmovzxdq	%xmm14, %ymm14          # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero
	vpmovzxdq	%xmm15, %ymm15          # ymm15 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpsllq	$4, %ymm1, %ymm17
	vpaddq	%ymm17, %ymm8, %ymm17
	vpaddq	%ymm20, %ymm17, %ymm18
	vpmovzxdq	%xmm16, %ymm16          # ymm16 = xmm16[0],zero,xmm16[1],zero,xmm16[2],zero,xmm16[3],zero
	vpaddq	%ymm21, %ymm17, %ymm19
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm18), %xmm4 {%k5}
	vpaddq	%ymm22, %ymm17, %ymm18
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm19), %xmm5 {%k5}
	vpaddq	%ymm23, %ymm17, %ymm17
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm18), %xmm6 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm17), %xmm7 {%k5}
	vpaddd	%xmm3, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm6, %xmm6
	vpaddd	%xmm3, %xmm7, %xmm7
	vpmovzxdq	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
	vpmovzxdq	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmuldq	%ymm13, %ymm4, %ymm4 {%k4} {z}
	vpaddq	%ymm4, %ymm0, %ymm0
	vpmuldq	%ymm14, %ymm5, %ymm4 {%k3} {z}
	vpmuldq	%ymm15, %ymm6, %ymm5 {%k2} {z}
	vpaddq	%ymm4, %ymm10, %ymm10
	vpaddq	%ymm5, %ymm11, %ymm11
	vpmuldq	%ymm16, %ymm7, %ymm4 {%k1} {z}
	vpaddq	%ymm4, %ymm12, %ymm12
	vpaddq	%ymm1, %ymm9, %ymm1
	addq	$256, %rsi                      # imm = 0x100
	addq	$-16, %rdi
	jne	.LBB94_100
# %bb.101:                              # %middle.block125
	vpaddq	%ymm0, %ymm10, %ymm0
	vpaddq	%ymm0, %ymm11, %ymm0
	vpaddq	%ymm0, %ymm12, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rsi
.LBB94_102:                             # %for.body.i13.i.preheader211
	movq	%r9, %rdi
	subq	%rdx, %rdi
	shlq	$4, %rdx
	leaq	(%r8,%rdx), %rcx
	addq	$8, %rcx
	jmp	.LBB94_103
	.p2align	4, 0x90
.LBB94_105:                             # %if.end.i24.i
                                        #   in Loop: Header=BB94_103 Depth=1
	addq	$16, %rcx
	decq	%rdi
	je	.LBB94_106
.LBB94_103:                             # %for.body.i13.i
                                        # =>This Inner Loop Header: Depth=1
	movslq	(%rcx), %rdx
	testq	%rdx, %rdx
	jns	.LBB94_105
# %bb.104:                              # %if.then.i20.i
                                        #   in Loop: Header=BB94_103 Depth=1
	movslq	-4(%rcx), %rax
	decq	%rax
	imulq	%rdx, %rax
	addq	%rax, %rsi
	jmp	.LBB94_105
.LBB94_106:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
	incq	%rbx
.LBB94_107:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit
	subq	%rsi, %rbx
	movzbl	33(%r13), %r12d
	addq	$7, %r12
	shrq	$3, %r12
	imulq	%rbx, %r12
	movl	%r12d, %edx
	negl	%edx
	andl	$7, %edx
	leaq	(%rdx,%r12), %rax
	shrq	$32, %rax
	jne	.LBB94_108
# %bb.109:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687
	movl	$6, %r15d
	subl	%r14d, %r15d
	andl	$-8, %r15d
	cmpl	$2, %r9d
	movl	$2, %eax
	cmovgl	%r9d, %eax
	movl	$14, -212(%rbp)
	leal	4(,%rax,4), %ebx
                                        # kill: def $eax killed $eax killed $rax
	shll	$2, %eax
	andl	$-8, %ebx
	leal	(%r15,%rbx), %ecx
	addl	%r12d, %ecx
	movq	%rdx, -64(%rbp)                 # 8-byte Spill
	addl	%edx, %ecx
	addl	$40, %ecx
	movl	%ecx, -208(%rbp)
	movabsq	$34359738374, %rcx              # imm = 0x800000006
	movq	%rcx, -204(%rbp)
	movslq	-52(%rbp), %rdx                 # 4-byte Folded Reload
	movq	_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE@GOTPCREL(%rip), %rcx
	movq	%rdx, -72(%rbp)                 # 8-byte Spill
	movzbl	(%rcx,%rdx), %ecx
	movl	%ecx, -196(%rbp)
	movabsq	$21474836481, %rcx              # imm = 0x500000001
	movq	%rcx, -192(%rbp)
	movl	%eax, -184(%rbp)
	leaq	-212(%rbp), %rdi
	movl	$32, %esi
	movl	$1, %edx
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	vzeroupper
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_110
# %bb.111:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676
	movl	-140(%rbp), %eax
	movl	-124(%rbp), %ecx
	movl	%eax, -180(%rbp)
	movl	%ecx, -176(%rbp)
	movl	-108(%rbp), %eax
	movl	%eax, -172(%rbp)
	movl	-92(%rbp), %eax
	movl	%eax, -168(%rbp)
	movslq	%ebx, %rsi
	leaq	-180(%rbp), %rdi
	movl	$1, %edx
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_112
# %bb.114:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664
	notl	%r14d
	movl	$1, -164(%rbp)
	movl	%r14d, -160(%rbp)
	leaq	-164(%rbp), %rdi
	movl	$8, %esi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	movl	$1, %edx
	testq	%rax, %rax
	je	.LBB94_115
# %bb.116:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652
	movl	%r15d, %esi
	leaq	-4464(%rbp), %rdi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_117
# %bb.118:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642
	movq	_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE@GOTPCREL(%rip), %rax
	movq	-72(%rbp), %rcx                 # 8-byte Reload
	movzbl	(%rax,%rcx), %eax
	movl	%eax, -156(%rbp)
	movl	%r12d, -152(%rbp)
	leaq	-156(%rbp), %rdi
	movl	$8, %esi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	xorl	%edx, %edx
	testq	%rax, %rax
	sete	%dl
	movl	$-11, %r14d
	jmp	.LBB94_119
.LBB94_108:                             # %cleanup278.thread
	leaq	.L.str.6.40(%rip), %rsi
	movq	%r15, %rdi
	vzeroupper
	callq	halide_error@PLT
	movl	$-6, %r14d
	jmp	.LBB94_161
.LBB94_110:
	movl	$-7, %r14d
	movl	$1, %edx
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	jmp	.LBB94_113
.LBB94_112:
	movl	$-8, %r14d
	movl	$1, %edx
.LBB94_113:                             # %cleanup278
	movq	-80(%rbp), %r11                 # 8-byte Reload
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	jmp	.LBB94_121
.LBB94_115:
	movl	$-9, %r14d
.LBB94_119:                             # %cleanup273
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	jmp	.LBB94_120
.LBB94_117:
	movl	$-10, %r14d
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	movl	$1, %edx
.LBB94_120:                             # %cleanup273
	movq	-80(%rbp), %r11                 # 8-byte Reload
.LBB94_121:                             # %cleanup278
	testl	%edx, %edx
	jne	.LBB94_162
.LBB94_125:                             # %if.end311
	movl	$4096, %eax                     # imm = 0x1000
	xorl	%edx, %edx
	divl	%r11d
	xorl	%r9d, %r9d
	movl	-92(%rbp), %esi
	testl	%esi, %esi
	jle	.LBB94_156
# %bb.126:                              # %for.body322.lr.ph
	movq	%rcx, -64(%rbp)                 # 8-byte Spill
	movl	-96(%rbp), %edi
	movl	%eax, %ecx
	imull	%r11d, %ecx
	movq	%rcx, -224(%rbp)                # 8-byte Spill
	movl	-112(%rbp), %r8d
	movl	-108(%rbp), %r10d
	vmovdqa64	.LCPI94_9(%rip), %xmm20 # xmm20 = [0,4,8,12]
	vmovdqa64	.LCPI94_16(%rip), %xmm21 # xmm21 = [2,6,10,14]
	movl	%r8d, %ecx
	movl	%r8d, -72(%rbp)                 # 4-byte Spill
	movl	%edi, %ecx
	movl	%edi, -52(%rbp)                 # 4-byte Spill
	movl	%eax, -148(%rbp)                # 4-byte Spill
.LBB94_127:                             # %for.body322
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB94_129 Depth 2
                                        #       Child Loop BB94_131 Depth 3
                                        #         Child Loop BB94_133 Depth 4
                                        #           Child Loop BB94_140 Depth 5
                                        #           Child Loop BB94_143 Depth 5
	testl	%r10d, %r10d
	jle	.LBB94_152
# %bb.128:                              # %for.body333.preheader
                                        #   in Loop: Header=BB94_127 Depth=1
	movl	-128(%rbp), %esi
	movl	-124(%rbp), %edi
	movl	%esi, %r15d
.LBB94_129:                             # %for.body333
                                        #   Parent Loop BB94_127 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB94_131 Depth 3
                                        #         Child Loop BB94_133 Depth 4
                                        #           Child Loop BB94_140 Depth 5
                                        #           Child Loop BB94_143 Depth 5
	testl	%edi, %edi
	jle	.LBB94_150
# %bb.130:                              # %for.body344.preheader
                                        #   in Loop: Header=BB94_129 Depth=2
	movl	-140(%rbp), %ecx
	jmp	.LBB94_131
.LBB94_147:                             # %for.inc389.loopexit
                                        #   in Loop: Header=BB94_131 Depth=3
	movl	-128(%rbp), %esi
	movl	-124(%rbp), %edi
	movl	%r14d, %r9d
.LBB94_148:                             # %for.inc389
                                        #   in Loop: Header=BB94_131 Depth=3
	incl	%r15d
	leal	(%rsi,%rdi), %eax
	cmpl	%eax, %r15d
	jge	.LBB94_149
.LBB94_131:                             # %for.body344
                                        #   Parent Loop BB94_127 Depth=1
                                        #     Parent Loop BB94_129 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB94_133 Depth 4
                                        #           Child Loop BB94_140 Depth 5
                                        #           Child Loop BB94_143 Depth 5
	testl	%ecx, %ecx
	jle	.LBB94_148
# %bb.132:                              # %for.body355.preheader
                                        #   in Loop: Header=BB94_131 Depth=3
	movl	-144(%rbp), %r12d
	jmp	.LBB94_133
	.p2align	4, 0x90
.LBB94_146:                             # %for.inc384
                                        #   in Loop: Header=BB94_133 Depth=4
	incl	%r12d
	movl	-140(%rbp), %ecx
	movl	-144(%rbp), %eax
	addl	%ecx, %eax
	movl	%r14d, %r9d
	cmpl	%eax, %r12d
	movq	-80(%rbp), %r11                 # 8-byte Reload
	vmovdqa64	.LCPI94_9(%rip), %xmm20 # xmm20 = [0,4,8,12]
	vmovdqa64	.LCPI94_16(%rip), %xmm21 # xmm21 = [2,6,10,14]
	jge	.LBB94_147
.LBB94_133:                             # %for.body355
                                        #   Parent Loop BB94_127 Depth=1
                                        #     Parent Loop BB94_129 Depth=2
                                        #       Parent Loop BB94_131 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB94_140 Depth 5
                                        #           Child Loop BB94_143 Depth 5
	leal	1(%r9), %r14d
	movl	%r12d, -368(%rbp)
	movl	%r15d, -364(%rbp)
	movl	-72(%rbp), %eax                 # 4-byte Reload
	movl	%eax, -360(%rbp)
	movl	-52(%rbp), %eax                 # 4-byte Reload
	movl	%eax, -356(%rbp)
	movl	36(%r13), %r10d
	testl	%r10d, %r10d
	jle	.LBB94_134
# %bb.135:                              # %for.body.lr.ph.i
                                        #   in Loop: Header=BB94_133 Depth=4
	movq	40(%r13), %r8
	movslq	8(%r8), %rsi
	movslq	(%r8), %rdi
	movslq	%r12d, %rdx
	subq	%rdi, %rdx
	imulq	%rsi, %rdx
	cmpl	$1, %r10d
	je	.LBB94_144
# %bb.136:                              # %for.body.i.for.body.i_crit_edge.preheader
                                        #   in Loop: Header=BB94_133 Depth=4
	movslq	24(%r8), %rsi
	movslq	16(%r8), %rbx
	movslq	%r15d, %rdi
	subq	%rbx, %rdi
	imulq	%rsi, %rdi
	addq	%rdx, %rdi
	cmpl	$2, %r10d
	jne	.LBB94_138
# %bb.137:                              #   in Loop: Header=BB94_133 Depth=4
	movq	%rdi, %rdx
	jmp	.LBB94_144
	.p2align	4, 0x90
.LBB94_134:                             #   in Loop: Header=BB94_133 Depth=4
	xorl	%edx, %edx
	jmp	.LBB94_144
.LBB94_138:                             # %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph
                                        #   in Loop: Header=BB94_133 Depth=4
	leaq	-2(%r10), %rbx
	movl	$2, %esi
	cmpq	$17, %rbx
	jb	.LBB94_142
# %bb.139:                              # %vector.ph170
                                        #   in Loop: Header=BB94_133 Depth=4
	movl	%ebx, %edx
	andl	$15, %edx
	testq	%rdx, %rdx
	movl	$16, %eax
	cmoveq	%rax, %rdx
	subq	%rdx, %rbx
	movq	%rbx, %rsi
	addq	$2, %rsi
	vmovq	%rdi, %xmm0
	movq	%r10, %rdi
	subq	%rdx, %rdi
	addq	$-2, %rdi
	leaq	224(%r8), %rbx
	vpxor	%xmm1, %xmm1, %xmm1
	xorl	%edx, %edx
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB94_140:                             # %vector.body168
                                        #   Parent Loop BB94_127 Depth=1
                                        #     Parent Loop BB94_129 Depth=2
                                        #       Parent Loop BB94_131 Depth=3
                                        #         Parent Loop BB94_133 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovdqu	-360(%rbp,%rdx,4), %xmm4
	vmovdqu	-344(%rbp,%rdx,4), %xmm5
	vmovdqu	-328(%rbp,%rdx,4), %xmm6
	vmovdqu	-312(%rbp,%rdx,4), %xmm7
	vmovdqu	-192(%rbx), %ymm8
	vmovdqu	-160(%rbx), %ymm9
	vmovdqu	-128(%rbx), %ymm10
	vmovdqu	-96(%rbx), %ymm11
	vmovdqa	%ymm8, %ymm12
	vpermt2d	%ymm9, %ymm20, %ymm12
	vmovdqa	%ymm10, %ymm13
	vmovdqu	-32(%rbx), %ymm14
	vpermt2d	%ymm11, %ymm20, %ymm13
	vmovdqu	-64(%rbx), %ymm15
	vmovdqa64	%ymm15, %ymm16
	vpermt2d	%ymm14, %ymm20, %ymm16
	vmovdqu64	32(%rbx), %ymm17
	vmovdqu64	(%rbx), %ymm18
	vmovdqa64	%ymm18, %ymm19
	vpermt2d	%ymm17, %ymm20, %ymm19
	vpermt2d	%ymm9, %ymm21, %ymm8
	vpermt2d	%ymm11, %ymm21, %ymm10
	vpermt2d	%ymm14, %ymm21, %ymm15
	vpermt2d	%ymm17, %ymm21, %ymm18
	vpmovzxdq	%xmm8, %ymm8            # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero
	vpmovzxdq	%xmm10, %ymm9           # ymm9 = xmm10[0],zero,xmm10[1],zero,xmm10[2],zero,xmm10[3],zero
	vpmovzxdq	%xmm15, %ymm10          # ymm10 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpmovzxdq	%xmm18, %ymm11          # ymm11 = xmm18[0],zero,xmm18[1],zero,xmm18[2],zero,xmm18[3],zero
	vpsubd	%xmm12, %xmm4, %xmm4
	vpsubd	%xmm13, %xmm5, %xmm5
	vpsubd	%xmm16, %xmm6, %xmm6
	vpsubd	%xmm19, %xmm7, %xmm7
	vpmovzxdq	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
	vpmuldq	%ymm8, %ymm4, %ymm4
	vpaddq	%ymm0, %ymm4, %ymm0
	vpmovzxdq	%xmm5, %ymm4            # ymm4 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
	vpmuldq	%ymm9, %ymm4, %ymm4
	vpaddq	%ymm1, %ymm4, %ymm1
	vpmovzxdq	%xmm6, %ymm4            # ymm4 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmuldq	%ymm10, %ymm4, %ymm4
	vpaddq	%ymm2, %ymm4, %ymm2
	vpmovzxdq	%xmm7, %ymm4            # ymm4 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmuldq	%ymm11, %ymm4, %ymm4
	vpaddq	%ymm3, %ymm4, %ymm3
	addq	$16, %rdx
	addq	$256, %rbx                      # imm = 0x100
	cmpq	%rdx, %rdi
	jne	.LBB94_140
# %bb.141:                              # %middle.block166
                                        #   in Loop: Header=BB94_133 Depth=4
	vpaddq	%ymm0, %ymm1, %ymm0
	vpaddq	%ymm0, %ymm2, %ymm0
	vpaddq	%ymm0, %ymm3, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdi
.LBB94_142:                             # %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader
                                        #   in Loop: Header=BB94_133 Depth=4
	movq	%rsi, %rdx
	shlq	$4, %rdx
	leaq	(%r8,%rdx), %rbx
	addq	$8, %rbx
	movq	%rdi, %rdx
	.p2align	4, 0x90
.LBB94_143:                             # %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge
                                        #   Parent Loop BB94_127 Depth=1
                                        #     Parent Loop BB94_129 Depth=2
                                        #       Parent Loop BB94_131 Depth=3
                                        #         Parent Loop BB94_133 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	movslq	-368(%rbp,%rsi,4), %rdi
	movslq	(%rbx), %rax
	movslq	-8(%rbx), %rcx
	subq	%rcx, %rdi
	imulq	%rax, %rdi
	addq	%rdi, %rdx
	incq	%rsi
	addq	$16, %rbx
	cmpq	%rsi, %r10
	jne	.LBB94_143
	.p2align	4, 0x90
.LBB94_144:                             # %_ZNK15halide_buffer_t10address_ofEPKi.exit
                                        #   in Loop: Header=BB94_133 Depth=4
	movzbl	33(%r13), %esi
	addq	$7, %rsi
	shrq	$3, %rsi
	imulq	%rdx, %rsi
	addq	16(%r13), %rsi
	imull	%r11d, %r9d
	movslq	%r9d, %rax
	leaq	(%rax,%rbp), %rdi
	addq	$-4464, %rdi                    # imm = 0xEE90
	movq	%r11, %rdx
	vzeroupper
	callq	memcpy@PLT
	cmpl	-148(%rbp), %r14d               # 4-byte Folded Reload
	jne	.LBB94_146
# %bb.145:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619
                                        #   in Loop: Header=BB94_133 Depth=4
	movl	$1, %edx
	leaq	-4464(%rbp), %rdi
	movq	-224(%rbp), %rsi                # 8-byte Reload
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	callq	fwrite@PLT
	xorl	%r14d, %r14d
	testq	%rax, %rax
	jne	.LBB94_146
	jmp	.LBB94_160
.LBB94_149:                             # %for.inc394.loopexit
                                        #   in Loop: Header=BB94_129 Depth=2
	movl	-112(%rbp), %r8d
	movl	-108(%rbp), %r10d
	movl	%esi, %r15d
.LBB94_150:                             # %for.inc394
                                        #   in Loop: Header=BB94_129 Depth=2
	movl	-72(%rbp), %ecx                 # 4-byte Reload
	incl	%ecx
	leal	(%r10,%r8), %eax
	movl	%ecx, -72(%rbp)                 # 4-byte Spill
	cmpl	%eax, %ecx
	jl	.LBB94_129
# %bb.151:                              # %for.inc399.loopexit
                                        #   in Loop: Header=BB94_127 Depth=1
	movl	-96(%rbp), %edi
	movl	-92(%rbp), %esi
	movl	%r8d, %eax
	movl	%r8d, -72(%rbp)                 # 4-byte Spill
	movq	-48(%rbp), %rbx                 # 8-byte Reload
.LBB94_152:                             # %for.inc399
                                        #   in Loop: Header=BB94_127 Depth=1
	movl	-52(%rbp), %edx                 # 4-byte Reload
	incl	%edx
	leal	(%rsi,%rdi), %ecx
	movl	%edx, -52(%rbp)                 # 4-byte Spill
	cmpl	%ecx, %edx
	jl	.LBB94_127
# %bb.153:                              # %for.end403
	testl	%r9d, %r9d
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	jle	.LBB94_156
# %bb.154:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612
	imull	%r11d, %r9d
	movslq	%r9d, %rsi
	leaq	-4464(%rbp), %rdi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	testq	%rax, %rax
	je	.LBB94_155
.LBB94_156:                             # %if.end412
	movq	$0, -368(%rbp)
	testl	%ecx, %ecx
	je	.LBB94_159
# %bb.157:                              # %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit
	movl	%ecx, %esi
	leaq	-368(%rbp), %rdi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB94_158
.LBB94_159:                             # %if.end423
	xorl	%r14d, %r14d
	jmp	.LBB94_162
.LBB94_160:                             # %cleanup425.loopexit
	movl	$-13, %r14d
.LBB94_161:                             # %cleanup433
	movq	-48(%rbp), %rbx                 # 8-byte Reload
.LBB94_162:                             # %cleanup433
	movq	%rbx, %rdi
	callq	fclose@PLT
	jmp	.LBB94_163
.LBB94_158:
	movl	$-16, %r14d
	jmp	.LBB94_162
.LBB94_82:
	addq	%rdx, %rcx
	jmp	.LBB94_83
.LBB94_155:
	movl	$-14, %r14d
	jmp	.LBB94_162
.Lfunc_end94:
	.size	halide_debug_to_file, .Lfunc_end94-halide_debug_to_file
                                        # -- End function
	.section	.text.halide_cache_cleanup,"ax",@progbits
	.weak	halide_cache_cleanup            # -- Begin function halide_cache_cleanup
	.p2align	4, 0x90
	.type	halide_cache_cleanup,@function
halide_cache_cleanup:                   # @halide_cache_cleanup
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_memoization_cache_cleanup@PLT # TAILCALL
.Lfunc_end95:
	.size	halide_cache_cleanup, .Lfunc_end95-halide_cache_cleanup
                                        # -- End function
	.section	.text.halide_memoization_cache_cleanup,"ax",@progbits
	.weak	halide_memoization_cache_cleanup # -- Begin function halide_memoization_cache_cleanup
	.p2align	4, 0x90
	.type	halide_memoization_cache_cleanup,@function
halide_memoization_cache_cleanup:       # @halide_memoization_cache_cleanup
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r14
	movl	$2048, %r15d                    # imm = 0x800
	addq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r15
	jmp	.LBB96_1
	.p2align	4, 0x90
.LBB96_3:                               # %while.end
                                        #   in Loop: Header=BB96_1 Depth=1
	addq	$8, %r14
	cmpq	%r15, %r14
	je	.LBB96_4
.LBB96_1:                               # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB96_2 Depth 2
	movq	(%r14), %rbx
	movq	$0, (%r14)
	testq	%rbx, %rbx
	je	.LBB96_3
	.p2align	4, 0x90
.LBB96_2:                               # %while.body
                                        #   Parent Loop BB96_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r12
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	%r12, %rbx
	testq	%r12, %r12
	jne	.LBB96_2
	jmp	.LBB96_3
.LBB96_4:                               # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end96:
	.size	halide_memoization_cache_cleanup, .Lfunc_end96-halide_memoization_cache_cleanup
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv # -- Begin function _ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,@function
_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv: # @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r14
	cmpl	$0, 56(%rdi)
	je	.LBB97_3
# %bb.1:                                # %for.body.lr.ph
	xorl	%ebx, %ebx
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB97_2:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	72(%r14), %rsi
	addq	%rbx, %rsi
	xorl	%edi, %edi
	callq	halide_device_free@PLT
	movq	72(%r14), %rax
	movq	16(%rax,%rbx), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_free@PLT
	incq	%r15
	movl	56(%r14), %eax
	addq	$56, %rbx
	cmpq	%rax, %r15
	jb	.LBB97_2
.LBB97_3:                               # %for.cond.cleanup
	movq	24(%r14), %rsi
	xorl	%edi, %edi
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_free@PLT                 # TAILCALL
.Lfunc_end97:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv, .Lfunc_end97-_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh # -- Begin function _ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,@function
_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh: # @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	-64(%rdi), %rax
	popq	%rbp
	retq
.Lfunc_end98:
	.size	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh, .Lfunc_end98-_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx # -- Begin function _ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx,@function
_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx: # @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, %r12
	movq	%rdx, %rbx
	movl	%esi, %r15d
	movq	%rdi, %r14
	testl	%esi, %esi
	js	.LBB99_4
	.p2align	4, 0x90
.LBB99_1:                               # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	movl	%r15d, %eax
	cmpq	$1, 24(%r14,%rax,8)
	jne	.LBB99_4
# %bb.2:                                # %while.body
                                        #   in Loop: Header=BB99_1 Depth=1
	leal	-1(%r15), %eax
	testl	%r15d, %r15d
	movl	%eax, %r15d
	jg	.LBB99_1
	jmp	.LBB99_3
.LBB99_4:                               # %while.end
	cmpl	$-1, %r15d
	je	.LBB99_3
# %bb.5:                                # %for.cond.preheader
	movslq	%r15d, %rax
	cmpq	$0, 24(%r14,%rax,8)
	je	.LBB99_8
# %bb.6:                                # %for.body.lr.ph
	decl	%r15d
	xorl	%r13d, %r13d
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	.p2align	4, 0x90
.LBB99_7:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	movl	%r15d, %esi
	movq	%rbx, %rdx
	movq	%r12, %rcx
	callq	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx@PLT
	movq	-48(%rbp), %rax                 # 8-byte Reload
	addq	152(%r14,%rax,8), %rbx
	addq	280(%r14,%rax,8), %r12
	incq	%r13
	cmpq	24(%r14,%rax,8), %r13
	jb	.LBB99_7
.LBB99_8:                               # %if.end
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB99_3:                               # %if.then
	addq	(%r14), %rbx
	addq	8(%r14), %r12
	movq	408(%r14), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	memcpy@PLT                      # TAILCALL
.Lfunc_end99:
	.size	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx, .Lfunc_end99-_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv # -- Begin function _ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv,@function
_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv: # @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	(%rdi), %rax
	cmpq	8(%rdi), %rax
	jne	.LBB100_2
# %bb.1:                                # %if.end
	popq	%rbp
	retq
.LBB100_2:                              # %if.then
	movq	16(%rdi), %rdx
	movl	$15, %esi
	xorl	%ecx, %ecx
	popq	%rbp
	jmp	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx@PLT # TAILCALL
.Lfunc_end100:
	.size	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv, .Lfunc_end100-_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
                                        # -- End function
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3                               # -- Begin function _ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
.LCPI101_0:
	.quad	1                               # 0x1
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI101_1:
	.long	2                               # 0x2
	.long	6                               # 0x6
	.long	10                              # 0xa
	.long	14                              # 0xe
.LCPI101_2:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.section	.text._ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b,@function
_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b: # @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$456, %rsp                      # imm = 0x1C8
	movq	%rdi, %r14
	testb	%dl, %dl
	je	.LBB101_2
# %bb.1:                                # %cond.true
	movq	16(%rsi), %rax
	movq	%rax, -488(%rbp)
	testb	%r8b, %r8b
	jne	.LBB101_4
.LBB101_5:                              # %cond.false6
	movq	(%rcx), %rax
	jmp	.LBB101_6
.LBB101_2:                              # %cond.false
	movq	(%rsi), %rax
	movq	%rax, -488(%rbp)
	testb	%r8b, %r8b
	je	.LBB101_5
.LBB101_4:                              # %cond.true4
	movq	16(%rcx), %rax
.LBB101_6:                              # %cond.end8
	movq	%rax, -480(%rbp)
	movzbl	33(%rsi), %r10d
	addq	$7, %r10
	shrq	$3, %r10
	movq	%r10, -80(%rbp)
	vpbroadcastq	.LCPI101_0(%rip), %ymm0 # ymm0 = [1,1,1,1]
	vmovdqu	%ymm0, -464(%rbp)
	vpxor	%xmm1, %xmm1, %xmm1
	vmovdqu	%ymm1, -336(%rbp)
	vmovdqu	%ymm1, -208(%rbp)
	vmovdqu	%ymm0, -432(%rbp)
	vmovdqu	%ymm1, -304(%rbp)
	vmovdqu	%ymm1, -176(%rbp)
	vmovdqu	%ymm0, -400(%rbp)
	vmovdqu	%ymm1, -272(%rbp)
	vmovdqu	%ymm1, -144(%rbp)
	vmovdqu	%ymm0, -368(%rbp)
	vmovdqu	%ymm1, -240(%rbp)
	vmovdqu	%ymm1, -112(%rbp)
	movl	36(%rsi), %edi
	testl	%edi, %edi
	movq	%rdi, -48(%rbp)                 # 8-byte Spill
	jle	.LBB101_7
# %bb.12:                               # %for.body19.lr.ph
	movq	40(%rsi), %r8
	movq	40(%rcx), %r11
	cmpl	$16, %edi
	ja	.LBB101_16
# %bb.13:
	xorl	%r15d, %r15d
	xorl	%eax, %eax
	jmp	.LBB101_14
.LBB101_7:
	xorl	%eax, %eax
	jmp	.LBB101_8
.LBB101_16:                             # %vector.ph
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %edx
	cmovneq	%rax, %rdx
	movq	%rdi, %r15
	subq	%rdx, %r15
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa64	.LCPI101_1(%rip), %xmm22 # xmm22 = [2,6,10,14]
	movl	$192, %eax
	vmovdqa64	.LCPI101_2(%rip), %xmm23 # xmm23 = [0,4,8,12]
	movq	%r15, %r9
	vpxor	%xmm3, %xmm3, %xmm3
	vpxor	%xmm4, %xmm4, %xmm4
	vpxor	%xmm5, %xmm5, %xmm5
	.p2align	4, 0x90
.LBB101_17:                             # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	-192(%r8,%rax), %ymm6
	vmovdqu	-160(%r8,%rax), %ymm7
	vmovdqu	-128(%r8,%rax), %ymm8
	vmovdqu	-96(%r8,%rax), %ymm9
	vmovdqa	%ymm6, %ymm10
	vpermt2d	%ymm7, %ymm22, %ymm10
	vmovdqa	%ymm8, %ymm11
	vpermt2d	%ymm9, %ymm22, %ymm11
	vmovdqu	-32(%r8,%rax), %ymm12
	vmovdqu	-64(%r8,%rax), %ymm13
	vmovdqa	%ymm13, %ymm14
	vpermt2d	%ymm12, %ymm22, %ymm14
	vmovdqu	32(%r8,%rax), %ymm15
	vmovdqu64	(%r8,%rax), %ymm16
	vmovdqa64	%ymm16, %ymm17
	vpermt2d	%ymm15, %ymm22, %ymm17
	vpmovzxdq	%xmm10, %ymm10          # ymm10 = xmm10[0],zero,xmm10[1],zero,xmm10[2],zero,xmm10[3],zero
	vpmovzxdq	%xmm11, %ymm11          # ymm11 = xmm11[0],zero,xmm11[1],zero,xmm11[2],zero,xmm11[3],zero
	vpmovzxdq	%xmm14, %ymm14          # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero
	vpmovzxdq	%xmm17, %ymm17          # ymm17 = xmm17[0],zero,xmm17[1],zero,xmm17[2],zero,xmm17[3],zero
	vmovdqu64	-192(%r11,%rax), %ymm18
	vmovdqu64	-128(%r11,%rax), %ymm19
	vmovdqu64	-64(%r11,%rax), %ymm20
	vmovdqu64	(%r11,%rax), %ymm21
	vpermt2d	-160(%r11,%rax), %ymm23, %ymm18
	vpermt2d	%ymm7, %ymm23, %ymm6
	vpsubd	%xmm6, %xmm18, %xmm6
	vpermt2d	-96(%r11,%rax), %ymm23, %ymm19
	vpermt2d	%ymm9, %ymm23, %ymm8
	vpsubd	%xmm8, %xmm19, %xmm7
	vpermt2d	-32(%r11,%rax), %ymm23, %ymm20
	vpermt2d	%ymm12, %ymm23, %ymm13
	vpsubd	%xmm13, %xmm20, %xmm1
	vpermt2d	32(%r11,%rax), %ymm23, %ymm21
	vpermt2d	%ymm15, %ymm23, %ymm16
	vpsubd	%xmm16, %xmm21, %xmm2
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmuldq	%ymm10, %ymm6, %ymm6
	vpaddq	%ymm0, %ymm6, %ymm0
	vpmovzxdq	%xmm7, %ymm6            # ymm6 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmuldq	%ymm11, %ymm6, %ymm6
	vpaddq	%ymm3, %ymm6, %ymm3
	vpmovzxdq	%xmm1, %ymm1            # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpmuldq	%ymm14, %ymm1, %ymm1
	vpaddq	%ymm4, %ymm1, %ymm4
	vpmovzxdq	%xmm2, %ymm1            # ymm1 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
	vpmuldq	%ymm17, %ymm1, %ymm1
	vpaddq	%ymm5, %ymm1, %ymm5
	addq	$256, %rax                      # imm = 0x100
	addq	$-16, %r9
	jne	.LBB101_17
# %bb.18:                               # %middle.block
	vpaddq	%ymm0, %ymm3, %ymm0
	vpaddq	%ymm0, %ymm4, %ymm0
	vpaddq	%ymm0, %ymm5, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	movq	-48(%rbp), %rdi                 # 8-byte Reload
.LBB101_14:                             # %for.body19.preheader
	movq	%rdi, %rdx
	subq	%r15, %rdx
	shlq	$4, %r15
	.p2align	4, 0x90
.LBB101_15:                             # %for.body19
                                        # =>This Inner Loop Header: Depth=1
	movslq	8(%r8,%r15), %r9
	movslq	(%r11,%r15), %rbx
	movslq	(%r8,%r15), %rdi
	subq	%rdi, %rbx
	imulq	%r9, %rbx
	addq	%rbx, %rax
	addq	$16, %r15
	decq	%rdx
	jne	.LBB101_15
.LBB101_8:                              # %for.cond.cleanup18
	imulq	%r10, %rax
	movq	%rax, -472(%rbp)
	movq	-48(%rbp), %r8                  # 8-byte Reload
	cmpl	36(%rcx), %r8d
	jne	.LBB101_11
# %bb.9:                                # %lor.lhs.false
	cmpl	$16, %r8d
	jg	.LBB101_11
# %bb.10:                               # %lor.lhs.false
	movzbl	33(%rcx), %eax
	addl	$7, %eax
	shrl	$3, %eax
	cmpl	%eax, %r10d
	jne	.LBB101_11
# %bb.19:                               # %if.end
	testl	%r10d, %r10d
	je	.LBB101_11
# %bb.20:                               # %for.cond54.preheader
	testl	%r8d, %r8d
	jle	.LBB101_32
# %bb.21:                               # %for.body58.lr.ph
	movq	%r14, -56(%rbp)                 # 8-byte Spill
	movq	40(%rcx), %r11
	movq	40(%rsi), %r9
	leaq	-208(%rbp), %rsi
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%r10, -64(%rbp)                 # 8-byte Spill
	movq	%r11, -72(%rbp)                 # 8-byte Spill
	jmp	.LBB101_22
	.p2align	4, 0x90
.LBB101_41:                             # %for.cond.cleanup94
                                        #   in Loop: Header=BB101_22 Depth=1
	movslq	4(%r11,%r13), %rcx
	movq	%rcx, -464(%rbp,%r14,8)
	movq	%rdx, -208(%rbp,%r14,8)
	movq	%r15, -336(%rbp,%r14,8)
	incq	%rax
	addq	$8, %rsi
	decq	%r12
	cmpq	%r8, %rax
	je	.LBB101_28
.LBB101_22:                             # %for.body58
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB101_25 Depth 2
                                        #     Child Loop BB101_38 Depth 2
                                        #     Child Loop BB101_42 Depth 2
	movq	%rax, %r13
	shlq	$4, %r13
	movslq	8(%r11,%r13), %rdx
	imulq	%r10, %rdx
	movl	$0, %ecx
	testq	%rax, %rax
	je	.LBB101_35
# %bb.23:                               # %for.body81.lr.ph
                                        #   in Loop: Header=BB101_22 Depth=1
	testq	%rdx, %rdx
	je	.LBB101_34
# %bb.24:                               # %for.body81.us.preheader
                                        #   in Loop: Header=BB101_22 Depth=1
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB101_25:                             # %for.body81.us
                                        #   Parent Loop BB101_22 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	-208(%rbp,%rcx,8), %rdx
	jb	.LBB101_35
# %bb.26:                               # %for.inc89.us
                                        #   in Loop: Header=BB101_25 Depth=2
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB101_25
# %bb.27:                               #   in Loop: Header=BB101_22 Depth=1
	movq	%rax, %rcx
	jmp	.LBB101_35
	.p2align	4, 0x90
.LBB101_34:                             # %for.body81.preheader
                                        #   in Loop: Header=BB101_22 Depth=1
	movl	%eax, %ecx
.LBB101_35:                             # %for.end91
                                        #   in Loop: Header=BB101_22 Depth=1
	movslq	8(%r9,%r13), %r15
	imulq	%r10, %r15
	movl	%ecx, %r14d
	cmpq	%r14, %rax
	jbe	.LBB101_41
# %bb.36:                               # %for.body95.preheader
                                        #   in Loop: Header=BB101_22 Depth=1
	movq	%r9, %r11
	movslq	%r14d, %rdi
	movl	%eax, %ecx
	subl	%edi, %ecx
	movq	%rdi, %r9
	notq	%r9
	addq	%rax, %r9
	movq	%rax, %rbx
	testb	$3, %cl
	je	.LBB101_40
# %bb.37:                               # %for.body95.prol.preheader
                                        #   in Loop: Header=BB101_22 Depth=1
	movl	%eax, %ecx
	subb	%r14b, %cl
	movzbl	%cl, %r8d
	andl	$3, %r8d
	negq	%r8
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB101_38:                             # %for.body95.prol
                                        #   Parent Loop BB101_22 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	-264(%rsi,%rbx,8), %rcx
	movq	-136(%rsi,%rbx,8), %r10
	movq	%rcx, -256(%rsi,%rbx,8)
	movq	-8(%rsi,%rbx,8), %rcx
	movq	%rcx, (%rsi,%rbx,8)
	movq	%r10, -128(%rsi,%rbx,8)
	decq	%rbx
	cmpq	%rbx, %r8
	jne	.LBB101_38
# %bb.39:                               # %for.body95.prol.loopexit.loopexit
                                        #   in Loop: Header=BB101_22 Depth=1
	subq	%r12, %rbx
	movq	-64(%rbp), %r10                 # 8-byte Reload
.LBB101_40:                             # %for.body95.prol.loopexit
                                        #   in Loop: Header=BB101_22 Depth=1
	cmpq	$3, %r9
	movq	-48(%rbp), %r8                  # 8-byte Reload
	movq	%r11, %r9
	movq	-72(%rbp), %r11                 # 8-byte Reload
	jb	.LBB101_41
	.p2align	4, 0x90
.LBB101_42:                             # %for.body95
                                        #   Parent Loop BB101_22 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	-216(%rbp,%rbx,8), %rcx
	movq	%rcx, -208(%rbp,%rbx,8)
	movq	-344(%rbp,%rbx,8), %rcx
	movq	%rcx, -336(%rbp,%rbx,8)
	movq	-224(%rbp,%rbx,8), %rcx
	movq	%rcx, -216(%rbp,%rbx,8)
	movq	-352(%rbp,%rbx,8), %rcx
	movq	%rcx, -344(%rbp,%rbx,8)
	vmovups	-496(%rbp,%rbx,8), %ymm0
	vmovups	%ymm0, -488(%rbp,%rbx,8)
	vmovups	-240(%rbp,%rbx,8), %xmm0
	vmovups	%xmm0, -232(%rbp,%rbx,8)
	vmovdqu	-368(%rbp,%rbx,8), %xmm0
	vmovdqu	%xmm0, -360(%rbp,%rbx,8)
	leaq	-4(%rbx), %rcx
	movq	%rcx, %rbx
	cmpq	%rdi, %rcx
	jg	.LBB101_42
	jmp	.LBB101_41
.LBB101_11:                             # %if.then
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, 384(%r14)
	vmovdqu	%ymm0, 352(%r14)
	vmovdqu	%ymm0, 320(%r14)
	vmovdqu	%ymm0, 288(%r14)
	vmovdqu	%ymm0, 256(%r14)
	vmovdqu	%ymm0, 224(%r14)
	vmovdqu	%ymm0, 192(%r14)
	vmovdqu	%ymm0, 160(%r14)
	vmovdqu	%ymm0, 128(%r14)
	vmovdqu	%ymm0, 96(%r14)
	vmovdqu	%ymm0, 64(%r14)
	vmovdqu	%ymm0, 32(%r14)
	vmovdqu	%ymm0, (%r14)
.LBB101_33:                             # %cleanup
	movq	%r14, %rax
	addq	$456, %rsp                      # imm = 0x1C8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB101_28:                             # %while.cond.preheader
	movq	-80(%rbp), %rax
	cmpq	-336(%rbp), %rax
	movq	-56(%rbp), %r14                 # 8-byte Reload
	jne	.LBB101_32
# %bb.29:                               # %land.rhs.lr.ph
	movq	-208(%rbp), %rcx
	.p2align	4, 0x90
.LBB101_30:                             # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rcx, %rax
	jne	.LBB101_32
# %bb.31:                               # %while.body
                                        #   in Loop: Header=BB101_30 Depth=1
	movq	%rcx, %rax
	imulq	-464(%rbp), %rax
	movq	%rax, -80(%rbp)
	vmovups	-456(%rbp), %ymm0
	vmovdqu	-424(%rbp), %ymm1
	vmovdqu	-392(%rbp), %ymm2
	vmovdqu	-328(%rbp), %ymm3
	vmovups	%ymm0, -464(%rbp)
	cmpq	-328(%rbp), %rax
	vmovdqu	%ymm3, -336(%rbp)
	vmovups	-200(%rbp), %ymm0
	movq	-200(%rbp), %rcx
	vmovups	%ymm0, -208(%rbp)
	vmovdqu	%ymm1, -432(%rbp)
	vmovups	-296(%rbp), %ymm0
	vmovups	%ymm0, -304(%rbp)
	vmovups	-168(%rbp), %ymm0
	vmovups	%ymm0, -176(%rbp)
	vmovdqu	%ymm2, -400(%rbp)
	vmovups	-264(%rbp), %ymm0
	vmovups	%ymm0, -272(%rbp)
	vmovups	-136(%rbp), %ymm0
	vmovups	%ymm0, -144(%rbp)
	vmovups	-360(%rbp), %xmm0
	vmovups	%xmm0, -368(%rbp)
	vmovups	-232(%rbp), %xmm0
	vmovups	%xmm0, -240(%rbp)
	vmovdqu	-104(%rbp), %xmm0
	vmovdqu	%xmm0, -112(%rbp)
	movq	-344(%rbp), %rdx
	movq	%rdx, -352(%rbp)
	movq	-216(%rbp), %rdx
	movq	%rdx, -224(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	movq	$1, -344(%rbp)
	movq	$0, -216(%rbp)
	movq	$0, -88(%rbp)
	je	.LBB101_30
.LBB101_32:                             # %while.end
	leaq	-488(%rbp), %rsi
	movl	$416, %edx                      # imm = 0x1A0
	movq	%r14, %rdi
	vzeroupper
	callq	memcpy@PLT
	jmp	.LBB101_33
.Lfunc_end101:
	.size	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b, .Lfunc_end101-_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m # -- Begin function _ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,@function
_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m: # @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	memcmp@PLT
	testl	%eax, %eax
	sete	%al
	popq	%rbp
	retq
.Lfunc_end102:
	.size	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m, .Lfunc_end102-_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t # -- Begin function _ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t,@function
_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t: # @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	36(%rdi), %r8d
	movb	$1, %al
	testl	%r8d, %r8d
	jle	.LBB103_8
# %bb.1:                                # %for.body.lr.ph
	movq	40(%rdi), %rdx
	shlq	$4, %r8
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB103_3:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	(%rdx,%rdi), %ecx
	cmpl	(%rsi,%rdi), %ecx
	jne	.LBB103_7
# %bb.4:                                # %land.lhs.true.i.i
                                        #   in Loop: Header=BB103_3 Depth=1
	movl	4(%rdx,%rdi), %ecx
	cmpl	4(%rsi,%rdi), %ecx
	jne	.LBB103_7
# %bb.5:                                # %land.lhs.true5.i.i
                                        #   in Loop: Header=BB103_3 Depth=1
	movl	8(%rdx,%rdi), %ecx
	cmpl	8(%rsi,%rdi), %ecx
	jne	.LBB103_7
# %bb.6:                                # %_ZNK18halide_dimension_tneERKS_.exit
                                        #   in Loop: Header=BB103_3 Depth=1
	movl	12(%rdx,%rdi), %ecx
	cmpl	12(%rsi,%rdi), %ecx
	jne	.LBB103_7
# %bb.2:                                # %for.cond
                                        #   in Loop: Header=BB103_3 Depth=1
	addq	$16, %rdi
	cmpq	%rdi, %r8
	jne	.LBB103_3
.LBB103_8:                              # %return
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.LBB103_7:
	xorl	%eax, %eax
                                        # kill: def $al killed $al killed $eax
	popq	%rbp
	retq
.Lfunc_end103:
	.size	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t, .Lfunc_end103-_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by # -- Begin function _ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by,@function
_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by: # @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%r9d, %r12d
	movq	%r8, %r14
	movq	%rsi, %r15
	movq	%rdi, %r13
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movq	%rdx, 32(%rdi)
	movl	%ecx, 48(%rdi)
	movl	$0, 52(%rdi)
	movl	%r9d, 56(%rdi)
	movslq	36(%r8), %rax
	movl	%eax, 60(%rdi)
	movl	%r9d, %ecx
	imulq	$56, %rcx, %rbx
	incl	%r12d
	imulq	%rax, %r12
	shlq	$4, %r12
	addq	%rbx, %r12
	leaq	(%rdx,%r12), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, 24(%r13)
	testq	%rax, %rax
	je	.LBB104_13
# %bb.1:                                # %if.end
	movq	%rax, 72(%r13)
	addq	%rax, %rbx
	movq	%rbx, 64(%r13)
	addq	%rax, %r12
	movq	%r12, 40(%r13)
	cmpq	$0, 32(%r13)
	je	.LBB104_5
# %bb.2:                                # %for.body.preheader
	movb	(%r15), %cl
	movb	%cl, (%r12)
	cmpq	$2, 32(%r13)
	jb	.LBB104_5
# %bb.3:                                # %for.body.for.body_crit_edge.preheader
	movl	$1, %ecx
	.p2align	4, 0x90
.LBB104_4:                              # %for.body.for.body_crit_edge
                                        # =>This Inner Loop Header: Depth=1
	movq	40(%r13), %rsi
	movzbl	(%r15,%rcx), %edx
	movb	%dl, (%rsi,%rcx)
	incq	%rcx
	cmpq	32(%r13), %rcx
	jb	.LBB104_4
.LBB104_5:                              # %for.cond23.preheader
	cmpl	$0, 60(%r13)
	jle	.LBB104_8
# %bb.6:                                # %for.body27.lr.ph
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB104_7:                              # %for.body27
                                        # =>This Inner Loop Header: Depth=1
	movq	40(%r14), %rsi
	movq	64(%r13), %rdi
	vmovups	(%rsi,%rcx), %xmm0
	vmovups	%xmm0, (%rdi,%rcx)
	incq	%rdx
	movslq	60(%r13), %rsi
	addq	$16, %rcx
	cmpq	%rsi, %rdx
	jl	.LBB104_7
.LBB104_8:                              # %for.cond36.preheader
	movq	32(%rbp), %r8
	movb	24(%rbp), %r9b
	cmpl	$0, 56(%r13)
	je	.LBB104_12
# %bb.9:                                # %for.body40.preheader
	movq	16(%rbp), %r11
	xorl	%r10d, %r10d
	jmp	.LBB104_10
	.p2align	4, 0x90
.LBB104_11:                             # %for.cond36.loopexit
                                        #   in Loop: Header=BB104_10 Depth=1
	movl	56(%r13), %ecx
	cmpq	%rcx, %r10
	jae	.LBB104_12
.LBB104_10:                             # %for.body40
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB104_16 Depth 2
	movq	%r10, %rdi
	movq	(%r11,%r10,8), %rcx
	movq	72(%r13), %rsi
	imulq	$56, %r10, %r15
	vmovups	(%rcx), %ymm0
	vmovups	24(%rcx), %ymm1
	vmovups	%ymm1, 24(%rsi,%r15)
	vmovups	%ymm0, (%rsi,%r15)
	incq	%r10
	movl	60(%r13), %r14d
	movl	%r10d, %ecx
	imull	%r14d, %ecx
	shlq	$4, %rcx
	addq	64(%r13), %rcx
	movq	72(%r13), %rsi
	movq	%rcx, 40(%rsi,%r15)
	testl	%r14d, %r14d
	jle	.LBB104_11
# %bb.14:                               # %for.body59.preheader
                                        #   in Loop: Header=BB104_10 Depth=1
	movq	(%r11,%rdi,8), %rsi
	movq	40(%rsi), %rsi
	vmovups	(%rsi), %xmm0
	vmovups	%xmm0, (%rcx)
	cmpl	$2, 60(%r13)
	jl	.LBB104_11
# %bb.15:                               # %for.body59.for.body59_crit_edge.preheader
                                        #   in Loop: Header=BB104_10 Depth=1
	movl	$1, %ecx
	movl	$16, %esi
	.p2align	4, 0x90
.LBB104_16:                             # %for.body59.for.body59_crit_edge
                                        #   Parent Loop BB104_10 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	72(%r13), %rbx
	movq	40(%rbx,%r15), %rbx
	movq	(%r11,%rdi,8), %rdx
	movq	40(%rdx), %rdx
	vmovups	(%rdx,%rsi), %xmm0
	vmovups	%xmm0, (%rbx,%rsi)
	incq	%rcx
	movslq	60(%r13), %rdx
	addq	$16, %rsi
	cmpq	%rdx, %rcx
	jl	.LBB104_16
	jmp	.LBB104_11
.LBB104_12:                             # %for.cond.cleanup39
	movb	%r9b, 88(%r13)
	movq	%r8, 80(%r13)
.LBB104_13:                             # %cleanup
	testq	%rax, %rax
	setne	%al
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end104:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by, .Lfunc_end104-_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal8djb_hashEPKhm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8djb_hashEPKhm # -- Begin function _ZN6Halide7Runtime8Internal8djb_hashEPKhm
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal8djb_hashEPKhm,@function
_ZN6Halide7Runtime8Internal8djb_hashEPKhm: # @_ZN6Halide7Runtime8Internal8djb_hashEPKhm
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	testq	%rsi, %rsi
	je	.LBB105_1
# %bb.2:                                # %for.body.preheader
	leaq	-1(%rsi), %rax
	movl	%esi, %r8d
	andl	$7, %r8d
	cmpq	$7, %rax
	jae	.LBB105_8
# %bb.3:
	movl	$5381, %eax                     # imm = 0x1505
	xorl	%edx, %edx
.LBB105_4:                              # %for.cond.cleanup.loopexit.unr-lcssa
	testq	%r8, %r8
	je	.LBB105_7
# %bb.5:                                # %for.body.epil.preheader
	addq	%rdx, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB105_6:                              # %for.body.epil
                                        # =>This Inner Loop Header: Depth=1
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	(%rdi,%rdx), %eax
	addl	%ecx, %eax
	incq	%rdx
	cmpq	%rdx, %r8
	jne	.LBB105_6
.LBB105_7:                              # %for.cond.cleanup
	popq	%rbp
	retq
.LBB105_1:
	movl	$5381, %eax                     # imm = 0x1505
	popq	%rbp
	retq
.LBB105_8:                              # %for.body.preheader.new
	andq	$-8, %rsi
	movl	$8, %edx
	movl	$177573, %ecx                   # imm = 0x2B5A5
	.p2align	4, 0x90
.LBB105_9:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movzbl	-8(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-7(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-6(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-5(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-4(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-3(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-2(%rdi,%rdx), %eax
	addl	%ecx, %eax
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	movzbl	-1(%rdi,%rdx), %eax
	addl	%ecx, %eax
	cmpq	%rdx, %rsi
	je	.LBB105_4
# %bb.10:                               # %for.body.for.body_crit_edge
                                        #   in Loop: Header=BB105_9 Depth=1
	movl	%eax, %ecx
	shll	$5, %ecx
	addl	%eax, %ecx
	addq	$8, %rdx
	jmp	.LBB105_9
.Lfunc_end105:
	.size	_ZN6Halide7Runtime8Internal8djb_hashEPKhm, .Lfunc_end105-_ZN6Halide7Runtime8Internal8djb_hashEPKhm
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function _ZN6Halide7Runtime8Internal11prune_cacheEv
.LCPI106_0:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI106_1:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI106_2:
	.quad	4                               # 0x4
.LCPI106_3:
	.quad	68                              # 0x44
.LCPI106_4:
	.quad	132                             # 0x84
.LCPI106_5:
	.quad	196                             # 0xc4
.LCPI106_6:
	.quad	16                              # 0x10
	.section	.text._ZN6Halide7Runtime8Internal11prune_cacheEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11prune_cacheEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal11prune_cacheEv,@function
_ZN6Halide7Runtime8Internal11prune_cacheEv: # @_ZN6Halide7Runtime8Internal11prune_cacheEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$224, %rsp
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r14
	movq	(%r14), %r13
	testq	%r13, %r13
	je	.LBB106_26
# %bb.1:                                # %entry
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r15
	movq	(%r15), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
	cmpq	%rcx, %rax
	jle	.LBB106_26
# %bb.2:                                # %while.body.preheader
	vmovdqa64	.LCPI106_0(%rip), %ymm17 # ymm17 = [0,1,2,3]
	vmovdqa64	.LCPI106_1(%rip), %xmm18 # xmm18 = [0,4,8,12]
	vpbroadcastq	.LCPI106_2(%rip), %ymm19 # ymm19 = [4,4,4,4]
	vpbroadcastq	.LCPI106_3(%rip), %ymm20 # ymm20 = [68,68,68,68]
	vpbroadcastq	.LCPI106_4(%rip), %ymm21 # ymm21 = [132,132,132,132]
	vpbroadcastq	.LCPI106_5(%rip), %ymm22 # ymm22 = [196,196,196,196]
	vpbroadcastq	.LCPI106_6(%rip), %ymm24 # ymm24 = [16,16,16,16]
	vpxord	%xmm25, %xmm25, %xmm25
	vmovdqa64	%ymm19, 160(%rsp)       # 32-byte Spill
	vmovdqa64	%ymm20, 128(%rsp)       # 32-byte Spill
	vmovdqa64	%ymm21, 96(%rsp)        # 32-byte Spill
	vmovdqa64	%ymm22, 64(%rsp)        # 32-byte Spill
	vmovdqa64	%ymm24, 32(%rsp)        # 32-byte Spill
	.p2align	4, 0x90
.LBB106_3:                              # %while.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB106_6 Depth 2
                                        #     Child Loop BB106_20 Depth 2
                                        #       Child Loop BB106_30 Depth 3
                                        #       Child Loop BB106_33 Depth 3
                                        #       Child Loop BB106_39 Depth 3
                                        #       Child Loop BB106_42 Depth 3
	movq	8(%r13), %rsi
	cmpl	$0, 52(%r13)
	jne	.LBB106_24
# %bb.4:                                # %if.then
                                        #   in Loop: Header=BB106_3 Depth=1
	movzbl	48(%r13), %ecx
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	(%rax,%rcx,8), %rax
	cmpq	%r13, %rax
	je	.LBB106_5
	.p2align	4, 0x90
.LBB106_6:                              # %while.cond9
                                        #   Parent Loop BB106_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB106_8
# %bb.7:                                # %land.rhs11
                                        #   in Loop: Header=BB106_6 Depth=2
	movq	(%rbx), %rax
	cmpq	%r13, %rax
	jne	.LBB106_6
	jmp	.LBB106_9
.LBB106_5:                              # %if.then6
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	(%r13), %rax
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rdx
	movq	%rax, (%rdx,%rcx,8)
	cmpq	%r13, (%r14)
	jne	.LBB106_12
	jmp	.LBB106_11
.LBB106_8:                              # %if.then18
                                        #   in Loop: Header=BB106_3 Depth=1
	xorl	%edi, %edi
	movq	%rsi, %r12
	leaq	.L.str.2.42(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movq	%r12, %rsi
	vpxord	%xmm25, %xmm25, %xmm25
	vmovdqa64	32(%rsp), %ymm24        # 32-byte Reload
	vmovdqa64	64(%rsp), %ymm22        # 32-byte Reload
	vmovdqa64	96(%rsp), %ymm21        # 32-byte Reload
	vmovdqa64	128(%rsp), %ymm20       # 32-byte Reload
	vmovdqa64	160(%rsp), %ymm19       # 32-byte Reload
	vmovdqa64	.LCPI106_1(%rip), %xmm18 # xmm18 = [0,4,8,12]
	vmovdqa64	.LCPI106_0(%rip), %ymm17 # ymm17 = [0,1,2,3]
.LBB106_9:                              # %do.end
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	(%r13), %rax
	movq	%rax, (%rbx)
	cmpq	%r13, (%r14)
	jne	.LBB106_12
.LBB106_11:                             # %if.then23
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	%rsi, (%r14)
.LBB106_12:                             # %if.end24
                                        #   in Loop: Header=BB106_3 Depth=1
	testq	%rsi, %rsi
	je	.LBB106_14
# %bb.13:                               # %if.then26
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	16(%r13), %rax
	movq	%rax, 16(%rsi)
.LBB106_14:                             # %if.end28
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	16(%r13), %rax
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rcx
	cmpq	%r13, (%rcx)
	je	.LBB106_15
# %bb.16:                               # %if.end32
                                        #   in Loop: Header=BB106_3 Depth=1
	testq	%rax, %rax
	je	.LBB106_18
.LBB106_17:                             # %if.then35
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	%rsi, 16(%r13)
.LBB106_18:                             # %if.end37
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movl	56(%r13), %r8d
	testq	%r8, %r8
	je	.LBB106_23
# %bb.19:                               # %for.body.lr.ph
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	72(%r13), %r9
	movq	(%r15), %r14
	xorl	%r15d, %r15d
	jmp	.LBB106_20
	.p2align	4, 0x90
.LBB106_45:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
                                        #   in Loop: Header=BB106_20 Depth=2
	notq	%rdx
	addq	%rdx, %rdi
.LBB106_46:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit
                                        #   in Loop: Header=BB106_20 Depth=2
	movzbl	33(%r9,%r10), %eax
	addq	$7, %rax
	shrq	$3, %rax
	imulq	%rdi, %rax
	addq	%rax, %r14
	incq	%r15
	cmpq	%r8, %r15
	je	.LBB106_22
.LBB106_20:                             # %for.body
                                        #   Parent Loop BB106_3 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB106_30 Depth 3
                                        #       Child Loop BB106_33 Depth 3
                                        #       Child Loop BB106_39 Depth 3
                                        #       Child Loop BB106_42 Depth 3
	imulq	$56, %r15, %r10
	movl	36(%r9,%r10), %eax
	testl	%eax, %eax
	jle	.LBB106_21
# %bb.27:                               # %for.body.lr.ph.i.i
                                        #   in Loop: Header=BB106_20 Depth=2
	movq	40(%r9,%r10), %r11
	vpbroadcastq	%r11, %ymm23
	cmpl	$17, %eax
	jae	.LBB106_29
# %bb.28:                               #   in Loop: Header=BB106_20 Depth=2
	xorl	%edi, %edi
	xorl	%edx, %edx
	jmp	.LBB106_32
	.p2align	4, 0x90
.LBB106_21:                             # %for.body._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge
                                        #   in Loop: Header=BB106_20 Depth=2
	movq	$-1, %rdi
	jmp	.LBB106_46
	.p2align	4, 0x90
.LBB106_29:                             # %vector.ph44
                                        #   in Loop: Header=BB106_20 Depth=2
	movl	%eax, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	movl	$16, %edx
	cmoveq	%rdx, %rcx
	movq	%rax, %rdi
	subq	%rcx, %rdi
	leaq	200(%r11), %rdx
	vpxor	%xmm14, %xmm14, %xmm14
	movq	%rdi, %rbx
	vmovdqa64	%ymm17, %ymm2
	vpxor	%xmm15, %xmm15, %xmm15
	vpxor	%xmm4, %xmm4, %xmm4
	vpxor	%xmm5, %xmm5, %xmm5
	vpcmpeqd	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
.LBB106_30:                             # %vector.body42
                                        #   Parent Loop BB106_3 Depth=1
                                        #     Parent Loop BB106_20 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	-192(%rdx), %ymm6
	vmovdqu	-128(%rdx), %ymm7
	vmovdqu	-64(%rdx), %ymm8
	vmovdqu	(%rdx), %ymm9
	vpermt2d	-160(%rdx), %ymm18, %ymm6
	vpermt2d	-96(%rdx), %ymm18, %ymm7
	vpermt2d	-32(%rdx), %ymm18, %ymm8
	vpermt2d	32(%rdx), %ymm18, %ymm9
	vpcmpgtd	%xmm25, %xmm6, %k4
	vpcmpgtd	%xmm25, %xmm7, %k3
	vpcmpgtd	%xmm25, %xmm8, %k2
	vpcmpgtd	%xmm25, %xmm9, %k1
	vpsllq	$4, %ymm2, %ymm10
	vpaddq	%ymm10, %ymm23, %ymm10
	vpaddq	%ymm19, %ymm10, %ymm11
	vpaddq	%ymm20, %ymm10, %ymm12
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm11), %xmm0 {%k5}
	vpaddq	%ymm21, %ymm10, %ymm11
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm12), %xmm1 {%k5}
	vpaddq	%ymm22, %ymm10, %ymm10
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm11), %xmm3 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm10), %xmm11 {%k5}
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmovzxdq	%xmm8, %ymm8            # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero
	vpmovzxdq	%xmm9, %ymm9            # ymm9 = xmm9[0],zero,xmm9[1],zero,xmm9[2],zero,xmm9[3],zero
	vpaddd	%xmm0, %xmm13, %xmm0
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm3, %xmm13, %xmm3
	vpmovsxdq	%xmm0, %ymm0
	vpmovsxdq	%xmm1, %ymm1
	vpmovsxdq	%xmm3, %ymm3
	vpmullq	%ymm6, %ymm0, %ymm0 {%k4} {z}
	vpmullq	%ymm7, %ymm1, %ymm1 {%k3} {z}
	vpmullq	%ymm8, %ymm3, %ymm3 {%k2} {z}
	vpaddd	%xmm13, %xmm11, %xmm6
	vpmovsxdq	%xmm6, %ymm6
	vpmullq	%ymm9, %ymm6, %ymm6 {%k1} {z}
	vpaddq	%ymm0, %ymm14, %ymm14
	vpaddq	%ymm1, %ymm15, %ymm15
	vpaddq	%ymm3, %ymm4, %ymm4
	vpaddq	%ymm6, %ymm5, %ymm5
	vpaddq	%ymm24, %ymm2, %ymm2
	addq	$256, %rdx                      # imm = 0x100
	addq	$-16, %rbx
	jne	.LBB106_30
# %bb.31:                               # %middle.block40
                                        #   in Loop: Header=BB106_20 Depth=2
	vpaddq	%ymm14, %ymm15, %ymm0
	vpaddq	%ymm0, %ymm4, %ymm0
	vpaddq	%ymm0, %ymm5, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
.LBB106_32:                             # %for.body.i.i.preheader
                                        #   in Loop: Header=BB106_20 Depth=2
	movq	%rax, %rbx
	subq	%rdi, %rbx
	shlq	$4, %rdi
	addq	%r11, %rdi
	addq	$8, %rdi
	jmp	.LBB106_33
	.p2align	4, 0x90
.LBB106_35:                             # %if.end.i.i
                                        #   in Loop: Header=BB106_33 Depth=3
	addq	$16, %rdi
	decq	%rbx
	je	.LBB106_36
.LBB106_33:                             # %for.body.i.i
                                        #   Parent Loop BB106_3 Depth=1
                                        #     Parent Loop BB106_20 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	(%rdi), %ecx
	testl	%ecx, %ecx
	jle	.LBB106_35
# %bb.34:                               # %if.then.i.i
                                        #   in Loop: Header=BB106_33 Depth=3
	movslq	-4(%rdi), %rsi
	decq	%rsi
	imulq	%rcx, %rsi
	addq	%rsi, %rdx
	jmp	.LBB106_35
	.p2align	4, 0x90
.LBB106_36:                             # %for.body.i13.i.preheader
                                        #   in Loop: Header=BB106_20 Depth=2
	cmpl	$17, %eax
	jae	.LBB106_38
# %bb.37:                               #   in Loop: Header=BB106_20 Depth=2
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	jmp	.LBB106_41
	.p2align	4, 0x90
.LBB106_38:                             # %vector.ph
                                        #   in Loop: Header=BB106_20 Depth=2
	movl	%eax, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	movl	$16, %esi
	cmoveq	%rsi, %rcx
	movq	%rax, %r12
	subq	%rcx, %r12
	leaq	200(%r11), %rbx
	vpxord	%xmm16, %xmm16, %xmm16
	movq	%r12, %rdi
	vmovdqa64	%ymm17, %ymm2
	vpxor	%xmm3, %xmm3, %xmm3
	vpxor	%xmm4, %xmm4, %xmm4
	vpxor	%xmm5, %xmm5, %xmm5
	vpcmpeqd	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
.LBB106_39:                             # %vector.body
                                        #   Parent Loop BB106_3 Depth=1
                                        #     Parent Loop BB106_20 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	-192(%rbx), %ymm0
	vmovdqu	-128(%rbx), %ymm6
	vmovdqu	-64(%rbx), %ymm7
	vmovdqu	(%rbx), %ymm8
	vpermt2d	-160(%rbx), %ymm18, %ymm0
	vpermt2d	-96(%rbx), %ymm18, %ymm6
	vpermt2d	-32(%rbx), %ymm18, %ymm7
	vpermt2d	32(%rbx), %ymm18, %ymm8
	vpmovd2m	%xmm0, %k4
	vpmovd2m	%xmm6, %k3
	vpmovd2m	%xmm7, %k2
	vpmovd2m	%xmm8, %k1
	vpmovzxdq	%xmm0, %ymm15           # ymm15 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
	vpmovzxdq	%xmm6, %ymm14           # ymm14 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm12           # ymm12 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpsllq	$4, %ymm2, %ymm9
	vpaddq	%ymm9, %ymm23, %ymm9
	vpaddq	%ymm19, %ymm9, %ymm10
	vpmovzxdq	%xmm8, %ymm8            # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero
	vpaddq	%ymm20, %ymm9, %ymm11
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm10), %xmm7 {%k5}
	vpaddq	%ymm21, %ymm9, %ymm10
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm11), %xmm6 {%k5}
	vpaddq	%ymm22, %ymm9, %ymm9
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm10), %xmm0 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm9), %xmm1 {%k5}
	vpaddd	%xmm7, %xmm13, %xmm7
	vpaddd	%xmm6, %xmm13, %xmm6
	vpaddd	%xmm0, %xmm13, %xmm0
	vpaddd	%xmm1, %xmm13, %xmm1
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
	vpmovzxdq	%xmm1, %ymm1            # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpmuldq	%ymm15, %ymm7, %ymm7 {%k4} {z}
	vpaddq	%ymm7, %ymm16, %ymm16
	vpmuldq	%ymm14, %ymm6, %ymm6 {%k3} {z}
	vpmuldq	%ymm12, %ymm0, %ymm0 {%k2} {z}
	vpaddq	%ymm6, %ymm3, %ymm3
	vpaddq	%ymm0, %ymm4, %ymm4
	vpmuldq	%ymm8, %ymm1, %ymm0 {%k1} {z}
	vpaddq	%ymm0, %ymm5, %ymm5
	vpaddq	%ymm24, %ymm2, %ymm2
	addq	$256, %rbx                      # imm = 0x100
	addq	$-16, %rdi
	jne	.LBB106_39
# %bb.40:                               # %middle.block
                                        #   in Loop: Header=BB106_20 Depth=2
	vpaddq	%ymm16, %ymm3, %ymm0
	vpaddq	%ymm0, %ymm4, %ymm0
	vpaddq	%ymm0, %ymm5, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdi
.LBB106_41:                             # %for.body.i13.i.preheader81
                                        #   in Loop: Header=BB106_20 Depth=2
	subq	%r12, %rax
	shlq	$4, %r12
	leaq	(%r11,%r12), %rbx
	addq	$8, %rbx
	jmp	.LBB106_42
	.p2align	4, 0x90
.LBB106_44:                             # %if.end.i24.i
                                        #   in Loop: Header=BB106_42 Depth=3
	addq	$16, %rbx
	decq	%rax
	je	.LBB106_45
.LBB106_42:                             # %for.body.i13.i
                                        #   Parent Loop BB106_3 Depth=1
                                        #     Parent Loop BB106_20 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movslq	(%rbx), %rcx
	testq	%rcx, %rcx
	jns	.LBB106_44
# %bb.43:                               # %if.then.i20.i
                                        #   in Loop: Header=BB106_42 Depth=3
	movslq	-4(%rbx), %rsi
	decq	%rsi
	imulq	%rcx, %rsi
	addq	%rsi, %rdi
	jmp	.LBB106_44
	.p2align	4, 0x90
.LBB106_22:                             # %for.cond.for.cond.cleanup_crit_edge
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r15
	movq	%r14, (%r15)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r14
.LBB106_23:                             # %for.cond.cleanup
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	%r13, %rdi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%r13, %rsi
	callq	halide_free@PLT
	vpxord	%xmm25, %xmm25, %xmm25
	vmovdqa64	32(%rsp), %ymm24        # 32-byte Reload
	vmovdqa64	64(%rsp), %ymm22        # 32-byte Reload
	vmovdqa64	96(%rsp), %ymm21        # 32-byte Reload
	vmovdqa64	128(%rsp), %ymm20       # 32-byte Reload
	vmovdqa64	160(%rsp), %ymm19       # 32-byte Reload
	vmovdqa64	.LCPI106_1(%rip), %xmm18 # xmm18 = [0,4,8,12]
	vmovdqa64	.LCPI106_0(%rip), %ymm17 # ymm17 = [0,1,2,3]
	movq	(%r15), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
	movq	24(%rsp), %rsi                  # 8-byte Reload
.LBB106_24:                             # %if.end41
                                        #   in Loop: Header=BB106_3 Depth=1
	testq	%rsi, %rsi
	je	.LBB106_26
# %bb.25:                               # %if.end41
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	%rsi, %r13
	cmpq	%rcx, %rax
	jg	.LBB106_3
	jmp	.LBB106_26
.LBB106_15:                             # %if.then30
                                        #   in Loop: Header=BB106_3 Depth=1
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rcx
	movq	%rax, (%rcx)
	testq	%rax, %rax
	jne	.LBB106_17
	jmp	.LBB106_18
.LBB106_26:                             # %while.end42
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end106:
	.size	_ZN6Halide7Runtime8Internal11prune_cacheEv, .Lfunc_end106-_ZN6Halide7Runtime8Internal11prune_cacheEv
                                        # -- End function
	.section	.text.halide_memoization_cache_set_size,"ax",@progbits
	.weak	halide_memoization_cache_set_size # -- Begin function halide_memoization_cache_set_size
	.p2align	4, 0x90
	.type	halide_memoization_cache_set_size,@function
halide_memoization_cache_set_size:      # @halide_memoization_cache_set_size
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	testq	%rdi, %rdi
	movl	$1048576, %ebx                  # imm = 0x100000
	cmovneq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rax
	movq	%rbx, (%rax)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end107:
	.size	halide_memoization_cache_set_size, .Lfunc_end107-halide_memoization_cache_set_size
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function halide_memoization_cache_lookup
.LCPI108_0:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI108_1:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI108_2:
	.quad	4                               # 0x4
.LCPI108_3:
	.quad	68                              # 0x44
.LCPI108_4:
	.quad	132                             # 0x84
.LCPI108_5:
	.quad	196                             # 0xc4
.LCPI108_6:
	.quad	16                              # 0x10
	.section	.text.halide_memoization_cache_lookup,"ax",@progbits
	.weak	halide_memoization_cache_lookup
	.p2align	4, 0x90
	.type	halide_memoization_cache_lookup,@function
halide_memoization_cache_lookup:        # @halide_memoization_cache_lookup
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$224, %rsp
	movq	%r9, 24(%rsp)                   # 8-byte Spill
	movl	%r8d, %r13d
	movq	%rcx, 32(%rsp)                  # 8-byte Spill
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movslq	%edx, %r12
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rsi, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal8djb_hashEPKhm@PLT
	movl	%eax, %r14d
	movzbl	%r14b, %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	(%rax,%rbx,8), %rbx
	testq	%rbx, %rbx
	movl	%r14d, 12(%rsp)                 # 4-byte Spill
	je	.LBB108_13
# %bb.1:                                # %while.body.lr.ph
	testl	%r13d, %r13d
	jle	.LBB108_18
# %bb.2:                                # %while.body.us.preheader
	movslq	%r13d, %r15
	jmp	.LBB108_3
	.p2align	4, 0x90
.LBB108_17:                             # %if.end73
                                        #   in Loop: Header=BB108_18 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	je	.LBB108_13
.LBB108_18:                             # %while.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%r14d, 48(%rbx)
	jne	.LBB108_17
# %bb.19:                               # %land.lhs.true
                                        #   in Loop: Header=BB108_18 Depth=1
	cmpq	%r12, 32(%rbx)
	jne	.LBB108_17
# %bb.20:                               # %land.lhs.true7
                                        #   in Loop: Header=BB108_18 Depth=1
	movq	40(%rbx), %rdi
	movq	16(%rsp), %rsi                  # 8-byte Reload
	movq	%r12, %rdx
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB108_17
# %bb.21:                               # %land.lhs.true10
                                        #   in Loop: Header=BB108_18 Depth=1
	movq	64(%rbx), %rsi
	movq	32(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	testb	%al, %al
	je	.LBB108_17
# %bb.22:                               # %land.lhs.true13
                                        #   in Loop: Header=BB108_18 Depth=1
	cmpl	%r13d, 56(%rbx)
	jne	.LBB108_17
.LBB108_23:                             # %if.then23
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %r14
	cmpq	(%r14), %rbx
	movq	24(%rsp), %r12                  # 8-byte Reload
	je	.LBB108_36
# %bb.24:                               # %do.body
	cmpq	$0, 8(%rbx)
	jne	.LBB108_26
# %bb.25:                               # %if.then27
	leaq	.L.str.3.43(%rip), %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB108_26:                             # %do.end
	movq	16(%rbx), %rax
	testq	%rax, %rax
	je	.LBB108_28
# %bb.27:                               # %if.then29
	movq	8(%rbx), %rcx
	movq	%rcx, 8(%rax)
	movq	8(%rbx), %rax
	jmp	.LBB108_31
.LBB108_11:                             # %for.cond.cleanup.us
                                        #   in Loop: Header=BB108_3 Depth=1
	testb	%al, %al
	movl	12(%rsp), %r14d                 # 4-byte Reload
	movl	64(%rsp), %r13d                 # 4-byte Reload
	jne	.LBB108_23
	.p2align	4, 0x90
.LBB108_12:                             # %if.end73.us
                                        #   in Loop: Header=BB108_3 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	je	.LBB108_13
.LBB108_3:                              # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB108_9 Depth 2
	cmpl	%r14d, 48(%rbx)
	jne	.LBB108_12
# %bb.4:                                # %land.lhs.true.us
                                        #   in Loop: Header=BB108_3 Depth=1
	cmpq	%r12, 32(%rbx)
	jne	.LBB108_12
# %bb.5:                                # %land.lhs.true7.us
                                        #   in Loop: Header=BB108_3 Depth=1
	movq	40(%rbx), %rdi
	movq	16(%rsp), %rsi                  # 8-byte Reload
	movq	%r12, %rdx
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB108_12
# %bb.6:                                # %land.lhs.true10.us
                                        #   in Loop: Header=BB108_3 Depth=1
	movq	64(%rbx), %rsi
	movq	32(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	testb	%al, %al
	je	.LBB108_12
# %bb.7:                                # %land.lhs.true13.us
                                        #   in Loop: Header=BB108_3 Depth=1
	cmpl	%r13d, 56(%rbx)
	jne	.LBB108_12
# %bb.8:                                # %for.cond.preheader.us
                                        #   in Loop: Header=BB108_3 Depth=1
	movl	%r13d, 64(%rsp)                 # 4-byte Spill
	movl	$1, %r13d
	movl	$5, %r14d
	.p2align	4, 0x90
.LBB108_9:                              # %for.body.us
                                        #   Parent Loop BB108_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	-8(%rax,%r13,8), %rdi
	movq	72(%rbx), %rax
	movq	(%rax,%r14,8), %rsi
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	cmpq	%r15, %r13
	jge	.LBB108_11
# %bb.10:                               # %for.body.us
                                        #   in Loop: Header=BB108_9 Depth=2
	incq	%r13
	addq	$7, %r14
	testb	%al, %al
	jne	.LBB108_9
	jmp	.LBB108_11
.LBB108_13:                             # %for.cond75.preheader
	movl	$1, %ebx
	testl	%r13d, %r13d
	jle	.LBB108_71
# %bb.14:                               # %for.body78.preheader
	movl	%r13d, %eax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movl	$1, %r13d
	movl	$4294967295, %r14d              # imm = 0xFFFFFFFF
	xorl	%r12d, %r12d
	vmovdqa64	.LCPI108_1(%rip), %xmm17 # xmm17 = [0,4,8,12]
	vpbroadcastq	.LCPI108_2(%rip), %ymm18 # ymm18 = [4,4,4,4]
	vpbroadcastq	.LCPI108_3(%rip), %ymm19 # ymm19 = [68,68,68,68]
	vpbroadcastq	.LCPI108_4(%rip), %ymm20 # ymm20 = [132,132,132,132]
	vpbroadcastq	.LCPI108_5(%rip), %ymm21 # ymm21 = [196,196,196,196]
	vpbroadcastq	.LCPI108_6(%rip), %ymm23 # ymm23 = [16,16,16,16]
	vpxord	%xmm16, %xmm16, %xmm16
	vmovdqa64	%ymm18, 32(%rsp)        # 32-byte Spill
	vmovdqa64	%ymm19, 64(%rsp)        # 32-byte Spill
	vmovdqa64	%ymm20, 160(%rsp)       # 32-byte Spill
	vmovdqa64	%ymm21, 128(%rsp)       # 32-byte Spill
	vmovdqa64	%ymm23, 96(%rsp)        # 32-byte Spill
	.p2align	4, 0x90
.LBB108_15:                             # %for.body78
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB108_48 Depth 2
                                        #     Child Loop BB108_51 Depth 2
                                        #     Child Loop BB108_57 Depth 2
                                        #     Child Loop BB108_60 Depth 2
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	(%rax,%r12,8), %r15
	movl	36(%r15), %eax
	testl	%eax, %eax
	jle	.LBB108_16
# %bb.45:                               # %for.body.lr.ph.i.i
                                        #   in Loop: Header=BB108_15 Depth=1
	movq	40(%r15), %r8
	vpbroadcastq	%r8, %ymm22
	cmpl	$17, %eax
	jae	.LBB108_47
# %bb.46:                               #   in Loop: Header=BB108_15 Depth=1
	xorl	%esi, %esi
	xorl	%edx, %edx
	jmp	.LBB108_50
	.p2align	4, 0x90
.LBB108_16:                             #   in Loop: Header=BB108_15 Depth=1
	movl	$1, %edx
	xorl	%esi, %esi
	jmp	.LBB108_64
	.p2align	4, 0x90
.LBB108_47:                             # %vector.ph54
                                        #   in Loop: Header=BB108_15 Depth=1
	movl	%eax, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	movl	$16, %edx
	cmoveq	%rdx, %rcx
	movq	%rax, %rsi
	subq	%rcx, %rsi
	leaq	200(%r8), %rdx
	vpxor	%xmm14, %xmm14, %xmm14
	movq	%rsi, %rdi
	vmovdqa	.LCPI108_0(%rip), %ymm2         # ymm2 = [0,1,2,3]
	vpxor	%xmm15, %xmm15, %xmm15
	vpxor	%xmm13, %xmm13, %xmm13
	vpxor	%xmm5, %xmm5, %xmm5
	.p2align	4, 0x90
.LBB108_48:                             # %vector.body52
                                        #   Parent Loop BB108_15 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqu	-192(%rdx), %ymm6
	vmovdqu	-128(%rdx), %ymm7
	vmovdqu	-64(%rdx), %ymm8
	vmovdqu	(%rdx), %ymm9
	vpermt2d	-160(%rdx), %ymm17, %ymm6
	vpermt2d	-96(%rdx), %ymm17, %ymm7
	vpermt2d	-32(%rdx), %ymm17, %ymm8
	vpermt2d	32(%rdx), %ymm17, %ymm9
	vpcmpgtd	%xmm16, %xmm6, %k4
	vpcmpgtd	%xmm16, %xmm7, %k3
	vpcmpgtd	%xmm16, %xmm8, %k2
	vpcmpgtd	%xmm16, %xmm9, %k1
	vpsllq	$4, %ymm2, %ymm10
	vpaddq	%ymm10, %ymm22, %ymm10
	vpaddq	%ymm18, %ymm10, %ymm11
	vpaddq	%ymm19, %ymm10, %ymm12
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm11), %xmm0 {%k5}
	vpaddq	%ymm20, %ymm10, %ymm11
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm12), %xmm1 {%k5}
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpaddq	%ymm21, %ymm10, %ymm10
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm11), %xmm3 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm10), %xmm11 {%k5}
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmovzxdq	%xmm8, %ymm8            # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero
	vpmovzxdq	%xmm9, %ymm9            # ymm9 = xmm9[0],zero,xmm9[1],zero,xmm9[2],zero,xmm9[3],zero
	vpaddd	%xmm4, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm1, %xmm1
	vpaddd	%xmm4, %xmm3, %xmm3
	vpmovsxdq	%xmm0, %ymm0
	vpmovsxdq	%xmm1, %ymm1
	vpmovsxdq	%xmm3, %ymm3
	vpmullq	%ymm6, %ymm0, %ymm0 {%k4} {z}
	vpmullq	%ymm7, %ymm1, %ymm1 {%k3} {z}
	vpmullq	%ymm8, %ymm3, %ymm3 {%k2} {z}
	vpaddd	%xmm4, %xmm11, %xmm6
	vpmovsxdq	%xmm6, %ymm6
	vpmullq	%ymm9, %ymm6, %ymm6 {%k1} {z}
	vpaddq	%ymm0, %ymm14, %ymm14
	vpaddq	%ymm1, %ymm15, %ymm15
	vpaddq	%ymm3, %ymm13, %ymm13
	vpaddq	%ymm6, %ymm5, %ymm5
	vpaddq	%ymm23, %ymm2, %ymm2
	addq	$256, %rdx                      # imm = 0x100
	addq	$-16, %rdi
	jne	.LBB108_48
# %bb.49:                               # %middle.block50
                                        #   in Loop: Header=BB108_15 Depth=1
	vpaddq	%ymm14, %ymm15, %ymm0
	vpaddq	%ymm0, %ymm13, %ymm0
	vpaddq	%ymm0, %ymm5, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
.LBB108_50:                             # %for.body.i.i.preheader
                                        #   in Loop: Header=BB108_15 Depth=1
	movq	%rax, %rcx
	subq	%rsi, %rcx
	shlq	$4, %rsi
	addq	%r8, %rsi
	addq	$8, %rsi
	jmp	.LBB108_51
	.p2align	4, 0x90
.LBB108_53:                             # %if.end.i.i
                                        #   in Loop: Header=BB108_51 Depth=2
	addq	$16, %rsi
	decq	%rcx
	je	.LBB108_54
.LBB108_51:                             # %for.body.i.i
                                        #   Parent Loop BB108_15 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%rsi), %edi
	testl	%edi, %edi
	jle	.LBB108_53
# %bb.52:                               # %if.then.i.i
                                        #   in Loop: Header=BB108_51 Depth=2
	movslq	-4(%rsi), %rbx
	decq	%rbx
	imulq	%rdi, %rbx
	addq	%rbx, %rdx
	jmp	.LBB108_53
	.p2align	4, 0x90
.LBB108_54:                             # %for.body.i13.i.preheader
                                        #   in Loop: Header=BB108_15 Depth=1
	cmpl	$17, %eax
	jae	.LBB108_56
# %bb.55:                               #   in Loop: Header=BB108_15 Depth=1
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	jmp	.LBB108_59
	.p2align	4, 0x90
.LBB108_56:                             # %vector.ph
                                        #   in Loop: Header=BB108_15 Depth=1
	movl	%eax, %esi
	andl	$15, %esi
	testq	%rsi, %rsi
	movl	$16, %ecx
	cmoveq	%rcx, %rsi
	movq	%rax, %rcx
	subq	%rsi, %rcx
	leaq	200(%r8), %rdi
	vpxord	%xmm16, %xmm16, %xmm16
	movq	%rcx, %rsi
	vmovdqa	.LCPI108_0(%rip), %ymm2         # ymm2 = [0,1,2,3]
	vpxor	%xmm3, %xmm3, %xmm3
	vpxor	%xmm4, %xmm4, %xmm4
	vpxor	%xmm5, %xmm5, %xmm5
	vpcmpeqd	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
.LBB108_57:                             # %vector.body
                                        #   Parent Loop BB108_15 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqu	-192(%rdi), %ymm0
	vmovdqu	-128(%rdi), %ymm6
	vmovdqu	-64(%rdi), %ymm7
	vmovdqu	(%rdi), %ymm8
	vpermt2d	-160(%rdi), %ymm17, %ymm0
	vpermt2d	-96(%rdi), %ymm17, %ymm6
	vpermt2d	-32(%rdi), %ymm17, %ymm7
	vpermt2d	32(%rdi), %ymm17, %ymm8
	vpmovd2m	%xmm0, %k4
	vpmovd2m	%xmm6, %k3
	vpmovd2m	%xmm7, %k2
	vpmovd2m	%xmm8, %k1
	vpmovzxdq	%xmm0, %ymm15           # ymm15 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
	vpmovzxdq	%xmm6, %ymm14           # ymm14 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm12           # ymm12 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpsllq	$4, %ymm2, %ymm9
	vpaddq	%ymm9, %ymm22, %ymm9
	vpaddq	%ymm18, %ymm9, %ymm10
	vpmovzxdq	%xmm8, %ymm8            # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero
	vpaddq	%ymm19, %ymm9, %ymm11
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm10), %xmm7 {%k5}
	vpaddq	%ymm20, %ymm9, %ymm10
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm11), %xmm6 {%k5}
	vpaddq	%ymm21, %ymm9, %ymm9
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm10), %xmm0 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm9), %xmm1 {%k5}
	vpaddd	%xmm7, %xmm13, %xmm7
	vpaddd	%xmm6, %xmm13, %xmm6
	vpaddd	%xmm0, %xmm13, %xmm0
	vpaddd	%xmm1, %xmm13, %xmm1
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
	vpmovzxdq	%xmm1, %ymm1            # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpmuldq	%ymm15, %ymm7, %ymm7 {%k4} {z}
	vpaddq	%ymm7, %ymm16, %ymm16
	vpmuldq	%ymm14, %ymm6, %ymm6 {%k3} {z}
	vpmuldq	%ymm12, %ymm0, %ymm0 {%k2} {z}
	vpaddq	%ymm6, %ymm3, %ymm3
	vpaddq	%ymm0, %ymm4, %ymm4
	vpmuldq	%ymm8, %ymm1, %ymm0 {%k1} {z}
	vpaddq	%ymm0, %ymm5, %ymm5
	vpaddq	%ymm23, %ymm2, %ymm2
	addq	$256, %rdi                      # imm = 0x100
	addq	$-16, %rsi
	jne	.LBB108_57
# %bb.58:                               # %middle.block
                                        #   in Loop: Header=BB108_15 Depth=1
	vpaddq	%ymm16, %ymm3, %ymm0
	vpaddq	%ymm0, %ymm4, %ymm0
	vpaddq	%ymm0, %ymm5, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rsi
.LBB108_59:                             # %for.body.i13.i.preheader91
                                        #   in Loop: Header=BB108_15 Depth=1
	subq	%rcx, %rax
	shlq	$4, %rcx
	addq	%r8, %rcx
	addq	$8, %rcx
	jmp	.LBB108_60
	.p2align	4, 0x90
.LBB108_62:                             # %if.end.i24.i
                                        #   in Loop: Header=BB108_60 Depth=2
	addq	$16, %rcx
	decq	%rax
	je	.LBB108_63
.LBB108_60:                             # %for.body.i13.i
                                        #   Parent Loop BB108_15 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movslq	(%rcx), %rdi
	testq	%rdi, %rdi
	jns	.LBB108_62
# %bb.61:                               # %if.then.i20.i
                                        #   in Loop: Header=BB108_60 Depth=2
	movslq	-4(%rcx), %rbx
	decq	%rbx
	imulq	%rdi, %rbx
	addq	%rbx, %rsi
	jmp	.LBB108_62
	.p2align	4, 0x90
.LBB108_63:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
                                        #   in Loop: Header=BB108_15 Depth=1
	incq	%rdx
.LBB108_64:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit
                                        #   in Loop: Header=BB108_15 Depth=1
	subq	%rsi, %rdx
	movzbl	33(%r15), %esi
	addq	$7, %rsi
	shrq	$3, %rsi
	imulq	%rdx, %rsi
	addq	$64, %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 16(%r15)
	testq	%rax, %rax
	je	.LBB108_65
# %bb.69:                               # %for.inc114
                                        #   in Loop: Header=BB108_15 Depth=1
	addq	$64, %rax
	movq	%rax, 16(%r15)
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	12(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 8(%rax)
	movq	$0, (%rax)
	incq	%r12
	incq	%r13
	incq	%r14
	cmpq	16(%rsp), %r12                  # 8-byte Folded Reload
	vmovdqa64	.LCPI108_1(%rip), %xmm17 # xmm17 = [0,4,8,12]
	vmovdqa64	32(%rsp), %ymm18        # 32-byte Reload
	vmovdqa64	64(%rsp), %ymm19        # 32-byte Reload
	vmovdqa64	160(%rsp), %ymm20       # 32-byte Reload
	vmovdqa64	128(%rsp), %ymm21       # 32-byte Reload
	vmovdqa64	96(%rsp), %ymm23        # 32-byte Reload
	vpxord	%xmm16, %xmm16, %xmm16
	jne	.LBB108_15
# %bb.70:
	movl	$1, %ebx
	jmp	.LBB108_71
.LBB108_65:                             # %for.cond89.preheader
	movl	$-1, %ebx
	testq	%r12, %r12
	je	.LBB108_71
# %bb.66:                               # %for.body92.preheader
	movq	24(%rsp), %r15                  # 8-byte Reload
	movq	(%rsp), %r12                    # 8-byte Reload
	.p2align	4, 0x90
.LBB108_67:                             # %for.body92
                                        # =>This Inner Loop Header: Depth=1
	movl	%r14d, %ebx
	movq	(%r15,%rbx,8), %rax
	movq	16(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r12, %rdi
	movq	%rax, %rsi
	callq	halide_free@PLT
	movq	(%r15,%rbx,8), %rax
	movq	$0, 16(%rax)
	decq	%r13
	decq	%r14
	cmpq	$1, %r13
	jg	.LBB108_67
# %bb.68:
	movl	$-1, %ebx
.LBB108_71:                             # %cleanup119
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	vzeroupper
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB108_28:                             # %do.body33
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	cmpq	%rbx, (%r15)
	je	.LBB108_30
# %bb.29:                               # %if.then35
	leaq	.L.str.4.44(%rip), %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB108_30:                             # %do.end38
	movq	8(%rbx), %rax
	movq	%rax, (%r15)
.LBB108_31:                             # %do.body41
	testq	%rax, %rax
	jne	.LBB108_33
# %bb.32:                               # %if.then44
	leaq	.L.str.5.45(%rip), %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	movq	8(%rbx), %rax
.LBB108_33:                             # %do.end47
	movq	16(%rbx), %rcx
	movq	%rcx, 16(%rax)
	movq	$0, 8(%rbx)
	movq	(%r14), %rax
	movq	%rax, 16(%rbx)
	testq	%rax, %rax
	je	.LBB108_35
# %bb.34:                               # %if.then54
	movq	%rbx, 8(%rax)
.LBB108_35:                             # %if.end56
	movq	%rbx, (%r14)
.LBB108_36:                             # %if.end57
	testl	%r13d, %r13d
	jle	.LBB108_44
# %bb.37:                               # %for.body62.lr.ph
	movl	%r13d, %ecx
	leaq	-1(%rcx), %rdx
	movl	%ecx, %r8d
	andl	$3, %r8d
	cmpq	$3, %rdx
	jae	.LBB108_39
# %bb.38:
	xorl	%edx, %edx
	jmp	.LBB108_41
.LBB108_39:                             # %for.body62.lr.ph.new
	andl	$-4, %ecx
	xorl	%esi, %esi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB108_40:                             # %for.body62
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12,%rdx,8), %rdi
	movq	72(%rbx), %rax
	vmovups	(%rax,%rsi), %ymm0
	vmovups	24(%rax,%rsi), %ymm1
	vmovups	%ymm1, 24(%rdi)
	vmovups	%ymm0, (%rdi)
	movq	8(%r12,%rdx,8), %rax
	movq	72(%rbx), %rdi
	vmovups	56(%rdi,%rsi), %ymm0
	vmovups	80(%rdi,%rsi), %ymm1
	vmovups	%ymm1, 24(%rax)
	vmovups	%ymm0, (%rax)
	movq	16(%r12,%rdx,8), %rax
	movq	72(%rbx), %rdi
	vmovups	112(%rdi,%rsi), %ymm0
	vmovups	136(%rdi,%rsi), %ymm1
	vmovups	%ymm1, 24(%rax)
	vmovups	%ymm0, (%rax)
	movq	24(%r12,%rdx,8), %rax
	movq	72(%rbx), %rdi
	vmovdqu	168(%rdi,%rsi), %ymm0
	vmovdqu	192(%rdi,%rsi), %ymm1
	vmovdqu	%ymm1, 24(%rax)
	vmovdqu	%ymm0, (%rax)
	addq	$4, %rdx
	addq	$224, %rsi
	cmpq	%rdx, %rcx
	jne	.LBB108_40
.LBB108_41:                             # %cleanup119.loopexit223.loopexit.unr-lcssa
	testq	%r8, %r8
	je	.LBB108_44
# %bb.42:                               # %for.body62.epil.preheader
	imulq	$56, %rdx, %rcx
	leaq	(%r12,%rdx,8), %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB108_43:                             # %for.body62.epil
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rdx,%rsi,8), %rax
	movq	72(%rbx), %rdi
	vmovdqu	(%rdi,%rcx), %ymm0
	vmovdqu	24(%rdi,%rcx), %ymm1
	vmovdqu	%ymm1, 24(%rax)
	vmovdqu	%ymm0, (%rax)
	addq	$56, %rcx
	incq	%rsi
	cmpq	%rsi, %r8
	jne	.LBB108_43
.LBB108_44:                             # %cleanup119.loopexit223
	addl	%r13d, 52(%rbx)
	xorl	%ebx, %ebx
	jmp	.LBB108_71
.Lfunc_end108:
	.size	halide_memoization_cache_lookup, .Lfunc_end108-halide_memoization_cache_lookup
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function halide_memoization_cache_store
.LCPI109_0:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI109_1:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI109_2:
	.quad	4                               # 0x4
.LCPI109_3:
	.quad	68                              # 0x44
.LCPI109_4:
	.quad	132                             # 0x84
.LCPI109_5:
	.quad	196                             # 0xc4
.LCPI109_6:
	.quad	16                              # 0x10
	.section	.text.halide_memoization_cache_store,"ax",@progbits
	.weak	halide_memoization_cache_store
	.p2align	4, 0x90
	.type	halide_memoization_cache_store,@function
halide_memoization_cache_store:         # @halide_memoization_cache_store
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movl	%r8d, %r15d
	movq	%rcx, %r13
	movl	%edx, %r12d
	movq	%rsi, -72(%rbp)                 # 8-byte Spill
	movq	%rdi, -80(%rbp)                 # 8-byte Spill
	movq	%r9, -56(%rbp)                  # 8-byte Spill
	movq	(%r9), %rax
	movq	16(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	8(%rax), %r14d
	movzbl	%r14b, %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	%rbx, -112(%rbp)                # 8-byte Spill
	movq	(%rax,%rbx,8), %rbx
	movslq	%r12d, %rax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	testq	%rbx, %rbx
	movl	%r15d, -60(%rbp)                # 4-byte Spill
	movq	%r14, -88(%rbp)                 # 8-byte Spill
	je	.LBB109_9
# %bb.1:                                # %while.body.lr.ph
	movq	%r14, %r12
	testl	%r15d, %r15d
	jle	.LBB109_19
# %bb.2:                                # %while.body.us.preheader
	movl	%r15d, %eax
	movq	%rax, -96(%rbp)                 # 8-byte Spill
	jmp	.LBB109_3
	.p2align	4, 0x90
.LBB109_24:                             # %if.end59
                                        #   in Loop: Header=BB109_19 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	je	.LBB109_9
.LBB109_19:                             # %while.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%r12d, 48(%rbx)
	jne	.LBB109_24
# %bb.20:                               # %land.lhs.true
                                        #   in Loop: Header=BB109_19 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	cmpq	%rax, 32(%rbx)
	jne	.LBB109_24
# %bb.21:                               # %land.lhs.true12
                                        #   in Loop: Header=BB109_19 Depth=1
	movq	40(%rbx), %rdi
	movq	-72(%rbp), %rsi                 # 8-byte Reload
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB109_24
# %bb.22:                               # %land.lhs.true15
                                        #   in Loop: Header=BB109_19 Depth=1
	movq	64(%rbx), %rsi
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	testb	%al, %al
	je	.LBB109_24
# %bb.23:                               # %land.lhs.true18
                                        #   in Loop: Header=BB109_19 Depth=1
	cmpl	%r15d, 56(%rbx)
	jne	.LBB109_24
	jmp	.LBB109_62
.LBB109_11:                             # %for.cond.cleanup.us
                                        #   in Loop: Header=BB109_3 Depth=1
	testb	%al, %al
	movl	-60(%rbp), %r15d                # 4-byte Reload
	movq	-104(%rbp), %r13                # 8-byte Reload
	movq	-88(%rbp), %r12                 # 8-byte Reload
	jne	.LBB109_12
	.p2align	4, 0x90
.LBB109_8:                              # %if.end59.us
                                        #   in Loop: Header=BB109_3 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	je	.LBB109_9
.LBB109_3:                              # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB109_17 Depth 2
	cmpl	%r12d, 48(%rbx)
	jne	.LBB109_8
# %bb.4:                                # %land.lhs.true.us
                                        #   in Loop: Header=BB109_3 Depth=1
	movq	-48(%rbp), %rax                 # 8-byte Reload
	cmpq	%rax, 32(%rbx)
	jne	.LBB109_8
# %bb.5:                                # %land.lhs.true12.us
                                        #   in Loop: Header=BB109_3 Depth=1
	movq	40(%rbx), %rdi
	movq	-72(%rbp), %rsi                 # 8-byte Reload
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB109_8
# %bb.6:                                # %land.lhs.true15.us
                                        #   in Loop: Header=BB109_3 Depth=1
	movq	64(%rbx), %rsi
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	testb	%al, %al
	je	.LBB109_8
# %bb.7:                                # %land.lhs.true18.us
                                        #   in Loop: Header=BB109_3 Depth=1
	cmpl	%r15d, 56(%rbx)
	jne	.LBB109_8
# %bb.16:                               # %for.body.lr.ph.us
                                        #   in Loop: Header=BB109_3 Depth=1
	movq	%r13, -104(%rbp)                # 8-byte Spill
	movq	72(%rbx), %rcx
	movb	$1, %r14b
	movl	$1, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB109_17:                             # %for.body.us
                                        #   Parent Loop BB109_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	-56(%rbp), %rax                 # 8-byte Reload
	movq	-8(%rax,%r15,8), %r13
	movq	40(%rcx,%r12), %rsi
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t@PLT
	movq	72(%rbx), %rcx
	movq	16(%rcx,%r12), %rdx
	cmpq	16(%r13), %rdx
	movzbl	%r14b, %r14d
	movl	$0, %edx
	cmovel	%edx, %r14d
	cmpq	-96(%rbp), %r15                 # 8-byte Folded Reload
	jae	.LBB109_11
# %bb.18:                               # %for.body.us
                                        #   in Loop: Header=BB109_17 Depth=2
	incq	%r15
	addq	$56, %r12
	testb	%al, %al
	jne	.LBB109_17
	jmp	.LBB109_11
.LBB109_9:                              # %for.cond61.preheader
	testl	%r15d, %r15d
	jle	.LBB109_10
# %bb.25:                               # %for.body64.preheader
	movl	%r15d, %r9d
	xorl	%r15d, %r15d
	vmovdqa64	.LCPI109_0(%rip), %ymm29 # ymm29 = [0,1,2,3]
	vmovdqa64	.LCPI109_1(%rip), %xmm28 # xmm28 = [0,4,8,12]
	vpbroadcastq	.LCPI109_2(%rip), %ymm22 # ymm22 = [4,4,4,4]
	vpbroadcastq	.LCPI109_3(%rip), %ymm23 # ymm23 = [68,68,68,68]
	vpbroadcastq	.LCPI109_4(%rip), %ymm24 # ymm24 = [132,132,132,132]
	vpbroadcastq	.LCPI109_5(%rip), %ymm25 # ymm25 = [196,196,196,196]
	movl	$16, %r8d
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vpbroadcastq	.LCPI109_6(%rip), %ymm26 # ymm26 = [16,16,16,16]
	vpxord	%xmm27, %xmm27, %xmm27
	xorl	%r14d, %r14d
	movq	-56(%rbp), %r12                 # 8-byte Reload
	jmp	.LBB109_26
	.p2align	4, 0x90
.LBB109_46:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
                                        #   in Loop: Header=BB109_26 Depth=1
	incq	%rax
.LBB109_47:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit
                                        #   in Loop: Header=BB109_26 Depth=1
	subq	%rdx, %rax
	movzbl	33(%r10), %ecx
	addq	$7, %rcx
	shrq	$3, %rcx
	imulq	%rax, %rcx
	addq	%rcx, %r14
	incq	%r15
	cmpq	%r9, %r15
	je	.LBB109_48
.LBB109_26:                             # %for.body64
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB109_31 Depth 2
                                        #     Child Loop BB109_34 Depth 2
                                        #     Child Loop BB109_40 Depth 2
                                        #     Child Loop BB109_43 Depth 2
	movq	(%r12,%r15,8), %r10
	movl	36(%r10), %edi
	testl	%edi, %edi
	jle	.LBB109_27
# %bb.28:                               # %for.body.lr.ph.i.i
                                        #   in Loop: Header=BB109_26 Depth=1
	movq	40(%r10), %r11
	vpbroadcastq	%r11, %ymm9
	cmpl	$17, %edi
	jae	.LBB109_30
# %bb.29:                               #   in Loop: Header=BB109_26 Depth=1
	xorl	%esi, %esi
	xorl	%eax, %eax
	jmp	.LBB109_33
	.p2align	4, 0x90
.LBB109_27:                             #   in Loop: Header=BB109_26 Depth=1
	movl	$1, %eax
	xorl	%edx, %edx
	jmp	.LBB109_47
	.p2align	4, 0x90
.LBB109_30:                             # %vector.ph40
                                        #   in Loop: Header=BB109_26 Depth=1
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	cmoveq	%r8, %rax
	movq	%rdi, %rsi
	subq	%rax, %rsi
	leaq	200(%r11), %rax
	vpxor	%xmm10, %xmm10, %xmm10
	movq	%rsi, %rdx
	vmovdqa64	%ymm29, %ymm11
	vpxor	%xmm12, %xmm12, %xmm12
	vpxor	%xmm13, %xmm13, %xmm13
	vpxor	%xmm14, %xmm14, %xmm14
	.p2align	4, 0x90
.LBB109_31:                             # %vector.body38
                                        #   Parent Loop BB109_26 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqu	-192(%rax), %ymm15
	vmovdqu64	-128(%rax), %ymm16
	vmovdqu64	-64(%rax), %ymm17
	vmovdqu64	(%rax), %ymm18
	vpermt2d	-160(%rax), %ymm28, %ymm15
	vpermt2d	-96(%rax), %ymm28, %ymm16
	vpermt2d	-32(%rax), %ymm28, %ymm17
	vpermt2d	32(%rax), %ymm28, %ymm18
	vpcmpgtd	%xmm27, %xmm15, %k4
	vpcmpgtd	%xmm27, %xmm16, %k3
	vpcmpgtd	%xmm27, %xmm17, %k2
	vpcmpgtd	%xmm27, %xmm18, %k1
	vpsllq	$4, %ymm11, %ymm19
	vpaddq	%ymm19, %ymm9, %ymm19
	vpaddq	%ymm22, %ymm19, %ymm20
	vpaddq	%ymm23, %ymm19, %ymm21
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm20), %xmm2 {%k5}
	vpaddq	%ymm24, %ymm19, %ymm20
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm21), %xmm3 {%k5}
	vpaddq	%ymm25, %ymm19, %ymm19
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm20), %xmm4 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm19), %xmm5 {%k5}
	vpmovzxdq	%xmm15, %ymm15          # ymm15 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpmovzxdq	%xmm16, %ymm16          # ymm16 = xmm16[0],zero,xmm16[1],zero,xmm16[2],zero,xmm16[3],zero
	vpmovzxdq	%xmm17, %ymm17          # ymm17 = xmm17[0],zero,xmm17[1],zero,xmm17[2],zero,xmm17[3],zero
	vpmovzxdq	%xmm18, %ymm18          # ymm18 = xmm18[0],zero,xmm18[1],zero,xmm18[2],zero,xmm18[3],zero
	vpaddd	%xmm2, %xmm8, %xmm2
	vpaddd	%xmm3, %xmm8, %xmm3
	vpaddd	%xmm4, %xmm8, %xmm4
	vpmovsxdq	%xmm2, %ymm2
	vpmovsxdq	%xmm3, %ymm3
	vpmovsxdq	%xmm4, %ymm4
	vpmullq	%ymm15, %ymm2, %ymm2 {%k4} {z}
	vpmullq	%ymm16, %ymm3, %ymm3 {%k3} {z}
	vpmullq	%ymm17, %ymm4, %ymm4 {%k2} {z}
	vpaddd	%xmm5, %xmm8, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vpmullq	%ymm18, %ymm5, %ymm5 {%k1} {z}
	vpaddq	%ymm2, %ymm10, %ymm10
	vpaddq	%ymm3, %ymm12, %ymm12
	vpaddq	%ymm4, %ymm13, %ymm13
	vpaddq	%ymm5, %ymm14, %ymm14
	vpaddq	%ymm26, %ymm11, %ymm11
	addq	$256, %rax                      # imm = 0x100
	addq	$-16, %rdx
	jne	.LBB109_31
# %bb.32:                               # %middle.block36
                                        #   in Loop: Header=BB109_26 Depth=1
	vpaddq	%ymm10, %ymm12, %ymm2
	vpaddq	%ymm2, %ymm13, %ymm2
	vpaddq	%ymm2, %ymm14, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vpaddq	%xmm3, %xmm2, %xmm2
	vpshufd	$238, %xmm2, %xmm3              # xmm3 = xmm2[2,3,2,3]
	vpaddq	%xmm3, %xmm2, %xmm2
	vmovq	%xmm2, %rax
.LBB109_33:                             # %for.body.i.i.preheader
                                        #   in Loop: Header=BB109_26 Depth=1
	movq	%rdi, %rdx
	subq	%rsi, %rdx
	shlq	$4, %rsi
	addq	%r11, %rsi
	addq	$8, %rsi
	jmp	.LBB109_34
	.p2align	4, 0x90
.LBB109_36:                             # %if.end.i.i
                                        #   in Loop: Header=BB109_34 Depth=2
	addq	$16, %rsi
	decq	%rdx
	je	.LBB109_37
.LBB109_34:                             # %for.body.i.i
                                        #   Parent Loop BB109_26 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%rsi), %ebx
	testl	%ebx, %ebx
	jle	.LBB109_36
# %bb.35:                               # %if.then.i.i
                                        #   in Loop: Header=BB109_34 Depth=2
	movslq	-4(%rsi), %rcx
	decq	%rcx
	imulq	%rbx, %rcx
	addq	%rcx, %rax
	jmp	.LBB109_36
	.p2align	4, 0x90
.LBB109_37:                             # %for.body.i13.i.preheader
                                        #   in Loop: Header=BB109_26 Depth=1
	cmpl	$17, %edi
	jae	.LBB109_39
# %bb.38:                               #   in Loop: Header=BB109_26 Depth=1
	xorl	%ebx, %ebx
	xorl	%edx, %edx
	jmp	.LBB109_42
	.p2align	4, 0x90
.LBB109_39:                             # %vector.ph
                                        #   in Loop: Header=BB109_26 Depth=1
	movl	%edi, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	cmoveq	%r8, %rcx
	movq	%rdi, %rbx
	subq	%rcx, %rbx
	leaq	200(%r11), %rdx
	vpxor	%xmm10, %xmm10, %xmm10
	movq	%rbx, %rsi
	vmovdqa64	%ymm29, %ymm11
	vpxor	%xmm12, %xmm12, %xmm12
	vpxor	%xmm13, %xmm13, %xmm13
	vpxor	%xmm14, %xmm14, %xmm14
	.p2align	4, 0x90
.LBB109_40:                             # %vector.body
                                        #   Parent Loop BB109_26 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqu	-192(%rdx), %ymm2
	vmovdqu	-128(%rdx), %ymm3
	vmovdqu	-64(%rdx), %ymm4
	vmovdqu	(%rdx), %ymm5
	vpermt2d	-160(%rdx), %ymm28, %ymm2
	vpermt2d	-96(%rdx), %ymm28, %ymm3
	vpermt2d	-32(%rdx), %ymm28, %ymm4
	vpermt2d	32(%rdx), %ymm28, %ymm5
	vpmovd2m	%xmm2, %k4
	vpmovd2m	%xmm3, %k3
	vpmovd2m	%xmm4, %k2
	vpmovd2m	%xmm5, %k1
	vpmovzxdq	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
	vpmovzxdq	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero
	vpmovzxdq	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
	vpsllq	$4, %ymm11, %ymm15
	vpaddq	%ymm15, %ymm9, %ymm15
	vpaddq	%ymm22, %ymm15, %ymm16
	vpmovzxdq	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
	vpaddq	%ymm23, %ymm15, %ymm17
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm16), %xmm7 {%k5}
	vpaddq	%ymm24, %ymm15, %ymm16
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm17), %xmm0 {%k5}
	vpaddq	%ymm25, %ymm15, %ymm15
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm16), %xmm1 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm15), %xmm6 {%k5}
	vpaddd	%xmm7, %xmm8, %xmm7
	vpaddd	%xmm0, %xmm8, %xmm0
	vpaddd	%xmm1, %xmm8, %xmm1
	vpaddd	%xmm6, %xmm8, %xmm6
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmovzxdq	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
	vpmovzxdq	%xmm1, %ymm1            # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmuldq	%ymm2, %ymm7, %ymm2 {%k4} {z}
	vpaddq	%ymm2, %ymm10, %ymm10
	vpmuldq	%ymm3, %ymm0, %ymm0 {%k3} {z}
	vpmuldq	%ymm4, %ymm1, %ymm1 {%k2} {z}
	vpaddq	%ymm0, %ymm12, %ymm12
	vpaddq	%ymm1, %ymm13, %ymm13
	vpmuldq	%ymm5, %ymm6, %ymm0 {%k1} {z}
	vpaddq	%ymm0, %ymm14, %ymm14
	vpaddq	%ymm26, %ymm11, %ymm11
	addq	$256, %rdx                      # imm = 0x100
	addq	$-16, %rsi
	jne	.LBB109_40
# %bb.41:                               # %middle.block
                                        #   in Loop: Header=BB109_26 Depth=1
	vpaddq	%ymm10, %ymm12, %ymm0
	vpaddq	%ymm0, %ymm13, %ymm0
	vpaddq	%ymm0, %ymm14, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
.LBB109_42:                             # %for.body.i13.i.preheader77
                                        #   in Loop: Header=BB109_26 Depth=1
	subq	%rbx, %rdi
	shlq	$4, %rbx
	leaq	(%r11,%rbx), %rsi
	addq	$8, %rsi
	jmp	.LBB109_43
	.p2align	4, 0x90
.LBB109_45:                             # %if.end.i24.i
                                        #   in Loop: Header=BB109_43 Depth=2
	addq	$16, %rsi
	decq	%rdi
	je	.LBB109_46
.LBB109_43:                             # %for.body.i13.i
                                        #   Parent Loop BB109_26 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movslq	(%rsi), %rbx
	testq	%rbx, %rbx
	jns	.LBB109_45
# %bb.44:                               # %if.then.i20.i
                                        #   in Loop: Header=BB109_43 Depth=2
	movslq	-4(%rsi), %rcx
	decq	%rcx
	imulq	%rbx, %rcx
	addq	%rcx, %rdx
	jmp	.LBB109_45
.LBB109_10:
	xorl	%r14d, %r14d
	movq	-56(%rbp), %r12                 # 8-byte Reload
.LBB109_48:                             # %for.cond.cleanup63
	movq	%r13, %r15
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	addq	%r14, (%rax)
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movl	$96, %esi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	movl	-60(%rbp), %ebx                 # 4-byte Reload
	je	.LBB109_50
# %bb.49:                               # %if.then76
	movb	16(%rbp), %al
	subq	$8, %rsp
	movzbl	%al, %eax
	movq	%r13, %rdi
	movq	-72(%rbp), %rsi                 # 8-byte Reload
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movq	-88(%rbp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	movq	%r15, %r8
	movl	%ebx, %r9d
	pushq	24(%rbp)
	pushq	%rax
	pushq	%r12
	callq	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by@PLT
	addq	$32, %rsp
	testb	%al, %al
	je	.LBB109_50
# %bb.55:                               # %if.end101
	movq	-112(%rbp), %rdx                # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rsi
	movq	(%rsi,%rdx,8), %rax
	movq	%rax, (%r13)
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	movq	%rcx, 16(%r13)
	testq	%rcx, %rcx
	je	.LBB109_57
# %bb.56:                               # %if.then106
	movq	%r13, 8(%rcx)
.LBB109_57:                             # %if.end107
	movq	%r13, (%rax)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	$0, (%rax)
	jne	.LBB109_59
# %bb.58:                               # %if.then109
	movq	%r13, (%rax)
.LBB109_59:                             # %if.end110
	movq	%r13, (%rsi,%rdx,8)
	movl	%ebx, 52(%r13)
	testl	%ebx, %ebx
	jle	.LBB109_62
# %bb.60:                               # %for.body117.preheader
	movl	%ebx, %r14d
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB109_61:                             # %for.body117
                                        # =>This Inner Loop Header: Depth=1
	movq	-56(%rbp), %rax                 # 8-byte Reload
	movq	(%rax,%rbx,8), %rax
	movq	16(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r13, (%rax)
	incq	%rbx
	cmpq	%rbx, %r14
	jne	.LBB109_61
	jmp	.LBB109_62
.LBB109_50:                             # %if.then83
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	subq	%r14, (%rax)
	testl	%ebx, %ebx
	jle	.LBB109_53
# %bb.51:                               # %for.body88.preheader
	movl	%ebx, %r14d
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB109_52:                             # %for.body88
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12,%rbx,8), %rax
	movq	16(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	incq	%rbx
	cmpq	%rbx, %r14
	jne	.LBB109_52
.LBB109_53:                             # %for.cond.cleanup87
	testq	%r13, %r13
	je	.LBB109_62
# %bb.54:                               # %if.then99
	movq	-80(%rbp), %rdi                 # 8-byte Reload
	movq	%r13, %rsi
	callq	halide_free@PLT
.LBB109_62:                             # %cleanup132
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%eax, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB109_12:                             # %do.body.us
	testb	$1, %r14b
	jne	.LBB109_13
# %bb.15:                               # %if.then42.us
	leaq	.L.str.9.46(%rip), %rsi
	movq	-80(%rbp), %rdi                 # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB109_13:                             # %for.body48.us.preheader
	xorl	%ebx, %ebx
	movq	-96(%rbp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB109_14:                             # %for.body48.us
                                        # =>This Inner Loop Header: Depth=1
	movq	-56(%rbp), %rax                 # 8-byte Reload
	movq	(%rax,%rbx,8), %rax
	movq	16(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	incq	%rbx
	cmpq	%rbx, %r14
	jne	.LBB109_14
	jmp	.LBB109_62
.Lfunc_end109:
	.size	halide_memoization_cache_store, .Lfunc_end109-halide_memoization_cache_store
                                        # -- End function
	.section	.text.halide_memoization_cache_release,"ax",@progbits
	.weak	halide_memoization_cache_release # -- Begin function halide_memoization_cache_release
	.p2align	4, 0x90
	.type	halide_memoization_cache_release,@function
halide_memoization_cache_release:       # @halide_memoization_cache_release
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	%rsi, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	(%rax), %rbx
	testq	%rbx, %rbx
	je	.LBB110_4
# %bb.1:                                # %if.else
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movl	52(%rbx), %eax
	testl	%eax, %eax
	jne	.LBB110_3
# %bb.2:                                # %if.then4
	leaq	.L.str.12.47(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	52(%rbx), %eax
.LBB110_3:                              # %do.end
	decl	%eax
	movl	%eax, 52(%rbx)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.LBB110_4:                              # %if.then
	movq	%r14, %rdi
	movq	%rax, %rsi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_free@PLT                 # TAILCALL
.Lfunc_end110:
	.size	halide_memoization_cache_release, .Lfunc_end110-halide_memoization_cache_release
                                        # -- End function
	.section	.text.halide_memoization_cache_evict,"ax",@progbits
	.weak	halide_memoization_cache_evict  # -- Begin function halide_memoization_cache_evict
	.p2align	4, 0x90
	.type	halide_memoization_cache_evict,@function
halide_memoization_cache_evict:         # @halide_memoization_cache_evict
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rsi, %r15
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r14
	movl	$2048, %eax                     # imm = 0x800
	addq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	%rax, -48(%rbp)                 # 8-byte Spill
	jmp	.LBB111_1
	.p2align	4, 0x90
.LBB111_13:                             # %if.end25
                                        #   in Loop: Header=BB111_1 Depth=1
	addq	$8, %r14
	cmpq	-48(%rbp), %r14                 # 8-byte Folded Reload
	je	.LBB111_14
.LBB111_1:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB111_3 Depth 2
	movq	(%r14), %r13
	testq	%r13, %r13
	je	.LBB111_13
# %bb.2:                                # %while.body.preheader
                                        #   in Loop: Header=BB111_1 Depth=1
	movq	%r14, %r12
	jmp	.LBB111_3
.LBB111_10:                             # %if.end
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rcx
.LBB111_11:                             # %if.end
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	%rax, (%rcx)
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	%r12, %rbx
.LBB111_12:                             # %if.end24
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	%rbx, %r12
	testq	%r13, %r13
	je	.LBB111_13
.LBB111_3:                              # %while.body
                                        #   Parent Loop BB111_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r13, %rbx
	movq	(%r13), %r13
	cmpb	$0, 88(%rbx)
	je	.LBB111_12
# %bb.4:                                # %land.lhs.true
                                        #   in Loop: Header=BB111_3 Depth=2
	cmpq	%r15, 80(%rbx)
	jne	.LBB111_12
# %bb.5:                                # %if.then7
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	%r13, (%r12)
	movq	8(%rbx), %rax
	movq	16(%rbx), %rcx
	testq	%rax, %rax
	je	.LBB111_7
# %bb.6:                                # %if.then9
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	%rcx, 16(%rax)
	movq	16(%rbx), %rcx
	testq	%rcx, %rcx
	je	.LBB111_10
	jmp	.LBB111_9
.LBB111_7:                              # %if.else
                                        #   in Loop: Header=BB111_3 Depth=2
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rdx
	movq	%rcx, (%rdx)
	testq	%rcx, %rcx
	je	.LBB111_10
.LBB111_9:                              #   in Loop: Header=BB111_3 Depth=2
	addq	$8, %rcx
	jmp	.LBB111_11
.LBB111_14:                             # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end111:
	.size	halide_memoization_cache_evict, .Lfunc_end111-halide_memoization_cache_evict
                                        # -- End function
	.section	.text.halide_string_to_string,"ax",@progbits
	.weak	halide_string_to_string         # -- Begin function halide_string_to_string
	.p2align	4, 0x90
	.type	halide_string_to_string,@function
halide_string_to_string:                # @halide_string_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB112_6
# %bb.1:                                # %if.end
	movq	%rsi, %rax
	testq	%rdx, %rdx
	leaq	.L.str.50(%rip), %rcx
	cmovneq	%rdx, %rcx
	.p2align	4, 0x90
.LBB112_2:                              # %if.end5
                                        # =>This Inner Loop Header: Depth=1
	movzbl	(%rcx), %edx
	movb	%dl, (%rdi)
	testb	%dl, %dl
	je	.LBB112_6
# %bb.3:                                # %if.end8
                                        #   in Loop: Header=BB112_2 Depth=1
	incq	%rdi
	incq	%rcx
	cmpq	%rdi, %rax
	jne	.LBB112_2
# %bb.4:                                # %if.then4
	movb	$0, -1(%rdi)
	popq	%rbp
	retq
.LBB112_6:
	movq	%rdi, %rax
	popq	%rbp
	retq
.Lfunc_end112:
	.size	halide_string_to_string, .Lfunc_end112-halide_string_to_string
                                        # -- End function
	.section	.text.halide_uint64_to_string,"ax",@progbits
	.weak	halide_uint64_to_string         # -- Begin function halide_uint64_to_string
	.p2align	4, 0x90
	.type	halide_uint64_to_string,@function
halide_uint64_to_string:                # @halide_uint64_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$40, %rsp
	movb	$0, -9(%rbp)
	leaq	-10(%rbp), %r8
	testq	%rdx, %rdx
	jne	.LBB113_2
# %bb.1:                                # %entry
	testl	%ecx, %ecx
	jle	.LBB113_5
.LBB113_2:                              # %for.body.preheader
	movl	$1, %r11d
	movabsq	$-3689348814741910323, %r9      # imm = 0xCCCCCCCCCCCCCCCD
	.p2align	4, 0x90
.LBB113_3:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdx, %rbx
	movl	%r11d, %r10d
	mulxq	%r9, %rdx, %rdx
	shrq	$3, %rdx
	leal	(%rdx,%rdx), %eax
	leal	(%rax,%rax,4), %r11d
	movl	%ebx, %eax
	subl	%r11d, %eax
	addb	$48, %al
	movb	%al, (%r8)
	decq	%r8
	leal	1(%r10), %r11d
	cmpq	$9, %rbx
	ja	.LBB113_3
# %bb.4:                                # %for.body
                                        #   in Loop: Header=BB113_3 Depth=1
	cmpl	%ecx, %r10d
	jl	.LBB113_3
.LBB113_5:                              # %for.cond.cleanup
	incq	%r8
	movq	%r8, %rdx
	callq	halide_string_to_string@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end113:
	.size	halide_uint64_to_string, .Lfunc_end113-halide_uint64_to_string
                                        # -- End function
	.section	.text.halide_int64_to_string,"ax",@progbits
	.weak	halide_int64_to_string          # -- Begin function halide_int64_to_string
	.p2align	4, 0x90
	.type	halide_int64_to_string,@function
halide_int64_to_string:                 # @halide_int64_to_string
# %bb.0:                                # %entry
	cmpq	%rsi, %rdi
	jae	.LBB114_3
# %bb.1:                                # %entry
	testq	%rdx, %rdx
	jns	.LBB114_3
# %bb.2:                                # %if.then
	pushq	%rbp
	movq	%rsp, %rbp
	movb	$45, (%rdi)
	incq	%rdi
	negq	%rdx
	popq	%rbp
.LBB114_3:                              # %if.end
	jmp	halide_uint64_to_string@PLT     # TAILCALL
.Lfunc_end114:
	.size	halide_int64_to_string, .Lfunc_end114-halide_int64_to_string
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function halide_double_to_string
.LCPI115_0:
	.quad	0x8000000000000000              # double -0
	.quad	0x8000000000000000              # double -0
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI115_1:
	.quad	0x3ff0000000000000              # double 1
.LCPI115_2:
	.quad	0x4024000000000000              # double 10
.LCPI115_3:
	.quad	0x412e848000000000              # double 1.0E+6
.LCPI115_4:
	.quad	0x3fe0000000000000              # double 0.5
	.section	.text.halide_double_to_string,"ax",@progbits
	.weak	halide_double_to_string
	.p2align	4, 0x90
	.type	halide_double_to_string,@function
halide_double_to_string:                # @halide_double_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$536, %rsp                      # imm = 0x218
	movl	%edx, %ebx
	movq	%rsi, %r12
	movq	%rdi, %r14
	vmovsd	%xmm0, -48(%rbp)
	movq	$0, -64(%rbp)
	leaq	-64(%rbp), %rdi
	leaq	-48(%rbp), %rsi
	movl	$8, %edx
	callq	memcpy@PLT
	movq	-64(%rbp), %rax
	movb	$52, %cl
	bzhiq	%rcx, %rax, %r13
	movq	%rax, %r15
	shrq	$52, %r15
	andl	$2047, %r15d                    # imm = 0x7FF
	cmpl	$2047, %r15d                    # imm = 0x7FF
	jne	.LBB115_9
# %bb.1:                                # %if.then
	testq	%r13, %r13
	je	.LBB115_6
# %bb.2:                                # %if.then4
	testq	%rax, %rax
	js	.LBB115_3
# %bb.5:                                # %if.else
	leaq	.L.str.2.58(%rip), %rdx
	jmp	.LBB115_4
.LBB115_9:                              # %if.else15
	testq	%r13, %r13
	jne	.LBB115_18
# %bb.10:                               # %if.else15
	testl	%r15d, %r15d
	jne	.LBB115_18
# %bb.11:                               # %if.then18
	testl	%ebx, %ebx
	je	.LBB115_15
# %bb.12:                               # %if.then20
	testq	%rax, %rax
	js	.LBB115_13
# %bb.14:                               # %if.else24
	leaq	.L.str.6.62(%rip), %rdx
	jmp	.LBB115_4
.LBB115_18:                             # %if.end32
	testq	%rax, %rax
	js	.LBB115_19
# %bb.20:                               # %if.end36
	testl	%ebx, %ebx
	je	.LBB115_35
.LBB115_21:                             # %while.condthread-pre-split
	vmovsd	-48(%rbp), %xmm0                # xmm0 = mem[0],zero
	xorl	%ebx, %ebx
	vmovsd	.LCPI115_1(%rip), %xmm1         # xmm1 = mem[0],zero
	vucomisd	%xmm0, %xmm1
	jbe	.LBB115_25
# %bb.22:                               # %while.body.preheader
	xorl	%ebx, %ebx
	vmovsd	.LCPI115_2(%rip), %xmm2         # xmm2 = mem[0],zero
	.p2align	4, 0x90
.LBB115_23:                             # %while.body
                                        # =>This Inner Loop Header: Depth=1
	vmulsd	%xmm2, %xmm0, %xmm0
	decl	%ebx
	vucomisd	%xmm0, %xmm1
	ja	.LBB115_23
# %bb.24:                               # %while.cond.while.cond40thread-pre-split_crit_edge
	vmovsd	%xmm0, -48(%rbp)
.LBB115_25:                             # %while.cond40thread-pre-split
	vucomisd	.LCPI115_2(%rip), %xmm0
	jb	.LBB115_29
# %bb.26:                               # %while.body42.preheader
	vmovsd	.LCPI115_2(%rip), %xmm1         # xmm1 = mem[0],zero
	.p2align	4, 0x90
.LBB115_27:                             # %while.body42
                                        # =>This Inner Loop Header: Depth=1
	vdivsd	%xmm1, %xmm0, %xmm0
	incl	%ebx
	vucomisd	%xmm1, %xmm0
	jae	.LBB115_27
# %bb.28:                               # %while.cond40.while.end43_crit_edge
	vmovsd	%xmm0, -48(%rbp)
.LBB115_29:                             # %while.end43
	vmovsd	.LCPI115_3(%rip), %xmm1         # xmm1 = mem[0],zero
	vfmadd213sd	.LCPI115_4(%rip), %xmm0, %xmm1 # xmm1 = (xmm0 * xmm1) + mem
	vcvttsd2usi	%xmm1, %rdx
	movabsq	$4835703278458516699, %rax      # imm = 0x431BDE82D7B634DB
	mulxq	%rax, %rax, %rax
	shrq	$18, %rax
	imulq	$-1000000, %rax, %r15           # imm = 0xFFF0BDC0
	addq	%rdx, %r15
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	movl	$6, %ecx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	js	.LBB115_31
# %bb.30:                               # %if.then53
	leaq	.L.str.11.67(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	jmp	.LBB115_32
.LBB115_6:                              # %if.else9
	testq	%rax, %rax
	js	.LBB115_7
# %bb.8:                                # %if.else13
	leaq	.L.str.4.60(%rip), %rdx
	jmp	.LBB115_4
.LBB115_3:                              # %if.then6
	leaq	.L.str.1.57(%rip), %rdx
	jmp	.LBB115_4
.LBB115_15:                             # %if.else26
	testq	%rax, %rax
	js	.LBB115_16
# %bb.17:                               # %if.else30
	leaq	.L.str.8.64(%rip), %rdx
	jmp	.LBB115_4
.LBB115_19:                             # %if.then34
	leaq	.L.str.9.65(%rip), %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r14
	vmovsd	-48(%rbp), %xmm0                # xmm0 = mem[0],zero
	vxorpd	.LCPI115_0(%rip), %xmm0, %xmm0
	vmovlpd	%xmm0, -48(%rbp)
	testl	%ebx, %ebx
	jne	.LBB115_21
.LBB115_35:                             # %if.else61
	testl	%r15d, %r15d
	je	.LBB115_36
# %bb.37:                               # %if.end65
	movabsq	$4503599627370495, %rax         # imm = 0xFFFFFFFFFFFFF
	incq	%rax
	orq	%rax, %r13
	xorl	%esi, %esi
	movl	%r15d, %edx
	subl	$1075, %edx                     # imm = 0x433
	jae	.LBB115_38
# %bb.39:                               # %if.then71
	movb	$51, %cl
	subb	%r15b, %cl
	cmpl	$1023, %r15d                    # imm = 0x3FF
	shrxq	%rcx, %r13, %rax
	shlxq	%rcx, %rax, %rcx
	cmovbq	%rsi, %rax
	cmovbq	%rsi, %rcx
	subq	%rcx, %r13
	vcvtusi2sd	%r13, %xmm1, %xmm0
	shlq	$52, %rdx
	movabsq	$4696837146684686336, %rcx      # imm = 0x412E848000000000
	addq	%rdx, %rcx
	vmovq	%rcx, %xmm1
	vfmadd213sd	.LCPI115_4(%rip), %xmm0, %xmm1 # xmm1 = (xmm0 * xmm1) + mem
	vcvttsd2usi	%xmm1, %rcx
	vcvtusi2sd	%rcx, %xmm2, %xmm0
	vucomisd	%xmm0, %xmm1
	setnp	%dl
	sete	%bl
	andb	%dl, %bl
	andb	%cl, %bl
	movzbl	%bl, %edx
	subq	%rdx, %rcx
	xorl	%r13d, %r13d
	cmpq	$1000000, %rcx                  # imm = 0xF4240
	sete	%r13b
	cmovneq	%rcx, %rsi
	movq	%rsi, -56(%rbp)                 # 8-byte Spill
	addq	%rax, %r13
	xorl	%r15d, %r15d
	jmp	.LBB115_40
.LBB115_7:                              # %if.then11
	leaq	.L.str.3.59(%rip), %rdx
	jmp	.LBB115_4
.LBB115_13:                             # %if.then22
	leaq	.L.str.5.61(%rip), %rdx
	jmp	.LBB115_4
.LBB115_31:                             # %if.else55
	leaq	.L.str.12.68(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	negl	%ebx
.LBB115_32:                             # %if.end58
	movl	%ebx, %edx
	movq	%r12, %rsi
	movl	$2, %ecx
	jmp	.LBB115_33
.LBB115_16:                             # %if.then28
	leaq	.L.str.7.63(%rip), %rdx
.LBB115_4:                              # %cleanup147
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	jmp	.LBB115_34
.LBB115_36:                             # %if.then63
	vxorpd	%xmm0, %xmm0, %xmm0
	movq	%r14, %rdi
	movq	%r12, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	jmp	.LBB115_34
.LBB115_38:
	xorl	%eax, %eax
	movq	%rax, -56(%rbp)                 # 8-byte Spill
	movq	%rdx, %r15
.LBB115_40:                             # %if.end104
	leaq	-64(%rbp), %rsi
	leaq	-96(%rbp), %rbx
	movq	%rbx, %rdi
	movq	%r13, %rdx
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testl	%r15d, %r15d
	jle	.LBB115_63
# %bb.41:                               # %for.cond111.preheader.preheader
	cmpl	$1, %r15d
	jne	.LBB115_42
.LBB115_56:                             # %for.cond.cleanup.loopexit.unr-lcssa
	testb	$1, %r15b
	je	.LBB115_63
# %bb.57:                               # %for.cond111.preheader.epil
	cmpq	%rbx, %rax
	je	.LBB115_58
# %bb.59:                               # %for.body115.epil.preheader
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB115_60:                             # %for.body115.epil
                                        # =>This Inner Loop Header: Depth=1
	movzbl	-1(%rax), %edx
	addb	$-48, %dl
	movzbl	%dl, %edx
	addl	%edx, %edx
	orl	%ecx, %edx
	leal	-10(%rdx), %esi
	xorl	%ecx, %ecx
	cmpb	$9, %dl
	setg	%cl
	movzbl	%sil, %esi
	cmovlel	%edx, %esi
	addb	$48, %sil
	movb	%sil, -1(%rax)
	leaq	-1(%rax), %rsi
	movq	%rsi, %rax
	cmpq	%rsi, %rbx
	jne	.LBB115_60
# %bb.61:                               # %for.cond.cleanup114.epil
	cmpb	$10, %dl
	jl	.LBB115_63
# %bb.62:                               # %if.then135.epil
	movb	$49, -1(%rbx)
	decq	%rbx
	jmp	.LBB115_63
.LBB115_42:                             # %for.cond111.preheader.preheader.new
	movl	%r15d, %r8d
	andl	$-2, %r8d
	jmp	.LBB115_43
	.p2align	4, 0x90
.LBB115_54:                             # %if.end137.1
                                        #   in Loop: Header=BB115_43 Depth=1
	movq	%rdx, %rbx
.LBB115_55:                             # %if.end137.1
                                        #   in Loop: Header=BB115_43 Depth=1
	addl	$-2, %r8d
	je	.LBB115_56
.LBB115_43:                             # %for.cond111.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB115_45 Depth 2
                                        #     Child Loop BB115_51 Depth 2
	movq	%rax, %rdx
	cmpq	%rbx, %rax
	je	.LBB115_49
# %bb.44:                               # %for.body115.preheader
                                        #   in Loop: Header=BB115_43 Depth=1
	xorl	%esi, %esi
	movq	%rax, %rdx
	.p2align	4, 0x90
.LBB115_45:                             # %for.body115
                                        #   Parent Loop BB115_43 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movzbl	-1(%rdx), %ecx
	addb	$-48, %cl
	movzbl	%cl, %edi
	addl	%edi, %edi
	orl	%esi, %edi
	leal	-10(%rdi), %ecx
	xorl	%esi, %esi
	cmpb	$9, %dil
	setg	%sil
	movzbl	%cl, %ecx
	cmovlel	%edi, %ecx
	addb	$48, %cl
	movb	%cl, -1(%rdx)
	leaq	-1(%rdx), %rcx
	movq	%rcx, %rdx
	cmpq	%rcx, %rbx
	jne	.LBB115_45
# %bb.46:                               # %for.cond.cleanup114
                                        #   in Loop: Header=BB115_43 Depth=1
	cmpb	$9, %dil
	jle	.LBB115_48
# %bb.47:                               # %if.then135
                                        #   in Loop: Header=BB115_43 Depth=1
	movb	$49, -1(%rbx)
	decq	%rbx
.LBB115_48:                             # %if.end137
                                        #   in Loop: Header=BB115_43 Depth=1
	movq	%rbx, %rdx
.LBB115_49:                             # %if.end137
                                        #   in Loop: Header=BB115_43 Depth=1
	movq	%rax, %rbx
	cmpq	%rdx, %rax
	je	.LBB115_55
# %bb.50:                               # %for.body115.1.preheader
                                        #   in Loop: Header=BB115_43 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rsi
	.p2align	4, 0x90
.LBB115_51:                             # %for.body115.1
                                        #   Parent Loop BB115_43 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movzbl	-1(%rsi), %ecx
	addb	$-48, %cl
	movzbl	%cl, %ebx
	addl	%ebx, %ebx
	orl	%edi, %ebx
	leal	-10(%rbx), %ecx
	xorl	%edi, %edi
	cmpb	$9, %bl
	setg	%dil
	movzbl	%cl, %ecx
	cmovlel	%ebx, %ecx
	addb	$48, %cl
	movb	%cl, -1(%rsi)
	leaq	-1(%rsi), %rcx
	movq	%rcx, %rsi
	cmpq	%rcx, %rdx
	jne	.LBB115_51
# %bb.52:                               # %for.cond.cleanup114.1
                                        #   in Loop: Header=BB115_43 Depth=1
	cmpb	$10, %bl
	jl	.LBB115_54
# %bb.53:                               # %if.then135.1
                                        #   in Loop: Header=BB115_43 Depth=1
	movb	$49, -1(%rdx)
	decq	%rdx
	jmp	.LBB115_54
.LBB115_58:
	movq	%rax, %rbx
.LBB115_63:                             # %for.cond.cleanup
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	movl	$6, %ecx
.LBB115_33:                             # %cleanup147
	callq	halide_int64_to_string@PLT
.LBB115_34:                             # %cleanup147
	addq	$536, %rsp                      # imm = 0x218
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end115:
	.size	halide_double_to_string, .Lfunc_end115-halide_double_to_string
                                        # -- End function
	.section	.text.halide_pointer_to_string,"ax",@progbits
	.weak	halide_pointer_to_string        # -- Begin function halide_pointer_to_string
	.p2align	4, 0x90
	.type	halide_pointer_to_string,@function
halide_pointer_to_string:               # @halide_pointer_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, -32(%rbp)
	movl	$0, -16(%rbp)
	movl	%edx, %eax
	andl	$15, %eax
	leaq	.L.str.13.71(%rip), %r8
	movb	(%rax,%r8), %cl
	leaq	-15(%rbp), %rax
	movb	%cl, -14(%rbp)
	movq	%rdx, %rcx
	shrq	$4, %rcx
	je	.LBB116_17
# %bb.1:                                # %for.cond
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-16(%rbp), %r9
	movb	%cl, -15(%rbp)
	movq	%rdx, %rcx
	shrq	$8, %rcx
	je	.LBB116_16
# %bb.2:                                # %for.cond.1
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-17(%rbp), %rax
	movb	%cl, -16(%rbp)
	movq	%rdx, %rcx
	shrq	$12, %rcx
	je	.LBB116_24
# %bb.3:                                # %for.cond.2
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-18(%rbp), %r9
	movb	%cl, -17(%rbp)
	movq	%rdx, %rcx
	shrq	$16, %rcx
	je	.LBB116_16
# %bb.4:                                # %for.cond.3
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-19(%rbp), %rax
	movb	%cl, -18(%rbp)
	movq	%rdx, %rcx
	shrq	$20, %rcx
	je	.LBB116_24
# %bb.5:                                # %for.cond.4
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-20(%rbp), %r9
	movb	%cl, -19(%rbp)
	movq	%rdx, %rcx
	shrq	$24, %rcx
	je	.LBB116_16
# %bb.6:                                # %for.cond.5
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-21(%rbp), %rax
	movb	%cl, -20(%rbp)
	movq	%rdx, %rcx
	shrq	$28, %rcx
	je	.LBB116_24
# %bb.7:                                # %for.cond.6
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-22(%rbp), %r9
	movb	%cl, -21(%rbp)
	movq	%rdx, %rcx
	shrq	$32, %rcx
	je	.LBB116_16
# %bb.8:                                # %for.cond.7
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-23(%rbp), %rax
	movb	%cl, -22(%rbp)
	movq	%rdx, %rcx
	shrq	$36, %rcx
	je	.LBB116_24
# %bb.9:                                # %for.cond.8
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-24(%rbp), %r9
	movb	%cl, -23(%rbp)
	movq	%rdx, %rcx
	shrq	$40, %rcx
	je	.LBB116_16
# %bb.10:                               # %for.cond.9
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-25(%rbp), %rax
	movb	%cl, -24(%rbp)
	movq	%rdx, %rcx
	shrq	$44, %rcx
	je	.LBB116_24
# %bb.11:                               # %for.cond.10
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-26(%rbp), %r9
	movb	%cl, -25(%rbp)
	movq	%rdx, %rcx
	shrq	$48, %rcx
	je	.LBB116_16
# %bb.12:                               # %for.cond.11
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-27(%rbp), %rax
	movb	%cl, -26(%rbp)
	movq	%rdx, %rcx
	shrq	$52, %rcx
	je	.LBB116_24
# %bb.13:                               # %for.cond.12
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-28(%rbp), %r9
	movb	%cl, -27(%rbp)
	movq	%rdx, %rcx
	shrq	$56, %rcx
	je	.LBB116_16
# %bb.14:                               # %for.cond.13
	andl	$15, %ecx
	movb	(%rcx,%r8), %cl
	leaq	-29(%rbp), %rax
	movb	%cl, -28(%rbp)
	shrq	$60, %rdx
	je	.LBB116_24
# %bb.15:                               # %for.cond.14
	movb	(%rdx,%r8), %cl
	movq	%rax, %rdx
	leaq	-30(%rbp), %rax
	movb	%cl, -29(%rbp)
	jmp	.LBB116_25
.LBB116_16:
	movq	%rax, %rdx
	movq	%r9, %rax
	jmp	.LBB116_25
.LBB116_17:
	leaq	-14(%rbp), %rdx
	jmp	.LBB116_25
.LBB116_24:
	movq	%r9, %rdx
.LBB116_25:                             # %cleanup
	movb	$120, (%rax)
	movb	$48, -2(%rdx)
	addq	$-2, %rdx
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end116:
	.size	halide_pointer_to_string, .Lfunc_end116-halide_pointer_to_string
                                        # -- End function
	.section	.text.halide_type_to_string,"ax",@progbits
	.weak	halide_type_to_string           # -- Begin function halide_type_to_string
	.p2align	4, 0x90
	.type	halide_type_to_string,@function
halide_type_to_string:                  # @halide_type_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdx, %rbx
	movq	%rsi, %r14
	movsbq	(%rdx), %rax
	cmpq	$3, %rax
	ja	.LBB117_1
# %bb.2:                                # %switch.lookup
	leaq	.Lswitch.table.halide_type_to_string(%rip), %rcx
	movq	(%rcx,%rax,8), %rdx
	jmp	.LBB117_3
.LBB117_1:
	leaq	.L.str.18.72(%rip), %rdx
.LBB117_3:                              # %sw.epilog
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movzbl	1(%rbx), %edx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	cmpw	$1, 2(%rbx)
	jne	.LBB117_5
# %bb.4:                                # %if.end
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.LBB117_5:                              # %if.then
	leaq	.L.str.19.77(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movzwl	2(%rbx), %edx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_uint64_to_string@PLT     # TAILCALL
.Lfunc_end117:
	.size	halide_type_to_string, .Lfunc_end117-halide_type_to_string
                                        # -- End function
	.section	.text.halide_buffer_to_string,"ax",@progbits
	.weak	halide_buffer_to_string         # -- Begin function halide_buffer_to_string
	.p2align	4, 0x90
	.type	halide_buffer_to_string,@function
halide_buffer_to_string:                # @halide_buffer_to_string
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	testq	%rdx, %rdx
	je	.LBB118_1
# %bb.3:                                # %if.end
	movq	%rdx, %r14
	leaq	.L.str.21.79(%rip), %rdx
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	(%r14), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.55(%rip), %r15
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	8(%r14), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_pointer_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	16(%r14), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_pointer_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	24(%r14), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	32(%r14), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_type_to_string@PLT
	cmpl	$0, 36(%r14)
	jle	.LBB118_6
# %bb.4:                                # %for.body.lr.ph
	xorl	%r15d, %r15d
	leaq	.L.str.55(%rip), %r12
	xorl	%r13d, %r13d
	.p2align	4, 0x90
.LBB118_5:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rdi
	movq	%rbx, %rsi
	leaq	.L.str.23.82(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	40(%r14), %rcx
	movslq	(%rcx,%r15), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movq	40(%r14), %rcx
	movslq	4(%rcx,%r15), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movq	40(%r14), %rcx
	movslq	8(%rcx,%r15), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	leaq	.L.str.24.83(%rip), %rdx
	callq	halide_string_to_string@PLT
	incq	%r13
	movslq	36(%r14), %rcx
	addq	$16, %r15
	cmpq	%rcx, %r13
	jl	.LBB118_5
.LBB118_6:                              # %for.cond.cleanup
	leaq	.L.str.8.119(%rip), %rdx
	movq	%rax, %rdi
	jmp	.LBB118_2
.LBB118_1:                              # %if.then
	leaq	.L.str.20.78(%rip), %rdx
.LBB118_2:                              # %if.then
	movq	%rbx, %rsi
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_string_to_string@PLT     # TAILCALL
.Lfunc_end118:
	.size	halide_buffer_to_string, .Lfunc_end118-halide_buffer_to_string
                                        # -- End function
	.section	.text.halide_malloc_alignment,"ax",@progbits
	.weak	halide_malloc_alignment         # -- Begin function halide_malloc_alignment
	.p2align	4, 0x90
	.type	halide_malloc_alignment,@function
halide_malloc_alignment:                # @halide_malloc_alignment
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$64, %eax
	popq	%rbp
	retq
.Lfunc_end119:
	.size	halide_malloc_alignment, .Lfunc_end119-halide_malloc_alignment
                                        # -- End function
	.section	.text.halide_reuse_device_allocations,"ax",@progbits
	.weak	halide_reuse_device_allocations # -- Begin function halide_reuse_device_allocations
	.p2align	4, 0x90
	.type	halide_reuse_device_allocations,@function
halide_reuse_device_allocations:        # @halide_reuse_device_allocations
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE@GOTPCREL(%rip), %rax
	movb	%sil, (%rax)
	xorl	%r15d, %r15d
	testl	%esi, %esi
	jne	.LBB120_4
# %bb.1:                                # %if.then
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal23device_allocation_poolsE@GOTPCREL(%rip), %rax
	movq	(%rax), %rbx
	xorl	%r15d, %r15d
	testq	%rbx, %rbx
	je	.LBB120_3
	.p2align	4, 0x90
.LBB120_5:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	callq	*(%rbx)
	testl	%eax, %eax
	cmovnel	%eax, %r15d
	movq	8(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB120_5
.LBB120_3:                              # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
.LBB120_4:                              # %if.end5
	movl	%r15d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end120:
	.size	halide_reuse_device_allocations, .Lfunc_end120-halide_reuse_device_allocations
                                        # -- End function
	.section	.text.halide_can_reuse_device_allocations,"ax",@progbits
	.weak	halide_can_reuse_device_allocations # -- Begin function halide_can_reuse_device_allocations
	.p2align	4, 0x90
	.type	halide_can_reuse_device_allocations,@function
halide_can_reuse_device_allocations:    # @halide_can_reuse_device_allocations
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE@GOTPCREL(%rip), %rax
	movb	(%rax), %al
	popq	%rbp
	retq
.Lfunc_end121:
	.size	halide_can_reuse_device_allocations, .Lfunc_end121-halide_can_reuse_device_allocations
                                        # -- End function
	.section	.text.halide_register_device_allocation_pool,"ax",@progbits
	.weak	halide_register_device_allocation_pool # -- Begin function halide_register_device_allocation_pool
	.p2align	4, 0x90
	.type	halide_register_device_allocation_pool,@function
halide_register_device_allocation_pool: # @halide_register_device_allocation_pool
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal23device_allocation_poolsE@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	movq	%rcx, 8(%rbx)
	movq	%rbx, (%rax)
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end122:
	.size	halide_register_device_allocation_pool, .Lfunc_end122-halide_register_device_allocation_pool
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t # -- Begin function _ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t,@function
_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t: # @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	24(%rsi), %rax
	xorl	%ebx, %ebx
	testb	$2, %al
	je	.LBB123_6
# %bb.1:                                # %if.end
	movl	$-14, %ebx
	testb	$1, %al
	jne	.LBB123_6
# %bb.2:                                # %if.end9
	movq	%rsi, %r14
	movq	8(%rsi), %rax
	testq	%rax, %rax
	je	.LBB123_3
# %bb.4:                                # %if.end15
	movq	%rdi, %r15
	movq	120(%rax), %rax
	movq	%r14, %rsi
	callq	*48(%rax)
	testl	%eax, %eax
	jne	.LBB123_6
# %bb.5:                                # %if.end23
	andb	$-3, 24(%r14)
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_msan_annotate_buffer_is_initialized@PLT
	xorl	%ebx, %ebx
	jmp	.LBB123_6
.LBB123_3:
	movl	$-19, %ebx
.LBB123_6:                              # %return
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end123:
	.size	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t, .Lfunc_end123-_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
                                        # -- End function
	.section	.text.halide_device_release,"ax",@progbits
	.weak	halide_device_release           # -- Begin function halide_device_release
	.p2align	4, 0x90
	.type	halide_device_release,@function
halide_device_release:                  # @halide_device_release
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	120(%rsi), %rax
	popq	%rbp
	jmpq	*40(%rax)                       # TAILCALL
.Lfunc_end124:
	.size	halide_device_release, .Lfunc_end124-halide_device_release
                                        # -- End function
	.section	.text.halide_copy_to_host,"ax",@progbits
	.weak	halide_copy_to_host             # -- Begin function halide_copy_to_host
	.p2align	4, 0x90
	.type	halide_copy_to_host,@function
halide_copy_to_host:                    # @halide_copy_to_host
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	testq	%r15, %r15
	je	.LBB125_1
# %bb.2:                                # %if.end.i
	movq	(%r15), %rax
	movq	8(%r15), %rcx
	testq	%rax, %rax
	je	.LBB125_5
# %bb.3:                                # %if.end.i
	testq	%rcx, %rcx
	jne	.LBB125_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB125_12
	jmp	.LBB125_11
.LBB125_1:                              # %if.then.i
	leaq	.L.str.6.88(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB125_12
	jmp	.LBB125_11
.LBB125_5:                              # %if.end10.i
	testq	%rcx, %rcx
	je	.LBB125_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB125_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB125_12
	jmp	.LBB125_11
.LBB125_8:                              # %if.end16.i
	movl	24(%r15), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB125_11
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB125_12
.LBB125_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t@PLT
	movl	%eax, %ebx
.LBB125_12:                             # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end125:
	.size	halide_copy_to_host, .Lfunc_end125-halide_copy_to_host
                                        # -- End function
	.section	.text.copy_to_device_already_locked,"ax",@progbits
	.weak	copy_to_device_already_locked   # -- Begin function copy_to_device_already_locked
	.p2align	4, 0x90
	.type	copy_to_device_already_locked,@function
copy_to_device_already_locked:          # @copy_to_device_already_locked
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB126_1
# %bb.2:                                # %if.end.i
	movq	(%r12), %rax
	movq	8(%r12), %rcx
	testq	%rax, %rax
	je	.LBB126_5
# %bb.3:                                # %if.end.i
	testq	%rcx, %rcx
	jne	.LBB126_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB126_21
	jmp	.LBB126_11
.LBB126_1:                              # %if.then.i
	leaq	.L.str.7.89(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB126_21
	jmp	.LBB126_11
.LBB126_5:                              # %if.end10.i
	testq	%rcx, %rcx
	je	.LBB126_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB126_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB126_21
	jmp	.LBB126_11
.LBB126_8:                              # %if.end16.i
	movl	24(%r12), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB126_11
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB126_21
.LBB126_11:                             # %if.end
	testq	%r15, %r15
	jne	.LBB126_13
# %bb.12:                               # %if.then2
	movq	8(%r12), %r15
	testq	%r15, %r15
	je	.LBB126_22
.LBB126_13:                             # %if.end11
	cmpq	$0, (%r12)
	je	.LBB126_16
# %bb.14:                               # %land.lhs.true
	cmpq	%r15, 8(%r12)
	je	.LBB126_17
# %bb.15:                               # %if.then14
	leaq	.L.str.9.90(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error@PLT
	movl	$-42, %ebx
	jmp	.LBB126_21
.LBB126_16:                             # %if.then18
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB126_21
.LBB126_17:                             # %if.end27
	movq	24(%r12), %rax
	xorl	%ebx, %ebx
	testb	$1, %al
	je	.LBB126_21
# %bb.18:                               # %if.then29
	movl	$-15, %ebx
	testb	$2, %al
	jne	.LBB126_21
# %bb.19:                               # %if.else
	movq	120(%r15), %rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*56(%rax)
	testl	%eax, %eax
	jne	.LBB126_21
# %bb.20:                               # %if.then46
	andb	$-2, 24(%r12)
	xorl	%ebx, %ebx
.LBB126_21:                             # %cleanup
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB126_22:                             # %if.then7
	movq	%r14, %rdi
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_no_device_interface@PLT # TAILCALL
.Lfunc_end126:
	.size	copy_to_device_already_locked, .Lfunc_end126-copy_to_device_already_locked
                                        # -- End function
	.section	.text.halide_device_malloc,"ax",@progbits
	.weak	halide_device_malloc            # -- Begin function halide_device_malloc
	.p2align	4, 0x90
	.type	halide_device_malloc,@function
halide_device_malloc:                   # @halide_device_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r14
	movq	%rsi, %rbx
	movq	%rdi, %r15
	testq	%rsi, %rsi
	je	.LBB127_1
# %bb.2:                                # %if.end.i
	movq	(%rbx), %rcx
	movq	8(%rbx), %rax
	testq	%rcx, %rcx
	je	.LBB127_5
# %bb.3:                                # %if.end.i
	testq	%rax, %rax
	jne	.LBB127_5
# %bb.4:                                # %if.then8.i
	movq	%r15, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB127_16
	jmp	.LBB127_11
.LBB127_1:                              # %if.then.i
	leaq	.L.str.17.91(%rip), %rsi
	movq	%r15, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB127_16
	jmp	.LBB127_11
.LBB127_5:                              # %if.end10.i
	testq	%rax, %rax
	je	.LBB127_8
# %bb.6:                                # %if.end10.i
	testq	%rcx, %rcx
	jne	.LBB127_8
# %bb.7:                                # %if.then14.i
	movq	%r15, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB127_16
	jmp	.LBB127_11
.LBB127_8:                              # %if.end16.i
	movl	24(%rbx), %ecx
	andl	$3, %ecx
	cmpl	$3, %ecx
	jne	.LBB127_12
# %bb.9:                                # %if.then24.i
	movq	%r15, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB127_16
.LBB127_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%rbx), %rax
.LBB127_12:                             # %if.end
	testq	%rax, %rax
	je	.LBB127_15
# %bb.13:                               # %if.end
	cmpq	%r14, %rax
	je	.LBB127_15
# %bb.14:                               # %if.then6
	leaq	.L.str.20.92(%rip), %rsi
	movq	%r15, %rdi
	callq	halide_error@PLT
	movl	$-42, %eax
	jmp	.LBB127_16
.LBB127_15:                             # %if.end7
	movq	120(%r14), %rax
	callq	*(%rax)
	movq	120(%r14), %rax
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	*16(%rax)
	movl	%eax, %ebx
	movq	120(%r14), %rax
	callq	*8(%rax)
	xorl	%eax, %eax
	testl	%ebx, %ebx
	sete	%al
	shll	$4, %eax
	addl	$-16, %eax
.LBB127_16:                             # %cleanup12
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end127:
	.size	halide_device_malloc, .Lfunc_end127-halide_device_malloc
                                        # -- End function
	.section	.text.halide_copy_to_device,"ax",@progbits
	.weak	halide_copy_to_device           # -- Begin function halide_copy_to_device
	.p2align	4, 0x90
	.type	halide_copy_to_device,@function
halide_copy_to_device:                  # @halide_copy_to_device
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r14
	movq	%rsi, %r15
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %r12
	movq	%r12, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	copy_to_device_already_locked@PLT
	movl	%eax, %ebx
	movq	%r12, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end128:
	.size	halide_copy_to_device, .Lfunc_end128-halide_copy_to_device
                                        # -- End function
	.section	.text.halide_device_sync,"ax",@progbits
	.weak	halide_device_sync              # -- Begin function halide_device_sync
	.p2align	4, 0x90
	.type	halide_device_sync,@function
halide_device_sync:                     # @halide_device_sync
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %rbx
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB129_1
# %bb.2:                                # %if.end.i
	movq	(%rbx), %rcx
	movq	8(%rbx), %rax
	testq	%rcx, %rcx
	je	.LBB129_5
# %bb.3:                                # %if.end.i
	testq	%rax, %rax
	jne	.LBB129_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB129_14
	jmp	.LBB129_11
.LBB129_1:                              # %if.then.i
	leaq	.L.str.16.93(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB129_14
	jmp	.LBB129_11
.LBB129_5:                              # %if.end10.i
	testq	%rax, %rax
	je	.LBB129_8
# %bb.6:                                # %if.end10.i
	testq	%rcx, %rcx
	jne	.LBB129_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	je	.LBB129_11
.LBB129_14:                             # %cleanup8
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.LBB129_8:                              # %if.end16.i
	movl	24(%rbx), %ecx
	andl	$3, %ecx
	cmpl	$3, %ecx
	jne	.LBB129_12
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB129_14
.LBB129_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%rbx), %rax
.LBB129_12:                             # %if.end
	testq	%rax, %rax
	je	.LBB129_15
# %bb.13:                               # %if.end5
	movq	120(%rax), %rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	*32(%rax)
	testl	%eax, %eax
	movl	$-17, %ecx
	cmovnel	%ecx, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.LBB129_15:                             # %if.then3
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_error_no_device_interface@PLT # TAILCALL
.Lfunc_end129:
	.size	halide_device_sync, .Lfunc_end129-halide_device_sync
                                        # -- End function
	.section	.text.halide_device_free,"ax",@progbits
	.weak	halide_device_free              # -- Begin function halide_device_free
	.p2align	4, 0x90
	.type	halide_device_free,@function
halide_device_free:                     # @halide_device_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB130_1
# %bb.2:                                # %if.end.i
	movq	(%r12), %rax
	movq	8(%r12), %rbx
	testq	%rax, %rax
	je	.LBB130_5
# %bb.3:                                # %if.end.i
	testq	%rbx, %rbx
	jne	.LBB130_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB130_17
	jmp	.LBB130_11
.LBB130_1:                              # %if.then.i
	leaq	.L.str.21.96(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB130_17
	jmp	.LBB130_11
.LBB130_5:                              # %if.end10.i
	testq	%rbx, %rbx
	je	.LBB130_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB130_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB130_17
	jmp	.LBB130_11
.LBB130_8:                              # %if.end16.i
	movl	24(%r12), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB130_12
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB130_17
.LBB130_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%r12), %rbx
.LBB130_12:                             # %if.end
	testq	%rbx, %rbx
	je	.LBB130_16
# %bb.13:                               # %if.then3
	movq	120(%rbx), %rax
	callq	*(%rax)
	movq	120(%rbx), %rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*24(%rax)
	movl	%eax, %r15d
	movq	120(%rbx), %rax
	callq	*8(%rax)
	cmpq	$0, (%r12)
	je	.LBB130_15
# %bb.14:                               # %if.then8
	leaq	.L.str.22.97(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB130_15:                             # %do.end
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB130_17
.LBB130_16:                             # %if.end11
	andb	$-3, 24(%r12)
	xorl	%eax, %eax
.LBB130_17:                             # %cleanup12
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end130:
	.size	halide_device_free, .Lfunc_end130-halide_device_free
                                        # -- End function
	.section	.text.halide_device_free_as_destructor,"ax",@progbits
	.weak	halide_device_free_as_destructor # -- Begin function halide_device_free_as_destructor
	.p2align	4, 0x90
	.type	halide_device_free_as_destructor,@function
halide_device_free_as_destructor:       # @halide_device_free_as_destructor
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT          # TAILCALL
.Lfunc_end131:
	.size	halide_device_free_as_destructor, .Lfunc_end131-halide_device_free_as_destructor
                                        # -- End function
	.section	.text.halide_device_and_host_malloc,"ax",@progbits
	.weak	halide_device_and_host_malloc   # -- Begin function halide_device_and_host_malloc
	.p2align	4, 0x90
	.type	halide_device_and_host_malloc,@function
halide_device_and_host_malloc:          # @halide_device_and_host_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r14
	movq	%rsi, %rbx
	movq	%rdi, %r15
	testq	%rsi, %rsi
	je	.LBB132_1
# %bb.2:                                # %if.end.i
	movq	(%rbx), %rcx
	movq	8(%rbx), %rax
	testq	%rcx, %rcx
	je	.LBB132_5
# %bb.3:                                # %if.end.i
	testq	%rax, %rax
	jne	.LBB132_5
# %bb.4:                                # %if.then8.i
	movq	%r15, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB132_18
	jmp	.LBB132_11
.LBB132_1:                              # %if.then.i
	leaq	.L.str.23.98(%rip), %rsi
	movq	%r15, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB132_18
	jmp	.LBB132_11
.LBB132_5:                              # %if.end10.i
	testq	%rax, %rax
	je	.LBB132_8
# %bb.6:                                # %if.end10.i
	testq	%rcx, %rcx
	jne	.LBB132_8
# %bb.7:                                # %if.then14.i
	movq	%r15, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB132_18
	jmp	.LBB132_11
.LBB132_8:                              # %if.end16.i
	movl	24(%rbx), %ecx
	andl	$3, %ecx
	cmpl	$3, %ecx
	jne	.LBB132_12
# %bb.9:                                # %if.then24.i
	movq	%r15, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB132_18
.LBB132_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%rbx), %rax
.LBB132_12:                             # %if.end
	testq	%rax, %rax
	je	.LBB132_15
# %bb.13:                               # %if.end
	cmpq	%r14, %rax
	je	.LBB132_15
# %bb.14:                               # %if.then6
	leaq	.L.str.25.99(%rip), %rsi
	movq	%r15, %rdi
	callq	halide_error@PLT
	movl	$-42, %eax
	jmp	.LBB132_18
.LBB132_15:                             # %if.end7
	movq	120(%r14), %rax
	callq	*(%rax)
	movq	120(%r14), %rax
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	*64(%rax)
	movl	%eax, %ebx
	movq	120(%r14), %rax
	callq	*8(%rax)
	testl	%ebx, %ebx
	je	.LBB132_16
# %bb.17:                               # %if.then12
	leaq	.L.str.26.100(%rip), %rsi
	movq	%r15, %rdi
	callq	halide_error@PLT
	movl	$-16, %eax
	jmp	.LBB132_18
.LBB132_16:
	xorl	%eax, %eax
.LBB132_18:                             # %cleanup14
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end132:
	.size	halide_device_and_host_malloc, .Lfunc_end132-halide_device_and_host_malloc
                                        # -- End function
	.section	.text.halide_device_and_host_free,"ax",@progbits
	.weak	halide_device_and_host_free     # -- Begin function halide_device_and_host_free
	.p2align	4, 0x90
	.type	halide_device_and_host_free,@function
halide_device_and_host_free:            # @halide_device_and_host_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB133_1
# %bb.2:                                # %if.end.i
	movq	(%r12), %rax
	movq	8(%r12), %rbx
	testq	%rax, %rax
	je	.LBB133_5
# %bb.3:                                # %if.end.i
	testq	%rbx, %rbx
	jne	.LBB133_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB133_19
	jmp	.LBB133_11
.LBB133_1:                              # %if.then.i
	leaq	.L.str.27.101(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB133_19
	jmp	.LBB133_11
.LBB133_5:                              # %if.end10.i
	testq	%rbx, %rbx
	je	.LBB133_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB133_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB133_19
	jmp	.LBB133_11
.LBB133_8:                              # %if.end16.i
	movl	24(%r12), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB133_12
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB133_19
.LBB133_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%r12), %rbx
.LBB133_12:                             # %if.end
	testq	%rbx, %rbx
	je	.LBB133_16
# %bb.13:                               # %if.then3
	movq	120(%rbx), %rax
	callq	*(%rax)
	movq	120(%rbx), %rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*72(%rax)
	movl	%eax, %r15d
	movq	120(%rbx), %rax
	callq	*8(%rax)
	cmpq	$0, (%r12)
	je	.LBB133_15
# %bb.14:                               # %if.then8
	leaq	.L.str.28.102(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_15:                             # %do.end
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB133_19
.LBB133_16:                             # %if.else11
	movq	16(%r12), %rsi
	testq	%rsi, %rsi
	je	.LBB133_18
# %bb.17:                               # %if.then13
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 16(%r12)
.LBB133_18:                             # %if.end17
	andb	$-3, 24(%r12)
	xorl	%eax, %eax
.LBB133_19:                             # %cleanup18
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end133:
	.size	halide_device_and_host_free, .Lfunc_end133-halide_device_and_host_free
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function halide_default_device_and_host_malloc
.LCPI134_0:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI134_1:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI134_2:
	.quad	4                               # 0x4
.LCPI134_3:
	.quad	68                              # 0x44
.LCPI134_4:
	.quad	132                             # 0x84
.LCPI134_5:
	.quad	196                             # 0xc4
.LCPI134_6:
	.quad	16                              # 0x10
	.section	.text.halide_default_device_and_host_malloc,"ax",@progbits
	.weak	halide_default_device_and_host_malloc
	.p2align	4, 0x90
	.type	halide_default_device_and_host_malloc,@function
halide_default_device_and_host_malloc:  # @halide_default_device_and_host_malloc
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB134_1
# %bb.2:                                # %if.end.i
	movq	(%r12), %rax
	movq	8(%r12), %rcx
	testq	%rax, %rax
	je	.LBB134_5
# %bb.3:                                # %if.end.i
	testq	%rcx, %rcx
	jne	.LBB134_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB134_37
	jmp	.LBB134_11
.LBB134_1:                              # %if.then.i
	leaq	.L.str.29.103(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB134_37
	jmp	.LBB134_11
.LBB134_5:                              # %if.end10.i
	testq	%rcx, %rcx
	je	.LBB134_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB134_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB134_37
	jmp	.LBB134_11
.LBB134_8:                              # %if.end16.i
	movl	24(%r12), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB134_11
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB134_37
.LBB134_11:                             # %if.end
	movl	36(%r12), %eax
	testl	%eax, %eax
	jle	.LBB134_12
# %bb.13:                               # %for.body.lr.ph.i.i
	movq	40(%r12), %r8
	cmpl	$17, %eax
	jae	.LBB134_15
# %bb.14:
	xorl	%esi, %esi
	xorl	%edx, %edx
	jmp	.LBB134_18
.LBB134_12:
	movl	$1, %edx
	xorl	%esi, %esi
	jmp	.LBB134_32
.LBB134_15:                             # %vector.ph
	movl	%eax, %edx
	andl	$15, %edx
	testq	%rdx, %rdx
	movl	$16, %edi
	cmovneq	%rdx, %rdi
	movq	%rax, %rsi
	subq	%rdi, %rsi
	leaq	200(%r8), %rdx
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	.LCPI134_0(%rip), %ymm1         # ymm1 = [0,1,2,3]
	vmovdqa64	.LCPI134_1(%rip), %xmm24 # xmm24 = [0,4,8,12]
	vpxor	%xmm3, %xmm3, %xmm3
	vpbroadcastq	%r8, %ymm9
	vpbroadcastq	.LCPI134_2(%rip), %ymm21 # ymm21 = [4,4,4,4]
	vpbroadcastq	.LCPI134_3(%rip), %ymm22 # ymm22 = [68,68,68,68]
	vpbroadcastq	.LCPI134_4(%rip), %ymm23 # ymm23 = [132,132,132,132]
	vpbroadcastq	.LCPI134_5(%rip), %ymm8 # ymm8 = [196,196,196,196]
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpbroadcastq	.LCPI134_6(%rip), %ymm10 # ymm10 = [16,16,16,16]
	movq	%rsi, %rdi
	vpxor	%xmm11, %xmm11, %xmm11
	vpxor	%xmm12, %xmm12, %xmm12
	vpxor	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
.LBB134_16:                             # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	-192(%rdx), %ymm14
	vmovdqu	-128(%rdx), %ymm15
	vmovdqu64	-64(%rdx), %ymm16
	vmovdqu64	(%rdx), %ymm17
	vpermt2d	-160(%rdx), %ymm24, %ymm14
	vpermt2d	-96(%rdx), %ymm24, %ymm15
	vpermt2d	-32(%rdx), %ymm24, %ymm16
	vpermt2d	32(%rdx), %ymm24, %ymm17
	vpcmpgtd	%xmm3, %xmm14, %k4
	vpcmpgtd	%xmm3, %xmm15, %k3
	vpcmpgtd	%xmm3, %xmm16, %k2
	vpcmpgtd	%xmm3, %xmm17, %k1
	vpsllq	$4, %ymm1, %ymm18
	vpaddq	%ymm18, %ymm9, %ymm18
	vpaddq	%ymm21, %ymm18, %ymm19
	vpaddq	%ymm22, %ymm18, %ymm20
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm19), %xmm5 {%k5}
	vpaddq	%ymm23, %ymm18, %ymm19
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm20), %xmm6 {%k5}
	vpaddq	%ymm8, %ymm18, %ymm18
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm19), %xmm7 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm18), %xmm2 {%k5}
	vpmovzxdq	%xmm14, %ymm14          # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero
	vpmovzxdq	%xmm15, %ymm15          # ymm15 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpmovzxdq	%xmm16, %ymm16          # ymm16 = xmm16[0],zero,xmm16[1],zero,xmm16[2],zero,xmm16[3],zero
	vpmovzxdq	%xmm17, %ymm17          # ymm17 = xmm17[0],zero,xmm17[1],zero,xmm17[2],zero,xmm17[3],zero
	vpaddd	%xmm4, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm7, %xmm7
	vpmovsxdq	%xmm5, %ymm5
	vpmovsxdq	%xmm6, %ymm6
	vpmovsxdq	%xmm7, %ymm7
	vpmullq	%ymm14, %ymm5, %ymm5 {%k4} {z}
	vpmullq	%ymm15, %ymm6, %ymm6 {%k3} {z}
	vpmullq	%ymm16, %ymm7, %ymm7 {%k2} {z}
	vpaddd	%xmm4, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpmullq	%ymm17, %ymm2, %ymm2 {%k1} {z}
	vpaddq	%ymm5, %ymm0, %ymm0
	vpaddq	%ymm6, %ymm11, %ymm11
	vpaddq	%ymm7, %ymm12, %ymm12
	vpaddq	%ymm2, %ymm13, %ymm13
	vpaddq	%ymm1, %ymm10, %ymm1
	addq	$256, %rdx                      # imm = 0x100
	addq	$-16, %rdi
	jne	.LBB134_16
# %bb.17:                               # %middle.block
	vpaddq	%ymm0, %ymm11, %ymm0
	vpaddq	%ymm0, %ymm12, %ymm0
	vpaddq	%ymm0, %ymm13, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
.LBB134_18:                             # %for.body.i.i.preheader
	movq	%rax, %rdi
	subq	%rsi, %rdi
	shlq	$4, %rsi
	addq	%r8, %rsi
	addq	$8, %rsi
	jmp	.LBB134_19
	.p2align	4, 0x90
.LBB134_21:                             # %if.end.i.i
                                        #   in Loop: Header=BB134_19 Depth=1
	addq	$16, %rsi
	decq	%rdi
	je	.LBB134_22
.LBB134_19:                             # %for.body.i.i
                                        # =>This Inner Loop Header: Depth=1
	movl	(%rsi), %ebx
	testl	%ebx, %ebx
	jle	.LBB134_21
# %bb.20:                               # %if.then.i.i
                                        #   in Loop: Header=BB134_19 Depth=1
	movslq	-4(%rsi), %rcx
	decq	%rcx
	imulq	%rbx, %rcx
	addq	%rcx, %rdx
	jmp	.LBB134_21
.LBB134_22:                             # %for.body.i13.i.preheader
	cmpl	$17, %eax
	jae	.LBB134_24
# %bb.23:
	xorl	%ebx, %ebx
	xorl	%esi, %esi
	jmp	.LBB134_27
.LBB134_24:                             # %vector.ph27
	movl	%eax, %ecx
	andl	$15, %ecx
	testq	%rcx, %rcx
	movl	$16, %esi
	cmovneq	%rcx, %rsi
	movq	%rax, %rbx
	subq	%rsi, %rbx
	leaq	200(%r8), %rdi
	vmovdqa	.LCPI134_0(%rip), %ymm1         # ymm1 = [0,1,2,3]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	.LCPI134_1(%rip), %xmm2         # xmm2 = [0,4,8,12]
	vpbroadcastq	%r8, %ymm8
	vpbroadcastq	.LCPI134_2(%rip), %ymm20 # ymm20 = [4,4,4,4]
	vpbroadcastq	.LCPI134_3(%rip), %ymm21 # ymm21 = [68,68,68,68]
	vpbroadcastq	.LCPI134_4(%rip), %ymm22 # ymm22 = [132,132,132,132]
	vpbroadcastq	.LCPI134_5(%rip), %ymm23 # ymm23 = [196,196,196,196]
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpbroadcastq	.LCPI134_6(%rip), %ymm9 # ymm9 = [16,16,16,16]
	movq	%rbx, %rsi
	vpxor	%xmm10, %xmm10, %xmm10
	vpxor	%xmm11, %xmm11, %xmm11
	vpxor	%xmm12, %xmm12, %xmm12
	.p2align	4, 0x90
.LBB134_25:                             # %vector.body25
                                        # =>This Inner Loop Header: Depth=1
	vmovdqu	-192(%rdi), %ymm13
	vmovdqu	-128(%rdi), %ymm14
	vmovdqu	-64(%rdi), %ymm15
	vmovdqu64	(%rdi), %ymm16
	vpermt2d	-160(%rdi), %ymm2, %ymm13
	vpermt2d	-96(%rdi), %ymm2, %ymm14
	vpermt2d	-32(%rdi), %ymm2, %ymm15
	vpermt2d	32(%rdi), %ymm2, %ymm16
	vpmovd2m	%xmm13, %k4
	vpmovd2m	%xmm14, %k3
	vpmovd2m	%xmm15, %k2
	vpmovd2m	%xmm16, %k1
	vpmovzxdq	%xmm13, %ymm13          # ymm13 = xmm13[0],zero,xmm13[1],zero,xmm13[2],zero,xmm13[3],zero
	vpmovzxdq	%xmm14, %ymm14          # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero
	vpmovzxdq	%xmm15, %ymm15          # ymm15 = xmm15[0],zero,xmm15[1],zero,xmm15[2],zero,xmm15[3],zero
	vpsllq	$4, %ymm1, %ymm17
	vpaddq	%ymm17, %ymm8, %ymm17
	vpaddq	%ymm20, %ymm17, %ymm18
	vpmovzxdq	%xmm16, %ymm16          # ymm16 = xmm16[0],zero,xmm16[1],zero,xmm16[2],zero,xmm16[3],zero
	vpaddq	%ymm21, %ymm17, %ymm19
	kmovq	%k4, %k5
	vpgatherqd	(,%ymm18), %xmm4 {%k5}
	vpaddq	%ymm22, %ymm17, %ymm18
	kmovq	%k3, %k5
	vpgatherqd	(,%ymm19), %xmm5 {%k5}
	vpaddq	%ymm23, %ymm17, %ymm17
	kmovq	%k2, %k5
	vpgatherqd	(,%ymm18), %xmm6 {%k5}
	kmovq	%k1, %k5
	vpgatherqd	(,%ymm17), %xmm7 {%k5}
	vpaddd	%xmm3, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm6, %xmm6
	vpaddd	%xmm3, %xmm7, %xmm7
	vpmovzxdq	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
	vpmovzxdq	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
	vpmovzxdq	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero
	vpmovzxdq	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero
	vpmuldq	%ymm13, %ymm4, %ymm4 {%k4} {z}
	vpaddq	%ymm4, %ymm0, %ymm0
	vpmuldq	%ymm14, %ymm5, %ymm4 {%k3} {z}
	vpmuldq	%ymm15, %ymm6, %ymm5 {%k2} {z}
	vpaddq	%ymm4, %ymm10, %ymm10
	vpaddq	%ymm5, %ymm11, %ymm11
	vpmuldq	%ymm16, %ymm7, %ymm4 {%k1} {z}
	vpaddq	%ymm4, %ymm12, %ymm12
	vpaddq	%ymm1, %ymm9, %ymm1
	addq	$256, %rdi                      # imm = 0x100
	addq	$-16, %rsi
	jne	.LBB134_25
# %bb.26:                               # %middle.block23
	vpaddq	%ymm0, %ymm10, %ymm0
	vpaddq	%ymm0, %ymm11, %ymm0
	vpaddq	%ymm0, %ymm12, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpaddq	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
	vpaddq	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rsi
.LBB134_27:                             # %for.body.i13.i.preheader64
	subq	%rbx, %rax
	shlq	$4, %rbx
	leaq	(%r8,%rbx), %rcx
	addq	$8, %rcx
	jmp	.LBB134_28
	.p2align	4, 0x90
.LBB134_30:                             # %if.end.i24.i
                                        #   in Loop: Header=BB134_28 Depth=1
	addq	$16, %rcx
	decq	%rax
	je	.LBB134_31
.LBB134_28:                             # %for.body.i13.i
                                        # =>This Inner Loop Header: Depth=1
	movslq	(%rcx), %rdi
	testq	%rdi, %rdi
	jns	.LBB134_30
# %bb.29:                               # %if.then.i20.i
                                        #   in Loop: Header=BB134_28 Depth=1
	movslq	-4(%rcx), %rbx
	decq	%rbx
	imulq	%rdi, %rbx
	addq	%rbx, %rsi
	jmp	.LBB134_30
.LBB134_31:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
	incq	%rdx
.LBB134_32:                             # %_ZNK15halide_buffer_t13size_in_bytesEv.exit
	subq	%rsi, %rdx
	movzbl	33(%r12), %esi
	addq	$7, %rsi
	shrq	$3, %rsi
	imulq	%rdx, %rsi
	movq	%r14, %rdi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 16(%r12)
	testq	%rax, %rax
	je	.LBB134_33
# %bb.34:                               # %if.end6
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	testl	%eax, %eax
	je	.LBB134_35
# %bb.36:                               # %if.then9
	movl	%eax, %ebx
	movq	16(%r12), %rsi
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 16(%r12)
	jmp	.LBB134_37
.LBB134_33:
	movl	$-1, %ebx
	jmp	.LBB134_37
.LBB134_35:
	xorl	%ebx, %ebx
.LBB134_37:                             # %cleanup13
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end134:
	.size	halide_default_device_and_host_malloc, .Lfunc_end134-halide_default_device_and_host_malloc
                                        # -- End function
	.section	.text.halide_default_device_and_host_free,"ax",@progbits
	.weak	halide_default_device_and_host_free # -- Begin function halide_default_device_and_host_free
	.p2align	4, 0x90
	.type	halide_default_device_and_host_free,@function
halide_default_device_and_host_free:    # @halide_default_device_and_host_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r15
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB135_1
# %bb.2:                                # %if.end.i
	movq	(%r15), %rax
	movq	8(%r15), %rcx
	testq	%rax, %rax
	je	.LBB135_5
# %bb.3:                                # %if.end.i
	testq	%rcx, %rcx
	jne	.LBB135_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB135_14
	jmp	.LBB135_11
.LBB135_1:                              # %if.then.i
	leaq	.L.str.30.104(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB135_14
	jmp	.LBB135_11
.LBB135_5:                              # %if.end10.i
	testq	%rcx, %rcx
	je	.LBB135_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB135_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB135_14
	jmp	.LBB135_11
.LBB135_8:                              # %if.end16.i
	movl	24(%r15), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB135_11
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB135_14
.LBB135_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_device_free@PLT
	movl	%eax, %ebx
	movq	16(%r15), %rsi
	testq	%rsi, %rsi
	je	.LBB135_13
# %bb.12:                               # %if.then2
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 16(%r15)
.LBB135_13:                             # %if.end5
	andb	$-4, 24(%r15)
.LBB135_14:                             # %cleanup
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end135:
	.size	halide_default_device_and_host_free, .Lfunc_end135-halide_default_device_and_host_free
                                        # -- End function
	.section	.text.halide_device_wrap_native,"ax",@progbits
	.weak	halide_device_wrap_native       # -- Begin function halide_device_wrap_native
	.p2align	4, 0x90
	.type	halide_device_wrap_native,@function
halide_device_wrap_native:              # @halide_device_wrap_native
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r15
	movq	%rdx, %r14
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%rsi, %rsi
	je	.LBB136_1
# %bb.2:                                # %if.end.i
	movq	(%rbx), %rcx
	movq	8(%rbx), %rax
	testq	%rcx, %rcx
	je	.LBB136_5
# %bb.3:                                # %if.end.i
	testq	%rax, %rax
	jne	.LBB136_5
# %bb.4:                                # %if.then8.i
	movq	%r12, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB136_16
	jmp	.LBB136_11
.LBB136_1:                              # %if.then.i
	leaq	.L.str.31.105(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB136_16
	jmp	.LBB136_11
.LBB136_5:                              # %if.end10.i
	testq	%rax, %rax
	je	.LBB136_8
# %bb.6:                                # %if.end10.i
	testq	%rcx, %rcx
	jne	.LBB136_8
# %bb.7:                                # %if.then14.i
	movq	%r12, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB136_16
	jmp	.LBB136_11
.LBB136_8:                              # %if.end16.i
	movl	24(%rbx), %ecx
	andl	$3, %ecx
	cmpl	$3, %ecx
	jne	.LBB136_12
# %bb.9:                                # %if.then24.i
	movq	%r12, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB136_16
.LBB136_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%rbx), %rax
.LBB136_12:                             # %if.end
	testq	%rax, %rax
	je	.LBB136_15
# %bb.13:                               # %if.end
	cmpq	%r15, %rax
	je	.LBB136_15
# %bb.14:                               # %if.then4
	leaq	.L.str.32.106(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_error@PLT
	movl	$-42, %eax
	jmp	.LBB136_16
.LBB136_15:                             # %if.end5
	movq	120(%r15), %rax
	callq	*(%rax)
	movq	%r15, 8(%rbx)
	movq	120(%r15), %rax
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	*112(%rax)
	movl	%eax, %ebx
	movq	120(%r15), %rax
	callq	*8(%rax)
	xorl	%eax, %eax
	testl	%ebx, %ebx
	sete	%al
	shll	$4, %eax
	addl	$-16, %eax
.LBB136_16:                             # %cleanup12
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end136:
	.size	halide_device_wrap_native, .Lfunc_end136-halide_device_wrap_native
                                        # -- End function
	.section	.text.halide_device_detach_native,"ax",@progbits
	.weak	halide_device_detach_native     # -- Begin function halide_device_detach_native
	.p2align	4, 0x90
	.type	halide_device_detach_native,@function
halide_device_detach_native:            # @halide_device_detach_native
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%rsi, %rsi
	je	.LBB137_1
# %bb.2:                                # %if.end.i
	movq	(%r12), %rax
	movq	8(%r12), %rbx
	testq	%rax, %rax
	je	.LBB137_5
# %bb.3:                                # %if.end.i
	testq	%rbx, %rbx
	jne	.LBB137_5
# %bb.4:                                # %if.then8.i
	movq	%r14, %rdi
	callq	halide_error_no_device_interface@PLT
	testl	%eax, %eax
	jne	.LBB137_17
	jmp	.LBB137_11
.LBB137_1:                              # %if.then.i
	leaq	.L.str.33.107(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_error_buffer_is_null@PLT
	testl	%eax, %eax
	jne	.LBB137_17
	jmp	.LBB137_11
.LBB137_5:                              # %if.end10.i
	testq	%rbx, %rbx
	je	.LBB137_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB137_8
# %bb.7:                                # %if.then14.i
	movq	%r14, %rdi
	callq	halide_error_device_interface_no_device@PLT
	testl	%eax, %eax
	jne	.LBB137_17
	jmp	.LBB137_11
.LBB137_8:                              # %if.end16.i
	movl	24(%r12), %eax
	andl	$3, %eax
	cmpl	$3, %eax
	jne	.LBB137_12
# %bb.9:                                # %if.then24.i
	movq	%r14, %rdi
	callq	halide_error_host_and_device_dirty@PLT
	testl	%eax, %eax
	jne	.LBB137_17
.LBB137_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	8(%r12), %rbx
.LBB137_12:                             # %if.end
	testq	%rbx, %rbx
	je	.LBB137_13
# %bb.14:                               # %if.then3
	movq	120(%rbx), %rax
	callq	*(%rax)
	movq	120(%rbx), %rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*120(%rax)
	movl	%eax, %r15d
	movq	120(%rbx), %rax
	callq	*8(%rax)
	cmpq	$0, (%r12)
	je	.LBB137_16
# %bb.15:                               # %if.then8
	leaq	.L.str.34.108(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB137_16:                             # %do.end
	testl	%r15d, %r15d
	movl	$-33, %eax
	cmovel	%r15d, %eax
	jmp	.LBB137_17
.LBB137_13:
	xorl	%eax, %eax
.LBB137_17:                             # %cleanup
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end137:
	.size	halide_device_detach_native, .Lfunc_end137-halide_device_detach_native
                                        # -- End function
	.section	.text.halide_default_device_wrap_native,"ax",@progbits
	.weak	halide_default_device_wrap_native # -- Begin function halide_default_device_wrap_native
	.p2align	4, 0x90
	.type	halide_default_device_wrap_native,@function
halide_default_device_wrap_native:      # @halide_default_device_wrap_native
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movl	$-32, %eax
	cmpq	$0, (%rsi)
	je	.LBB138_1
# %bb.2:                                # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.LBB138_1:                              # %if.end
	movq	%rdx, %r14
	movq	%rsi, %rbx
	movq	8(%rsi), %rax
	movq	120(%rax), %rax
	callq	*(%rax)
	movq	%r14, (%rbx)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end138:
	.size	halide_default_device_wrap_native, .Lfunc_end138-halide_default_device_wrap_native
                                        # -- End function
	.section	.text.halide_default_device_detach_native,"ax",@progbits
	.weak	halide_default_device_detach_native # -- Begin function halide_default_device_detach_native
	.p2align	4, 0x90
	.type	halide_default_device_detach_native,@function
halide_default_device_detach_native:    # @halide_default_device_detach_native
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %r14
	testq	%rsi, %rsi
	je	.LBB139_1
# %bb.2:                                # %if.end.i
	movq	(%r14), %rax
	movq	8(%r14), %rcx
	testq	%rax, %rax
	je	.LBB139_5
# %bb.3:                                # %if.end.i
	testq	%rcx, %rcx
	jne	.LBB139_5
# %bb.4:                                # %if.then8.i
	callq	halide_error_no_device_interface@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB139_14
	jmp	.LBB139_11
.LBB139_1:                              # %if.then.i
	leaq	.L.str.35(%rip), %rsi
	callq	halide_error_buffer_is_null@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB139_14
	jmp	.LBB139_11
.LBB139_5:                              # %if.end10.i
	testq	%rcx, %rcx
	je	.LBB139_8
# %bb.6:                                # %if.end10.i
	testq	%rax, %rax
	jne	.LBB139_8
# %bb.7:                                # %if.then14.i
	callq	halide_error_device_interface_no_device@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB139_14
	jmp	.LBB139_11
.LBB139_8:                              # %if.end16.i
	movl	24(%r14), %ecx
	andl	$3, %ecx
	cmpl	$3, %ecx
	jne	.LBB139_12
# %bb.9:                                # %if.then24.i
	callq	halide_error_host_and_device_dirty@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB139_14
.LBB139_11:                             # %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	movq	(%r14), %rax
.LBB139_12:                             # %if.end
	xorl	%ebx, %ebx
	testq	%rax, %rax
	je	.LBB139_14
# %bb.13:                               # %if.end3
	movq	8(%r14), %rax
	movq	120(%rax), %rax
	callq	*8(%rax)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r14)
.LBB139_14:                             # %cleanup
	movl	%ebx, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end139:
	.size	halide_default_device_detach_native, .Lfunc_end139-halide_default_device_detach_native
                                        # -- End function
	.section	.text.halide_device_and_host_free_as_destructor,"ax",@progbits
	.weak	halide_device_and_host_free_as_destructor # -- Begin function halide_device_and_host_free_as_destructor
	.p2align	4, 0x90
	.type	halide_device_and_host_free_as_destructor,@function
halide_device_and_host_free_as_destructor: # @halide_device_and_host_free_as_destructor
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_and_host_free@PLT # TAILCALL
.Lfunc_end140:
	.size	halide_device_and_host_free_as_destructor, .Lfunc_end140-halide_device_and_host_free_as_destructor
                                        # -- End function
	.section	.text.halide_device_host_nop_free,"ax",@progbits
	.weak	halide_device_host_nop_free     # -- Begin function halide_device_host_nop_free
	.p2align	4, 0x90
	.type	halide_device_host_nop_free,@function
halide_device_host_nop_free:            # @halide_device_host_nop_free
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end141:
	.size	halide_device_host_nop_free, .Lfunc_end141-halide_device_host_nop_free
                                        # -- End function
	.section	.text.halide_default_buffer_copy,"ax",@progbits
	.weak	halide_default_buffer_copy      # -- Begin function halide_default_buffer_copy
	.p2align	4, 0x90
	.type	halide_default_buffer_copy,@function
halide_default_buffer_copy:             # @halide_default_buffer_copy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$-39, %eax
	popq	%rbp
	retq
.Lfunc_end142:
	.size	halide_default_buffer_copy, .Lfunc_end142-halide_default_buffer_copy
                                        # -- End function
	.section	.text.halide_buffer_copy_already_locked,"ax",@progbits
	.weak	halide_buffer_copy_already_locked # -- Begin function halide_buffer_copy_already_locked
	.p2align	4, 0x90
	.type	halide_buffer_copy_already_locked,@function
halide_buffer_copy_already_locked:      # @halide_buffer_copy_already_locked
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$456, %rsp                      # imm = 0x1C8
	movq	%rdx, %r13
	movq	%rsi, -56(%rbp)                 # 8-byte Spill
	testq	%rdx, %rdx
	je	.LBB143_5
# %bb.1:                                # %land.lhs.true
	movq	8(%rcx), %rax
	testq	%rax, %rax
	je	.LBB143_4
# %bb.2:                                # %land.lhs.true
	cmpq	%r13, %rax
	je	.LBB143_4
# %bb.3:                                # %if.then
	leaq	.L.str.41(%rip), %rsi
	callq	halide_error@PLT
	movl	$-42, %eax
	jmp	.LBB143_39
.LBB143_4:                              # %land.lhs.true5
	cmpq	$0, (%rcx)
	je	.LBB143_13
.LBB143_5:                              # %if.end13
	movq	-56(%rbp), %rax                 # 8-byte Reload
	cmpq	$0, (%rax)
	movq	16(%rax), %rax
	je	.LBB143_9
# %bb.6:                                # %land.rhs
	testq	%rax, %rax
	je	.LBB143_12
# %bb.7:                                # %land.end.thread264
	movq	-56(%rbp), %rax                 # 8-byte Reload
	movq	24(%rax), %rax
	movl	%eax, %r12d
	andb	$1, %r12b
	testb	$2, %al
	jne	.LBB143_11
.LBB143_8:
	xorl	%esi, %esi
	xorl	%ebx, %ebx
	movq	16(%rcx), %r14
	testq	%r13, %r13
	setne	%r8b
	je	.LBB143_16
	jmp	.LBB143_17
.LBB143_9:                              # %land.end
	testq	%rax, %rax
	je	.LBB143_14
# %bb.10:                               # %land.end.land.rhs26_crit_edge
	movq	-56(%rbp), %rax                 # 8-byte Reload
	movq	24(%rax), %rax
	movb	$1, %r12b
	testb	$2, %al
	je	.LBB143_8
.LBB143_11:                             # %lor.rhs28
	movq	-56(%rbp), %rax                 # 8-byte Reload
	cmpq	$0, 8(%rax)
	setne	%bl
	xorl	%esi, %esi
	movq	16(%rcx), %r14
	testq	%r13, %r13
	setne	%r8b
	je	.LBB143_16
	jmp	.LBB143_17
.LBB143_12:
	movb	$1, %sil
	xorl	%r12d, %r12d
	jmp	.LBB143_15
.LBB143_13:                             # %if.then7
	movq	%rdi, %rbx
	movq	%rcx, %rsi
	movq	%r13, %rdx
	movq	%rcx, %r14
	callq	halide_device_malloc@PLT
	movq	%rbx, %rdi
	movq	%r14, %rcx
	testl	%eax, %eax
	jne	.LBB143_39
	jmp	.LBB143_5
.LBB143_14:
	movb	$1, %sil
	movb	$1, %r12b
.LBB143_15:
	movb	$1, %bl
	movq	16(%rcx), %r14
	testq	%r13, %r13
	setne	%r8b
	jne	.LBB143_17
.LBB143_16:                             # %land.end32
	movl	$-34, %eax
	testq	%r14, %r14
	je	.LBB143_39
.LBB143_17:                             # %if.end41
	testq	%r13, %r13
	sete	%al
	orb	%r12b, %al
	jne	.LBB143_19
# %bb.18:                               # %if.end49
	movq	120(%r13), %rax
	movq	%rdi, %r15
	movl	%esi, -60(%rbp)                 # 4-byte Spill
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movq	%r13, %rdx
	movq	%rcx, -72(%rbp)                 # 8-byte Spill
	movb	%r8b, -41(%rbp)                 # 1-byte Spill
	callq	*80(%rax)
	movb	-41(%rbp), %r8b                 # 1-byte Reload
	movl	-60(%rbp), %esi                 # 4-byte Reload
	movq	%r15, %rdi
	movq	-72(%rbp), %rcx                 # 8-byte Reload
	cmpl	$-42, %eax
	jne	.LBB143_33
.LBB143_19:                             # %if.then51
	testq	%r14, %r14
	sete	%dl
	movl	$-42, %eax
	testb	%dl, %sil
	jne	.LBB143_39
# %bb.20:                               # %if.end58
	orb	%r8b, %bl
	je	.LBB143_26
# %bb.21:                               # %if.else
	orb	%r12b, %r8b
	je	.LBB143_27
# %bb.22:                               # %if.else81
	testq	%r14, %r14
	sete	%dl
	orb	%dl, %r12b
	je	.LBB143_30
# %bb.23:                               # %if.else98
	testq	%r13, %r13
	je	.LBB143_39
# %bb.24:                               # %if.then100
	movq	%rdi, %rbx
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movq	%rcx, %r14
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t@PLT
	testl	%eax, %eax
	jne	.LBB143_39
# %bb.25:                               # %if.then105
	movq	%r14, %rcx
	movq	120(%r13), %rax
	movq	%rbx, %rdi
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movq	%r13, %rdx
	callq	*80(%rax)
	jmp	.LBB143_32
.LBB143_26:                             # %if.end117.thread258
	leaq	-488(%rbp), %r14
	movq	%rdi, %r15
	movq	%r14, %rdi
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movl	$1, %edx
	movq	%rcx, %rbx
	movl	$1, %r8d
	callq	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv@PLT
	movq	%rbx, %rcx
	jmp	.LBB143_34
.LBB143_27:                             # %if.then66
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movq	8(%rsi), %rax
	movq	120(%rax), %rax
	movq	%rdi, %rbx
	xorl	%edx, %edx
	movq	%rcx, %r14
	callq	*80(%rax)
	movq	%r14, %rcx
	cmpl	$-42, %eax
	jne	.LBB143_33
# %bb.28:                               # %if.then74
	movq	%rbx, %rdi
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t@PLT
	testl	%eax, %eax
	jne	.LBB143_39
# %bb.29:                               # %if.then77
	movq	%r14, %rcx
	movq	%rbx, %rdi
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	xorl	%edx, %edx
	callq	halide_buffer_copy_already_locked@PLT
	jmp	.LBB143_32
.LBB143_30:                             # %if.then85
	movq	-56(%rbp), %rsi                 # 8-byte Reload
	movq	8(%rsi), %rax
	movq	120(%rax), %rax
	movq	%rdi, %rbx
	xorl	%edx, %edx
	movq	%rcx, %r14
	callq	*80(%rax)
	testl	%eax, %eax
	jne	.LBB143_39
# %bb.31:                               # %if.then95
	movq	%r14, %rsi
	orb	$1, 24(%r14)
	movq	%rbx, %rdi
	movq	%r13, %rdx
	callq	copy_to_device_already_locked@PLT
.LBB143_32:                             # %if.end117
	movq	%r14, %rcx
.LBB143_33:                             # %if.end117
	testl	%eax, %eax
	jne	.LBB143_39
.LBB143_34:                             # %land.lhs.true126
	xorl	%eax, %eax
	cmpq	-56(%rbp), %rcx                 # 8-byte Folded Reload
	je	.LBB143_39
# %bb.35:                               # %if.then128
	movq	24(%rcx), %rdx
	andq	$-4, %rdx
	testq	%r13, %r13
	je	.LBB143_37
# %bb.36:                               # %if.then130
	orq	$2, %rdx
	jmp	.LBB143_38
.LBB143_37:                             # %if.else133
	orq	$1, %rdx
.LBB143_38:                             # %cleanup143
	movq	%rdx, 24(%rcx)
.LBB143_39:                             # %cleanup143
	addq	$456, %rsp                      # imm = 0x1C8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end143:
	.size	halide_buffer_copy_already_locked, .Lfunc_end143-halide_buffer_copy_already_locked
                                        # -- End function
	.section	.text.halide_buffer_copy,"ax",@progbits
	.weak	halide_buffer_copy              # -- Begin function halide_buffer_copy
	.p2align	4, 0x90
	.type	halide_buffer_copy,@function
halide_buffer_copy:                     # @halide_buffer_copy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r15
	movq	%rdx, %rbx
	movq	%rsi, %r14
	movq	%rdi, %r12
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	testq	%rbx, %rbx
	je	.LBB144_2
# %bb.1:                                # %if.then
	movq	120(%rbx), %rax
	callq	*(%rax)
.LBB144_2:                              # %if.end
	movq	8(%r14), %rax
	testq	%rax, %rax
	je	.LBB144_4
# %bb.3:                                # %if.then12
	movq	120(%rax), %rax
	callq	*(%rax)
.LBB144_4:                              # %if.end16
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	movq	%r15, %rcx
	callq	halide_buffer_copy_already_locked@PLT
	movl	%eax, %r15d
	testq	%rbx, %rbx
	je	.LBB144_6
# %bb.5:                                # %if.then18
	movq	120(%rbx), %rax
	callq	*8(%rax)
.LBB144_6:                              # %if.end20
	movq	8(%r14), %rax
	testq	%rax, %rax
	je	.LBB144_8
# %bb.7:                                # %if.then23
	movq	120(%rax), %rax
	callq	*8(%rax)
.LBB144_8:                              # %if.end27
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r15d, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end144:
	.size	halide_buffer_copy, .Lfunc_end144-halide_buffer_copy
                                        # -- End function
	.section	.text.halide_default_device_crop,"ax",@progbits
	.weak	halide_default_device_crop      # -- Begin function halide_default_device_crop
	.p2align	4, 0x90
	.type	halide_default_device_crop,@function
halide_default_device_crop:             # @halide_default_device_crop
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.58(%rip), %rsi
	callq	halide_error@PLT
	movl	$-40, %eax
	popq	%rbp
	retq
.Lfunc_end145:
	.size	halide_default_device_crop, .Lfunc_end145-halide_default_device_crop
                                        # -- End function
	.section	.text.halide_default_device_slice,"ax",@progbits
	.weak	halide_default_device_slice     # -- Begin function halide_default_device_slice
	.p2align	4, 0x90
	.type	halide_default_device_slice,@function
halide_default_device_slice:            # @halide_default_device_slice
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.59(%rip), %rsi
	callq	halide_error@PLT
	movl	$-40, %eax
	popq	%rbp
	retq
.Lfunc_end146:
	.size	halide_default_device_slice, .Lfunc_end146-halide_default_device_slice
                                        # -- End function
	.section	.text.halide_device_crop,"ax",@progbits
	.weak	halide_device_crop              # -- Begin function halide_device_crop
	.p2align	4, 0x90
	.type	halide_device_crop,@function
halide_device_crop:                     # @halide_device_crop
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	cmpq	$0, (%rbx)
	je	.LBB147_1
# %bb.2:                                # %if.end
	cmpq	$0, (%r15)
	je	.LBB147_5
# %bb.3:                                # %if.then3
	leaq	.L.str.60(%rip), %rsi
	jmp	.LBB147_4
.LBB147_1:
	xorl	%ebx, %ebx
	jmp	.LBB147_8
.LBB147_5:                              # %if.end4
	movl	36(%rbx), %eax
	cmpl	36(%r15), %eax
	jne	.LBB147_6
# %bb.7:                                # %if.end7
	movq	8(%rbx), %rax
	movq	120(%rax), %rax
	callq	*(%rax)
	movq	8(%rbx), %rax
	movq	120(%rax), %rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	*88(%rax)
	movl	%eax, %ebx
	jmp	.LBB147_8
.LBB147_6:                              # %if.then6
	leaq	.L.str.61(%rip), %rsi
.LBB147_4:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_error@PLT
	movl	$-41, %ebx
.LBB147_8:                              # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end147:
	.size	halide_device_crop, .Lfunc_end147-halide_device_crop
                                        # -- End function
	.section	.text.halide_device_slice,"ax",@progbits
	.weak	halide_device_slice             # -- Begin function halide_device_slice
	.p2align	4, 0x90
	.type	halide_device_slice,@function
halide_device_slice:                    # @halide_device_slice
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%r8, %r13
	movl	%ecx, %r15d
	movl	%edx, %r12d
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	cmpq	$0, (%rbx)
	je	.LBB148_1
# %bb.2:                                # %if.end
	cmpq	$0, (%r13)
	je	.LBB148_5
# %bb.3:                                # %if.then3
	leaq	.L.str.60(%rip), %rsi
	jmp	.LBB148_4
.LBB148_1:
	xorl	%ebx, %ebx
	jmp	.LBB148_8
.LBB148_5:                              # %if.end4
	movl	36(%r13), %eax
	incl	%eax
	cmpl	%eax, 36(%rbx)
	jne	.LBB148_6
# %bb.7:                                # %if.end7
	movq	8(%rbx), %rax
	movq	120(%rax), %rax
	callq	*(%rax)
	movq	8(%rbx), %rax
	movq	120(%rax), %rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movl	%r12d, %edx
	movl	%r15d, %ecx
	movq	%r13, %r8
	callq	*96(%rax)
	movl	%eax, %ebx
	jmp	.LBB148_8
.LBB148_6:                              # %if.then6
	leaq	.L.str.64(%rip), %rsi
.LBB148_4:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_error@PLT
	movl	$-41, %ebx
.LBB148_8:                              # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end148:
	.size	halide_device_slice, .Lfunc_end148-halide_device_slice
                                        # -- End function
	.section	.text.halide_default_device_release_crop,"ax",@progbits
	.weak	halide_default_device_release_crop # -- Begin function halide_default_device_release_crop
	.p2align	4, 0x90
	.type	halide_default_device_release_crop,@function
halide_default_device_release_crop:     # @halide_default_device_release_crop
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	$0, (%rsi)
	je	.LBB149_1
# %bb.2:                                # %if.end
	leaq	.L.str.58(%rip), %rsi
	callq	halide_error@PLT
	movl	$-40, %eax
	popq	%rbp
	retq
.LBB149_1:
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end149:
	.size	halide_default_device_release_crop, .Lfunc_end149-halide_default_device_release_crop
                                        # -- End function
	.section	.text.halide_device_release_crop,"ax",@progbits
	.weak	halide_device_release_crop      # -- Begin function halide_device_release_crop
	.p2align	4, 0x90
	.type	halide_device_release_crop,@function
halide_device_release_crop:             # @halide_device_release_crop
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	cmpq	$0, (%rsi)
	je	.LBB150_2
# %bb.1:                                # %if.then
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %r15
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	movq	8(%rbx), %r12
	movq	120(%r12), %rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	*104(%rax)
	movl	%eax, %r14d
	movq	$0, (%rbx)
	movq	120(%r12), %rax
	callq	*8(%rax)
	movq	$0, 8(%rbx)
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	jmp	.LBB150_3
.LBB150_2:                              # %return
	xorl	%eax, %eax
.LBB150_3:                              # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end150:
	.size	halide_device_release_crop, .Lfunc_end150-halide_device_release_crop
                                        # -- End function
	.section	.text.halide_float16_bits_to_float,"ax",@progbits
	.weak	halide_float16_bits_to_float    # -- Begin function halide_float16_bits_to_float
	.p2align	4, 0x90
	.type	halide_float16_bits_to_float,@function
halide_float16_bits_to_float:           # @halide_float16_bits_to_float
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%edi, %ecx
	shrl	$10, %ecx
	andl	$31, %ecx
	movl	%edi, %eax
	andl	$1023, %eax                     # imm = 0x3FF
	je	.LBB151_3
# %bb.1:                                # %entry
	testl	%ecx, %ecx
	jne	.LBB151_3
# %bb.2:                                # %if.then
	lzcntl	%eax, %ecx
	movl	%ecx, %edx
	xorb	$31, %dl
	btrl	%edx, %eax
	movb	$23, %sil
	subb	%dl, %sil
	shlxl	%esi, %eax, %edx
	shll	$23, %ecx
	movl	$1124073472, %eax               # imm = 0x43000000
	subl	%ecx, %eax
	jmp	.LBB151_7
.LBB151_3:                              # %if.else
	shll	$13, %eax
	testl	%ecx, %ecx
	je	.LBB151_4
# %bb.5:                                # %if.else18
	movl	$2139095040, %edx               # imm = 0x7F800000
	cmpl	$31, %ecx
	je	.LBB151_7
# %bb.6:                                # %if.else21
	shll	$23, %ecx
	addl	$939524096, %ecx                # imm = 0x38000000
	movl	%ecx, %edx
	jmp	.LBB151_7
.LBB151_4:
	xorl	%edx, %edx
.LBB151_7:                              # %if.end28
	orl	%edx, %eax
	movswl	%di, %ecx
	andl	$-2147483648, %ecx              # imm = 0x80000000
	orl	%eax, %ecx
	vmovd	%ecx, %xmm0
	popq	%rbp
	retq
.Lfunc_end151:
	.size	halide_float16_bits_to_float, .Lfunc_end151-halide_float16_bits_to_float
                                        # -- End function
	.section	.text.halide_float16_bits_to_double,"ax",@progbits
	.weak	halide_float16_bits_to_double   # -- Begin function halide_float16_bits_to_double
	.p2align	4, 0x90
	.type	halide_float16_bits_to_double,@function
halide_float16_bits_to_double:          # @halide_float16_bits_to_double
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_float16_bits_to_float@PLT
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	popq	%rbp
	retq
.Lfunc_end152:
	.size	halide_float16_bits_to_double, .Lfunc_end152-halide_float16_bits_to_double
                                        # -- End function
	.section	.text.halide_error_bounds_inference_call_failed,"ax",@progbits
	.weak	halide_error_bounds_inference_call_failed # -- Begin function halide_error_bounds_inference_call_failed
	.p2align	4, 0x90
	.type	halide_error_bounds_inference_call_failed,@function
halide_error_bounds_inference_call_failed: # @halide_error_bounds_inference_call_failed
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r12
	movq	%rdi, %r15
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB153_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r13
	movb	$0, 1023(%rbx)
	leaq	.L.str.111(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	jmp	.LBB153_3
.LBB153_1:                              # %entry.split
	leaq	.L.str.111(%rip), %rdx
	xorl	%r13d, %r13d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB153_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r13, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.112(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB153_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r15, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB153_6
.LBB153_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r15, %rdi
.LBB153_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end153:
	.size	halide_error_bounds_inference_call_failed, .Lfunc_end153-halide_error_bounds_inference_call_failed
                                        # -- End function
	.section	.text.halide_error_extern_stage_failed,"ax",@progbits
	.weak	halide_error_extern_stage_failed # -- Begin function halide_error_extern_stage_failed
	.p2align	4, 0x90
	.type	halide_error_extern_stage_failed,@function
halide_error_extern_stage_failed:       # @halide_error_extern_stage_failed
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r12
	movq	%rdi, %r15
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB154_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r13
	movb	$0, 1023(%rbx)
	leaq	.L.str.2.113(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	jmp	.LBB154_3
.LBB154_1:                              # %entry.split
	leaq	.L.str.2.113(%rip), %rdx
	xorl	%r13d, %r13d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB154_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r13, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.112(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB154_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r15, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB154_6
.LBB154_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r15, %rdi
.LBB154_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end154:
	.size	halide_error_extern_stage_failed, .Lfunc_end154-halide_error_extern_stage_failed
                                        # -- End function
	.section	.text.halide_error_explicit_bounds_too_small,"ax",@progbits
	.weak	halide_error_explicit_bounds_too_small # -- Begin function halide_error_explicit_bounds_too_small
	.p2align	4, 0x90
	.type	halide_error_explicit_bounds_too_small,@function
halide_error_explicit_bounds_too_small: # @halide_error_explicit_bounds_too_small
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -48(%rbp)                 # 4-byte Spill
	movl	%r8d, -44(%rbp)                 # 4-byte Spill
	movl	%ecx, %r14d
	movq	%rdx, %r13
	movq	%rsi, %r12
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB155_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
	leaq	.L.str.3.114(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB155_3
.LBB155_1:                              # %entry.split
	leaq	.L.str.3.114(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB155_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.4.115(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.5.116(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.117(%rip), %r14
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.7.118(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movslq	16(%rbp), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.119(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB155_4
# %bb.5:                                # %if.else.i
	subq	%r15, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r15, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r15, %rsi
	jmp	.LBB155_6
.LBB155_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
.LBB155_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	callq	free@PLT
	movl	$-2, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end155:
	.size	halide_error_explicit_bounds_too_small, .Lfunc_end155-halide_error_explicit_bounds_too_small
                                        # -- End function
	.section	.text.halide_error_bad_type,"ax",@progbits
	.weak	halide_error_bad_type           # -- Begin function halide_error_bad_type
	.p2align	4, 0x90
	.type	halide_error_bad_type,@function
halide_error_bad_type:                  # @halide_error_bad_type
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$32, %rsp
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	%edx, -56(%rbp)
	movl	%ecx, -52(%rbp)
	movl	$0, -48(%rbp)
	movl	$0, -40(%rbp)
	leaq	-48(%rbp), %rdi
	leaq	-52(%rbp), %rsi
	movl	$4, %edx
	callq	memcpy@PLT
	leaq	-40(%rbp), %rdi
	leaq	-56(%rbp), %rsi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB156_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB156_3
.LBB156_1:                              # %entry.split
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB156_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.9.120(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	-48(%rbp), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_type_to_string@PLT
	leaq	.L.str.10.121(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	-40(%rbp), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_type_to_string@PLT
	testq	%r15, %r15
	je	.LBB156_4
# %bb.5:                                # %if.else.i
	subq	%r15, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r15, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB156_6
.LBB156_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB156_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	callq	free@PLT
	movl	$-3, %eax
	addq	$32, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end156:
	.size	halide_error_bad_type, .Lfunc_end156-halide_error_bad_type
                                        # -- End function
	.section	.text.halide_error_bad_dimensions,"ax",@progbits
	.weak	halide_error_bad_dimensions     # -- Begin function halide_error_bad_dimensions
	.p2align	4, 0x90
	.type	halide_error_bad_dimensions,@function
halide_error_bad_dimensions:            # @halide_error_bad_dimensions
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, %r13d
	movl	%edx, %r15d
	movq	%rsi, %r14
	movq	%rdi, -48(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB157_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB157_3
.LBB157_1:                              # %entry.split
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB157_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.11.122(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.12.123(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.13.124(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB157_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB157_6
.LBB157_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-48(%rbp), %rdi                 # 8-byte Reload
.LBB157_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-43, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end157:
	.size	halide_error_bad_dimensions, .Lfunc_end157-halide_error_bad_dimensions
                                        # -- End function
	.section	.text.halide_error_access_out_of_bounds,"ax",@progbits
	.weak	halide_error_access_out_of_bounds # -- Begin function halide_error_access_out_of_bounds
	.p2align	4, 0x90
	.type	halide_error_access_out_of_bounds,@function
halide_error_access_out_of_bounds:      # @halide_error_access_out_of_bounds
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rsi, %r14
	cmpl	%r9d, %ecx
	jge	.LBB158_7
# %bb.1:                                # %if.then
	movl	%r9d, %r13d
	movl	%ecx, %r15d
	movl	%edx, -44(%rbp)                 # 4-byte Spill
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB158_2
# %bb.3:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB158_4
.LBB158_7:                              # %if.else
	movl	%r8d, %ebx
	movl	16(%rbp), %r13d
	cmpl	%r13d, %r8d
	jle	.LBB158_14
# %bb.8:                                # %if.then8
	movl	%edx, -44(%rbp)                 # 4-byte Spill
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB158_9
# %bb.10:                               # %if.then6.i59
	leaq	1023(%r12), %r15
	movb	$0, 1023(%r12)
	movq	%r12, %rdi
	movq	%r15, %rsi
	jmp	.LBB158_11
.LBB158_2:                              # %if.then.split
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB158_4:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	leaq	.L.str.14.125(%rip), %rdx
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.15.126(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.16.127(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB158_6
.LBB158_12:                             # %if.else.i100
	subq	%r12, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_error@PLT
	jmp	.LBB158_13
.LBB158_9:                              # %if.then8.split
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB158_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.14.125(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	%ebx, %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.17.128(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.16.127(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%r15, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	jne	.LBB158_12
.LBB158_6:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
	callq	halide_error@PLT
	xorl	%r12d, %r12d
.LBB158_13:                             # %if.end17.sink.split
	movq	%r12, %rdi
	callq	free@PLT
.LBB158_14:                             # %if.end17
	movl	$-4, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end158:
	.size	halide_error_access_out_of_bounds, .Lfunc_end158-halide_error_access_out_of_bounds
                                        # -- End function
	.section	.text.halide_error_buffer_allocation_too_large,"ax",@progbits
	.weak	halide_error_buffer_allocation_too_large # -- Begin function halide_error_buffer_allocation_too_large
	.p2align	4, 0x90
	.type	halide_error_buffer_allocation_too_large,@function
halide_error_buffer_allocation_too_large: # @halide_error_buffer_allocation_too_large
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB159_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.18.129(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB159_3
.LBB159_1:                              # %entry.split
	leaq	.L.str.18.129(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB159_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.20.131(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	testq	%r13, %r13
	je	.LBB159_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB159_6
.LBB159_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB159_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-5, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end159:
	.size	halide_error_buffer_allocation_too_large, .Lfunc_end159-halide_error_buffer_allocation_too_large
                                        # -- End function
	.section	.text.halide_error_buffer_extents_negative,"ax",@progbits
	.weak	halide_error_buffer_extents_negative # -- Begin function halide_error_buffer_extents_negative
	.p2align	4, 0x90
	.type	halide_error_buffer_extents_negative,@function
halide_error_buffer_extents_negative:   # @halide_error_buffer_extents_negative
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)                 # 4-byte Spill
	movl	%edx, %r13d
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB160_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.21.132(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB160_3
.LBB160_1:                              # %entry.split
	leaq	.L.str.21.132(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB160_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.22.133(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.23.134(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.119(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB160_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB160_6
.LBB160_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB160_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-28, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end160:
	.size	halide_error_buffer_extents_negative, .Lfunc_end160-halide_error_buffer_extents_negative
                                        # -- End function
	.section	.text.halide_error_buffer_extents_too_large,"ax",@progbits
	.weak	halide_error_buffer_extents_too_large # -- Begin function halide_error_buffer_extents_too_large
	.p2align	4, 0x90
	.type	halide_error_buffer_extents_too_large,@function
halide_error_buffer_extents_too_large:  # @halide_error_buffer_extents_too_large
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB161_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.24.135(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB161_3
.LBB161_1:                              # %entry.split
	leaq	.L.str.24.135(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB161_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.20.131(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r13, %r13
	je	.LBB161_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB161_6
.LBB161_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB161_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-6, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end161:
	.size	halide_error_buffer_extents_too_large, .Lfunc_end161-halide_error_buffer_extents_too_large
                                        # -- End function
	.section	.text.halide_error_constraints_make_required_region_smaller,"ax",@progbits
	.weak	halide_error_constraints_make_required_region_smaller # -- Begin function halide_error_constraints_make_required_region_smaller
	.p2align	4, 0x90
	.type	halide_error_constraints_make_required_region_smaller,@function
halide_error_constraints_make_required_region_smaller: # @halide_error_constraints_make_required_region_smaller
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
                                        # kill: def $r8d killed $r8d def $r8
                                        # kill: def $ecx killed $ecx def $rcx
	movl	%edx, %r14d
	movq	%rsi, %r12
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	16(%rbp), %eax
	leal	-1(%r13,%rax), %eax
	movl	%eax, -44(%rbp)                 # 4-byte Spill
	movq	%rcx, -64(%rbp)                 # 8-byte Spill
	leal	(%rcx,%r8), %eax
	decl	%eax
	movl	%eax, -48(%rbp)                 # 4-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB162_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
	leaq	.L.str.25.136(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB162_3
.LBB162_1:                              # %entry.split
	leaq	.L.str.25.136(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB162_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.26.137(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.27.138(%rip), %r14
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.28.139(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.117(%rip), %r12
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.140(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-64(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB162_4
# %bb.5:                                # %if.else.i
	subq	%r15, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r15, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r15, %rsi
	jmp	.LBB162_6
.LBB162_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
.LBB162_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	callq	free@PLT
	movl	$-7, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end162:
	.size	halide_error_constraints_make_required_region_smaller, .Lfunc_end162-halide_error_constraints_make_required_region_smaller
                                        # -- End function
	.section	.text.halide_error_constraint_violated,"ax",@progbits
	.weak	halide_error_constraint_violated # -- Begin function halide_error_constraint_violated
	.p2align	4, 0x90
	.type	halide_error_constraint_violated,@function
halide_error_constraint_violated:       # @halide_error_constraint_violated
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r8d, -44(%rbp)                 # 4-byte Spill
	movq	%rcx, %r13
	movl	%edx, %r15d
	movq	%rsi, %r14
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB163_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.31.142(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB163_3
.LBB163_1:                              # %entry.split
	leaq	.L.str.31.142(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB163_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.32.143(%rip), %r14
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.33.144(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.119(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB163_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB163_6
.LBB163_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
.LBB163_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-8, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end163:
	.size	halide_error_constraint_violated, .Lfunc_end163-halide_error_constraint_violated
                                        # -- End function
	.section	.text.halide_error_param_too_small_i64,"ax",@progbits
	.weak	halide_error_param_too_small_i64 # -- Begin function halide_error_param_too_small_i64
	.p2align	4, 0x90
	.type	halide_error_param_too_small_i64,@function
halide_error_param_too_small_i64:       # @halide_error_param_too_small_i64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB164_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB164_3
.LBB164_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB164_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.35.146(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r13, %r13
	je	.LBB164_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB164_6
.LBB164_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB164_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end164:
	.size	halide_error_param_too_small_i64, .Lfunc_end164-halide_error_param_too_small_i64
                                        # -- End function
	.section	.text.halide_error_param_too_small_u64,"ax",@progbits
	.weak	halide_error_param_too_small_u64 # -- Begin function halide_error_param_too_small_u64
	.p2align	4, 0x90
	.type	halide_error_param_too_small_u64,@function
halide_error_param_too_small_u64:       # @halide_error_param_too_small_u64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB165_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB165_3
.LBB165_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB165_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.35.146(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	testq	%r13, %r13
	je	.LBB165_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB165_6
.LBB165_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB165_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end165:
	.size	halide_error_param_too_small_u64, .Lfunc_end165-halide_error_param_too_small_u64
                                        # -- End function
	.section	.text.halide_error_param_too_small_f64,"ax",@progbits
	.weak	halide_error_param_too_small_f64 # -- Begin function halide_error_param_too_small_f64
	.p2align	4, 0x90
	.type	halide_error_param_too_small_f64,@function
halide_error_param_too_small_f64:       # @halide_error_param_too_small_f64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -48(%rbp)                # 8-byte Spill
	vmovsd	%xmm0, -40(%rbp)                # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB166_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB166_3
.LBB166_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB166_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	leaq	.L.str.35.146(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB166_4
# %bb.5:                                # %if.else.i
	subq	%r15, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r15, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB166_6
.LBB166_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB166_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	callq	free@PLT
	movl	$-9, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end166:
	.size	halide_error_param_too_small_f64, .Lfunc_end166-halide_error_param_too_small_f64
                                        # -- End function
	.section	.text.halide_error_param_too_large_i64,"ax",@progbits
	.weak	halide_error_param_too_large_i64 # -- Begin function halide_error_param_too_large_i64
	.p2align	4, 0x90
	.type	halide_error_param_too_large_i64,@function
halide_error_param_too_large_i64:       # @halide_error_param_too_large_i64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB167_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB167_3
.LBB167_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB167_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.36(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r13, %r13
	je	.LBB167_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB167_6
.LBB167_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB167_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end167:
	.size	halide_error_param_too_large_i64, .Lfunc_end167-halide_error_param_too_large_i64
                                        # -- End function
	.section	.text.halide_error_param_too_large_u64,"ax",@progbits
	.weak	halide_error_param_too_large_u64 # -- Begin function halide_error_param_too_large_u64
	.p2align	4, 0x90
	.type	halide_error_param_too_large_u64,@function
halide_error_param_too_large_u64:       # @halide_error_param_too_large_u64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB168_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB168_3
.LBB168_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB168_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.36(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	testq	%r13, %r13
	je	.LBB168_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB168_6
.LBB168_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB168_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end168:
	.size	halide_error_param_too_large_u64, .Lfunc_end168-halide_error_param_too_large_u64
                                        # -- End function
	.section	.text.halide_error_param_too_large_f64,"ax",@progbits
	.weak	halide_error_param_too_large_f64 # -- Begin function halide_error_param_too_large_f64
	.p2align	4, 0x90
	.type	halide_error_param_too_large_f64,@function
halide_error_param_too_large_f64:       # @halide_error_param_too_large_f64
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -48(%rbp)                # 8-byte Spill
	vmovsd	%xmm0, -40(%rbp)                # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB169_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
	leaq	.L.str.34.145(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	jmp	.LBB169_3
.LBB169_1:                              # %entry.split
	leaq	.L.str.34.145(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB169_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.19.130(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	leaq	.L.str.36(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	$1, %edx
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB169_4
# %bb.5:                                # %if.else.i
	subq	%r15, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r15, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB169_6
.LBB169_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB169_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	callq	free@PLT
	movl	$-10, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end169:
	.size	halide_error_param_too_large_f64, .Lfunc_end169-halide_error_param_too_large_f64
                                        # -- End function
	.section	.text.halide_error_out_of_memory,"ax",@progbits
	.weak	halide_error_out_of_memory      # -- Begin function halide_error_out_of_memory
	.p2align	4, 0x90
	.type	halide_error_out_of_memory,@function
halide_error_out_of_memory:             # @halide_error_out_of_memory
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.37(%rip), %rsi
	callq	halide_error@PLT
	movl	$-11, %eax
	popq	%rbp
	retq
.Lfunc_end170:
	.size	halide_error_out_of_memory, .Lfunc_end170-halide_error_out_of_memory
                                        # -- End function
	.section	.text.halide_error_buffer_argument_is_null,"ax",@progbits
	.weak	halide_error_buffer_argument_is_null # -- Begin function halide_error_buffer_argument_is_null
	.p2align	4, 0x90
	.type	halide_error_buffer_argument_is_null,@function
halide_error_buffer_argument_is_null:   # @halide_error_buffer_argument_is_null
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB171_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
	leaq	.L.str.38(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB171_3
.LBB171_1:                              # %entry.split
	leaq	.L.str.38(%rip), %rdx
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB171_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.39(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB171_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB171_6
.LBB171_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB171_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-12, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end171:
	.size	halide_error_buffer_argument_is_null, .Lfunc_end171-halide_error_buffer_argument_is_null
                                        # -- End function
	.section	.text.halide_error_debug_to_file_failed,"ax",@progbits
	.weak	halide_error_debug_to_file_failed # -- Begin function halide_error_debug_to_file_failed
	.p2align	4, 0x90
	.type	halide_error_debug_to_file_failed,@function
halide_error_debug_to_file_failed:      # @halide_error_debug_to_file_failed
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)                 # 4-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB172_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.40(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB172_3
.LBB172_1:                              # %entry.split
	leaq	.L.str.40(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB172_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.41.147(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.42(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	testq	%r13, %r13
	je	.LBB172_4
# %bb.5:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r13, %rsi
	jmp	.LBB172_6
.LBB172_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB172_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-13, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end172:
	.size	halide_error_debug_to_file_failed, .Lfunc_end172-halide_error_debug_to_file_failed
                                        # -- End function
	.section	.text.halide_error_unaligned_host_ptr,"ax",@progbits
	.weak	halide_error_unaligned_host_ptr # -- Begin function halide_error_unaligned_host_ptr
	.p2align	4, 0x90
	.type	halide_error_unaligned_host_ptr,@function
halide_error_unaligned_host_ptr:        # @halide_error_unaligned_host_ptr
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r15d
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB173_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.43(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB173_3
.LBB173_1:                              # %entry.split
	leaq	.L.str.43(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB173_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.44(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.45(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB173_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB173_6
.LBB173_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB173_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-24, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end173:
	.size	halide_error_unaligned_host_ptr, .Lfunc_end173-halide_error_unaligned_host_ptr
                                        # -- End function
	.section	.text.halide_error_device_dirty_with_no_device_support,"ax",@progbits
	.weak	halide_error_device_dirty_with_no_device_support # -- Begin function halide_error_device_dirty_with_no_device_support
	.p2align	4, 0x90
	.type	halide_error_device_dirty_with_no_device_support,@function
halide_error_device_dirty_with_no_device_support: # @halide_error_device_dirty_with_no_device_support
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB174_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
	leaq	.L.str.46(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB174_3
.LBB174_1:                              # %entry.split
	leaq	.L.str.46(%rip), %rdx
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB174_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.47(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.48(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB174_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB174_6
.LBB174_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB174_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-44, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end174:
	.size	halide_error_device_dirty_with_no_device_support, .Lfunc_end174-halide_error_device_dirty_with_no_device_support
                                        # -- End function
	.section	.text.halide_error_host_is_null,"ax",@progbits
	.weak	halide_error_host_is_null       # -- Begin function halide_error_host_is_null
	.p2align	4, 0x90
	.type	halide_error_host_is_null,@function
halide_error_host_is_null:              # @halide_error_host_is_null
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB175_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
	leaq	.L.str.43(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB175_3
.LBB175_1:                              # %entry.split
	leaq	.L.str.43(%rip), %rdx
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB175_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.49(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB175_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB175_6
.LBB175_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB175_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-34, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end175:
	.size	halide_error_host_is_null, .Lfunc_end175-halide_error_host_is_null
                                        # -- End function
	.section	.text.halide_error_bad_fold,"ax",@progbits
	.weak	halide_error_bad_fold           # -- Begin function halide_error_bad_fold
	.p2align	4, 0x90
	.type	halide_error_bad_fold,@function
halide_error_bad_fold:                  # @halide_error_bad_fold
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB176_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.50.148(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB176_3
.LBB176_1:                              # %entry.split
	leaq	.L.str.50.148(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB176_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.51(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.52(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB176_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB176_6
.LBB176_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB176_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-25, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end176:
	.size	halide_error_bad_fold, .Lfunc_end176-halide_error_bad_fold
                                        # -- End function
	.section	.text.halide_error_bad_extern_fold,"ax",@progbits
	.weak	halide_error_bad_extern_fold    # -- Begin function halide_error_bad_extern_fold
	.p2align	4, 0x90
	.type	halide_error_bad_extern_fold,@function
halide_error_bad_extern_fold:           # @halide_error_bad_extern_fold
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r12d
                                        # kill: def $r8d killed $r8d def $r8
	movl	%ecx, %r15d
	movl	16(%rbp), %eax
	cmpl	%r9d, %ecx
	movq	%rdi, -48(%rbp)                 # 8-byte Spill
	movq	%rsi, -56(%rbp)                 # 8-byte Spill
	jl	.LBB177_2
# %bb.1:                                # %lor.lhs.false
	leal	(%r8,%r15), %r14d
	addl	%r12d, %eax
	cmpl	%eax, %r14d
	jle	.LBB177_8
.LBB177_2:                              # %if.then
	movl	%edx, %ebx
	movq	%r8, -64(%rbp)                  # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB177_3
# %bb.4:                                # %if.then6.i
	leaq	1023(%r13), %r14
	movb	$0, 1023(%r13)
	leaq	.L.str.53(%rip), %rdx
	movq	%r13, %rdi
	movq	%r14, %rsi
	jmp	.LBB177_5
.LBB177_3:                              # %if.then.split
	leaq	.L.str.53(%rip), %rdx
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB177_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movslq	%ebx, %rdx
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.51(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.54(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.55(%rip), %rbx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movq	-64(%rbp), %rcx                 # 8-byte Reload
	addl	%r15d, %ecx
	decl	%ecx
	movslq	%ecx, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.56(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.57(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r12d, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movl	16(%rbp), %ecx
	addl	%r12d, %ecx
	decl	%ecx
	movslq	%ecx, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.58.149(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	testq	%r13, %r13
	je	.LBB177_6
# %bb.7:                                # %if.else.i
	subq	%r13, %rax
	incq	%rax
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	jmp	.LBB177_13
.LBB177_8:                              # %if.else
	movl	%edx, %r12d
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r13
	testq	%rax, %rax
	je	.LBB177_9
# %bb.10:                               # %if.then6.i107
	leaq	1023(%r13), %rbx
	movb	$0, 1023(%r13)
	leaq	.L.str.53(%rip), %rdx
	movq	%r13, %rdi
	movq	%rbx, %rsi
	jmp	.LBB177_11
.LBB177_9:                              # %if.else.split
	leaq	.L.str.53(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB177_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110
	callq	halide_string_to_string@PLT
	movslq	%r12d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.51(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.54(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.55(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	decl	%r14d
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.56(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.59.150(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.60.151(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	16(%rbp), %ecx
	movslq	%ecx, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.30.141(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r13, %r13
	je	.LBB177_6
# %bb.12:                               # %if.else.i167
	subq	%r13, %rax
	incq	%rax
	movq	-48(%rbp), %r12                 # 8-byte Reload
	movq	%r12, %rdi
	movq	%r13, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r12, %rdi
.LBB177_13:                             # %if.end
	movq	%r13, %rsi
	callq	halide_error@PLT
	jmp	.LBB177_14
.LBB177_6:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-48(%rbp), %rdi                 # 8-byte Reload
	callq	halide_error@PLT
	xorl	%r13d, %r13d
.LBB177_14:                             # %if.end
	movq	%r13, %rdi
	callq	free@PLT
	movl	$-35, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end177:
	.size	halide_error_bad_extern_fold, .Lfunc_end177-halide_error_bad_extern_fold
                                        # -- End function
	.section	.text.halide_error_fold_factor_too_small,"ax",@progbits
	.weak	halide_error_fold_factor_too_small # -- Begin function halide_error_fold_factor_too_small
	.p2align	4, 0x90
	.type	halide_error_fold_factor_too_small,@function
halide_error_fold_factor_too_small:     # @halide_error_fold_factor_too_small
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)                 # 4-byte Spill
	movq	%r8, -64(%rbp)                  # 8-byte Spill
	movl	%ecx, %r13d
	movq	%rdx, %r14
	movq	%rsi, %r15
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB178_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.61.152(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB178_3
.LBB178_1:                              # %entry.split
	leaq	.L.str.61.152(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB178_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.62(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.51(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.63(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-64(%rbp), %rdx                 # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.32.143(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.64.153(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB178_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB178_6
.LBB178_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
.LBB178_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-26, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end178:
	.size	halide_error_fold_factor_too_small, .Lfunc_end178-halide_error_fold_factor_too_small
                                        # -- End function
	.section	.text.halide_error_requirement_failed,"ax",@progbits
	.weak	halide_error_requirement_failed # -- Begin function halide_error_requirement_failed
	.p2align	4, 0x90
	.type	halide_error_requirement_failed,@function
halide_error_requirement_failed:        # @halide_error_requirement_failed
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB179_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r13
	movb	$0, 1023(%rbx)
	leaq	.L.str.65(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	jmp	.LBB179_3
.LBB179_1:                              # %entry.split
	leaq	.L.str.65(%rip), %rdx
	xorl	%r13d, %r13d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB179_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r13, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.66(%rip), %rdx
	movq	%rax, %rdi
	movq	%r13, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r13, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB179_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB179_6
.LBB179_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB179_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-27, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end179:
	.size	halide_error_requirement_failed, .Lfunc_end179-halide_error_requirement_failed
                                        # -- End function
	.section	.text.halide_error_specialize_fail,"ax",@progbits
	.weak	halide_error_specialize_fail    # -- Begin function halide_error_specialize_fail
	.p2align	4, 0x90
	.type	halide_error_specialize_fail,@function
halide_error_specialize_fail:           # @halide_error_specialize_fail
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB180_1
# %bb.2:                                # %if.else.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
	leaq	.L.str.67(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	subq	%rbx, %rax
	leaq	1(%rax), %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB180_3
.LBB180_1:                              # %if.then.i
	leaq	.L.str.67(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB180_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-31, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end180:
	.size	halide_error_specialize_fail, .Lfunc_end180-halide_error_specialize_fail
                                        # -- End function
	.section	.text.halide_error_no_device_interface,"ax",@progbits
	.weak	halide_error_no_device_interface # -- Begin function halide_error_no_device_interface
	.p2align	4, 0x90
	.type	halide_error_no_device_interface,@function
halide_error_no_device_interface:       # @halide_error_no_device_interface
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB181_1
# %bb.2:                                # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.68(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	subq	%rbx, %rax
	leaq	1(%rax), %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB181_3
.LBB181_1:                              # %if.then.i
	leaq	.L.str.68(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB181_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-19, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end181:
	.size	halide_error_no_device_interface, .Lfunc_end181-halide_error_no_device_interface
                                        # -- End function
	.section	.text.halide_error_device_interface_no_device,"ax",@progbits
	.weak	halide_error_device_interface_no_device # -- Begin function halide_error_device_interface_no_device
	.p2align	4, 0x90
	.type	halide_error_device_interface_no_device,@function
halide_error_device_interface_no_device: # @halide_error_device_interface_no_device
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB182_1
# %bb.2:                                # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.69(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	subq	%rbx, %rax
	leaq	1(%rax), %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB182_3
.LBB182_1:                              # %if.then.i
	leaq	.L.str.69(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB182_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-36, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end182:
	.size	halide_error_device_interface_no_device, .Lfunc_end182-halide_error_device_interface_no_device
                                        # -- End function
	.section	.text.halide_error_host_and_device_dirty,"ax",@progbits
	.weak	halide_error_host_and_device_dirty # -- Begin function halide_error_host_and_device_dirty
	.p2align	4, 0x90
	.type	halide_error_host_and_device_dirty,@function
halide_error_host_and_device_dirty:     # @halide_error_host_and_device_dirty
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB183_1
# %bb.2:                                # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.70(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	subq	%rbx, %rax
	leaq	1(%rax), %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB183_3
.LBB183_1:                              # %if.then.i
	leaq	.L.str.70(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB183_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-37, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end183:
	.size	halide_error_host_and_device_dirty, .Lfunc_end183-halide_error_host_and_device_dirty
                                        # -- End function
	.section	.text.halide_error_buffer_is_null,"ax",@progbits
	.weak	halide_error_buffer_is_null     # -- Begin function halide_error_buffer_is_null
	.p2align	4, 0x90
	.type	halide_error_buffer_is_null,@function
halide_error_buffer_is_null:            # @halide_error_buffer_is_null
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB184_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
	leaq	.L.str.71(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB184_3
.LBB184_1:                              # %entry.split
	leaq	.L.str.71(%rip), %rdx
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB184_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.72(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB184_4
# %bb.5:                                # %if.else.i
	subq	%rbx, %rax
	incq	%rax
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB184_6
.LBB184_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB184_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-38, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end184:
	.size	halide_error_buffer_is_null, .Lfunc_end184-halide_error_buffer_is_null
                                        # -- End function
	.section	.text.halide_error_storage_bound_too_small,"ax",@progbits
	.weak	halide_error_storage_bound_too_small # -- Begin function halide_error_storage_bound_too_small
	.p2align	4, 0x90
	.type	halide_error_storage_bound_too_small,@function
halide_error_storage_bound_too_small:   # @halide_error_storage_bound_too_small
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r8d, -44(%rbp)                 # 4-byte Spill
	movl	%ecx, %r14d
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r12
	testq	%rax, %rax
	je	.LBB185_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
	leaq	.L.str.73(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	jmp	.LBB185_3
.LBB185_1:                              # %entry.split
	leaq	.L.str.73(%rip), %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	xorl	%esi, %esi
.LBB185_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.62(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.51(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.74(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx                 # 4-byte Folded Reload
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.64.153(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB185_4
# %bb.5:                                # %if.else.i
	subq	%r12, %rax
	incq	%rax
	movq	-56(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB185_6
.LBB185_4:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-56(%rbp), %rdi                 # 8-byte Reload
.LBB185_6:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-45, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end185:
	.size	halide_error_storage_bound_too_small, .Lfunc_end185-halide_error_storage_bound_too_small
                                        # -- End function
	.section	.text.halide_error_device_crop_failed,"ax",@progbits
	.weak	halide_error_device_crop_failed # -- Begin function halide_error_device_crop_failed
	.p2align	4, 0x90
	.type	halide_error_device_crop_failed,@function
halide_error_device_crop_failed:        # @halide_error_device_crop_failed
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %rbx
	testq	%rax, %rax
	je	.LBB186_1
# %bb.2:                                # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.75(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	subq	%rbx, %rax
	leaq	1(%rax), %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB186_3
.LBB186_1:                              # %if.then.i
	leaq	.L.str.75(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.29.163(%rip), %rsi
	movq	%r14, %rdi
.LBB186_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movl	$-41, %eax
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end186:
	.size	halide_error_device_crop_failed, .Lfunc_end186-halide_error_device_crop_failed
                                        # -- End function
	.section	.text.halide_profiler_shutdown,"ax",@progbits
	.weak	halide_profiler_shutdown        # -- Begin function halide_profiler_shutdown
	.p2align	4, 0x90
	.type	halide_profiler_shutdown,@function
halide_profiler_shutdown:               # @halide_profiler_shutdown
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	40(%rax), %rdi
	testq	%rdi, %rdi
	je	.LBB187_1
# %bb.2:                                # %if.end
	movq	%rax, %rbx
	movl	$-2, 16(%rax)
	callq	halide_join_thread@PLT
	movq	$0, 40(%rbx)
	movl	$-1, 16(%rbx)
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_profiler_report_unlocked@PLT
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	halide_profiler_reset_unlocked@PLT # TAILCALL
.LBB187_1:                              # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end187:
	.size	halide_profiler_shutdown, .Lfunc_end187-halide_profiler_shutdown
                                        # -- End function
	.section	.text.halide_profiler_get_state,"ax",@progbits
	.weak	halide_profiler_get_state       # -- Begin function halide_profiler_get_state
	.p2align	4, 0x90
	.type	halide_profiler_get_state,@function
halide_profiler_get_state:              # @halide_profiler_get_state
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	_ZZ25halide_profiler_get_stateE1s(%rip), %rax
	popq	%rbp
	retq
.Lfunc_end188:
	.size	halide_profiler_get_state, .Lfunc_end188-halide_profiler_get_state
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function halide_profiler_report_unlocked
.LCPI189_0:
	.long	0x49742400                      # float 1.0E+6
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI189_1:
	.quad	0x3ddb7cdfd9d7bdbb              # double 1.0E-10
	.section	.text.halide_profiler_report_unlocked,"ax",@progbits
	.weak	halide_profiler_report_unlocked
	.p2align	4, 0x90
	.type	halide_profiler_report_unlocked,@function
halide_profiler_report_unlocked:        # @halide_profiler_report_unlocked
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%rsi, %rbx
	movq	%rdi, %r12
	movl	$1024, %edi                     # imm = 0x400
	callq	malloc@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	movq	%r12, -72(%rbp)                 # 8-byte Spill
	je	.LBB189_1
# %bb.2:                                # %if.then6.i
	leaq	1023(%r15), %r14
	movb	$0, 1023(%r15)
	movq	24(%rbx), %rsi
	movq	%r15, %r12
	testq	%rsi, %rsi
	jne	.LBB189_4
	jmp	.LBB189_8
.LBB189_1:
	xorl	%r14d, %r14d
	movq	24(%rbx), %rsi
	movq	%r15, %r12
	testq	%rsi, %rsi
	je	.LBB189_8
.LBB189_4:                              # %for.body.lr.ph
	movl	$1, %eax
	subq	%r15, %rax
	movq	%rax, -104(%rbp)                # 8-byte Spill
	leaq	.L.str.20.177(%rip), %r13
	movq	%r15, %r12
	jmp	.LBB189_5
	.p2align	4, 0x90
.LBB189_71:                             # %cleanup181
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	64(%rsi), %rsi
	testq	%rsi, %rsi
	je	.LBB189_8
.LBB189_5:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB189_22 Depth 2
                                        #     Child Loop BB189_25 Depth 2
                                        #       Child Loop BB189_32 Depth 3
                                        #       Child Loop BB189_37 Depth 3
                                        #       Child Loop BB189_44 Depth 3
                                        #       Child Loop BB189_50 Depth 3
                                        #       Child Loop BB189_55 Depth 3
                                        #       Child Loop BB189_57 Depth 3
	vcvtusi2ssq	(%rsi), %xmm2, %xmm0
	vdivss	.LCPI189_0(%rip), %xmm0, %xmm0
	cmpl	$0, 80(%rsi)
	je	.LBB189_71
# %bb.6:                                # %if.end
                                        #   in Loop: Header=BB189_5 Depth=1
	testq	%r15, %r15
	vmovss	%xmm0, -56(%rbp)                # 4-byte Spill
	je	.LBB189_7
# %bb.12:                               # %if.then.i278
                                        #   in Loop: Header=BB189_5 Depth=1
	movb	$0, (%r15)
	movq	32(%rsi), %rax
	movq	40(%rsi), %rcx
	movq	%rax, -64(%rbp)                 # 8-byte Spill
	movq	%rcx, -96(%rbp)                 # 8-byte Spill
	cmpq	%rcx, %rax
	sete	%r12b
	movq	48(%rsi), %rdx
	movq	%r15, %rdi
	jmp	.LBB189_13
	.p2align	4, 0x90
.LBB189_7:                              # %if.end.split
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	32(%rsi), %rax
	movq	40(%rsi), %rcx
	movq	%rax, -64(%rbp)                 # 8-byte Spill
	movq	%rcx, -96(%rbp)                 # 8-byte Spill
	cmpq	%rcx, %rax
	sete	%r12b
	movq	48(%rsi), %rdx
	xorl	%edi, %edi
.LBB189_13:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	%rsi, %rbx
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movb	%r12b, -41(%rbp)                # 1-byte Spill
	movq	%r14, %rsi
	leaq	.L.str.7.164(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.8.165(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-56(%rbp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.9.166(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.10.167(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	84(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.11.168(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	80(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.12.169(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rbx, -80(%rbp)                 # 8-byte Spill
	vcvtsi2ssl	80(%rbx), %xmm2, %xmm0
	vmovss	-56(%rbp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.13.170(%rip), %rdx
	callq	halide_string_to_string@PLT
	testb	%r12b, %r12b
	jne	.LBB189_15
# %bb.14:                               # %if.then24
                                        #   in Loop: Header=BB189_5 Depth=1
	vcvtusi2sdq	-64(%rbp), %xmm2, %xmm0 # 8-byte Folded Reload
	vcvtusi2sdq	-96(%rbp), %xmm2, %xmm1 # 8-byte Folded Reload
	vaddsd	.LCPI189_1(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -56(%rbp)                # 4-byte Spill
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.14.171(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-56(%rbp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.7.164(%rip), %rdx
	callq	halide_string_to_string@PLT
.LBB189_15:                             # %if.end28
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	-72(%rbp), %rbx                 # 8-byte Reload
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.15.172(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-80(%rbp), %r12                 # 8-byte Reload
	movslq	88(%r12), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.16.173(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	16(%r12), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.17.174(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
	leaq	.L.str.29.163(%rip), %rsi
	testq	%r15, %r15
	je	.LBB189_17
# %bb.16:                               # %if.then.i352
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	-104(%rbp), %rax                # 8-byte Reload
	leaq	(%rax,%r12), %rdx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rsi
.LBB189_17:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	%rbx, %rdi
	callq	halide_print@PLT
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	cmpq	$0, (%rsi)
	jne	.LBB189_23
# %bb.18:                               # %lor.end
                                        #   in Loop: Header=BB189_5 Depth=1
	cmpq	$0, 24(%rsi)
	je	.LBB189_19
.LBB189_23:                             # %for.cond53.preheader
                                        #   in Loop: Header=BB189_5 Depth=1
	cmpl	$0, 72(%rsi)
	jle	.LBB189_71
# %bb.24:                               # %for.body57.lr.ph
                                        #   in Loop: Header=BB189_5 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB189_25
	.p2align	4, 0x90
.LBB189_69:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	%rbx, %rdi
	callq	halide_print@PLT
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	movq	-96(%rbp), %rcx                 # 8-byte Reload
.LBB189_70:                             # %cleanup172
                                        #   in Loop: Header=BB189_25 Depth=2
	incq	%rcx
	movslq	72(%rsi), %rax
	cmpq	%rax, %rcx
	jge	.LBB189_71
.LBB189_25:                             # %for.body57
                                        #   Parent Loop BB189_5 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB189_32 Depth 3
                                        #       Child Loop BB189_37 Depth 3
                                        #       Child Loop BB189_44 Depth 3
                                        #       Child Loop BB189_50 Depth 3
                                        #       Child Loop BB189_55 Depth 3
                                        #       Child Loop BB189_57 Depth 3
	testq	%r15, %r15
	je	.LBB189_27
# %bb.26:                               # %if.then.i356
                                        #   in Loop: Header=BB189_25 Depth=2
	movb	$0, (%r15)
.LBB189_27:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	56(%rsi), %rbx
	leaq	(%rcx,%rcx,8), %rax
	leaq	(%rbx,%rax,8), %rdx
	testq	%rcx, %rcx
	jne	.LBB189_29
# %bb.28:                               # %land.lhs.true
                                        #   in Loop: Header=BB189_25 Depth=2
	cmpq	$0, (%rdx)
	movq	%r15, %r12
	je	.LBB189_70
.LBB189_29:                             # %if.end66
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	%rdx, -88(%rbp)                 # 8-byte Spill
	movq	%rcx, -96(%rbp)                 # 8-byte Spill
	movq	%r15, %rdi
	movq	%r14, %rsi
	leaq	.L.str.18.175(%rip), %rdx
	movq	%rax, %r12
	callq	halide_string_to_string@PLT
	movq	%rbx, -56(%rbp)                 # 8-byte Spill
	movq	%r12, -64(%rbp)                 # 8-byte Spill
	movq	56(%rbx,%r12,8), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.19.176(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r15, %rcx
	cmpq	$24, %rcx
	ja	.LBB189_30
# %bb.31:                               # %while.body.preheader
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-88(%rbp), %rbx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB189_32:                             # %while.body
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r15, %rcx
	cmpq	$25, %rcx
	jb	.LBB189_32
	jmp	.LBB189_33
	.p2align	4, 0x90
.LBB189_30:                             #   in Loop: Header=BB189_25 Depth=2
	movq	-88(%rbp), %rbx                 # 8-byte Reload
.LBB189_33:                             # %while.end
                                        #   in Loop: Header=BB189_25 Depth=2
	vcvtusi2ssq	(%rbx), %xmm2, %xmm0
	movq	-80(%rbp), %rcx                 # 8-byte Reload
	vcvtsi2ssl	80(%rcx), %xmm2, %xmm1
	vmulss	.LCPI189_0(%rip), %xmm1, %xmm1
	vdivss	%xmm1, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	testq	%rax, %rax
	je	.LBB189_34
# %bb.35:                               # %if.then.i393
                                        #   in Loop: Header=BB189_25 Depth=2
	addq	$-3, %rax
	cmpq	%r15, %rax
	cmovbq	%r15, %rax
	movb	$0, (%rax)
	jmp	.LBB189_36
	.p2align	4, 0x90
.LBB189_34:                             #   in Loop: Header=BB189_25 Depth=2
	xorl	%eax, %eax
.LBB189_36:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.21.178(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	subq	%r15, %rax
	cmpq	$34, %rax
	ja	.LBB189_38
	.p2align	4, 0x90
.LBB189_37:                             # %while.body86
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	subq	%r15, %rax
	cmpq	$35, %rax
	jb	.LBB189_37
.LBB189_38:                             # %while.end88
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-80(%rbp), %rax                 # 8-byte Reload
	movq	(%rax), %rcx
	testq	%rcx, %rcx
	je	.LBB189_39
# %bb.40:                               # %if.then91
                                        #   in Loop: Header=BB189_25 Depth=2
	imulq	$100, (%rbx), %rax
	movq	%rax, %rdx
	orq	%rcx, %rdx
	shrq	$32, %rdx
	je	.LBB189_41
# %bb.42:                               #   in Loop: Header=BB189_25 Depth=2
	xorl	%edx, %edx
	divq	%rcx
	movq	%rax, %r12
	jmp	.LBB189_43
	.p2align	4, 0x90
.LBB189_39:                             #   in Loop: Header=BB189_25 Depth=2
	xorl	%r12d, %r12d
	jmp	.LBB189_43
	.p2align	4, 0x90
.LBB189_41:                             #   in Loop: Header=BB189_25 Depth=2
                                        # kill: def $eax killed $eax killed $rax
	xorl	%edx, %edx
	divl	%ecx
	movl	%eax, %r12d
.LBB189_43:                             # %if.end97
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	%r14, %rsi
	leaq	.L.str.22.179(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	%r12d, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.23.180(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r15, %rcx
	leaq	.L.str.7.164(%rip), %r12
	cmpq	$42, %rcx
	ja	.LBB189_45
	.p2align	4, 0x90
.LBB189_44:                             # %while.body105
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r15, %rcx
	cmpq	$43, %rcx
	jb	.LBB189_44
.LBB189_45:                             # %while.end107
                                        #   in Loop: Header=BB189_25 Depth=2
	movl	$58, %ebx
	cmpb	$0, -41(%rbp)                   # 1-byte Folded Reload
	jne	.LBB189_51
# %bb.46:                               # %if.then109
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-56(%rbp), %rcx                 # 8-byte Reload
	movq	-64(%rbp), %rdx                 # 8-byte Reload
	vcvtusi2sdq	40(%rcx,%rdx,8), %xmm2, %xmm0
	vcvtusi2sdq	48(%rcx,%rdx,8), %xmm2, %xmm1
	vaddsd	.LCPI189_1(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -88(%rbp)                # 4-byte Spill
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.24.181(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-88(%rbp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	halide_double_to_string@PLT
	testq	%rax, %rax
	je	.LBB189_47
# %bb.48:                               # %if.then.i441
                                        #   in Loop: Header=BB189_25 Depth=2
	addq	$-3, %rax
	cmpq	%r15, %rax
	cmovbq	%r15, %rax
	movb	$0, (%rax)
	jmp	.LBB189_49
.LBB189_47:                             #   in Loop: Header=BB189_25 Depth=2
	xorl	%eax, %eax
.LBB189_49:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	movq	-64(%rbp), %rsi                 # 8-byte Reload
	movq	%rax, %rcx
	subq	%r15, %rcx
	movl	$73, %ebx
	cmpq	$57, %rcx
	ja	.LBB189_52
	.p2align	4, 0x90
.LBB189_50:                             # %while.body124
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r15, %rcx
	cmpq	$58, %rcx
	jb	.LBB189_50
.LBB189_51:                             #   in Loop: Header=BB189_25 Depth=2
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	movq	-64(%rbp), %rsi                 # 8-byte Reload
.LBB189_52:                             # %if.end127
                                        #   in Loop: Header=BB189_25 Depth=2
	cmpq	$0, 16(%rdx,%rsi,8)
	je	.LBB189_65
# %bb.53:                               # %if.then130
                                        #   in Loop: Header=BB189_25 Depth=2
	leaq	(%rdx,%rsi,8), %r12
	addq	$16, %r12
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.25.182(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%r12), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
	.p2align	4, 0x90
.LBB189_55:                             # %while.body138
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rcx
	subq	%r15, %rcx
	movq	%rax, %rdi
	movq	%r14, %rsi
	cmpq	%rbx, %rcx
	jae	.LBB189_56
# %bb.54:                               # %while.body138
                                        #   in Loop: Header=BB189_55 Depth=3
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	jmp	.LBB189_55
	.p2align	4, 0x90
.LBB189_56:                             # %while.end140
                                        #   in Loop: Header=BB189_25 Depth=2
	leaq	.L.str.26.183(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-56(%rbp), %rcx                 # 8-byte Reload
	movq	-64(%rbp), %rdx                 # 8-byte Reload
	movslq	64(%rcx,%rdx,8), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	addq	$15, %rbx
	subq	%r15, %rax
	cmpq	%rbx, %rax
	jae	.LBB189_59
	.p2align	4, 0x90
.LBB189_57:                             # %while.body148
                                        #   Parent Loop BB189_5 Depth=1
                                        #     Parent Loop BB189_25 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r14, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	subq	%r15, %rax
	cmpq	%rbx, %rax
	jb	.LBB189_57
.LBB189_59:                             # %while.end150
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	movq	-64(%rbp), %rsi                 # 8-byte Reload
	leaq	(%rdx,%rsi,8), %rax
	addq	$64, %rax
	movslq	(%rax), %rcx
	testq	%rcx, %rcx
	je	.LBB189_60
# %bb.61:                               # %if.then153
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	24(%rdx,%rsi,8), %rax
	movq	%rax, %rdx
	orq	%rcx, %rdx
	shrq	$32, %rdx
	je	.LBB189_62
# %bb.63:                               #   in Loop: Header=BB189_25 Depth=2
	xorl	%edx, %edx
	divq	%rcx
	movq	%rax, %r12
	jmp	.LBB189_64
	.p2align	4, 0x90
.LBB189_60:                             #   in Loop: Header=BB189_25 Depth=2
	xorl	%r12d, %r12d
	jmp	.LBB189_64
.LBB189_62:                             #   in Loop: Header=BB189_25 Depth=2
                                        # kill: def $eax killed $eax killed $rax
	xorl	%edx, %edx
	divl	%ecx
	movl	%eax, %r12d
.LBB189_64:                             # %if.end159
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	%r14, %rsi
	leaq	.L.str.27.184(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	%r12d, %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.7.164(%rip), %r12
	movq	-56(%rbp), %rdx                 # 8-byte Reload
	movq	-64(%rbp), %rsi                 # 8-byte Reload
.LBB189_65:                             # %if.end162
                                        #   in Loop: Header=BB189_25 Depth=2
	cmpq	$0, 32(%rdx,%rsi,8)
	je	.LBB189_67
# %bb.66:                               # %if.then165
                                        #   in Loop: Header=BB189_25 Depth=2
	leaq	(%rdx,%rsi,8), %rbx
	addq	$32, %rbx
	movq	%rax, %rdi
	movq	%r14, %rsi
	leaq	.L.str.28.185(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	movl	$1, %ecx
	callq	halide_uint64_to_string@PLT
.LBB189_67:                             # %if.end169
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-72(%rbp), %rbx                 # 8-byte Reload
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %r12
	leaq	.L.str.29.163(%rip), %rsi
	testq	%r15, %r15
	je	.LBB189_69
# %bb.68:                               # %if.then.i379
                                        #   in Loop: Header=BB189_25 Depth=2
	movq	-104(%rbp), %rax                # 8-byte Reload
	leaq	(%rax,%r12), %rdx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rsi
	jmp	.LBB189_69
.LBB189_19:                             # %for.cond41.preheader
                                        #   in Loop: Header=BB189_5 Depth=1
	movl	72(%rsi), %ecx
	testl	%ecx, %ecx
	jle	.LBB189_71
# %bb.20:                               # %for.body44.lr.ph
                                        #   in Loop: Header=BB189_5 Depth=1
	movq	56(%rsi), %rax
	shlq	$3, %rcx
	leaq	(%rcx,%rcx,8), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB189_22:                             # %for.body44
                                        #   Parent Loop BB189_5 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	$0, 32(%rax,%rdx)
	jne	.LBB189_23
# %bb.21:                               # %for.cond41
                                        #   in Loop: Header=BB189_22 Depth=2
	addq	$72, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB189_22
	jmp	.LBB189_71
.LBB189_8:                              # %for.cond.cleanup
	testq	%r15, %r15
	je	.LBB189_9
# %bb.10:                               # %if.else.i
	subq	%r15, %r12
	incq	%r12
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	movq	%r15, %rsi
	movq	%r12, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	jmp	.LBB189_11
.LBB189_9:                              # %if.then.i
	leaq	.L.str.29.163(%rip), %rsi
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	callq	halide_error@PLT
.LBB189_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EED2Ev.exit
	movq	%r15, %rdi
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	free@PLT                        # TAILCALL
.Lfunc_end189:
	.size	halide_profiler_report_unlocked, .Lfunc_end189-halide_profiler_report_unlocked
                                        # -- End function
	.section	.text.halide_profiler_reset_unlocked,"ax",@progbits
	.weak	halide_profiler_reset_unlocked  # -- Begin function halide_profiler_reset_unlocked
	.p2align	4, 0x90
	.type	halide_profiler_reset_unlocked,@function
halide_profiler_reset_unlocked:         # @halide_profiler_reset_unlocked
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	24(%rdi), %rbx
	testq	%rbx, %rbx
	je	.LBB190_3
	.p2align	4, 0x90
.LBB190_1:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	64(%rbx), %rax
	movq	%rax, 24(%r14)
	movq	56(%rbx), %rdi
	callq	free@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movq	24(%r14), %rbx
	testq	%rbx, %rbx
	jne	.LBB190_1
.LBB190_3:                              # %while.end
	movl	$0, 12(%r14)
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end190:
	.size	halide_profiler_reset_unlocked, .Lfunc_end190-halide_profiler_reset_unlocked
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy # -- Begin function _ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,@function
_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy: # @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r12
	movl	%esi, %r14d
	movq	%rdi, %r13
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r15
	movq	24(%rax), %rbx
	jmp	.LBB191_1
	.p2align	4, 0x90
.LBB191_4:                              # %for.inc
                                        #   in Loop: Header=BB191_1 Depth=1
	movq	64(%rbx), %rbx
.LBB191_1:                              # %entry
                                        # =>This Inner Loop Header: Depth=1
	testq	%rbx, %rbx
	je	.LBB191_5
# %bb.2:                                # %for.body
                                        #   in Loop: Header=BB191_1 Depth=1
	cmpq	%r13, 48(%rbx)
	jne	.LBB191_4
# %bb.3:                                # %land.lhs.true
                                        #   in Loop: Header=BB191_1 Depth=1
	cmpl	%r14d, 72(%rbx)
	jne	.LBB191_4
	jmp	.LBB191_17
.LBB191_5:                              # %for.end
	movl	$96, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB191_16
# %bb.6:                                # %if.end7
	movq	%rax, %rbx
	movq	24(%r15), %rax
	movq	%rax, 64(%rbx)
	movq	%r13, 48(%rbx)
	movl	12(%r15), %eax
	movl	%eax, 76(%rbx)
	movl	%r14d, 72(%rbx)
	movq	$0, 80(%rbx)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, (%rbx)
	movl	$0, 88(%rbx)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 32(%rbx)
	movslq	%r14d, %rax
	shlq	$3, %rax
	leaq	(%rax,%rax,8), %rdi
	vzeroupper
	callq	malloc@PLT
	movq	%rax, 56(%rbx)
	testq	%rax, %rax
	je	.LBB191_15
# %bb.7:                                # %for.cond17.preheader
	testl	%r14d, %r14d
	jle	.LBB191_12
# %bb.8:                                # %for.body20.lr.ph
	movl	%r14d, %r8d
	cmpl	$1, %r14d
	jne	.LBB191_13
# %bb.9:
	xorl	%edx, %edx
	jmp	.LBB191_10
.LBB191_15:                             # %if.then15
	movq	%rbx, %rdi
	callq	free@PLT
.LBB191_16:                             # %cleanup62
	xorl	%ebx, %ebx
	jmp	.LBB191_17
.LBB191_13:                             # %for.body20.lr.ph.new
	movl	%r8d, %esi
	andl	$-2, %esi
	leaq	136(%rax), %rdi
	xorl	%edx, %edx
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	.p2align	4, 0x90
.LBB191_14:                             # %for.body20
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, -136(%rdi)
	movq	(%r12,%rdx,8), %rcx
	movq	%rcx, -80(%rdi)
	movl	$0, -72(%rdi)
	vmovups	%ymm0, -128(%rdi)
	vmovups	%xmm1, -96(%rdi)
	movq	$0, -64(%rdi)
	movq	8(%r12,%rdx,8), %rcx
	movq	%rcx, -8(%rdi)
	movl	$0, (%rdi)
	vmovups	%ymm0, -56(%rdi)
	vmovups	%xmm1, -24(%rdi)
	addq	$2, %rdx
	addq	$144, %rdi
	cmpq	%rdx, %rsi
	jne	.LBB191_14
.LBB191_10:                             # %for.cond.cleanup19.loopexit.unr-lcssa
	testb	$1, %r8b
	je	.LBB191_12
# %bb.11:                               # %for.body20.epil
	leaq	(%rdx,%rdx,8), %rcx
	movq	$0, (%rax,%rcx,8)
	movq	(%r12,%rdx,8), %rdx
	movq	%rdx, 56(%rax,%rcx,8)
	movl	$0, 64(%rax,%rcx,8)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, 8(%rax,%rcx,8)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 40(%rax,%rcx,8)
.LBB191_12:                             # %for.cond.cleanup19
	addl	%r14d, 12(%r15)
	movq	%rbx, 24(%r15)
.LBB191_17:                             # %cleanup62
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end191:
	.size	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy, .Lfunc_end191-_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function _ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
.LCPI192_0:
	.zero	8
	.quad	1                               # 0x1
	.section	.text._ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,@function
_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi: # @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	24(%rdi), %r8
	testq	%r8, %r8
	je	.LBB192_8
# %bb.1:                                # %for.body.preheader
	xorl	%r10d, %r10d
	movq	%r8, %rax
	jmp	.LBB192_3
	.p2align	4, 0x90
.LBB192_2:                              # %if.end23
                                        #   in Loop: Header=BB192_3 Depth=1
	movq	64(%r11), %rax
	movq	%r11, %r10
	testq	%rax, %rax
	je	.LBB192_8
.LBB192_3:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %r11
	movslq	76(%rax), %r9
	cmpl	%esi, %r9d
	jg	.LBB192_2
# %bb.4:                                # %land.lhs.true
                                        #   in Loop: Header=BB192_3 Depth=1
	movl	72(%r11), %eax
	addl	%r9d, %eax
	cmpl	%esi, %eax
	jle	.LBB192_2
# %bb.5:                                # %if.then
	testq	%r10, %r10
	je	.LBB192_7
# %bb.6:                                # %if.then4
	movq	64(%r11), %rax
	movq	%rax, 64(%r10)
	movq	%r8, 64(%r11)
	movq	%r11, 24(%rdi)
.LBB192_7:                              # %if.end
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	shlq	$3, %rax
	addq	56(%r11), %rax
	negq	%r9
	leaq	(%r9,%r9,8), %rsi
	addq	%rdx, (%rax,%rsi,8)
	movslq	%ecx, %rcx
	vmovdqa	.LCPI192_0(%rip), %xmm0         # xmm0 = <u,1>
	vpinsrq	$0, %rcx, %xmm0, %xmm0
	vpaddq	40(%rax,%rsi,8), %xmm0, %xmm1
	vmovdqu	%xmm1, 40(%rax,%rsi,8)
	addq	%rdx, (%r11)
	incl	84(%r11)
	vpaddq	32(%r11), %xmm0, %xmm0
	vmovdqu	%xmm0, 32(%r11)
.LBB192_8:                              # %cleanup25
	popq	%rbp
	retq
.Lfunc_end192:
	.size	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi, .Lfunc_end192-_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
                                        # -- End function
	.section	.text.halide_profiler_sample,"ax",@progbits
	.weak	halide_profiler_sample          # -- Begin function halide_profiler_sample
	.p2align	4, 0x90
	.type	halide_profiler_sample,@function
halide_profiler_sample:                 # @halide_profiler_sample
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	32(%rdi), %rax
	testq	%rax, %rax
	je	.LBB193_2
# %bb.1:                                # %if.then
	leaq	-32(%rbp), %rdi
	leaq	-28(%rbp), %rsi
	callq	*%rax
	jmp	.LBB193_3
.LBB193_2:                              # %if.else
	movl	16(%rbx), %eax
	movl	%eax, -32(%rbp)
	movl	20(%rbx), %eax
	movl	%eax, -28(%rbp)
.LBB193_3:                              # %if.end
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %r15
	movl	-32(%rbp), %esi
	movl	$-1, %eax
	cmpl	$-2, %esi
	je	.LBB193_7
# %bb.4:                                # %if.else4
	testl	%esi, %esi
	js	.LBB193_6
# %bb.5:                                # %if.then6
	movq	%r15, %rdx
	subq	(%r14), %rdx
	movl	-28(%rbp), %ecx
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi@PLT
.LBB193_6:                              # %if.end8
	movq	%r15, (%r14)
	movl	8(%rbx), %eax
.LBB193_7:                              # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end193:
	.size	halide_profiler_sample, .Lfunc_end193-halide_profiler_sample
                                        # -- End function
	.section	.text._ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv # -- Begin function _ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,@function
_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv: # @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r15
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	cmpl	$-2, 16(%r15)
	jne	.LBB194_1
.LBB194_6:                              # %while.end8
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB194_1:                              # %while.body.lr.ph
	leaq	-32(%rbp), %r14
	jmp	.LBB194_2
	.p2align	4, 0x90
.LBB194_5:                              # %while.end
                                        #   in Loop: Header=BB194_2 Depth=1
	cmpl	$-2, 16(%r15)
	je	.LBB194_6
.LBB194_2:                              # %while.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB194_4 Depth 2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, -32(%rbp)
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_profiler_sample@PLT
	testl	%eax, %eax
	js	.LBB194_5
# %bb.3:                                # %if.end.preheader
                                        #   in Loop: Header=BB194_2 Depth=1
	movl	%eax, %ebx
	.p2align	4, 0x90
.LBB194_4:                              # %if.end
                                        #   Parent Loop BB194_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%edi, %edi
	movl	%ebx, %esi
	callq	halide_sleep_ms@PLT
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_profiler_sample@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jns	.LBB194_4
	jmp	.LBB194_5
.Lfunc_end194:
	.size	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv, .Lfunc_end194-_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
                                        # -- End function
	.section	.text.halide_profiler_get_pipeline_state,"ax",@progbits
	.weak	halide_profiler_get_pipeline_state # -- Begin function halide_profiler_get_pipeline_state
	.p2align	4, 0x90
	.type	halide_profiler_get_pipeline_state,@function
halide_profiler_get_pipeline_state:     # @halide_profiler_get_pipeline_state
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r15
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	movq	24(%r14), %rbx
	testq	%rbx, %rbx
	je	.LBB195_4
	.p2align	4, 0x90
.LBB195_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%r15, 48(%rbx)
	je	.LBB195_5
# %bb.3:                                # %for.inc
                                        #   in Loop: Header=BB195_2 Depth=1
	movq	64(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB195_2
.LBB195_4:
	xorl	%ebx, %ebx
.LBB195_5:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end195:
	.size	halide_profiler_get_pipeline_state, .Lfunc_end195-halide_profiler_get_pipeline_state
                                        # -- End function
	.section	.text.halide_profiler_pipeline_start,"ax",@progbits
	.weak	halide_profiler_pipeline_start  # -- Begin function halide_profiler_pipeline_start
	.p2align	4, 0x90
	.type	halide_profiler_pipeline_start,@function
halide_profiler_pipeline_start:         # @halide_profiler_pipeline_start
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, %r15
	movl	%edx, %r12d
	movq	%rsi, %r13
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	cmpq	$0, 40(%rbx)
	jne	.LBB196_2
# %bb.1:                                # %if.then
	movq	%r14, %rdi
	callq	halide_start_clock@PLT
	movq	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv@GOTPCREL(%rip), %rdi
	xorl	%esi, %esi
	callq	halide_spawn_thread@PLT
	movq	%rax, 40(%rbx)
.LBB196_2:                              # %if.end
	movq	%r13, %rdi
	movl	%r12d, %esi
	movq	%r15, %rdx
	callq	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy@PLT
	testq	%rax, %rax
	je	.LBB196_3
# %bb.4:                                # %if.end8
	incl	80(%rax)
	movl	76(%rax), %r14d
	jmp	.LBB196_5
.LBB196_3:                              # %if.then6
	movq	%r14, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r14d
.LBB196_5:                              # %cleanup
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end196:
	.size	halide_profiler_pipeline_start, .Lfunc_end196-halide_profiler_pipeline_start
                                        # -- End function
	.section	.text.halide_profiler_stack_peak_update,"ax",@progbits
	.weak	halide_profiler_stack_peak_update # -- Begin function halide_profiler_stack_peak_update
	.p2align	4, 0x90
	.type	halide_profiler_stack_peak_update,@function
halide_profiler_stack_peak_update:      # @halide_profiler_stack_peak_update
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdx, %r14
	movq	%rsi, %rbx
	testq	%rsi, %rsi
	je	.LBB197_1
# %bb.2:                                # %do.end
	movl	72(%rbx), %eax
	testl	%eax, %eax
	jg	.LBB197_3
	jmp	.LBB197_10
.LBB197_1:                              # %if.then
	leaq	.L.str.186(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	72(%rbx), %eax
	testl	%eax, %eax
	jle	.LBB197_10
.LBB197_3:                              # %for.body.lr.ph
	xorl	%edx, %edx
	jmp	.LBB197_4
	.p2align	4, 0x90
.LBB197_8:                              # %for.inc.loopexit
                                        #   in Loop: Header=BB197_4 Depth=1
	movl	72(%rbx), %eax
.LBB197_9:                              # %for.inc
                                        #   in Loop: Header=BB197_4 Depth=1
	incq	%rdx
	movslq	%eax, %rcx
	cmpq	%rcx, %rdx
	jge	.LBB197_10
.LBB197_4:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB197_6 Depth 2
	movq	(%r14,%rdx,8), %rsi
	testq	%rsi, %rsi
	je	.LBB197_9
# %bb.5:                                # %if.then3
                                        #   in Loop: Header=BB197_4 Depth=1
	movq	56(%rbx), %rax
	leaq	(%rdx,%rdx,8), %rcx
	leaq	(%rax,%rcx,8), %rdi
	addq	$32, %rdi
	movq	32(%rax,%rcx,8), %rcx
	.p2align	4, 0x90
.LBB197_6:                              # %while.cond.i
                                        #   Parent Loop BB197_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rsi, %rcx
	jae	.LBB197_8
# %bb.7:                                # %while.body.i
                                        #   in Loop: Header=BB197_6 Depth=2
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB197_6
	jmp	.LBB197_8
.LBB197_10:                             # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end197:
	.size	halide_profiler_stack_peak_update, .Lfunc_end197-halide_profiler_stack_peak_update
                                        # -- End function
	.section	.text.halide_profiler_memory_allocate,"ax",@progbits
	.weak	halide_profiler_memory_allocate # -- Begin function halide_profiler_memory_allocate
	.p2align	4, 0x90
	.type	halide_profiler_memory_allocate,@function
halide_profiler_memory_allocate:        # @halide_profiler_memory_allocate
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	testq	%rcx, %rcx
	je	.LBB198_13
# %bb.1:                                # %if.end
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%rsi, %rsi
	je	.LBB198_2
# %bb.3:                                # %do.body4
	testl	%r15d, %r15d
	js	.LBB198_4
.LBB198_5:                              # %do.body10
	cmpl	%r15d, 72(%rbx)
	jg	.LBB198_7
.LBB198_6:                              # %if.then12
	leaq	.L.str.3.189(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB198_7:                              # %do.end15
	movq	56(%rbx), %rdx
	lock		incl	88(%rbx)
	lock		addq	%r14, 24(%rbx)
	movslq	%r15d, %rsi
	movq	%r14, %rdi
	lock		xaddq	%rdi, 8(%rbx)
	addq	%r14, %rdi
	movq	16(%rbx), %rcx
	.p2align	4, 0x90
.LBB198_8:                              # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rdi, %rcx
	jae	.LBB198_10
# %bb.9:                                # %while.body.i
                                        #   in Loop: Header=BB198_8 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rdi, 16(%rbx)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB198_8
.LBB198_10:                             # %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit
	leaq	(%rsi,%rsi,8), %rax
	lock		incl	64(%rdx,%rax,8)
	lock		addq	%r14, 24(%rdx,%rax,8)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rdx,%rax,8)
	addq	%r14, %rsi
	leaq	(%rdx,%rax,8), %rdi
	addq	$16, %rdi
	movq	16(%rdx,%rax,8), %rcx
	.p2align	4, 0x90
.LBB198_11:                             # %while.cond.i43
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB198_13
# %bb.12:                               # %while.body.i45
                                        #   in Loop: Header=BB198_11 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB198_11
.LBB198_13:                             # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB198_2:                              # %if.then2
	leaq	.L.str.1.187(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	testl	%r15d, %r15d
	jns	.LBB198_5
.LBB198_4:                              # %if.then6
	leaq	.L.str.2.188(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	cmpl	%r15d, 72(%rbx)
	jle	.LBB198_6
	jmp	.LBB198_7
.Lfunc_end198:
	.size	halide_profiler_memory_allocate, .Lfunc_end198-halide_profiler_memory_allocate
                                        # -- End function
	.section	.text.halide_profiler_memory_free,"ax",@progbits
	.weak	halide_profiler_memory_free     # -- Begin function halide_profiler_memory_free
	.p2align	4, 0x90
	.type	halide_profiler_memory_free,@function
halide_profiler_memory_free:            # @halide_profiler_memory_free
# %bb.0:                                # %entry
	testq	%rcx, %rcx
	je	.LBB199_8
# %bb.1:                                # %if.end
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r15
	movl	%edx, %r14d
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%rsi, %rsi
	je	.LBB199_2
# %bb.3:                                # %do.body4
	testl	%r14d, %r14d
	js	.LBB199_4
.LBB199_5:                              # %do.body10
	cmpl	%r14d, 72(%rbx)
	jg	.LBB199_7
.LBB199_6:                              # %if.then12
	leaq	.L.str.6.192(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB199_7:                              # %do.end15
	movq	56(%rbx), %rax
	lock		subq	%r15, 8(%rbx)
	movslq	%r14d, %rcx
	leaq	(%rcx,%rcx,8), %rcx
	lock		subq	%r15, 8(%rax,%rcx,8)
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
.LBB199_8:                              # %return
	retq
.LBB199_2:                              # %if.then2
	leaq	.L.str.4.190(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	testl	%r14d, %r14d
	jns	.LBB199_5
.LBB199_4:                              # %if.then6
	leaq	.L.str.5.191(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	cmpl	%r14d, 72(%rbx)
	jle	.LBB199_6
	jmp	.LBB199_7
.Lfunc_end199:
	.size	halide_profiler_memory_free, .Lfunc_end199-halide_profiler_memory_free
                                        # -- End function
	.section	.text.halide_profiler_report,"ax",@progbits
	.weak	halide_profiler_report          # -- Begin function halide_profiler_report
	.p2align	4, 0x90
	.type	halide_profiler_report,@function
halide_profiler_report:                 # @halide_profiler_report
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_profiler_report_unlocked@PLT
	movq	%rbx, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end200:
	.size	halide_profiler_report, .Lfunc_end200-halide_profiler_report
                                        # -- End function
	.section	.text.halide_profiler_reset,"ax",@progbits
	.weak	halide_profiler_reset           # -- Begin function halide_profiler_reset
	.p2align	4, 0x90
	.type	halide_profiler_reset,@function
halide_profiler_reset:                  # @halide_profiler_reset
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rax, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	callq	halide_profiler_reset_unlocked@PLT
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	halide_mutex_unlock@PLT         # TAILCALL
.Lfunc_end201:
	.size	halide_profiler_reset, .Lfunc_end201-halide_profiler_reset
                                        # -- End function
	.section	.text.halide_profiler_pipeline_end,"ax",@progbits
	.weak	halide_profiler_pipeline_end    # -- Begin function halide_profiler_pipeline_end
	.p2align	4, 0x90
	.type	halide_profiler_pipeline_end,@function
halide_profiler_pipeline_end:           # @halide_profiler_pipeline_end
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$-1, 16(%rsi)
	popq	%rbp
	retq
.Lfunc_end202:
	.size	halide_profiler_pipeline_end, .Lfunc_end202-halide_profiler_pipeline_end
                                        # -- End function
	.section	.text.halide_msan_annotate_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_memory_is_initialized # -- Begin function halide_msan_annotate_memory_is_initialized
	.p2align	4, 0x90
	.type	halide_msan_annotate_memory_is_initialized,@function
halide_msan_annotate_memory_is_initialized: # @halide_msan_annotate_memory_is_initialized
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end203:
	.size	halide_msan_annotate_memory_is_initialized, .Lfunc_end203-halide_msan_annotate_memory_is_initialized
                                        # -- End function
	.section	.text.halide_msan_check_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_check_memory_is_initialized # -- Begin function halide_msan_check_memory_is_initialized
	.p2align	4, 0x90
	.type	halide_msan_check_memory_is_initialized,@function
halide_msan_check_memory_is_initialized: # @halide_msan_check_memory_is_initialized
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end204:
	.size	halide_msan_check_memory_is_initialized, .Lfunc_end204-halide_msan_check_memory_is_initialized
                                        # -- End function
	.section	.text.halide_msan_check_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_check_buffer_is_initialized # -- Begin function halide_msan_check_buffer_is_initialized
	.p2align	4, 0x90
	.type	halide_msan_check_buffer_is_initialized,@function
halide_msan_check_buffer_is_initialized: # @halide_msan_check_buffer_is_initialized
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end205:
	.size	halide_msan_check_buffer_is_initialized, .Lfunc_end205-halide_msan_check_buffer_is_initialized
                                        # -- End function
	.section	.text.halide_msan_annotate_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized # -- Begin function halide_msan_annotate_buffer_is_initialized
	.p2align	4, 0x90
	.type	halide_msan_annotate_buffer_is_initialized,@function
halide_msan_annotate_buffer_is_initialized: # @halide_msan_annotate_buffer_is_initialized
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end206:
	.size	halide_msan_annotate_buffer_is_initialized, .Lfunc_end206-halide_msan_annotate_buffer_is_initialized
                                        # -- End function
	.section	.text.halide_msan_annotate_buffer_is_initialized_as_destructor,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized_as_destructor # -- Begin function halide_msan_annotate_buffer_is_initialized_as_destructor
	.p2align	4, 0x90
	.type	halide_msan_annotate_buffer_is_initialized_as_destructor,@function
halide_msan_annotate_buffer_is_initialized_as_destructor: # @halide_msan_annotate_buffer_is_initialized_as_destructor
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end207:
	.size	halide_msan_annotate_buffer_is_initialized_as_destructor, .Lfunc_end207-halide_msan_annotate_buffer_is_initialized_as_destructor
                                        # -- End function
	.section	.text.halide_default_can_use_target_features,"ax",@progbits
	.weak	halide_default_can_use_target_features # -- Begin function halide_default_can_use_target_features
	.p2align	4, 0x90
	.type	halide_default_can_use_target_features,@function
halide_default_can_use_target_features: # @halide_default_can_use_target_features
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$32, %rsp
	movq	%rsi, %r14
	movl	%edi, %ebx
	movq	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE@GOTPCREL(%rip), %r12
	cmpb	$0, (%r12)
	je	.LBB208_1
# %bb.2:                                # %if.end
	movq	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	cmpl	$2, %ebx
	jne	.LBB208_3
.LBB208_4:                              # %if.end2
	movq	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rdx
	andq	(%r14), %rdx
	jne	.LBB208_5
	jmp	.LBB208_6
.LBB208_1:                              # %if.then
	leaq	-64(%rbp), %r15
	movq	%r15, %rdi
	callq	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv@PLT
	movq	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE@GOTPCREL(%rip), %rdi
	movl	$32, %edx
	movq	%r15, %rsi
	callq	memcpy@PLT
	movb	$1, (%r12)
	movq	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	cmpl	$2, %ebx
	je	.LBB208_4
.LBB208_3:                              # %if.then1
	leaq	.L.str.197(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
	movq	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rdx
	andq	(%r14), %rdx
	je	.LBB208_6
.LBB208_5:                              # %if.then7
	movq	16(%rcx), %rsi
	xorl	%eax, %eax
	andnq	%rdx, %rsi, %rdx
	jne	.LBB208_9
.LBB208_6:                              # %for.inc.critedge
	movq	8(%rcx), %rdx
	andq	8(%r14), %rdx
	je	.LBB208_8
# %bb.7:                                # %if.then7.1
	movq	24(%rcx), %rcx
	xorl	%eax, %eax
	andnq	%rdx, %rcx, %rcx
	jne	.LBB208_9
.LBB208_8:                              # %for.inc.critedge.1
	movl	$1, %eax
.LBB208_9:                              # %cleanup15
	addq	$32, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end208:
	.size	halide_default_can_use_target_features, .Lfunc_end208-halide_default_can_use_target_features
                                        # -- End function
	.section	.text.halide_set_custom_can_use_target_features,"ax",@progbits
	.weak	halide_set_custom_can_use_target_features # -- Begin function halide_set_custom_can_use_target_features
	.p2align	4, 0x90
	.type	halide_set_custom_can_use_target_features,@function
halide_set_custom_can_use_target_features: # @halide_set_custom_can_use_target_features
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end209:
	.size	halide_set_custom_can_use_target_features, .Lfunc_end209-halide_set_custom_can_use_target_features
                                        # -- End function
	.section	.text.halide_can_use_target_features,"ax",@progbits
	.weak	halide_can_use_target_features  # -- Begin function halide_can_use_target_features
	.p2align	4, 0x90
	.type	halide_can_use_target_features,@function
halide_can_use_target_features:         # @halide_can_use_target_features
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                           # TAILCALL
.Lfunc_end210:
	.size	halide_can_use_target_features, .Lfunc_end210-halide_can_use_target_features
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function _ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
.LCPI211_0:
	.quad	34084860461808                  # 0x1f00000002f0
	.quad	0                               # 0x0
	.section	.text._ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
	.p2align	4, 0x90
	.type	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,@function
_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv: # @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	vmovaps	.LCPI211_0(%rip), %xmm0         # xmm0 = [34084860461808,0]
	movq	%rdi, %r8
	vmovups	%ymm0, (%rdi)
	movq	$1, -56(%rbp)
	#APP

	xchgq	%rsi, %rbx
	movl	-56(%rbp), %eax
	movl	-52(%rbp), %ecx
	cpuid
	movl	%eax, -56(%rbp)
	movl	%ebx, -52(%rbp)
	movl	%ecx, -48(%rbp)
	movl	%edx, -44(%rbp)
	xchgq	%rsi, %rbx

	#NO_APP
	movl	-48(%rbp), %eax
	movq	%rax, %rcx
	shrq	$15, %rcx
	andl	$16, %ecx
	movq	%rax, %rdx
	shrq	$23, %rdx
	andl	$32, %edx
	orq	%rcx, %rdx
	movq	%rax, %rcx
	shrq	$20, %rcx
	andl	$512, %ecx                      # imm = 0x200
	orq	%rdx, %rcx
	movq	%rax, %rdi
	shrq	$5, %rdi
	andl	$128, %edi
	orq	%rcx, %rdi
	testl	$805834752, %eax                # imm = 0x30081000
	je	.LBB211_2
# %bb.1:
	movq	%rdi, 16(%r8)
.LBB211_2:
	andl	$1879048192, %eax               # imm = 0x70000000
	cmpl	$1879048192, %eax               # imm = 0x70000000
	jne	.LBB211_10
# %bb.3:                                # %if.then30
	movq	$7, -24(%rbp)
	#APP

	xchgq	%rsi, %rbx
	movl	-24(%rbp), %eax
	movl	-20(%rbp), %ecx
	cpuid
	movl	%eax, -24(%rbp)
	movl	%ebx, -20(%rbp)
	movl	%ecx, -16(%rbp)
	movl	%edx, -12(%rbp)
	xchgq	%rsi, %rbx

	#NO_APP
	movl	-20(%rbp), %eax
	testb	$32, %al
	je	.LBB211_5
# %bb.4:                                # %if.then35
	orq	$64, %rdi
	movq	%rdi, 16(%r8)
.LBB211_5:                              # %if.end36
	movl	%eax, %ecx
	andl	$268500992, %ecx                # imm = 0x10010000
	cmpl	$268500992, %ecx                # imm = 0x10010000
	jne	.LBB211_10
# %bb.6:                                # %if.then40
	movl	%eax, %ecx
	andl	$469827584, %ecx                # imm = 0x1C010000
	xorl	%edx, %edx
	cmpl	$469827584, %ecx                # imm = 0x1C010000
	sete	%dl
	shlq	$41, %rdx
	orq	%rdi, %rdx
	movabsq	$1099511627776, %rcx            # imm = 0x10000000000
	orq	%rdx, %rcx
	movl	%eax, %esi
	andl	$-805109760, %esi               # imm = 0xD0030000
	movabsq	$5497558138880, %rdi            # imm = 0x50000000000
	orq	%rdx, %rdi
	cmpl	$-805109760, %esi               # imm = 0xD0030000
	cmovneq	%rcx, %rdi
	movq	%rdi, 16(%r8)
	andl	$-803012608, %eax               # imm = 0xD0230000
	cmpl	$-803012608, %eax               # imm = 0xD0230000
	jne	.LBB211_10
# %bb.7:                                # %if.then54
	movabsq	$8796093022208, %rax            # imm = 0x80000000000
	orq	%rdi, %rax
	movq	%rax, 16(%r8)
	movabsq	$4294967303, %rax               # imm = 0x100000007
	movq	%rax, -40(%rbp)
	#APP

	xchgq	%rsi, %rbx
	movl	-40(%rbp), %eax
	movl	-36(%rbp), %ecx
	cpuid
	movl	%eax, -40(%rbp)
	movl	%ebx, -36(%rbp)
	movl	%ecx, -32(%rbp)
	movl	%edx, -28(%rbp)
	xchgq	%rsi, %rbx

	#NO_APP
	testb	$8, -15(%rbp)
	je	.LBB211_10
# %bb.8:                                # %land.lhs.true59
	testb	$32, -40(%rbp)
	je	.LBB211_10
# %bb.9:                                # %if.then63
	movabsq	$26388279066624, %rax           # imm = 0x180000000000
	orq	%rax, %rdi
	movq	%rdi, 16(%r8)
.LBB211_10:                             # %if.end67
	movq	%r8, %rax
	popq	%rbx
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end211:
	.size	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv, .Lfunc_end211-_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
                                        # -- End function
	.section	.text.halide_use_jit_module,"ax",@progbits
	.weak	halide_use_jit_module           # -- Begin function halide_use_jit_module
	.p2align	4, 0x90
	.type	halide_use_jit_module,@function
halide_use_jit_module:                  # @halide_use_jit_module
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end212:
	.size	halide_use_jit_module, .Lfunc_end212-halide_use_jit_module
                                        # -- End function
	.section	.text.halide_release_jit_module,"ax",@progbits
	.weak	halide_release_jit_module       # -- Begin function halide_release_jit_module
	.p2align	4, 0x90
	.type	halide_release_jit_module,@function
halide_release_jit_module:              # @halide_release_jit_module
# %bb.0:                                # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end213:
	.size	halide_release_jit_module, .Lfunc_end213-halide_release_jit_module
                                        # -- End function
	.section	.rodata,"a",@progbits
	.p2align	6                               # -- Begin function sobel5x5
.LCPI214_0:
	.long	16                              # 0x10
	.long	17                              # 0x11
	.long	18                              # 0x12
	.long	19                              # 0x13
	.long	20                              # 0x14
	.long	21                              # 0x15
	.long	22                              # 0x16
	.long	23                              # 0x17
	.long	24                              # 0x18
	.long	25                              # 0x19
	.long	26                              # 0x1a
	.long	27                              # 0x1b
	.long	28                              # 0x1c
	.long	29                              # 0x1d
	.long	30                              # 0x1e
	.long	31                              # 0x1f
.LCPI214_1:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	15                              # 0xf
	.section	.text.sobel5x5,"ax",@progbits
	.globl	sobel5x5
	.p2align	4, 0x90
	.type	sobel5x5,@function
sobel5x5:                               # @sobel5x5
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$3848, %rsp                     # imm = 0xF08
	movq	16(%rdi), %rax
	movq	40(%rdi), %rdi
	movl	(%rdi), %r11d
	movl	16(%rdi), %r9d
	movl	20(%rdi), %r8d
	movslq	24(%rdi), %rcx
	movq	%rcx, 40(%rsp)                  # 8-byte Spill
	movq	16(%rsi), %rcx
	movq	%rcx, 568(%rsp)                 # 8-byte Spill
	movq	40(%rsi), %rsi
	movl	4(%rsi), %r10d
	movl	16(%rsi), %ebx
	movl	20(%rsi), %ebp
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	addl	$17, %ecx
	movl	%ecx, %edx
	sarl	$4, %edx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %edx
	leal	15(%rbp), %ecx
	sarl	$4, %ecx
	cmpl	%ecx, %edx
	movl	%ecx, 2172(%rsp)                # 4-byte Spill
	cmovgel	%ecx, %edx
	movq	%r8, 136(%rsp)                  # 8-byte Spill
	movq	%r9, 776(%rsp)                  # 8-byte Spill
	leal	(%r8,%r9), %ecx
	movl	%ebx, 112(%rsp)                 # 4-byte Spill
	subl	%ebx, %ecx
	addl	$17, %ebp
	cmpl	%ebp, %ecx
	cmovll	%ecx, %ebp
	addl	$-2, %ebp
	sarl	$4, %ebp
	cmpl	%ebp, %edx
	cmovgl	%edx, %ebp
	movq	%rbp, 616(%rsp)                 # 8-byte Spill
	movq	%r10, 768(%rsp)                 # 8-byte Spill
	leal	31(%r10), %ebp
	sarl	$5, %ebp
	movq	%rdi, 3064(%rsp)                # 8-byte Spill
	movl	4(%rdi), %ecx
	addl	%r11d, %ecx
	movq	%rcx, 608(%rsp)                 # 8-byte Spill
	vpbroadcastd	%r11d, %zmm0
	vmovdqu64	%zmm0, 3072(%rsp)       # 64-byte Spill
	movl	(%rsi), %ecx
	movq	%rcx, 312(%rsp)                 # 8-byte Spill
	movq	%rsi, 2120(%rsp)                # 8-byte Spill
	movslq	24(%rsi), %rsi
	movq	%rbp, 448(%rsp)                 # 8-byte Spill
	movl	%ebp, %ecx
	movq	%rcx, 1400(%rsp)                # 8-byte Spill
	movl	%edx, 232(%rsp)                 # 4-byte Spill
	testl	%edx, %edx
	movl	%r11d, -8(%rsp)                 # 4-byte Spill
	movq	%rsi, 200(%rsp)                 # 8-byte Spill
	jle	.LBB214_8
# %bb.1:                                # %"for output.s0.y.y.preheader"
	cmpl	$0, 768(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_8
# %bb.2:                                # %"for output.s0.y.y.us.preheader"
	movq	608(%rsp), %rcx                 # 8-byte Reload
	decl	%ecx
	movq	136(%rsp), %rdx                 # 8-byte Reload
	leal	1(%rdx), %edi
	decl	%edx
	movl	%edx, 856(%rsp)                 # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	%zmm0, 3200(%rsp)       # 64-byte Spill
	movl	232(%rsp), %ecx                 # 4-byte Reload
	movq	%rcx, 2128(%rsp)                # 8-byte Spill
	movl	%esi, %edx
	movl	112(%rsp), %ecx                 # 4-byte Reload
	imull	%ecx, %edx
	negl	%edx
	movl	%edx, 844(%rsp)                 # 4-byte Spill
	movl	%ecx, %edx
	subl	776(%rsp), %edx                 # 4-byte Folded Reload
	movl	%edx, 476(%rsp)                 # 4-byte Spill
	xorl	%edx, %edx
	movl	$2, %ebp
	movl	%edi, 860(%rsp)                 # 4-byte Spill
	.p2align	4, 0x90
.LBB214_3:                              # %"for output.s0.y.y.us"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_4 Depth 2
                                        #       Child Loop BB214_5 Depth 3
	movq	%rdx, 2136(%rsp)                # 8-byte Spill
	movl	%ecx, 848(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	imulq	%rsi, %rcx
	addq	568(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2144(%rsp)                # 8-byte Spill
	movl	844(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_4:                              # %"for output.s0.x.x.us"
                                        #   Parent Loop BB214_3 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_5 Depth 3
	movl	%ecx, 852(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	2144(%rsp), %rcx                # 8-byte Folded Reload
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	%rdx, 2152(%rsp)                # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3200(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2176(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1024(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3776(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3712(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1216(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1536(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3648(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1472(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3584(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3520(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3456(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3392(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3328(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3264(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 2160(%rsp)                # 8-byte Spill
	movl	476(%rsp), %ecx                 # 4-byte Reload
                                        # kill: def $ecx killed $ecx def $rcx
	.p2align	4, 0x90
.LBB214_5:                              # %"for output.s0.y.yi.us"
                                        #   Parent Loop BB214_3 Depth=1
                                        #     Parent Loop BB214_4 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	136(%rsp), %rsi                 # 8-byte Reload
	cmpl	%esi, %ecx
	movl	%ecx, %edx
                                        # kill: def $esi killed $esi killed $rsi
	cmovgl	%edi, %edx
	cmovll	%ecx, %esi
	movl	%esi, 1664(%rsp)                # 4-byte Spill
	cmpl	$2, %edx
	cmovlel	%ebp, %edx
	addl	$-2, %edx
	imull	40(%rsp), %edx                  # 4-byte Folded Reload
	subl	%r11d, %edx
	vpbroadcastd	%edx, %zmm22
	vmovdqu64	3776(%rsp), %zmm19      # 64-byte Reload
	vpaddd	%zmm19, %zmm22, %zmm0
	vmovdqu64	3712(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm22, %zmm2
	vmovd	%xmm2, %edx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %ebx
	vextracti128	$1, %ymm2, %xmm3
	movslq	%edx, %r14
	vpextrd	$1, %xmm3, %edx
	vpextrd	$2, %xmm3, %r13d
	vpextrd	$3, %xmm3, %r10d
	movslq	%esi, %r9
	vextracti32x4	$2, %zmm2, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r8d
	movslq	%edi, %rdi
	vpextrd	$3, %xmm1, 960(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	movslq	%ebx, %rsi
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm2, 896(%rsp)            # 4-byte Folded Spill
	movslq	%ebx, %rbx
	movslq	%r12d, %r12
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpinsrb	$1, (%rax,%r12), %xmm4, %xmm4
	vmovd	%xmm3, %ebx
	vpextrd	$2, %xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm3
	vpextrd	$3, %xmm0, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm0, %xmm4
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r12d
	vextracti32x4	$2, %zmm0, %xmm4
	movslq	%edx, %rdx
	movzbl	(%rax,%r14), %ebx
	vmovd	%ebx, %xmm5
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm1, %ebx
	vpinsrb	$1, (%rax,%r9), %xmm5, %xmm1
	vpextrd	$1, %xmm4, %ebp
	movslq	%r13d, %r9
	movslq	%ebx, %r13
	vpinsrb	$2, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %edi
	movslq	%r10d, %rbx
	vpinsrb	$3, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %r10d
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %r14d
	vpinsrb	$9, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ebp
	movslq	%r15d, %rdx
	movslq	%ecx, %rcx
	movq	%rcx, 1408(%rsp)                # 8-byte Spill
	vpinsrb	$6, (%rax,%r9), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %r15d
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rdx), %xmm1, %xmm2
	vmovd	%xmm0, %edx
	vmovdqu64	3584(%rsp), %zmm26      # 64-byte Reload
	vpaddd	%zmm26, %zmm22, %zmm0
	vmovdqu64	3520(%rsp), %zmm27      # 64-byte Reload
	vpaddd	%zmm27, %zmm22, %zmm1
	vmovd	%xmm1, %edi
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edi, %r11
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vmovd	%xmm0, %ebx
	movslq	%edx, %r13
	movslq	%ebx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm4
	vpextrd	$1, %xmm0, %edx
	movslq	%r8d, %rcx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$2, %xmm0, %edx
	movslq	960(%rsp), %rsi                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm0, %ebx
	movslq	704(%rsp), %rdx                 # 4-byte Folded Reload
	movq	%rdx, 1152(%rsp)                # 8-byte Spill
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm0, %xmm5
	vpinsrb	$3, (%rax,%rbx), %xmm4, %xmm4
	vmovd	%xmm5, %ebx
	movslq	896(%rsp), %rdx                 # 4-byte Folded Reload
	movq	%rdx, 704(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ebx
	movslq	%r12d, %rdx
	movq	%rdx, 960(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm5, %ebx
	movslq	%r10d, %r10
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$3, %xmm5, %r12d
	movslq	%r14d, %rbx
	movslq	%r12d, %r14
	vextracti32x4	$2, %zmm0, %xmm5
	vpinsrb	$7, (%rax,%r14), %xmm4, %xmm4
	vmovd	%xmm5, %r14d
	movslq	%ebp, %r12
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %r14
	vpextrd	$2, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rcx
	vextracti128	$1, %ymm1, %xmm6
	vpinsrb	$11, (%rax,%r10), %xmm3, %xmm3
	vmovd	%xmm6, %r10d
	movzbl	(%rax,%r11), %ebp
	vmovd	%ebp, %xmm7
	vpextrd	$1, %xmm6, %r11d
	movslq	%r15d, %r9
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %r15d
	vpinsrb	$11, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r10d, %r10
	vpextrd	$3, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm4, %xmm4
	movslq	%r11d, %r11
	vpextrd	$3, %xmm6, %esi
	vextracti32x4	$2, %zmm1, %xmm5
	vpinsrb	$12, (%rax,%r13), %xmm3, %xmm3
	vmovd	%xmm5, %edi
	vpinsrb	$2, (%rax,%r14), %xmm7, %xmm6
	vpextrd	$1, %xmm5, %r14d
	movslq	%r15d, %r15
	movslq	%edi, %r13
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%r14d, %r10
	vpinsrb	$5, (%rax,%r11), %xmm6, %xmm5
	vpextrd	$1, %xmm1, %r11d
	movq	1408(%rsp), %rdx                # 8-byte Reload
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	vmovd	%xmm1, %r14d
	vpinsrb	$6, (%rax,%r15), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %r15d
	movslq	%ecx, %rdx
	movslq	%r14d, %r8
	vpinsrb	$7, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %r14d
	movslq	%edi, %rcx
	movslq	%r11d, %r11
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$8, (%rax,%r13), %xmm5, %xmm1
	vmovd	%xmm0, %esi
	movslq	%r15d, %rdi
	movslq	%esi, %r15
	vpinsrb	$9, (%rax,%r10), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %esi
	movslq	%r14d, %rbp
	movq	%rbp, 896(%rsp)                 # 8-byte Spill
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$12, (%rax,%r15), %xmm4, %xmm0
	movslq	%ecx, %r14
	vpinsrb	$12, (%rax,%r8), %xmm1, %xmm4
	movq	2048(%rsp), %rcx                # 8-byte Reload
	addl	$3, %ecx
	vpinsrb	$13, (%rax,%rbx), %xmm3, %xmm3
	movq	136(%rsp), %rbx                 # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovgel	%ebx, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovlel	%ebx, %ecx
	movq	1152(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm2, %xmm2
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm6
	vpinsrb	$13, (%rax,%rsi), %xmm0, %xmm0
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm5
	vmovdqa64	%zmm6, %zmm12
	vmovd	%xmm5, %ecx
	movslq	%ecx, %r10
	vpinsrb	$13, (%rax,%r11), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %esi
	vpinsrb	$14, (%rax,%r12), %xmm3, %xmm3
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$3, %xmm5, %ecx
	movq	704(%rsp), %rsi                 # 8-byte Reload
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm6
	movslq	%ecx, %r15
	vextracti128	$1, %ymm5, %xmm2
	vpextrd	$1, %xmm2, %r11d
	vpinsrb	$14, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %ebx
	vpinsrb	$14, (%rax,%rdi), %xmm4, %xmm0
	vmovd	%xmm2, %esi
	vextracti32x4	$2, %zmm5, %xmm4
	vpextrd	$1, %xmm4, %edi
	vpinsrb	$15, (%rax,%r9), %xmm3, %xmm8
	vpextrd	$2, %xmm4, %r8d
	vextracti32x4	$3, %zmm5, %xmm2
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	movq	960(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm11
	vpextrd	$2, %xmm1, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm4, 960(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm5
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm5, %ecx
	movslq	%r11d, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %ecx
	movslq	%edx, %r11
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %r9d
	vpinsrb	$15, (%rax,%r14), %xmm7, %xmm10
	vmovd	%xmm4, %ecx
	movzbl	(%rax,%r10), %edx
	vmovd	%edx, %xmm4
	vpextrd	$2, %xmm2, %r10d
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%r12), %xmm4, %xmm4
	vpextrd	$3, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	movq	896(%rsp), %rbx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rbx), %xmm0, %xmm9
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbx), %xmm3, %xmm0
	vmovd	%xmm2, %r12d
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%r13), %xmm4, %xmm3
	vmovd	%xmm2, %ebx
	movslq	%edi, %r13
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%r8d, %r8
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ebp
	vextracti32x4	$3, %zmm1, %xmm5
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r11), %xmm3, %xmm1
	vpextrd	$1, %xmm5, %r15d
	vpinsrb	$8, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, 896(%rsp)            # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm5, %ecx
	vmovdqu64	%zmm12, 1728(%rsp)      # 64-byte Spill
	vpaddd	%zmm26, %zmm12, %zmm2
	vpaddd	%zmm27, %zmm12, %zmm3
	vmovd	%xmm3, %esi
	vpinsrb	$10, (%rax,%r8), %xmm1, %xmm1
	vpextrd	$1, %xmm3, %edx
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm4
	movslq	%esi, %r13
	vpextrd	$3, %xmm3, %esi
	vmovd	%xmm2, %ebp
	vpextrd	$3, %xmm5, %r14d
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpextrd	$1, %xmm2, %ebp
	movslq	%edx, %rdi
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%ebx, %rbp
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %edx
	movslq	%esi, %r11
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm5
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm5, %edx
	movslq	%r12d, %r12
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %esi
	movslq	%ecx, %r8
	movslq	%esi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %ecx
	movslq	960(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %ecx
	movslq	%r9d, %r9
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm2, %xmm5
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm5, %ebx
	movslq	%r10d, %rcx
	movq	%rcx, 1152(%rsp)                # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %ebx
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	movq	%rcx, 960(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %ebx
	movslq	%r15d, %r15
	vextracti128	$1, %ymm3, %xmm6
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm0, %xmm0
	vmovd	%xmm6, %r10d
	movslq	896(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	%r10d, %r10
	movzbl	(%rax,%r13), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$1, %xmm6, %ecx
	movslq	%r14d, %rsi
	movq	%rsi, 704(%rsp)                 # 8-byte Spill
	movslq	%ecx, %r13
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edi
	movslq	%edi, %r14
	vpinsrb	$2, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %ebp
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm3, %xmm6
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	vpinsrb	$3, (%rax,%r11), %xmm7, %xmm5
	vpextrd	$1, %xmm6, %edi
	movslq	%ebp, %rbp
	movslq	%edx, %r11
	vpinsrb	$4, (%rax,%r10), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %ecx
	movslq	%edi, %r10
	vpinsrb	$5, (%rax,%r13), %xmm5, %xmm5
	vpextrd	$3, %xmm6, %edi
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%ecx, %r13
	vpinsrb	$6, (%rax,%r14), %xmm5, %xmm5
	vpextrd	$1, %xmm3, %ecx
	vpinsrb	$12, (%rax,%r8), %xmm4, %xmm4
	vmovd	%xmm3, %edx
	vpinsrb	$7, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %ebp
	movslq	%edi, %rdi
	movslq	%edx, %r8
	vpinsrb	$8, (%rax,%r11), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %edx
	movslq	%ecx, %rcx
	movslq	%ebp, %rsi
	movq	%rsi, 1408(%rsp)                # 8-byte Spill
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$9, (%rax,%r10), %xmm5, %xmm3
	vmovd	%xmm2, %ebp
	movslq	%edx, %rdx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	movslq	%ebp, %rdx
	vpinsrb	$10, (%rax,%r13), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$11, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edi
	vpinsrb	$12, (%rax,%r12), %xmm1, %xmm1
	movslq	%edi, %r12
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rsi
	movq	%rsi, 1088(%rsp)                # 8-byte Spill
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm2
	vpinsrb	$12, (%rax,%r8), %xmm3, %xmm3
	movl	1664(%rsp), %edx                # 4-byte Reload
	testl	%edx, %edx
	movl	$1, %edi
	cmovlel	%edi, %edx
	vpinsrb	$13, (%rax,%r15), %xmm4, %xmm4
	decl	%edx
	imull	40(%rsp), %edx                  # 4-byte Folded Reload
	subl	-8(%rsp), %edx                  # 4-byte Folded Reload
	vpbroadcastd	%edx, %zmm6
	vpinsrb	$13, (%rax,%r9), %xmm1, %xmm5
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm0
	vmovdqa64	%zmm6, %zmm14
	vmovd	%xmm0, %edx
	movslq	%edx, %r15
	vpinsrb	$13, (%rax,%rbp), %xmm2, %xmm6
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %r8d
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm7
	movslq	%edx, %r13
	vpextrd	$3, %xmm0, %r10d
	vextracti128	$1, %ymm0, %xmm2
	vpinsrb	$14, (%rax,%rbx), %xmm4, %xmm13
	vpextrd	$1, %xmm2, %r11d
	vpextrd	$2, %xmm2, %r14d
	movq	1152(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm12
	vpextrd	$3, %xmm2, 1280(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm3
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm3, %r9d
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm5
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$14, (%rax,%r12), %xmm6, %xmm6
	movslq	%r8d, %r8
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	1408(%rsp), %rdx                # 8-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm7, %xmm15
	vpextrd	$3, %xmm1, %ebp
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$3, %xmm3, 1152(%rsp)           # 4-byte Folded Spill
	movslq	%r10d, %rdi
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$2, %xmm4, %ebp
	movq	704(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm13, %xmm7
	vmovdqa	%xmm7, 1408(%rsp)               # 16-byte Spill
	vmovd	%xmm2, %r10d
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm2
	vpextrd	$3, %xmm4, %ebp
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$7, (%rax,%rbp), %xmm2, %xmm2
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %r12d
	movslq	%r10d, %rsi
	movzbl	(%rax,%r15), %ebp
	vmovd	%ebp, %xmm5
	vpextrd	$2, %xmm0, %ebp
	movq	960(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm30
	vmovd	%xmm3, %r15d
	vpinsrb	$1, (%rax,%r13), %xmm5, %xmm3
	vpextrd	$3, %xmm0, %r10d
	movslq	%r11d, %r11
	movslq	%r15d, %rdx
	vpinsrb	$2, (%rax,%r8), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %r8d
	movslq	%r14d, %rcx
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %r15d
	movq	1088(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm17
	vmovd	%xmm0, %edi
	vpinsrb	$4, (%rax,%rsi), %xmm3, %xmm0
	vpextrd	$3, %xmm4, %r13d
	movslq	1280(%rsp), %rsi                # 4-byte Folded Reload
	movslq	%edi, %rdi
	movq	%rdi, 960(%rsp)                 # 8-byte Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$5, (%rax,%r11), %xmm0, %xmm0
	vmovd	%xmm1, %edi
	movslq	%r9d, %r9
	movslq	%edi, %rdi
	movq	%rdi, 704(%rsp)                 # 8-byte Spill
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %edi
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %r11d
	movq	896(%rsp), %rsi                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rsi), %xmm15, %xmm13
	vpextrd	$3, %xmm1, %r14d
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm4
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm1
	vmovdqu64	%zmm14, 1664(%rsp)      # 64-byte Spill
	vpaddd	%zmm26, %zmm14, %zmm2
	vpaddd	%zmm27, %zmm14, %zmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r8
	vmovd	%xmm2, %ecx
	movslq	%ebx, %rdx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %esi
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %ebx
	movslq	%r12d, %r12
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %ebx
	movslq	%ebp, %rsi
	movq	%rsi, 896(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm2, %xmm5
	vpinsrb	$3, (%rax,%rbx), %xmm0, %xmm0
	vmovd	%xmm5, %ebx
	movslq	%r10d, %rsi
	movq	%rsi, 512(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %ebp
	movslq	%r15d, %r15
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %ebp
	movslq	%r13d, %r10
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm2, %xmm5
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm3, %ebp
	vpinsrb	$9, (%rax,%r9), %xmm4, %xmm4
	movslq	%ebp, %r9
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%r8), %ebx
	movslq	%r11d, %r13
	vmovd	%ebx, %xmm6
	vpextrd	$1, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %r8
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%r15), %xmm1, %xmm1
	vmovd	%xmm7, %ebp
	vpinsrb	$1, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%r14d, %r9
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %r14d
	vpinsrb	$10, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ebp, %r11
	vpextrd	$2, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm0, %xmm0
	movslq	%ebx, %r15
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$2, %zmm3, %xmm7
	vpinsrb	$11, (%rax,%r10), %xmm1, %xmm1
	vmovd	%xmm7, %edx
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%r14d, %rbp
	movslq	%edx, %r8
	vpinsrb	$4, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%r15), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %r10d
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%esi, %r11
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm3, %ebp
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %r14
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm3, %edx
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm5
	vpextrd	$2, %xmm3, %ebx
	movslq	%r10d, %rcx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%r8), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %esi
	movslq	%ebp, %r10
	movslq	%ebx, %rbp
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$9, (%rax,%r11), %xmm5, %xmm3
	vmovd	%xmm2, %ebx
	movslq	%esi, %rsi
	movq	%rsi, 1280(%rsp)                # 8-byte Spill
	movslq	%ebx, %r8
	vpinsrb	$10, (%rax,%r14), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %ecx
	movq	704(%rsp), %rsi                 # 8-byte Reload
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm5
	movslq	%ecx, %r14
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 416(%rsp)                 # 8-byte Spill
	movq	960(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm2
	movq	2048(%rsp), %r11                # 8-byte Reload
	leal	2(%r11), %ecx
	movq	136(%rsp), %rsi                 # 8-byte Reload
	cmpl	%esi, %ecx
	cmovgel	%esi, %ecx
	testl	%ecx, %ecx
	vpinsrb	$12, (%rax,%r8), %xmm0, %xmm0
	movl	$1, %esi
	cmovlel	%esi, %ecx
	decl	%ecx
	movq	40(%rsp), %r15                  # 8-byte Reload
	imull	%r15d, %ecx
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm4
	vpbroadcastd	%ecx, %zmm6
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm3
	vmovdqa64	%zmm6, %zmm24
	vmovd	%xmm3, %ecx
	vpinsrb	$13, (%rax,%rdi), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm3, %edx
	movslq	%edx, %r8
	vpinsrb	$13, (%rax,%r12), %xmm2, %xmm15
	vpextrd	$2, %xmm3, %edi
	vpextrd	$3, %xmm3, %edx
	vpinsrb	$13, (%rax,%rbx), %xmm0, %xmm14
	vextracti128	$1, %ymm3, %xmm2
	vpextrd	$1, %xmm2, %r12d
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%r10), %xmm4, %xmm16
	vpextrd	$2, %xmm1, %ebx
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm2, %esi
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm2, %r10d
	vpinsrb	$14, (%rax,%r13), %xmm5, %xmm5
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, %r13d
	vextracti128	$1, %ymm1, %xmm6
	movq	896(%rsp), %rbx                 # 8-byte Reload
	vpinsrb	$14, (%rax,%rbx), %xmm15, %xmm4
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$2, %xmm0, 672(%rsp)            # 4-byte Folded Spill
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm3, %xmm23
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$3, %xmm0, 1088(%rsp)           # 4-byte Folded Spill
	vpinsrb	$1, (%rax,%r8), %xmm3, %xmm3
	vpextrd	$1, %xmm23, 1152(%rsp)          # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%r14), %xmm14, %xmm14
	vmovd	%xmm2, %ecx
	vpinsrb	$2, (%rax,%rdi), %xmm3, %xmm2
	vpextrd	$2, %xmm23, 896(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movslq	%r12d, %rbx
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm23, 704(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm6, %ecx
	vpinsrb	$14, (%rax,%rbp), %xmm16, %xmm15
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm6, %ebp
	vextracti32x4	$2, %zmm1, %xmm3
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm18
	vpextrd	$1, %xmm3, %edi
	vpinsrb	$5, (%rax,%rbx), %xmm2, %xmm6
	vpinsrb	$15, (%rax,%r9), %xmm5, %xmm25
	vmovd	%xmm0, %edx
	vpextrd	$2, %xmm3, %r8d
	vextracti32x4	$3, %zmm1, %xmm2
	movq	512(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm12
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm1
	vmovdqu64	%zmm24, 960(%rsp)       # 64-byte Spill
	vpaddd	%zmm26, %zmm24, %zmm29
	vmovd	%xmm29, %ecx
	vpextrd	$3, %xmm3, 640(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, 512(%rsp)            # 4-byte Folded Spill
	movq	416(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm14, %xmm14
	vpaddd	%zmm27, %zmm24, %zmm16
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	movq	1280(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$15, (%rax,%rsi), %xmm15, %xmm15
	vpextrd	$3, %xmm29, %esi
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, 416(%rsp)            # 4-byte Folded Spill
	movslq	%esi, %rcx
	vextracti32x4	$1, %ymm29, %xmm5
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm5, %ecx
	movslq	%ebp, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %ecx
	vpinsrb	$7, (%rax,%rsi), %xmm18, %xmm6
	vmovd	%xmm16, %esi
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, 1280(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$1, %xmm16, %ecx
	movslq	%r10d, %rsi
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm3, %ecx
	vpextrd	$3, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm5
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$1, %ymm16, %xmm3
	vpinsrb	$8, (%rax,%rdx), %xmm1, %xmm6
	vmovd	%xmm3, %edx
	vpinsrb	$2, (%rax,%rsi), %xmm7, %xmm1
	vpextrd	$1, %xmm3, %esi
	vinserti128	$1, %xmm8, %ymm11, %ymm4
	vmovdqu	%ymm4, 1920(%rsp)               # 32-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm3, %ecx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm3, %edx
	vextracti32x4	$2, %zmm16, %xmm7
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm7, %esi
	vextracti32x4	$2, %zmm29, %xmm11
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm8
	vmovd	%xmm7, %ecx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm0
	vpextrd	$2, %xmm7, 1792(%rsp)           # 4-byte Folded Spill
	movslq	%edi, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movl	856(%rsp), %ecx                 # 4-byte Reload
	cmpl	%ecx, %r11d
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm4
	cmovll	%r11d, %ecx
	movl	%ecx, %edx
	sarl	$31, %edx
	andnl	%ecx, %edx, %ecx
	vpextrd	$3, %xmm7, 256(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm10, %ymm9, %ymm28
	imull	%r15d, %ecx
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm1
	vpextrd	$1, %xmm11, %edx
	movslq	%r13d, %rdi
	movslq	%esi, %rcx
	movslq	%edx, %rdx
	vpaddd	%zmm19, %zmm1, %zmm24
	vpaddd	%zmm20, %zmm1, %zmm31
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm5
	vmovd	%xmm24, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$1, %xmm24, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm31, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm31, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdx), %xmm8, %xmm9
	vpextrd	$2, %xmm31, %edx
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm8
	vmovd	%xmm23, 1856(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm24, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm2, 160(%rsp)                # 4-byte Folded Spill
	movslq	%edx, %rcx
	vmovdqa64	%zmm1, %zmm19
	vpaddd	%zmm26, %zmm1, %zmm23
	vpaddd	%zmm27, %zmm1, %zmm27
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm23, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm10
	vmovd	%xmm27, %edx
	vmovdqu64	2176(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm22, %zmm2
	vmovdqu64	1024(%rsp), %zmm3       # 64-byte Reload
	vpaddd	%zmm3, %zmm22, %zmm0
	movslq	%edx, %rsi
	vextracti32x4	$1, %ymm0, %xmm26
	vmovd	%xmm26, 480(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm26, 1600(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 176(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm26, 576(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm26
	vmovd	%xmm26, 384(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm26, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm26, -72(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm2, %xmm26
	movzbl	(%rax,%rsi), %esi
	vpextrd	$1, %xmm26, -60(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm26, %r13d
	vmovd	%xmm26, %r15d
	vextracti32x4	$2, %zmm2, %xmm26
	vpextrd	$1, %xmm26, %r12d
	vpextrd	$2, %xmm26, %r14d
	vmovd	%xmm26, %ebx
	vpextrd	$3, %xmm26, %ebp
	vmovd	%esi, %xmm26
	vpextrd	$1, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm26, %xmm26
	movslq	%r8d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm1
	vmovdqa	%xmm1, 320(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm26, %xmm26
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %edi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	movzbl	(%rax,%rsi), %esi
	vmovd	%xmm2, %r8d
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %r10d
	vpextrd	$3, %xmm2, %r11d
	vmovd	%esi, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r15d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vextracti32x4	$3, %zmm0, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r9d
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %ebx
	vmovd	%ecx, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	vinserti32x4	$1, 1408(%rsp), %ymm30, %ymm2 # 16-byte Folded Reload
	vmovdqu	%ymm2, 576(%rsp)                # 32-byte Spill
	vmovdqu64	1216(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm19, %zmm30
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm30, %ecx
	vinserti32x4	$1, %xmm17, %ymm13, %ymm4
	vmovdqu	%ymm4, 384(%rsp)                # 32-byte Spill
	movslq	%ecx, %rbp
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm13
	vinserti32x4	$1, %xmm25, %ymm12, %ymm0
	vmovdqu	%ymm0, 1408(%rsp)               # 32-byte Spill
	vpaddd	%zmm2, %zmm22, %zmm1
	vmovdqu64	1536(%rsp), %zmm0       # 64-byte Reload
	vpaddd	%zmm0, %zmm22, %zmm4
	vmovdqa64	%zmm0, %zmm2
	vextracti128	$1, %ymm4, %xmm0
	vmovd	%xmm0, 480(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 176(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 352(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm0
	movzbl	(%rax,%rbp), %ebp
	vpextrd	$1, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, %r12d
	vmovd	%xmm0, %r15d
	vextracti32x4	$2, %zmm1, %xmm0
	vpextrd	$1, %xmm0, %r13d
	vpextrd	$2, %xmm0, %r14d
	vmovd	%xmm0, %r11d
	vpextrd	$3, %xmm0, %ebx
	vmovd	%ebp, %xmm0
	vpextrd	$1, %xmm30, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm30, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rbp), %ebp
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %esi
	vmovd	%ebp, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm25
	vextracti32x4	$2, %zmm4, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edi
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	480(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	1600(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	176(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	352(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	movslq	%esi, %rdx
	vextracti32x4	$3, %zmm4, %xmm4
	movslq	672(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$10, (%rax,%rsi), %xmm5, %xmm12
	vpextrd	$2, %xmm4, %esi
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %edx
	vmovd	%xmm4, %edi
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm9, %xmm9
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	vextracti32x4	$3, %zmm16, %xmm4
	movslq	%ecx, %rcx
	movslq	1792(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm31, %ecx
	vpinsrb	$10, (%rax,%rdi), %xmm8, %xmm21
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm24, %edi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm25, %ymm1, %ymm25
	vinserti128	$1, %xmm14, %ymm15, %ymm1
	vmovdqu	%ymm1, 672(%rsp)                # 32-byte Spill
	vpaddd	1472(%rsp), %zmm22, %zmm1       # 64-byte Folded Reload
	vextracti128	$1, %ymm1, %xmm5
	vpextrd	$1, %xmm5, 1792(%rsp)           # 4-byte Folded Spill
	vpinsrb	$3, (%rax,%rdi), %xmm6, %xmm15
	vpextrd	$2, %xmm5, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, 1600(%rsp)           # 4-byte Folded Spill
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm16
	vmovd	%xmm5, 176(%rsp)                # 4-byte Folded Spill
	vpaddd	%zmm2, %zmm19, %zmm14
	vmovdqa64	%zmm2, %zmm8
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm4, -96(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm4, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm4, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 144(%rsp)            # 4-byte Folded Spill
	vmovd	%ecx, %xmm4
	vpextrd	$3, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm10, %xmm10
	vpextrd	$1, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm5, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -60(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm5, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm27, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm26, %xmm5
	vpextrd	$2, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm6
	vmovdqu64	3648(%rsp), %zmm26      # 64-byte Reload
	vpaddd	%zmm26, %zmm22, %zmm7
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -84(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 208(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, 80(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 240(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm7, %xmm1
	vpextrd	$1, %xmm1, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -100(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %r12d
	vextracti32x4	$2, %zmm7, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %ebx
	vmovd	%xmm1, %r11d
	vpaddd	%zmm18, %zmm19, %zmm4
	vmovd	%xmm4, %r13d
	movslq	%r13d, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%xmm7, %r13d
	vpextrd	$1, %xmm7, %ecx
	vpextrd	$2, %xmm7, %edx
	vpextrd	$3, %xmm7, %esi
	vextracti32x4	$3, %zmm7, %xmm1
	vmovd	%xmm1, %edi
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%ebp, %xmm1
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r13d, %rbp
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm7
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r12d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vpmovzxbw	%ymm13, %zmm13          # zmm13 = ymm13[0],zero,ymm13[1],zero,ymm13[2],zero,ymm13[3],zero,ymm13[4],zero,ymm13[5],zero,ymm13[6],zero,ymm13[7],zero,ymm13[8],zero,ymm13[9],zero,ymm13[10],zero,ymm13[11],zero,ymm13[12],zero,ymm13[13],zero,ymm13[14],zero,ymm13[15],zero,ymm13[16],zero,ymm13[17],zero,ymm13[18],zero,ymm13[19],zero,ymm13[20],zero,ymm13[21],zero,ymm13[22],zero,ymm13[23],zero,ymm13[24],zero,ymm13[25],zero,ymm13[26],zero,ymm13[27],zero,ymm13[28],zero,ymm13[29],zero,ymm13[30],zero,ymm13[31],zero
	vpsllw	$2, %zmm13, %zmm13
	vpmovzxbw	%ymm25, %zmm22          # zmm22 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	vpaddw	%zmm22, %zmm22, %zmm22
	vpaddw	%zmm22, %zmm13, %zmm25
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm13
	vinserti128	$1, %xmm7, %ymm2, %ymm0
	vpmovzxbw	1920(%rsp), %zmm2       # 32-byte Folded Reload
                                        # zmm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm2, %zmm2, %zmm7
	vmovdqu64	%zmm7, 1792(%rsp)       # 64-byte Spill
	vpmovzxbw	%ymm28, %zmm2           # zmm2 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddw	%zmm2, %zmm2, %zmm2
	vmovdqu64	%zmm2, 1920(%rsp)       # 64-byte Spill
	vpaddw	%zmm7, %zmm2, %zmm2
	vpaddw	%zmm25, %zmm2, %zmm2
	vpextrd	$3, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm28
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm6
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm2, %zmm20
	vpaddd	%zmm3, %zmm19, %zmm17
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, %r9d
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm25
	vextracti32x4	$3, %zmm29, %xmm0
	vmovd	%xmm0, -12(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm31, %xmm0
	vmovd	%xmm0, 16(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm31, %xmm0
	vextracti32x4	$3, %zmm31, %xmm2
	vpextrd	$1, %xmm0, 208(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 64(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm0, 80(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm24, %xmm0
	vpextrd	$1, %xmm2, 176(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 352(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 480(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm24, %xmm2
	vpextrd	$1, %xmm0, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 4(%rsp)                  # 4-byte Folded Spill
	vextracti32x4	$1, %ymm27, %xmm0
	vpextrd	$1, %xmm2, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 48(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm2, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -60(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm23, %xmm2
	vpextrd	$1, %xmm0, 36(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm0, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm14, %xmm0
	vpextrd	$1, %xmm2, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, (%rsp)                   # 4-byte Folded Spill
	vextracti32x4	$1, %ymm30, %xmm2
	vpextrd	$1, %xmm0, 32(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1728(%rsp), %zmm1       # 64-byte Reload
	vpaddd	%zmm3, %zmm1, %zmm0
	vpextrd	$1, %xmm2, 24(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm2, -4(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -52(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm17, %xmm2
	vpextrd	$1, %xmm2, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, -100(%rsp)               # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm2
	vpextrd	$1, %xmm2, 2624(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2752(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, 2496(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2816(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, 2880(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, 2688(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2368(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm1, %zmm2
	vpextrd	$1, %xmm0, 132(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 280(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 288(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r10d
	vextracti32x4	$3, %zmm0, %xmm0
	vpextrd	$1, %xmm0, 2944(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 2304(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm0, 2560(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 2432(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	vpextrd	$1, %xmm0, 884(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 304(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %r11d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$1, %xmm2, %r15d
	vpextrd	$2, %xmm2, %r12d
	vpextrd	$3, %xmm2, %r13d
	vmovd	%xmm2, %ecx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm0, 888(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 300(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r14d
	vpextrd	$3, %xmm0, 892(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm1, %zmm0
	vpextrd	$1, %xmm2, %edi
	vmovd	%xmm2, %edx
	vpextrd	$2, %xmm2, %r8d
	vpextrd	$3, %xmm2, %ebx
	vextracti128	$1, %ymm0, %xmm2
	vpextrd	$1, %xmm2, 128(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 124(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 120(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 808(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, 3136(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 464(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 816(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 832(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm4, %xmm2
	movslq	%ecx, %rcx
	vmovd	%xmm2, 864(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 880(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 236(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 116(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1216(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm1, %zmm2
	vmovd	%xmm0, 792(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 784(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 308(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 800(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, 456(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 624(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 824(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 632(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, 876(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 872(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 868(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %ebp
	vmovd	%ecx, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	304(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	884(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	888(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	300(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	892(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm3
	vpextrd	$1, %xmm3, %r12d
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r14d
	vmovd	%xmm3, %r13d
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm29
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	320(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm31
	movslq	%edi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm12, %xmm12
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm9, %xmm9
	movslq	%ebx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm11
	movslq	%r10d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %r8d
	vextracti32x4	$3, %zmm2, %xmm0
	vmovd	%xmm0, %r9d
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %r11d
	vmovd	%ecx, %xmm0
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm21, %xmm2
	movslq	132(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm15, %xmm3
	movslq	280(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm16, %xmm6
	movslq	288(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm10, %xmm8
	movslq	2496(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	movslq	2624(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm13, %xmm10
	movslq	2752(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm28, %xmm13
	movslq	2816(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	864(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm29, %xmm15
	movslq	2688(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm31, %xmm16
	movslq	2880(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	1856(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm12, %xmm12
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm9, %xmm9
	movslq	2368(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm29
	movslq	2560(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm31
	movslq	2944(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm8, %xmm8
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm28
	movslq	%esi, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm5, %xmm5
	movslq	%edi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm10, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm13, %xmm3
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	880(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm15, %xmm10
	movslq	876(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm13
	movslq	872(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm12
	movslq	868(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r13d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm29, %xmm15
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm31, %xmm16
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm29
	movslq	%r14d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm8, %xmm8
	movslq	%r9d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm5, %xmm5
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r11d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	792(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	784(%rsp), %rdx                 # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rdx), %xmm7, %xmm7
	movslq	236(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm10, %xmm6
	movslq	308(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm10
	movslq	800(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm12, %xmm12
	movslq	808(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm9, %xmm9
	movslq	128(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm15, %xmm15
	movslq	124(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm16, %xmm31
	movslq	120(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm13
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm29, %xmm7
	movslq	816(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm13, %xmm13
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm8, %xmm29
	movslq	3136(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm13, %xmm8
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm13
	movslq	464(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm8, %xmm5
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm16
	movslq	832(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm5
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm2
	movslq	456(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm3
	movslq	116(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm8
	movslq	624(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm10, %xmm5
	movslq	824(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm6
	vinserti32x4	$1, %xmm11, %ymm28, %ymm10
	movslq	632(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm17, %ecx
	vinserti128	$1, %xmm0, %ymm3, %ymm11
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm25, %xmm3
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm9, %xmm0
	vmovdqa	%xmm0, 96(%rsp)                 # 16-byte Spill
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm15, %xmm28
	vextracti32x4	$3, %zmm24, %xmm24
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm24, 1856(%rsp)              # 4-byte Folded Spill
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm15
	vpextrd	$1, %xmm24, 416(%rsp)           # 4-byte Folded Spill
	vpmovzxbw	%ymm10, %zmm9           # zmm9 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpsllw	$2, %zmm9, %zmm9
	vpmovzxbw	%ymm11, %zmm10          # zmm10 = ymm11[0],zero,ymm11[1],zero,ymm11[2],zero,ymm11[3],zero,ymm11[4],zero,ymm11[5],zero,ymm11[6],zero,ymm11[7],zero,ymm11[8],zero,ymm11[9],zero,ymm11[10],zero,ymm11[11],zero,ymm11[12],zero,ymm11[13],zero,ymm11[14],zero,ymm11[15],zero,ymm11[16],zero,ymm11[17],zero,ymm11[18],zero,ymm11[19],zero,ymm11[20],zero,ymm11[21],zero,ymm11[22],zero,ymm11[23],zero,ymm11[24],zero,ymm11[25],zero,ymm11[26],zero,ymm11[27],zero,ymm11[28],zero,ymm11[29],zero,ymm11[30],zero,ymm11[31],zero
	vpaddw	%zmm10, %zmm10, %zmm10
	vpaddw	%zmm10, %zmm9, %zmm11
	vextracti32x4	$2, %zmm27, %xmm3
	vextracti32x4	$3, %zmm27, %xmm0
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	vmovdqu	%ymm5, 1152(%rsp)               # 32-byte Spill
	vextracti32x4	$2, %zmm23, %xmm5
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm15, %xmm10
	vpextrd	$2, %xmm24, 1280(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm24, 896(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm14, %xmm6
	vmovd	%xmm3, %r8d
	vpextrd	$1, %xmm3, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 640(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm30, %xmm3
	vmovd	%xmm0, 160(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 512(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 1088(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm17, %xmm0
	vmovd	%xmm5, %r9d
	vpextrd	$1, %xmm5, -12(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm5, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -108(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm4, %xmm9
	vmovd	%xmm6, %r10d
	vpextrd	$1, %xmm6, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 256(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm26, %zmm19, %zmm15
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, 36(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -112(%rsp)           # 4-byte Folded Spill
	vmovdqu64	1472(%rsp), %zmm22      # 64-byte Reload
	vpaddd	%zmm22, %zmm19, %zmm5
	vmovdqa64	%zmm19, %zmm25
	vmovd	%xmm0, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm5, %xmm0
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vmovd	%xmm9, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm9, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm9, -56(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, -84(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm15, %xmm3
	vmovd	%xmm0, %r14d
	vpextrd	$1, %xmm0, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 144(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm15, %xmm0
	vmovd	%xmm3, %ebp
	vpextrd	$1, %xmm3, %ebx
	vpextrd	$2, %xmm3, %edx
	vpextrd	$3, %xmm3, %esi
	vpaddd	%zmm22, %zmm1, %zmm3
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 4(%rsp)                  # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm0
	vpextrd	$1, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 32(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm3, %xmm6
	vpextrd	$1, %xmm6, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm15, %ecx
	vmovd	%xmm6, 28(%rsp)                 # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm31, %xmm24
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm27
	movslq	%ebx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm29, %xmm9
	movslq	%edx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm13, %xmm13
	movslq	%esi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm16, %xmm16
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpaddd	%zmm26, %zmm1, %zmm21
	vmovd	%xmm3, 2432(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, 2368(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, (%rsp)               # 4-byte Folded Spill
	vextracti32x4	$3, %zmm3, %xmm3
	movzbl	(%rax,%rcx), %edx
	vpextrd	$1, %xmm3, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 1728(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm3, 16(%rsp)                 # 4-byte Folded Spill
	vextracti32x4	$1, %ymm21, %xmm3
	vpextrd	$1, %xmm3, %ebp
	vpextrd	$2, %xmm3, %r15d
	vmovd	%xmm3, %ebx
	vpextrd	$3, %xmm3, %r13d
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$2, %zmm21, %xmm6
	vpextrd	$1, %xmm6, %r12d
	vpextrd	$2, %xmm6, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 2304(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm5, %ecx
	vmovd	%xmm6, %r11d
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r10d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm29
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm31
	vmovd	%xmm21, %ecx
	vpextrd	$1, %xmm21, %r14d
	vpextrd	$2, %xmm21, %edx
	vpextrd	$3, %xmm21, %esi
	vextracti32x4	$3, %zmm21, %xmm3
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, %r8d
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r10d
	vmovd	%ecx, %xmm3
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm8, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm10, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm24, %xmm6
	movslq	%ebx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm27, %xmm8
	movslq	%ebp, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r15d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm13, %xmm10
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm16, %xmm13
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm29, %xmm16
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm27
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm24
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm29
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm8, %xmm8
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm21
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	movslq	2368(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm10, %xmm3
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm13, %xmm13
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm16, %xmm2
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm27, %xmm6
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm24, %xmm16
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm29, %xmm24
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm8, %xmm10
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm9, %xmm8
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm13, %xmm9
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm13
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm1
	vmovdqa	%xmm1, 256(%rsp)                # 16-byte Spill
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm16, %xmm1
	vmovdqa	%xmm1, 640(%rsp)                # 16-byte Spill
	vinserti32x4	$1, %xmm21, %ymm0, %ymm7
	vpmovzxbw	576(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm29
	vpmovzxbw	384(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm27
	vpaddw	%zmm29, %zmm27, %zmm0
	vpaddw	%zmm11, %zmm0, %zmm11
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1856(%rsp), %rdx                # 4-byte Folded Reload
	movslq	160(%rsp), %rsi                 # 4-byte Folded Reload
	vextracti32x4	$3, %zmm23, %xmm0
	movslq	-80(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm31, %xmm2
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	movslq	320(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	144(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rdx), %xmm24, %xmm1
	vmovdqa	%xmm1, 1856(%rsp)               # 16-byte Spill
	vpmovzxbw	%ymm7, %zmm7            # zmm7 = ymm7[0],zero,ymm7[1],zero,ymm7[2],zero,ymm7[3],zero,ymm7[4],zero,ymm7[5],zero,ymm7[6],zero,ymm7[7],zero,ymm7[8],zero,ymm7[9],zero,ymm7[10],zero,ymm7[11],zero,ymm7[12],zero,ymm7[13],zero,ymm7[14],zero,ymm7[15],zero,ymm7[16],zero,ymm7[17],zero,ymm7[18],zero,ymm7[19],zero,ymm7[20],zero,ymm7[21],zero,ymm7[22],zero,ymm7[23],zero,ymm7[24],zero,ymm7[25],zero,ymm7[26],zero,ymm7[27],zero,ymm7[28],zero,ymm7[29],zero,ymm7[30],zero,ymm7[31],zero
	vpaddw	%zmm7, %zmm7, %zmm7
	vpaddw	%zmm7, %zmm11, %zmm7
	vpcmpltuw	%zmm7, %zmm20, %k1
	vpsubw	%zmm7, %zmm20, %zmm31
	vpsubw	%zmm20, %zmm7, %zmm31 {%k1}
	vmovdqu64	1664(%rsp), %zmm19      # 64-byte Reload
	vmovdqu64	1024(%rsp), %zmm12      # 64-byte Reload
	vpaddd	%zmm12, %zmm19, %zmm7
	vextracti128	$1, %ymm7, %xmm11
	vpextrd	$1, %xmm11, 384(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm1
	vmovdqa	%xmm1, 160(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm11, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm11, -72(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm8, %xmm21
	vmovd	%xmm11, 144(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm8
	vpextrd	$1, %xmm8, -60(%rsp)            # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm23
	vmovd	%xmm8, 208(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm8, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm8, 48(%rsp)             # 4-byte Folded Spill
	vmovdqu64	2176(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm19, %zmm3
	vpextrd	$1, %xmm7, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, %r8d
	vextracti32x4	$3, %zmm7, %xmm7
	vpextrd	$1, %xmm7, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 80(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm7, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm7
	vpextrd	$1, %xmm7, %r9d
	vmovd	%xmm7, %esi
	vpextrd	$2, %xmm7, %r11d
	vpextrd	$3, %xmm7, %r10d
	vextracti32x4	$2, %zmm3, %xmm7
	vpextrd	$1, %xmm3, -12(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -16(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %ecx
	vextracti32x4	$3, %zmm3, %xmm3
	vpextrd	$1, %xmm7, %ebp
	vpextrd	$2, %xmm7, %edi
	vmovd	%xmm7, 36(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm7, %ebx
	vmovdqu64	1536(%rsp), %zmm11      # 64-byte Reload
	vpaddd	%zmm11, %zmm19, %zmm7
	vpextrd	$1, %xmm3, 20(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm3, -4(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 16(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 32(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm7, %xmm3
	vpextrd	$1, %xmm3, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -92(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, -68(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm3
	vpextrd	$1, %xmm3, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -76(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm19, %zmm3
	vmovdqa64	%zmm18, %zmm20
	vpextrd	$1, %xmm7, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, -44(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -32(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm7, %xmm7
	vpextrd	$1, %xmm7, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -24(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, -28(%rsp)                # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm7
	vpextrd	$1, %xmm7, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 24(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm7, 2432(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 12(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm7
	vpextrd	$1, %xmm7, 8(%rsp)              # 4-byte Folded Spill
	vmovd	%xmm7, 2368(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 4(%rsp)              # 4-byte Folded Spill
	vpextrd	$3, %xmm7, (%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm5, %xmm7
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm2, %xmm24
	vmovd	%xmm3, %edx
	vpextrd	$1, %xmm3, 2944(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 2304(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$3, %zmm3, %xmm2
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 576(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1728(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm26, %zmm19, %zmm3
	vmovd	%xmm2, 2688(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 2752(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2816(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2880(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm0
	vmovd	%xmm0, 2624(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 2560(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 2496(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 288(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm0, %r13d
	vpextrd	$3, %xmm0, %r15d
	vmovd	%xmm0, %r14d
	vmovd	%ecx, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm18
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %r8d
	vpextrd	$3, %xmm3, %r9d
	vextracti32x4	$3, %zmm3, %xmm2
	vmovd	%xmm2, %r10d
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %r11d
	vpextrd	$3, %xmm2, %ebp
	vmovd	%ecx, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm10
	movslq	%edx, %rcx
	movslq	2944(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	2368(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	2688(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	2752(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	2816(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	2880(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-40(%rsp), %rdx                 # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm0
	movslq	%esi, %rcx
	movslq	%edi, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	2624(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	2560(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	2496(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	288(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm14, %xmm3
	movslq	%ebx, %rcx
	movslq	%r11d, %rdx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm3, 208(%rsp)            # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm3, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 384(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %ecx
	vextracti32x4	$3, %zmm30, %xmm14
	vmovd	%xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm9, %xmm16
	movslq	%ebp, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm30
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm13, %xmm2
	vinserti32x4	$1, 96(%rsp), %ymm28, %ymm28 # 16-byte Folded Reload
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm3
	vinserti32x4	$1, %xmm18, %ymm10, %ymm10
	vinserti32x4	$1, %xmm1, %ymm0, %ymm18
	vpaddd	%zmm22, %zmm19, %zmm0
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm6
	movzbl	(%rax,%rcx), %r12d
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -92(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %ecx
	vmovdqu64	960(%rsp), %zmm9        # 64-byte Reload
	vpaddd	%zmm12, %zmm9, %zmm0
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r14d
	vpextrd	$3, %xmm6, %r15d
	vmovd	%xmm6, %ebx
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -60(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 64(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, 48(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm1, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 80(%rsp)             # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm9, %zmm1
	vpextrd	$1, %xmm14, 240(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm14, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm14, 1664(%rsp)          # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, -64(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm6, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, %r10d
	vpextrd	$3, %xmm6, %r11d
	vextracti32x4	$2, %zmm1, %xmm6
	vmovd	%xmm6, %r8d
	vpextrd	$1, %xmm6, %r9d
	vpextrd	$2, %xmm6, %esi
	vpextrd	$3, %xmm6, %edi
	vmovd	%r12d, %xmm6
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	movslq	-100(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	-108(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	movslq	-96(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edx, %rbp
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ecx, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r13d, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r14d, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r15d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm12
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r15d
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm1, %r12d
	vpextrd	$2, %xmm1, %r13d
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%ebx, %xmm1
	movslq	%ebp, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r14d, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r15d, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	-64(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbx), %xmm1, %xmm1
	movslq	-68(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r10d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r11d, %rbp
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r8d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r9d, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm19
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm0, %ebp
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %r8d
	vpextrd	$3, %xmm0, %r9d
	vmovd	%ecx, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm13
	vextracti32x4	$3, %zmm17, %xmm0
	vmovd	%xmm0, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm4, 80(%rsp)             # 4-byte Folded Spill
	vmovdqa64	%zmm9, %zmm6
	vpaddd	%zmm11, %zmm9, %zmm0
	vpextrd	$2, %xmm4, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -60(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -100(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm9, %zmm1
	vpextrd	$1, %xmm0, -24(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -112(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -52(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vpextrd	$1, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -48(%rsp)                # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm0
	vpextrd	$1, %xmm0, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r13d
	vpextrd	$3, %xmm0, -12(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	vmovd	%xmm1, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %ebx
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm0, %r11d
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vpaddd	%zmm22, %zmm9, %zmm4
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vextracti128	$1, %ymm4, %xmm0
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -44(%rsp)                # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %esi
	vpaddd	%zmm26, %zmm9, %zmm1
	vpextrd	$2, %xmm7, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 960(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, %r12d
	vpextrd	$1, %xmm6, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, %r11d
	vextracti32x4	$2, %zmm1, %xmm6
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r14d
	vpextrd	$3, %xmm6, %r15d
	vmovd	%esi, %xmm6
	movslq	-24(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	movslq	-28(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	movslq	-52(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	-100(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	movslq	-104(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	movslq	-96(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	movslq	-108(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	movslq	-76(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	movslq	-84(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	movslq	-88(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	movslq	-92(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	movslq	-48(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	movslq	-64(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	movslq	-68(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	-56(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm20
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rsi), %esi
	vpextrd	$1, %xmm1, %ebx
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r8d
	vmovd	%esi, %xmm1
	movslq	%edi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm6
	vextracti32x4	$2, %zmm4, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %edi
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm7
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	256(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm17
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm7, %xmm1
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, %ebp
	vpextrd	$2, %xmm4, %ebx
	vpextrd	$3, %xmm4, %r10d
	vmovd	%xmm4, %edx
	vextracti32x4	$3, %zmm15, %xmm22
	vmovd	%xmm22, %esi
	movslq	%esi, %rsi
	vmovdqa	640(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$12, (%rax,%rsi), %xmm4, %xmm14
	movslq	-44(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	416(%rsp), %rsi                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm4, %xmm4
	movslq	-40(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	movslq	176(%rsp), %rsi                 # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm7                # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm7, %xmm7
	movslq	-36(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	movslq	320(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm21, %xmm21
	movslq	-32(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	movslq	512(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm23, %xmm11
	movslq	%edi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm1, %xmm1
	movslq	240(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm16, %xmm9
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm16
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm15
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm21, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm11, %xmm3
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm30, %ymm12, %ymm8
	vpmovzxbw	%ymm10, %zmm10          # zmm10 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpaddw	%zmm10, %zmm10, %zmm11
	vpmovzxbw	1408(%rsp), %zmm10      # 32-byte Folded Reload
                                        # zmm10 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm18, %zmm12          # zmm12 = ymm18[0],zero,ymm18[1],zero,ymm18[2],zero,ymm18[3],zero,ymm18[4],zero,ymm18[5],zero,ymm18[6],zero,ymm18[7],zero,ymm18[8],zero,ymm18[9],zero,ymm18[10],zero,ymm18[11],zero,ymm18[12],zero,ymm18[13],zero,ymm18[14],zero,ymm18[15],zero,ymm18[16],zero,ymm18[17],zero,ymm18[18],zero,ymm18[19],zero,ymm18[20],zero,ymm18[21],zero,ymm18[22],zero,ymm18[23],zero,ymm18[24],zero,ymm18[25],zero,ymm18[26],zero,ymm18[27],zero,ymm18[28],zero,ymm18[29],zero,ymm18[30],zero,ymm18[31],zero
	vpmovzxbw	%ymm8, %zmm8            # zmm8 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpaddw	%zmm8, %zmm12, %zmm12
	vpmovzxbw	672(%rsp), %zmm8        # 32-byte Folded Reload
                                        # zmm8 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm10, %zmm8, %zmm18
	vpaddw	%zmm11, %zmm18, %zmm11
	vpaddw	%zmm12, %zmm11, %zmm18
	vinserti32x4	$1, %xmm19, %ymm13, %ymm11
	vinserti32x4	$1, %xmm0, %ymm20, %ymm0
	vinserti128	$1, %xmm6, %ymm1, %ymm1
	vpmovzxbw	%ymm11, %zmm6           # zmm6 = ymm11[0],zero,ymm11[1],zero,ymm11[2],zero,ymm11[3],zero,ymm11[4],zero,ymm11[5],zero,ymm11[6],zero,ymm11[7],zero,ymm11[8],zero,ymm11[9],zero,ymm11[10],zero,ymm11[11],zero,ymm11[12],zero,ymm11[13],zero,ymm11[14],zero,ymm11[15],zero,ymm11[16],zero,ymm11[17],zero,ymm11[18],zero,ymm11[19],zero,ymm11[20],zero,ymm11[21],zero,ymm11[22],zero,ymm11[23],zero,ymm11[24],zero,ymm11[25],zero,ymm11[26],zero,ymm11[27],zero,ymm11[28],zero,ymm11[29],zero,ymm11[30],zero,ymm11[31],zero
	vpaddw	%zmm6, %zmm6, %zmm6
	vpmovzxbw	1152(%rsp), %zmm12      # 32-byte Folded Reload
                                        # zmm12 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm28, %zmm11          # zmm11 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddw	%zmm1, %zmm0, %zmm0
	vpaddw	%zmm12, %zmm11, %zmm1
	vpaddw	%zmm6, %zmm1, %zmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vextracti32x4	$3, %zmm5, %xmm6
	movslq	960(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rdx), %xmm24, %xmm5
	vpextrd	$1, %xmm6, %esi
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vmovd	%xmm6, %edx
	vpinsrb	$11, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %r11d
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm5, %xmm20
	vpaddw	%zmm0, %zmm1, %zmm1
	vpcmpltuw	%zmm1, %zmm18, %k1
	vpsubw	%zmm1, %zmm18, %zmm19
	vpsubw	%zmm18, %zmm1, %zmm19 {%k1}
	vpaddd	3456(%rsp), %zmm25, %zmm13      # 64-byte Folded Reload
	vmovd	%xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm1
	vpextrd	$1, %xmm13, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm6, 960(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm6
	vpextrd	$2, %xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm13, %ecx
	movslq	%ecx, %rcx
	movslq	1728(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm22, %edi
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	movslq	704(%rsp), %rdx                 # 4-byte Folded Reload
	vpextrd	$2, %xmm22, %r14d
	vextracti128	$1, %ymm13, %xmm7
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm22, 704(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm2, %ymm3, %ymm3
	vpmovzxbw	%ymm1, %zmm18           # zmm18 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm10, %zmm10, %zmm1
	vpaddw	1792(%rsp), %zmm1, %zmm1        # 64-byte Folded Reload
	vpsllw	$2, %zmm18, %zmm6
	vpaddw	%zmm12, %zmm12, %zmm10
	vpaddw	%zmm6, %zmm10, %zmm6
	vpaddw	%zmm1, %zmm29, %zmm10
	vpaddw	%zmm8, %zmm8, %zmm1
	vpaddw	1920(%rsp), %zmm1, %zmm8        # 64-byte Folded Reload
	movslq	144(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm17, %xmm0
	vpmovzxbw	%ymm3, %zmm12           # zmm12 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpextrd	$3, %xmm7, %edx
	vpsllw	$2, %zmm12, %zmm3
	vpaddw	%zmm11, %zmm11, %zmm7
	vpaddw	%zmm3, %zmm7, %zmm3
	vpaddw	%zmm8, %zmm27, %zmm7
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm13, %xmm1
	vpinsrb	$7, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$1, %xmm1, %edx
	movslq	352(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm1, %edx
	vpaddd	3392(%rsp), %zmm25, %zmm1       # 64-byte Folded Reload
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm13, %xmm2
	vpinsrb	$11, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm2, %edx
	movslq	%edi, %rdi
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm1, %edx
	vpinsrb	$13, (%rax,%rdi), %xmm14, %xmm5
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	movzbl	(%rax,%rdx), %edx
	vpaddw	%zmm6, %zmm10, %zmm8
	vmovd	%edx, %xmm6
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rsi), %xmm20, %xmm11
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vpaddw	%zmm3, %zmm7, %zmm13
	vextracti128	$1, %ymm1, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm10
	vmovd	%xmm3, %edi
	vpinsrb	$2, (%rax,%rdx), %xmm6, %xmm4
	vpextrd	$1, %xmm3, %edx
	movslq	96(%rsp), %rbx                  # 4-byte Folded Reload
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %esi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %edi
	vextracti32x4	$2, %zmm1, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %edx
	vpinsrb	$14, (%rax,%rbx), %xmm9, %xmm6
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$6, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rbp), %xmm16, %xmm9
	vmovd	%xmm3, %esi
	vpextrd	$3, %xmm3, %ecx
	movslq	48(%rsp), %rbp                  # 4-byte Folded Reload
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$7, (%rax,%rdi), %xmm4, %xmm3
	vpextrd	$1, %xmm1, %r12d
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %r15d
	vpinsrb	$14, (%rax,%rbp), %xmm15, %xmm7
	vmovd	%xmm1, %esi
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, 896(%rsp)            # 4-byte Folded Spill
	movslq	384(%rsp), %r13                 # 4-byte Folded Reload
	movslq	1664(%rsp), %rdx                # 4-byte Folded Reload
	movslq	64(%rsp), %rbp                  # 4-byte Folded Reload
	movslq	%r14d, %r14
	movslq	%ebx, %rdi
	vpinsrb	$14, (%rax,%rbp), %xmm0, %xmm1
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm2, %ebx
	vpinsrb	$10, (%rax,%rdi), %xmm3, %xmm0
	vpcmpltuw	%zmm13, %zmm8, %k1
	movslq	%esi, %rsi
	vpextrd	$3, %xmm2, %r10d
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpsubw	%zmm13, %zmm8, %zmm3
	movslq	%r12d, %rcx
	vpinsrb	$12, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm14
	vpsubw	%zmm8, %zmm13, %zmm3 {%k1}
	vpaddd	3328(%rsp), %zmm25, %zmm8       # 64-byte Folded Reload
	vpaddd	3264(%rsp), %zmm25, %zmm4       # 64-byte Folded Reload
	vextracti128	$1, %ymm4, %xmm2
	vmovd	%xmm2, %r12d
	vpinsrb	$14, (%rax,%r14), %xmm5, %xmm13
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, %r9d
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm2, %r8d
	vmovd	%xmm8, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%r13), %xmm9, %xmm9
	vpextrd	$2, %xmm8, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vinserti128	$1, %xmm6, %ymm9, %ymm9
	vpextrd	$3, %xmm8, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm8, %xmm0
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm0, %r13d
	vpinsrb	$2, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %ebp
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %esi
	movslq	%r11d, %rcx
	movslq	%r13d, %r13
	movslq	%ebp, %rbp
	vinserti128	$1, %xmm7, %ymm1, %ymm15
	vextracti32x4	$2, %zmm8, %xmm1
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %r13d
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$14, (%rax,%rcx), %xmm11, %xmm7
	vmovd	%xmm1, %ecx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$2, %zmm4, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %esi
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%r13d, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %r13d
	vpinsrb	$14, (%rax,%rbx), %xmm10, %xmm5
	vmovd	%xmm1, %ebx
	movslq	704(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm8, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %ebp
	vpinsrb	$14, (%rax,%r15), %xmm14, %xmm6
	vpextrd	$2, %xmm1, %r15d
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm13, %xmm0
	vmovd	%xmm1, %edx
	vpextrd	$3, %xmm1, %edi
	movslq	960(%rsp), %r11                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%r11), %xmm7, %xmm1
	movl	-8(%rsp), %r11d                 # 4-byte Reload
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r10d, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm5, %xmm5
	movslq	%ebp, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	movslq	%r15d, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %ebp
	movzbl	(%rax,%rdx), %edx
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovd	%edx, %xmm1
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r12d, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r14d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r9d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r8d, %rdx
	movl	$2, %ebp
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %ecx
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	movl	860(%rsp), %edi                 # 4-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vpmovzxbw	%ymm9, %zmm6            # zmm6 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpextrd	$3, %xmm4, %ecx
	vpaddw	%zmm6, %zmm6, %zmm4
	vpmovzxbw	%ymm15, %zmm7           # zmm7 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm7, %zmm18, %zmm8
	vpaddw	%zmm8, %zmm4, %zmm4
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpmovzxbw	%ymm5, %zmm2            # zmm2 = ymm5[0],zero,ymm5[1],zero,ymm5[2],zero,ymm5[3],zero,ymm5[4],zero,ymm5[5],zero,ymm5[6],zero,ymm5[7],zero,ymm5[8],zero,ymm5[9],zero,ymm5[10],zero,ymm5[11],zero,ymm5[12],zero,ymm5[13],zero,ymm5[14],zero,ymm5[15],zero,ymm5[16],zero,ymm5[17],zero,ymm5[18],zero,ymm5[19],zero,ymm5[20],zero,ymm5[21],zero,ymm5[22],zero,ymm5[23],zero,ymm5[24],zero,ymm5[25],zero,ymm5[26],zero,ymm5[27],zero,ymm5[28],zero,ymm5[29],zero,ymm5[30],zero,ymm5[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm2, %zmm0, %zmm2
	vpaddw	%zmm0, %zmm0, %zmm0
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm0, %zmm0
	vpaddw	%zmm2, %zmm4, %zmm1
	vpaddw	%zmm12, %zmm6, %zmm2
	vpaddw	%zmm7, %zmm2, %zmm2
	vpaddw	%zmm0, %zmm2, %zmm0
	vpcmpltuw	%zmm0, %zmm1, %k1
	vpsubw	%zmm0, %zmm1, %zmm2
	vpsubw	%zmm1, %zmm0, %zmm2 {%k1}
	vpaddw	%zmm31, %zmm19, %zmm0
	vpaddw	%zmm3, %zmm2, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	1984(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	2048(%rsp), %rcx                # 8-byte Reload
	incl	%ecx
	decq	2160(%rsp)                      # 8-byte Folded Spill
	jne	.LBB214_5
# %bb.6:                                # %"end for output.s0.y.yi.us"
                                        #   in Loop: Header=BB214_4 Depth=2
	movq	2152(%rsp), %rdx                # 8-byte Reload
	incq	%rdx
	movl	852(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	1400(%rsp), %rdx                # 8-byte Folded Reload
	jne	.LBB214_4
# %bb.7:                                # %"end for output.s0.x.x.loopexit.us"
                                        #   in Loop: Header=BB214_3 Depth=1
	movq	2136(%rsp), %rdx                # 8-byte Reload
	incq	%rdx
	movl	848(%rsp), %ecx                 # 4-byte Reload
	addl	$16, %ecx
	addl	$16, 476(%rsp)                  # 4-byte Folded Spill
	cmpq	2128(%rsp), %rdx                # 8-byte Folded Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB214_3
.LBB214_8:                              # %"end for output.s0.y.y"
	movl	%r11d, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	subl	%esi, %ecx
	addl	$34, %ecx
	movl	%ecx, %edx
	sarl	$5, %edx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %edx
	movq	448(%rsp), %rcx                 # 8-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%ecx, %edx
	movq	608(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %edi
	subl	%esi, %edi
	movq	200(%rsp), %rsi                 # 8-byte Reload
	movq	768(%rsp), %rcx                 # 8-byte Reload
	addl	$34, %ecx
	cmpl	%ecx, %edi
	cmovgel	%ecx, %edi
	addl	$-3, %edi
	sarl	$5, %edi
	cmpl	%edx, %edi
	movl	%edx, 236(%rsp)                 # 4-byte Spill
	cmovlel	%edx, %edi
	movq	%rdi, 624(%rsp)                 # 8-byte Spill
	movq	616(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	movl	232(%rsp), %ebp                 # 4-byte Reload
	subl	%ebp, %ecx
	movq	40(%rsp), %r14                  # 8-byte Reload
	jle	.LBB214_26
# %bb.9:                                # %"for output.s0.y.y.rebased.preheader"
	movq	608(%rsp), %rdx                 # 8-byte Reload
	decl	%edx
	vpbroadcastd	%edx, %zmm0
	vmovdqu64	%zmm0, 3136(%rsp)       # 64-byte Spill
	movq	624(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %edx
	movl	236(%rsp), %r8d                 # 4-byte Reload
	subl	%r8d, %edx
	movq	%rdx, 632(%rsp)                 # 8-byte Spill
	movq	448(%rsp), %rdx                 # 8-byte Reload
	subl	%ebx, %edx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	movl	%r8d, %edx
	movq	%rdx, 816(%rsp)                 # 8-byte Spill
	movslq	%ebx, %r10
	movl	%ecx, %ecx
	movq	%rcx, 800(%rsp)                 # 8-byte Spill
	shll	$4, %ebp
	movl	112(%rsp), %edx                 # 4-byte Reload
	addl	%edx, %ebp
	movl	%esi, %r9d
	imull	%edx, %r9d
	movl	%r8d, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rdx                 # 8-byte Reload
	leal	(%rdx,%rcx), %r11d
	subl	%r9d, %ecx
	movq	%rcx, 784(%rsp)                 # 8-byte Spill
	movslq	%r9d, %r8
	negl	%r9d
	movl	%r9d, 300(%rsp)                 # 4-byte Spill
	movl	%ebp, %ebx
	subl	776(%rsp), %ebx                 # 4-byte Folded Reload
	movl	%r14d, %edx
	imull	%ebx, %edx
	movl	-8(%rsp), %ecx                  # 4-byte Reload
	subl	%ecx, %edx
	movl	%edx, 132(%rsp)                 # 4-byte Spill
	movl	%r14d, %edx
	shll	$4, %edx
	movl	%edx, 308(%rsp)                 # 4-byte Spill
	leal	1(%rbx), %edx
	imull	%r14d, %edx
	subl	%ecx, %edx
	movl	%edx, 128(%rsp)                 # 4-byte Spill
	leal	-1(%rbx), %edx
	imull	%r14d, %edx
	subl	%ecx, %edx
	movl	%edx, 124(%rsp)                 # 4-byte Spill
	leal	2(%rbx), %edx
	imull	%r14d, %edx
	subl	%ecx, %edx
	movl	%edx, 120(%rsp)                 # 4-byte Spill
	movq	%rbx, 456(%rsp)                 # 8-byte Spill
	leal	-2(%rbx), %edx
	imull	%r14d, %edx
	subl	%ecx, %edx
	movl	%edx, 116(%rsp)                 # 4-byte Spill
	subl	%ecx, %r11d
	movl	%r11d, 304(%rsp)                # 4-byte Spill
	shlq	$5, %r10
	subq	%r8, %r10
	addq	568(%rsp), %r10                 # 8-byte Folded Reload
	movq	%r10, 792(%rsp)                 # 8-byte Spill
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_10:                             # %"for output.s0.y.y.rebased"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_12 Depth 2
                                        #       Child Loop BB214_13 Depth 3
                                        #     Child Loop BB214_17 Depth 2
                                        #       Child Loop BB214_18 Depth 3
                                        #     Child Loop BB214_22 Depth 2
                                        #       Child Loop BB214_23 Depth 3
	movq	%rdx, 808(%rsp)                 # 8-byte Spill
	movl	%ebp, 232(%rsp)                 # 4-byte Spill
	movslq	%ebp, %rcx
	imulq	%rsi, %rcx
	movq	%rcx, 280(%rsp)                 # 8-byte Spill
	cmpl	$0, 236(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_15
# %bb.11:                               # %"for output.s0.x.x1.preheader"
                                        #   in Loop: Header=BB214_10 Depth=1
	movq	568(%rsp), %rcx                 # 8-byte Reload
	movq	280(%rsp), %rdx                 # 8-byte Reload
	addq	%rdx, %rcx
	movq	%rcx, 824(%rsp)                 # 8-byte Spill
	movl	300(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_12:                             # %"for output.s0.x.x1"
                                        #   Parent Loop BB214_10 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_13 Depth 3
	movl	%ecx, 464(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	824(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	%rdx, 832(%rsp)                 # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3136(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2368(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2304(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1984(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2176(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2240(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1024(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2944(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2880(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2816(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2752(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2688(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2624(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2560(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2496(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	116(%rsp), %ecx                 # 4-byte Reload
	movl	120(%rsp), %r13d                # 4-byte Reload
	movl	124(%rsp), %r14d                # 4-byte Reload
	movl	128(%rsp), %r10d                # 4-byte Reload
	movl	132(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 1216(%rsp)                # 4-byte Spill
	.p2align	4, 0x90
.LBB214_13:                             # %"for output.s0.y.yi4"
                                        #   Parent Loop BB214_10 Depth=1
                                        #     Parent Loop BB214_12 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 1536(%rsp)                # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	1984(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm0, %zmm1
	vmovdqa64	%zmm2, %zmm14
	vmovdqu64	2176(%rsp), %zmm3       # 64-byte Reload
	vpaddd	%zmm3, %zmm0, %zmm2
	vmovdqa64	%zmm3, %zmm13
	vmovdqa64	%zmm0, %zmm6
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	movslq	%ecx, %rdi
	movslq	%edx, %rdx
	vpextrd	$3, %xmm2, %ecx
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$1, %xmm3, %r15d
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %r9d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$3, %xmm3, %r8d
	vpextrd	$1, %xmm0, %r11d
	vpextrd	$2, %xmm0, 896(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %r12d
	vextracti32x4	$3, %zmm2, %xmm22
	vpextrd	$1, %xmm22, 512(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm22, 1088(%rsp)          # 4-byte Folded Spill
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm2
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %ebp
	vpinsrb	$2, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm22, 416(%rsp)           # 4-byte Folded Spill
	movslq	%ebp, %rbx
	vextracti128	$1, %ymm1, %xmm3
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm4
	vpextrd	$1, %xmm3, %ebp
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm5
	vpextrd	$2, %xmm3, %edi
	vpinsrb	$3, (%rax,%rbx), %xmm2, %xmm4
	vmovd	%xmm3, %ebx
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %esi
	vpinsrb	$3, (%rax,%rcx), %xmm5, %xmm5
	vmovdqu64	2816(%rsp), %zmm21      # 64-byte Reload
	vmovdqa64	%zmm6, %zmm3
	vmovdqu64	%zmm6, 1152(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm6, %zmm20
	vmovd	%xmm20, %ecx
	movslq	%ecx, %rdx
	movslq	%r12d, %r12
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$1, %xmm20, %edx
	movslq	%edx, %rdx
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm7
	vpextrd	$1, %xmm2, %ebx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpextrd	$2, %xmm2, %edx
	vextracti32x4	$3, %zmm1, %xmm9
	vpinsrb	$4, (%rax,%r12), %xmm5, %xmm5
	vmovdqu64	2752(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm3, %zmm6
	vpextrd	$2, %xmm20, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$3, %xmm20, %ecx
	vpinsrb	$2, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$3, %xmm2, %r12d
	movslq	%ecx, %rcx
	vextracti32x4	$1, %ymm20, %xmm1
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm1, %ecx
	movslq	%r15d, %r15
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm6, %ecx
	vpinsrb	$5, (%rax,%r15), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %ebp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$1, %xmm9, %r15d
	movslq	%ebp, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %ecx
	movslq	%r9d, %rbp
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rdi), %xmm4, %xmm4
	vmovd	%xmm0, %edi
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm11
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %ebp
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm0, %ecx
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebp
	movslq	%esi, %r9
	movslq	%ecx, %rcx
	movslq	%ebp, %rsi
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rsi), %xmm7, %xmm12
	vmovd	%xmm2, %esi
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm2
	movl	%r13d, 1472(%rsp)               # 4-byte Spill
	vpbroadcastd	%r13d, %zmm3
	vpaddd	%zmm14, %zmm3, %zmm10
	vmovdqa64	%zmm14, %zmm25
	vmovdqa64	%zmm3, %zmm17
	vmovd	%xmm10, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r9), %xmm4, %xmm3
	vpextrd	$1, %xmm10, %r9d
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm4
	vpextrd	$2, %xmm9, %r13d
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm10, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$3, %xmm10, %ecx
	movslq	%ecx, %rcx
	movslq	%r8d, %rbp
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm7
	vpextrd	$3, %xmm9, 672(%rsp)            # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rbp), %xmm11, %xmm5
	movslq	%esi, %rcx
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm12, %xmm1
	vpextrd	$3, %xmm0, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	vmovd	%xmm0, %esi
	vextracti32x4	$2, %zmm20, %xmm24
	vpaddd	%zmm13, %zmm17, %zmm29
	vmovdqa64	%zmm13, %zmm26
	vextracti128	$1, %ymm10, %xmm4
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm11
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm3
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm4
	vmovd	%xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rdi), %xmm5, %xmm3
	vpextrd	$1, %xmm29, %ebp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$1, %xmm0, %r8d
	movslq	%ebp, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm24, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm7
	movslq	%esi, %rcx
	vpextrd	$3, %xmm29, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm12
	vextracti32x4	$1, %ymm29, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$3, (%rax,%rsi), %xmm5, %xmm2
	vpextrd	$1, %xmm1, %esi
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %esi
	vextracti32x4	$2, %zmm29, %xmm13
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm10, %xmm14
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm14, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm4, %xmm16
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm15
	vmovdqu64	%zmm17, 1408(%rsp)      # 64-byte Spill
	vpaddd	%zmm21, %zmm17, %zmm30
	vmovd	%xmm30, %ecx
	vpextrd	$2, %xmm0, %esi
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%r11d, %rbp
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$9, (%rax,%rbx), %xmm11, %xmm2
	vextracti32x4	$3, %zmm6, %xmm11
	vpextrd	$1, %xmm24, %ebx
	vextracti32x4	$1, %ymm30, %xmm0
	vpinsrb	$9, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %ebp
	vpaddd	%zmm8, %zmm17, %zmm6
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm30, %xmm27
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm27, %ebp
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbx), %xmm7, %xmm4
	vpextrd	$1, %xmm6, %edi
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm7
	vpextrd	$1, %xmm11, %ebx
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %edi
	movslq	%edi, %rdi
	movslq	%r8d, %rbp
	vpinsrb	$3, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm11, %r8d
	vpinsrb	$9, (%rax,%rbp), %xmm12, %xmm12
	vpextrd	$1, %xmm14, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm16, %xmm18
	vpextrd	$1, %xmm13, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm15, %xmm15
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm27, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm1, %xmm19
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm7, %xmm1
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm7
	vpextrd	$3, %xmm5, %edi
	movslq	896(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm6, %xmm1
	vpinsrb	$7, (%rax,%rdi), %xmm7, %xmm5
	vmovd	%xmm1, %edi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm5, %xmm23
	vpextrd	$3, %xmm11, 1600(%rsp)          # 4-byte Folded Spill
	vpinsrb	$10, (%rax,%rdx), %xmm2, %xmm17
	vpextrd	$2, %xmm24, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm20, %xmm31
	vpinsrb	$10, (%rax,%rbp), %xmm3, %xmm16
	movl	%r14d, 1664(%rsp)               # 4-byte Spill
	vpbroadcastd	%r14d, %zmm0
	vpaddd	%zmm25, %zmm0, %zmm3
	vmovdqa64	%zmm0, %zmm20
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdx), %xmm4, %xmm25
	vpextrd	$1, %xmm3, %edx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm2
	vpextrd	$3, %xmm24, %edi
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm31, %r11d
	vpinsrb	$10, (%rax,%rsi), %xmm12, %xmm4
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm18, %xmm5
	vpextrd	$2, %xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm15, %xmm28
	vpextrd	$2, %xmm27, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm19, %xmm24
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%rdx), %xmm23, %xmm15
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm0
	vpextrd	$3, %xmm7, %edx
	vpaddd	%zmm26, %zmm20, %zmm12
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm3, %xmm2
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %edx
	movslq	%r12d, %rsi
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm26
	vmovd	%xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rsi), %xmm17, %xmm18
	vpextrd	$1, %xmm12, %ebp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm31, %r9d
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm12, %edx
	movslq	%edx, %rdx
	movslq	704(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm13, %esi
	vpinsrb	$11, (%rax,%rbp), %xmm16, %xmm19
	movslq	%edi, %rdx
	vpextrd	$3, %xmm14, %edi
	vextracti128	$1, %ymm12, %xmm7
	vpinsrb	$11, (%rax,%rdx), %xmm25, %xmm17
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm7, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm16
	movslq	%edi, %rcx
	vpextrd	$3, %xmm27, %ebp
	vextracti32x4	$2, %zmm12, %xmm1
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm27
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rsi), %xmm28, %xmm14
	vmovd	%xmm22, %ecx
	vmovdqu64	%zmm20, 896(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm20, %zmm23
	vmovd	%xmm23, %esi
	movslq	%esi, %rdi
	movzbl	(%rax,%rdi), %esi
	movslq	%ebp, %rdi
	vmovd	%esi, %xmm4
	vpextrd	$1, %xmm23, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdi), %xmm24, %xmm25
	vpextrd	$2, %xmm23, %edi
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$3, %xmm2, %esi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm2
	vpextrd	$3, %xmm23, %edi
	movslq	%edi, %rdi
	vextracti32x4	$1, %ymm23, %xmm4
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %edi
	vpinsrb	$11, (%rax,%rdx), %xmm15, %xmm13
	vmovd	%xmm9, %edx
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm4, %edi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rsi), %xmm26, %xmm26
	vmovd	%xmm11, %esi
	vpextrd	$3, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm0, %xmm28
	vmovd	%xmm31, %edi
	vextracti32x4	$3, %zmm29, %xmm1
	vpaddd	%zmm8, %zmm20, %zmm4
	vmovdqa64	%zmm8, %zmm22
	vextracti32x4	$2, %zmm23, %xmm0
	vpinsrb	$12, (%rax,%rdx), %xmm18, %xmm9
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm24
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm19, %xmm15
	vpextrd	$1, %xmm4, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$3, %xmm31, 640(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movslq	%edi, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm1, %edi
	vextracti128	$1, %ymm4, %xmm2
	vpinsrb	$12, (%rax,%rdx), %xmm17, %xmm17
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %ecx
	movslq	%esi, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpinsrb	$12, (%rax,%rdx), %xmm16, %xmm19
	vextracti32x4	$3, %zmm10, %xmm2
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm4, %xmm5
	vpinsrb	$12, (%rax,%rdx), %xmm27, %xmm16
	vmovd	%xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$1, %xmm5, %edx
	vextracti32x4	$3, %zmm6, %xmm0
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm7, %xmm6
	vpextrd	$2, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %edx
	movslq	%edi, %rsi
	vextracti32x4	$3, %zmm30, %xmm8
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm6, %xmm29
	vpinsrb	$12, (%rax,%rsi), %xmm14, %xmm14
	vextracti32x4	$3, %zmm12, %xmm10
	movl	%r10d, 960(%rsp)                # 4-byte Spill
	vpbroadcastd	%r10d, %zmm5
	vmovdqu64	1984(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm5, %zmm30
	vmovdqa64	%zmm5, %zmm6
	vmovd	%xmm30, %edx
	movslq	%edx, %rdi
	vmovd	%xmm8, %edx
	movzbl	(%rax,%rdi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm30, %esi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm30, %esi
	vpinsrb	$12, (%rax,%rdx), %xmm25, %xmm27
	movslq	%esi, %rdx
	vpextrd	$3, %xmm30, %esi
	vpinsrb	$2, (%rax,%rdx), %xmm7, %xmm7
	vmovd	%xmm0, %edx
	movslq	%esi, %rsi
	vextracti32x4	$1, %ymm30, %xmm5
	vpinsrb	$3, (%rax,%rsi), %xmm7, %xmm7
	vmovd	%xmm5, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm5, %esi
	vpinsrb	$12, (%rax,%rdx), %xmm13, %xmm25
	vmovd	%xmm10, %edx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$3, %xmm5, %esi
	vextracti32x4	$3, %zmm3, %xmm13
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm7, %xmm3
	vmovd	%xmm13, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm4, %xmm12
	vextracti32x4	$3, %zmm23, %xmm11
	vmovdqa64	%zmm6, %zmm4
	vmovdqu64	2176(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm6, %zmm7
	vextracti32x4	$2, %zmm30, %xmm5
	vpinsrb	$12, (%rax,%rsi), %xmm26, %xmm6
	vmovdqa	%xmm6, 144(%rsp)                # 16-byte Spill
	vmovd	%xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm7, %esi
	vpinsrb	$12, (%rax,%rdx), %xmm28, %xmm6
	vmovdqa	%xmm6, 1856(%rsp)               # 16-byte Spill
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm28
	vmovd	%xmm11, %edx
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm5
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$3, %xmm7, %esi
	vpinsrb	$12, (%rax,%rdx), %xmm24, %xmm3
	vmovdqa	%xmm3, 160(%rsp)                # 16-byte Spill
	vmovd	%xmm12, %edx
	movslq	%esi, %rsi
	vextracti128	$1, %ymm7, %xmm3
	vpinsrb	$3, (%rax,%rsi), %xmm5, %xmm5
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$1, %xmm3, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %esi
	vextracti32x4	$2, %zmm7, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm5, %xmm6
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rdx), %xmm29, %xmm5
	vpextrd	$1, %xmm3, %edx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm3, %esi
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm3, %edx
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm7, %xmm24
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm3
	vmovd	%xmm24, %esi
	vextracti32x4	$3, %zmm30, %xmm23
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm23, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm28, %xmm7
	movslq	%esi, %rsi
	vpaddd	%zmm21, %zmm4, %zmm6
	vmovdqu64	%zmm4, 704(%rsp)        # 64-byte Spill
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm3
	vmovdqa	%xmm3, 1280(%rsp)               # 16-byte Spill
	vpextrd	$1, %xmm6, %edi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	movslq	%r15d, %rdi
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm28
	vpextrd	$3, %xmm1, 1728(%rsp)           # 4-byte Folded Spill
	vpinsrb	$13, (%rax,%rdi), %xmm9, %xmm3
	movslq	512(%rsp), %rsi                 # 4-byte Folded Reload
	vpextrd	$1, %xmm2, %edi
	vextracti128	$1, %ymm6, %xmm1
	vpinsrb	$13, (%rax,%rsi), %xmm15, %xmm31
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm28, %xmm9
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm9, %xmm9
	vpextrd	$2, %xmm1, %esi
	movslq	%r11d, %rbp
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm9, %xmm9
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm9, %xmm9
	vpextrd	$2, %xmm2, %r11d
	vpinsrb	$13, (%rax,%rbp), %xmm17, %xmm1
	movslq	%ebx, %rsi
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm2
	vpinsrb	$13, (%rax,%rsi), %xmm19, %xmm30
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm9, %xmm9
	vpextrd	$1, %xmm2, %esi
	vpaddd	%zmm22, %zmm4, %zmm15
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm9, %xmm9
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm9, %xmm17
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm6, %xmm4
	vmovdqa	%xmm4, 176(%rsp)                # 16-byte Spill
	vpinsrb	$11, (%rax,%rsi), %xmm17, %xmm2
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rsi), %xmm2, %xmm2
	vmovdqa	%xmm2, 1792(%rsp)               # 16-byte Spill
	vmovd	%xmm15, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rdi), %xmm16, %xmm2
	vpextrd	$1, %xmm15, %edi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$1, %xmm0, %esi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$2, %xmm15, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm15, %edi
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rdi), %xmm6, %xmm19
	vpextrd	$2, %xmm0, %r15d
	vpinsrb	$13, (%rax,%rcx), %xmm14, %xmm16
	vpextrd	$1, %xmm8, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm15, %xmm6
	vpinsrb	$13, (%rax,%rcx), %xmm27, %xmm17
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm19, %xmm14
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm14, %xmm14
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm14, %xmm14
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm15, %xmm6
	vpinsrb	$7, (%rax,%rcx), %xmm14, %xmm14
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm14, %xmm14
	vpextrd	$1, %xmm6, %ecx
	movslq	%r13d, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm14, %xmm14
	vpextrd	$2, %xmm6, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm14, %xmm14
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm14, %xmm28
	vpextrd	$3, %xmm0, %r13d
	vpinsrb	$13, (%rax,%rsi), %xmm25, %xmm25
	movl	1216(%rsp), %ecx                # 4-byte Reload
	vpbroadcastd	%ecx, %zmm26
	vpaddd	%zmm20, %zmm26, %zmm29
	vmovd	%xmm29, %esi
	movslq	%esi, %rbx
	vpinsrb	$14, (%rax,%rbp), %xmm3, %xmm0
	vpextrd	$1, %xmm29, %ebp
	movzbl	(%rax,%rbx), %esi
	vmovd	%esi, %xmm3
	vpextrd	$2, %xmm8, %r12d
	movslq	%ebp, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm29, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm29, %esi
	movslq	%esi, %rsi
	movslq	1088(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm14
	vpextrd	$3, %xmm8, %r14d
	vpinsrb	$14, (%rax,%rbx), %xmm31, %xmm31
	movslq	%r9d, %rsi
	vpextrd	$1, %xmm10, 576(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm29, %xmm6
	vpinsrb	$14, (%rax,%rsi), %xmm1, %xmm3
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm14, %xmm1
	vpextrd	$1, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm29, %xmm6
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %esi
	movslq	%r8d, %rbx
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm10, 512(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rbx), %xmm30, %xmm27
	movslq	%r11d, %rsi
	vpextrd	$3, %xmm6, %ebx
	vpaddd	%zmm18, %zmm26, %zmm8
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm30
	vmovd	%xmm8, %esi
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbx), %xmm1, %xmm4
	vpextrd	$3, %xmm10, 1088(%rsp)          # 4-byte Folded Spill
	movslq	%esi, %rsi
	vextracti128	$1, %ymm8, %xmm1
	movzbl	(%rax,%rsi), %esi
	vpextrd	$1, %xmm8, %ebx
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r11d
	vpextrd	$3, %xmm1, %ecx
	vmovd	%esi, %xmm1
	movslq	%ebx, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm8, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm16, %xmm2
	vpextrd	$3, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r8d, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r10d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	movslq	672(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%r11d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm13, 384(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm13, 1920(%rsp)          # 4-byte Folded Spill
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm13, 672(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm8, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm10
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinserti128	$1, %xmm0, %ymm10, %ymm6
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm10
	vpaddd	%zmm21, %zmm26, %zmm16
	vmovd	%xmm16, %ecx
	vextracti32x4	$1, %ymm16, %xmm0
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %r11d
	vmovd	%xmm0, %r9d
	vextracti32x4	$2, %zmm16, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm16, %esi
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %edi
	vpextrd	$3, %xmm0, %r8d
	vmovd	%ecx, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	movslq	%edi, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm12, 64(%rsp)            # 4-byte Folded Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm31
	vpextrd	$2, %xmm12, 256(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm12, 416(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm22, %zmm26, %zmm17
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm12
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 480(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, 640(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm17, %ecx
	vextracti32x4	$1, %ymm17, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm3
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	movslq	%edi, %rdx
	vinserti128	$1, %xmm12, %ymm1, %ymm1
	vmovdqu	%ymm1, 1728(%rsp)               # 32-byte Spill
	vextracti32x4	$2, %zmm17, %xmm1
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movslq	%r14d, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpinsrb	$15, (%rax,%rsi), %xmm13, %xmm11
	movslq	%edx, %rdx
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rcx), %xmm25, %xmm12
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm3, %ymm2, %ymm1
	vmovdqu	%ymm1, 1600(%rsp)               # 32-byte Spill
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm30
	vmovdqu64	2368(%rsp), %zmm27      # 64-byte Reload
	vmovdqu64	1152(%rsp), %zmm0       # 64-byte Reload
	vpaddd	%zmm27, %zmm0, %zmm1
	vinserti128	$1, %xmm11, %ymm12, %ymm2
	vmovdqu	%ymm2, 320(%rsp)                # 32-byte Spill
	vmovdqu64	2304(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm0, %zmm2
	vmovdqa64	%zmm0, %zmm12
	vextracti128	$1, %ymm2, %xmm0
	vmovd	%xmm0, 352(%rsp)                # 4-byte Folded Spill
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vextracti32x4	$3, %zmm15, %xmm13
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm18
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti128	$1, %ymm1, %xmm3
	vpextrd	$1, %xmm3, %r10d
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r8d
	vmovd	%xmm3, %edi
	vextracti32x4	$3, %zmm29, %xmm19
	vmovd	%xmm19, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm3
	vmovdqa	%xmm3, 240(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ebp
	vpextrd	$2, %xmm3, %ebx
	vpextrd	$3, %xmm3, %esi
	vmovd	%xmm3, %edx
	vextracti32x4	$3, %zmm8, %xmm11
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm3
	vmovdqa	%xmm3, 208(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm8
	vextracti32x4	$2, %zmm2, %xmm3
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %esi
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %ebx
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm2, %ebp
	vextracti32x4	$3, %zmm2, %xmm2
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm2, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -60(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -72(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1024(%rsp), %zmm9       # 64-byte Reload
	vpaddd	%zmm9, %zmm12, %zmm2
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, -28(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm1
	vmovd	%xmm1, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2240(%rsp), %zmm29      # 64-byte Reload
	vpaddd	%zmm29, %zmm12, %zmm1
	vmovd	%xmm2, %r10d
	vpextrd	$1, %xmm2, %r11d
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, -32(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -36(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r12d
	vmovd	%xmm2, %edx
	vmovd	%ecx, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm10
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %ebp
	vpextrd	$3, %xmm3, %r9d
	vmovd	%xmm3, %ebx
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r8d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r13d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r12d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm0
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edi
	vextracti32x4	$3, %zmm16, %xmm28
	vmovd	%xmm28, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm31, %xmm25
	movslq	%r9d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm16
	movslq	%r10d, %rbp
	movslq	%r11d, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm8, %xmm2
	movslq	%r14d, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm10, %xmm0
	movslq	%r15d, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm16, %xmm8
	movslq	-28(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm2, %xmm2
	movslq	-48(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	80(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	movslq	-100(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm8, %xmm8
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm8, %xmm8
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm8, %xmm10
	vinserti128	$1, %xmm2, %ymm0, %ymm8
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$1, %xmm24, 80(%rsp)            # 4-byte Folded Spill
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-36(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti32x4	$1, %xmm10, %ymm0, %ymm16
	vpaddd	%zmm29, %zmm26, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rdx
	vpextrd	$2, %xmm24, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm24, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm23, %r8d
	vpextrd	$2, %xmm23, -60(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$3, %xmm23, -72(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	movzbl	(%rax,%rdx), %edx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edi
	vpextrd	$2, %xmm2, %ebp
	vpextrd	$3, %xmm2, %r14d
	vmovdqu64	2880(%rsp), %zmm4       # 64-byte Reload
	vpaddd	%zmm4, %zmm12, %zmm23
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %r9d
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %r11d
	vextracti32x4	$1, %ymm23, %xmm0
	vpextrd	$1, %xmm1, %ebx
	vmovd	%xmm0, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm0
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	movslq	%esi, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	movslq	%edi, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	movslq	%ebp, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm23, %xmm2
	vpextrd	$1, %xmm2, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, -92(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$3, %zmm17, %xmm10
	vmovd	%xmm10, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm30, %xmm30
	movslq	%r14d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	movslq	384(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm15
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm24
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm5, %xmm31
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm3
	vmovdqa	%xmm3, 576(%rsp)                # 16-byte Spill
	vextracti32x4	$3, %zmm1, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)                # 16-byte Spill
	vmovdqu64	2944(%rsp), %zmm14      # 64-byte Reload
	vpaddd	%zmm14, %zmm12, %zmm0
	vpextrd	$1, %xmm23, -100(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm23, 144(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm23, 48(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm23, %r8d
	vextracti32x4	$3, %zmm23, %xmm1
	vpextrd	$1, %xmm1, 1152(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 1856(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 160(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, -28(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm0, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 32(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -12(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm1, 36(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -40(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1408(%rsp), %zmm21      # 64-byte Reload
	vpaddd	%zmm20, %zmm21, %zmm1
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vextracti128	$1, %ymm1, %xmm0
	vmovd	%xmm0, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	vmovd	%xmm0, -48(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm27, %zmm21, %zmm0
	vmovd	%xmm1, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 4(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 12(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 16(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, 28(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -24(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, (%rsp)                   # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 2432(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %r13d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm1, %r12d
	vpextrd	$2, %xmm1, %r11d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vmovd	%ecx, %xmm1
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm22
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edx
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %edi
	vpextrd	$3, %xmm0, %r8d
	vmovd	%ecx, %xmm0
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	1856(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r11d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r10d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm1
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	movslq	4(%rsp), %rdx                   # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rdx), %xmm7, %xmm7
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	vpmovzxbw	%ymm8, %zmm8            # zmm8 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpsllw	$2, %zmm8, %zmm8
	vpmovzxbw	%ymm16, %zmm16          # zmm16 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddw	%zmm16, %zmm16, %zmm16
	vpaddw	%zmm16, %zmm8, %zmm8
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$1, %xmm13, 48(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm22, %ymm0, %ymm0
	vpmovzxbw	%ymm6, %zmm6            # zmm6 = ymm6[0],zero,ymm6[1],zero,ymm6[2],zero,ymm6[3],zero,ymm6[4],zero,ymm6[5],zero,ymm6[6],zero,ymm6[7],zero,ymm6[8],zero,ymm6[9],zero,ymm6[10],zero,ymm6[11],zero,ymm6[12],zero,ymm6[13],zero,ymm6[14],zero,ymm6[15],zero,ymm6[16],zero,ymm6[17],zero,ymm6[18],zero,ymm6[19],zero,ymm6[20],zero,ymm6[21],zero,ymm6[22],zero,ymm6[23],zero,ymm6[24],zero,ymm6[25],zero,ymm6[26],zero,ymm6[27],zero,ymm6[28],zero,ymm6[29],zero,ymm6[30],zero,ymm6[31],zero
	vpaddw	%zmm6, %zmm6, %zmm16
	vmovdqu64	%zmm16, 1856(%rsp)      # 64-byte Spill
	vpmovzxbw	1728(%rsp), %zmm6       # 32-byte Folded Reload
                                        # zmm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm6, %zmm6, %zmm3
	vmovdqu64	%zmm3, 1152(%rsp)       # 64-byte Spill
	vpaddw	%zmm16, %zmm3, %zmm6
	vpaddw	%zmm8, %zmm6, %zmm6
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm8
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm6, %zmm7
	vinserti32x4	$1, %xmm1, %ymm8, %ymm16
	vpaddd	%zmm29, %zmm21, %zmm8
	vpaddd	%zmm9, %zmm21, %zmm1
	vpextrd	$2, %xmm13, 1728(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm13, 160(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm6, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm6
	vmovd	%xmm6, %r12d
	vpextrd	$1, %xmm6, %r14d
	vpextrd	$2, %xmm6, %r13d
	vpextrd	$3, %xmm6, %r15d
	vextracti32x4	$2, %zmm8, %xmm6
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %r9d
	vpextrd	$2, %xmm6, %r10d
	vpextrd	$3, %xmm6, %r11d
	vpaddd	%zmm9, %zmm26, %zmm6
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm8, %edx
	vpextrd	$2, %xmm8, %esi
	vpextrd	$3, %xmm8, %edi
	vmovd	%xmm8, %ecx
	vextracti32x4	$3, %zmm8, %xmm0
	vpextrd	$1, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm0, %r8d
	vpextrd	$3, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%ebx, %xmm0
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm0, %xmm13
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm22
	vextracti32x4	$2, %zmm1, %xmm0
	vpextrd	$1, %xmm0, %ecx
	vpextrd	$2, %xmm0, %edx
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpinsrb	$1, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	movslq	80(%rsp), %rbp                  # 4-byte Folded Reload
	vmovdqa	1280(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm3, %xmm23
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm0, %xmm0
	movslq	64(%rsp), %rbp                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm0, %xmm0
	movslq	144(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm0, %xmm0
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	movslq	-104(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %ebp
	vmovd	%xmm1, %ebx
	vmovaps	176(%rsp), %xmm3                # 16-byte Reload
	vextractps	$1, %xmm3, %esi
	movslq	%esi, %rsi
	vmovdqa	1792(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	vmovdqa	%xmm1, 64(%rsp)                 # 16-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	movslq	48(%rsp), %rdx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm18, %xmm1
	vmovdqa	%xmm1, 48(%rsp)                 # 16-byte Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm0
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm22, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	movslq	-108(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	vextractps	$2, %xmm3, 80(%rsp)     # 4-byte Folded Spill
	vpmovzxbw	%ymm16, %zmm1           # zmm1 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpsllw	$2, %zmm1, %zmm1
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm1, %zmm16
	vextracti128	$1, %ymm6, %xmm0
	vextractps	$3, %xmm3, 176(%rsp)    # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm1
	vpextrd	$1, %xmm11, -76(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 144(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm11, 1792(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm27, %zmm26, %zmm11
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vpaddd	%zmm20, %zmm26, %zmm0
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, -108(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm11, %xmm1
	vmovd	%xmm1, %r14d
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm11, %xmm1
	vmovd	%xmm1, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vmovdqa64	%zmm4, %zmm18
	vpaddd	%zmm4, %zmm21, %zmm22
	vextracti32x4	$1, %ymm22, %xmm1
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 1280(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm19, %r13d
	movslq	%r13d, %rbp
	vmovdqa	240(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 240(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm13, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm6, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm17
	vmovdqa64	%zmm14, %zmm3
	vpaddd	%zmm14, %zmm21, %zmm13
	vextracti128	$1, %ymm13, %xmm1
	vpextrd	$1, %xmm1, %r13d
	vpextrd	$2, %xmm1, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -52(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, %r11d
	vmovd	%xmm11, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	208(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm4, %xmm4
	vmovdqa	%xmm4, 208(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r15d, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r12d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	-68(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	-56(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	movslq	-64(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm14
	vextracti32x4	$2, %zmm13, %xmm1
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r15d
	vpextrd	$3, %xmm1, %r12d
	vmovd	%xmm1, %r8d
	movslq	-100(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbx), %xmm14, %xmm14
	vpextrd	$1, %xmm13, %ebx
	vmovd	%xmm13, %edx
	vpextrd	$2, %xmm13, %esi
	vpextrd	$3, %xmm13, %ecx
	vextracti32x4	$3, %zmm13, %xmm1
	vpextrd	$1, %xmm1, %r14d
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 1408(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, %edi
	vpextrd	$1, %xmm28, %r10d
	movslq	%r10d, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm25, %xmm21
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rbp), %xmm14, %xmm1
	vextracti32x4	$3, %zmm11, %xmm14
	vmovd	%xmm14, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm12
	movslq	%edx, %rdx
	movslq	%ebx, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm1
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm13
	vextracti32x4	$2, %zmm22, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %edi
	vmovd	%xmm22, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm22, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm22, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm22, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vextracti32x4	$3, %zmm22, %xmm11
	vpextrd	$1, %xmm11, %ebp
	vpextrd	$2, %xmm11, %ebx
	vpextrd	$3, %xmm11, %esi
	vmovd	%xmm11, %edx
	vpextrd	$1, %xmm10, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm30, %xmm11
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm13, %xmm13
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm15, %xmm30
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm4
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm24, %xmm8
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm31, %xmm31
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm13
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	1408(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm13, %xmm13
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vinserti128	$1, %xmm13, %ymm4, %ymm4
	vpmovzxbw	1600(%rsp), %zmm13      # 32-byte Folded Reload
                                        # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm13, %zmm13, %zmm1
	vmovdqu64	%zmm1, 1408(%rsp)       # 64-byte Spill
	vpmovzxbw	320(%rsp), %zmm22       # 32-byte Folded Reload
                                        # zmm22 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm22, %zmm22, %zmm6
	vmovdqu64	%zmm6, 1280(%rsp)       # 64-byte Spill
	vpaddw	%zmm1, %zmm6, %zmm24
	vpaddw	%zmm16, %zmm24, %zmm16
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm19, -84(%rsp)           # 4-byte Folded Spill
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm16, %zmm4
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$2, %zmm0, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm6, %ecx
	vpcmpltuw	%zmm4, %zmm7, %k1
	vpsubw	%zmm4, %zmm7, %zmm24
	vpsubw	%zmm7, %zmm4, %zmm24 {%k1}
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm25
	vpaddd	%zmm3, %zmm26, %zmm15
	vmovdqa64	%zmm3, %zmm16
	vmovd	%xmm15, %ecx
	vpextrd	$3, %xmm19, -108(%rsp)          # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm15, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm15, %edx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edi
	vpextrd	$2, %xmm2, %ebp
	vpextrd	$3, %xmm2, %ebx
	vmovd	%ecx, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm6
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm7
	vextracti32x4	$2, %zmm15, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$1, %xmm2, %ecx
	vinserti128	$1, %xmm1, %ymm6, %ymm1
	vmovdqu	%ymm1, 512(%rsp)                # 32-byte Spill
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm1
	vmovdqa	%xmm1, 1088(%rsp)               # 16-byte Spill
	vpextrd	$2, %xmm10, -88(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm10, -96(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm26, %zmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm28, -92(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm7, %edx
	vpextrd	$3, %xmm28, -112(%rsp)          # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm9, %r14d
	vpextrd	$2, %xmm9, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r15d
	vmovdqu64	896(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm27, %zmm4, %zmm6
	vpextrd	$3, %xmm5, 1600(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vmovd	%xmm5, %edi
	vpextrd	$2, %xmm5, %r11d
	vpextrd	$3, %xmm5, %r8d
	vextracti128	$1, %ymm7, %xmm5
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	640(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbp), %xmm8, %xmm9
	vpextrd	$2, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 672(%rsp)                # 16-byte Spill
	vextracti32x4	$2, %zmm6, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%xmm6, %r13d
	movslq	%r13d, %r13
	vpextrd	$1, %xmm6, %r12d
	movslq	%r12d, %r12
	movzbl	(%rax,%r13), %ebx
	vmovd	%ebx, %xmm1
	vpinsrb	$1, (%rax,%r12), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	416(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbx), %xmm31, %xmm10
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r10d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r11d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm6, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm28
	vpextrd	$1, %xmm0, %r8d
	vinserti128	$1, %xmm9, %ymm10, %ymm1
	vmovdqu	%ymm1, 640(%rsp)                # 32-byte Spill
	vpextrd	$2, %xmm0, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1920(%rsp)           # 4-byte Folded Spill
	vmovdqa64	%zmm20, %zmm19
	vpaddd	%zmm20, %zmm4, %zmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm14, %esi
	vpextrd	$2, %xmm14, -104(%rsp)          # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vpextrd	$3, %xmm14, 256(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm6
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %edx
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%xmm1, %ecx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm6, %xmm1
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r9d, %rdx
	vmovdqa	384(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm14
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm17, %xmm10
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm13
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm25, %xmm22
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm5, 416(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm8
	vpextrd	$1, %xmm5, %esi
	vpextrd	$2, %xmm5, %r9d
	vpextrd	$3, %xmm5, %r8d
	vpaddd	%zmm29, %zmm4, %zmm5
	vextracti128	$1, %ymm5, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r11d
	vmovd	%xmm1, %r12d
	vpextrd	$1, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm28, %xmm28
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm8, %xmm1
	movslq	-60(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	576(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm8
	movslq	%r9d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	movslq	96(%rsp), %rdx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm23, %xmm12
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	movslq	-72(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm8, %xmm8
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm17
	vextracti32x4	$2, %zmm5, %xmm1
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edi
	vmovd	%xmm1, %ebx
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm12
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm5, %xmm25
	vmovd	%xmm25, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, 576(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm8, %ymm12, %ymm30
	vpextrd	$3, %xmm2, 384(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1024(%rsp), %zmm9       # 64-byte Reload
	vpaddd	%zmm9, %zmm4, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 352(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %r10d
	vpextrd	$2, %xmm0, %r8d
	vmovd	%xmm0, %ebp
	vpextrd	$3, %xmm0, %r9d
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm17, %xmm31
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm5
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %edx
	vpextrd	$3, %xmm5, %edi
	vmovd	%xmm5, %ebx
	vpextrd	$1, %xmm25, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm17
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebp, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r10d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r8d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r9d, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebx, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, -76(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm16, %zmm4, %zmm2
	vpextrd	$3, %xmm1, 96(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vmovd	%xmm1, %r14d
	vextracti32x4	$2, %zmm2, %xmm1
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm2, %ecx
	vmovd	%xmm1, %esi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%edx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm5
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm8
	vpextrd	$2, %xmm25, -68(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm4, %zmm0
	vpextrd	$3, %xmm25, 896(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm2
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %r11d
	vpextrd	$3, %xmm2, %r8d
	vmovd	%xmm2, %ebp
	vextracti32x4	$2, %zmm0, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %ebx
	vmovd	%xmm2, %edi
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %r9d
	vpextrd	$3, %xmm2, %r10d
	vmovd	%ecx, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %r8d
	vpextrd	$2, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	704(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm27, %zmm4, %zmm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %r13d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, %r14d
	vmovd	%xmm1, %ebx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm8, %xmm25
	movslq	%r9d, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm1
	movslq	80(%rsp), %rbp                  # 4-byte Folded Reload
	vmovdqa	64(%rsp), %xmm2                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm2, %xmm2
	movslq	%r10d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm1, %xmm1
	movslq	1728(%rsp), %rbp                # 4-byte Folded Reload
	vmovdqa	48(%rsp), %xmm6                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm8
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	240(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm6, %xmm12
	movslq	%r8d, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm1, %xmm23
	movslq	144(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	208(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm1, %xmm20
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm0, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %edi
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	movslq	-92(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm21, %xmm21
	movslq	%ebx, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-88(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm11, %xmm11
	movslq	%r13d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r15d, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm14, %xmm27
	movslq	%r12d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	movslq	%r14d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm8, %xmm8
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm14
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm20, %xmm20
	vinserti128	$1, %xmm2, %ymm8, %ymm12
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpaddd	%zmm19, %zmm4, %zmm2
	vextracti128	$1, %ymm2, %xmm6
	vpextrd	$1, %xmm6, %r10d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vmovd	%xmm6, %r14d
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm6
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm0
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm21, %xmm1
	vpextrd	$1, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm8
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %ebx
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %esi
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm11, %xmm11
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %ecx
	vinserti32x4	$1, %xmm14, %ymm20, %ymm20
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm14
	vpextrd	$2, %xmm3, 48(%rsp)             # 4-byte Folded Spill
	vinserti32x4	$1, %xmm1, %ymm11, %ymm21
	vpextrd	$3, %xmm3, 176(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm29, %zmm4, %zmm0
	vpextrd	$2, %xmm2, 1728(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm6, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 160(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm9, %zmm4, %zmm11
	vextracti128	$1, %ymm11, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vmovd	%xmm1, %r14d
	vpextrd	$3, %xmm1, %r13d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %edi
	movslq	%edi, %rbx
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm0, %ecx
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vmovd	%ebx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm10, %xmm3
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$1, %xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm29
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm11, %ecx
	vextracti32x4	$2, %zmm11, %xmm2
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edi
	vmovd	%xmm2, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm2
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm22, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm6
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm28, %xmm28
	movslq	%r15d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm31, %xmm31
	movslq	%r12d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm17
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm5
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm5, %ymm3, %ymm10
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm11, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm23, %xmm23
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm5
	vextracti32x4	$2, %zmm7, %xmm2
	vpextrd	$1, %xmm2, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %r10d
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r11d
	vpaddd	%zmm16, %zmm4, %zmm11
	vmovd	%xmm11, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm9, %r12d
	vpextrd	$3, %xmm9, %r14d
	vextracti128	$1, %ymm11, %xmm2
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %ebx
	vmovd	%xmm2, %edx
	vpextrd	$3, %xmm2, %esi
	vmovd	%edi, %xmm2
	vpextrd	$1, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm11, %edi
	vextracti32x4	$2, %zmm11, %xmm3
	vpextrd	$1, %xmm3, %r8d
	vpextrd	$2, %xmm3, %r13d
	vpextrd	$3, %xmm3, %r9d
	vmovd	%xmm3, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	movslq	64(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm8, %xmm3
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm2, %xmm2
	movslq	1728(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm14, %xmm6
	movslq	%ebp, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r12d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r15d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm5, %xmm5
	movslq	%esi, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm2, %xmm2
	movslq	256(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm29, %xmm8
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm9
	movslq	%r8d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm27
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm14
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm11, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm28
	vpaddd	%zmm18, %zmm4, %zmm11
	vextracti128	$1, %ymm11, %xmm2
	vpextrd	$1, %xmm2, %r15d
	vpextrd	$2, %xmm2, %r9d
	vpextrd	$3, %xmm2, %r8d
	vpextrd	$1, %xmm0, %edi
	vmovd	%xmm2, %ebp
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm28, %xmm2
	vpextrd	$2, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm28
	movslq	896(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm13, %xmm13
	vmovd	%xmm11, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm11, %edi
	vextracti32x4	$2, %zmm11, %xmm2
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm2
	vpextrd	$3, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	movslq	96(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm17, %xmm17
	movslq	%ebp, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm25, %xmm4
	movslq	%r15d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	movslq	-60(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm23, %xmm23
	movslq	%r9d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm25
	vextracti32x4	$3, %zmm7, %xmm7
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %r13d
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %r15d
	movslq	%ebx, %rcx
	vextracti32x4	$3, %zmm11, %xmm3
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm3, %ecx
	movslq	1792(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %esi
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %ebp
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vmovd	%xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm7, %r9d
	movslq	%r14d, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm7, %r8d
	vpextrd	$3, %xmm0, %ecx
	movslq	%r11d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm5, %xmm3
	vextracti32x4	$3, %zmm15, %xmm0
	vpextrd	$1, %xmm0, %ebx
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm5
	vmovd	%xmm0, %r11d
	movslq	%esi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %esi
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm7
	vpextrd	$3, %xmm0, %edx
	vinserti128	$1, %xmm8, %ymm9, %ymm15
	vinserti32x4	$1, %xmm27, %ymm14, %ymm0
	vinserti32x4	$1, %xmm13, %ymm17, %ymm8
	vinserti32x4	$1, %xmm4, %ymm23, %ymm4
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	512(%rsp), %zmm11       # 32-byte Folded Reload
                                        # zmm11 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm9
	vpmovzxbw	%ymm8, %zmm0            # zmm0 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm0, %zmm4
	vpmovzxbw	640(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm11, %zmm0, %zmm8
	vpaddw	%zmm9, %zmm8, %zmm8
	vpaddw	%zmm4, %zmm8, %zmm8
	vinserti32x4	$1, %xmm25, %ymm6, %ymm4
	vinserti128	$1, %xmm1, %ymm3, %ymm1
	vinserti128	$1, %xmm5, %ymm7, %ymm3
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm5
	vpmovzxbw	%ymm30, %zmm6           # zmm6 = ymm30[0],zero,ymm30[1],zero,ymm30[2],zero,ymm30[3],zero,ymm30[4],zero,ymm30[5],zero,ymm30[6],zero,ymm30[7],zero,ymm30[8],zero,ymm30[9],zero,ymm30[10],zero,ymm30[11],zero,ymm30[12],zero,ymm30[13],zero,ymm30[14],zero,ymm30[15],zero,ymm30[16],zero,ymm30[17],zero,ymm30[18],zero,ymm30[19],zero,ymm30[20],zero,ymm30[21],zero,ymm30[22],zero,ymm30[23],zero,ymm30[24],zero,ymm30[25],zero,ymm30[26],zero,ymm30[27],zero,ymm30[28],zero,ymm30[29],zero,ymm30[30],zero,ymm30[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm3, %zmm3            # zmm3 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpmovzxbw	%ymm12, %zmm4           # zmm4 = ymm12[0],zero,ymm12[1],zero,ymm12[2],zero,ymm12[3],zero,ymm12[4],zero,ymm12[5],zero,ymm12[6],zero,ymm12[7],zero,ymm12[8],zero,ymm12[9],zero,ymm12[10],zero,ymm12[11],zero,ymm12[12],zero,ymm12[13],zero,ymm12[14],zero,ymm12[15],zero,ymm12[16],zero,ymm12[17],zero,ymm12[18],zero,ymm12[19],zero,ymm12[20],zero,ymm12[21],zero,ymm12[22],zero,ymm12[23],zero,ymm12[24],zero,ymm12[25],zero,ymm12[26],zero,ymm12[27],zero,ymm12[28],zero,ymm12[29],zero,ymm12[30],zero,ymm12[31],zero
	vpaddw	%zmm6, %zmm4, %zmm3
	vpaddw	%zmm5, %zmm3, %zmm3
	movslq	416(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	576(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	384(%rsp), %r14                 # 4-byte Folded Reload
	vmovdqa	1088(%rsp), %xmm2               # 16-byte Reload
	vpinsrb	$10, (%rax,%rdi), %xmm2, %xmm7
	movslq	%r11d, %rdi
	vmovdqa	672(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$7, (%rax,%rbp), %xmm2, %xmm2
	vpaddw	%zmm1, %zmm3, %zmm3
	vpcmpltuw	%zmm3, %zmm8, %k1
	vpsubw	%zmm3, %zmm8, %zmm12
	vpsubw	%zmm8, %zmm3, %zmm12 {%k1}
	vpaddd	2624(%rsp), %zmm26, %zmm5       # 64-byte Folded Reload
	vextracti128	$1, %ymm5, %xmm3
	vmovd	%xmm3, 704(%rsp)                # 4-byte Folded Spill
	vpinsrb	$11, (%rax,%r14), %xmm7, %xmm7
	vpextrd	$1, %xmm3, 896(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 1088(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$3, %xmm3, 416(%rsp)            # 4-byte Folded Spill
	movslq	%r10d, %rdi
	movslq	480(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	1600(%rsp), %r10                # 4-byte Folded Reload
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%r8d, %r8
	movslq	%r13d, %r13
	movslq	%r15d, %r15
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rbx), %xmm7, %xmm3
	vpinsrb	$10, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm3, %ymm2, %ymm9
	vextracti32x4	$2, %zmm5, %xmm2
	vpextrd	$1, %xmm2, %r12d
	vpextrd	$2, %xmm2, %r14d
	vpaddw	%zmm11, %zmm11, %zmm3
	vmovd	%xmm2, %r15d
	vpextrd	$3, %xmm2, 512(%rsp)            # 4-byte Folded Spill
	vpaddd	2688(%rsp), %zmm26, %zmm2       # 64-byte Folded Reload
	vmovd	%xmm2, %edx
	vpaddw	%zmm6, %zmm6, %zmm6
	movslq	%edx, %rdx
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %esi
	vpaddw	%zmm6, %zmm3, %zmm3
	movslq	%ebx, %rbx
	movslq	%esi, %r13
	vpextrd	$3, %xmm2, %edi
	vextracti128	$1, %ymm2, %xmm6
	movslq	%edi, %rdi
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %ecx
	vpextrd	$2, %xmm6, %esi
	vmovdqu64	1408(%rsp), %zmm1       # 64-byte Reload
	vpaddw	1856(%rsp), %zmm1, %zmm7        # 64-byte Folded Reload
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm1
	vpextrd	$3, %xmm6, %edx
	vextracti32x4	$2, %zmm2, %xmm6
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %ebx
	vpinsrb	$2, (%rax,%r13), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %r13d
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm6, %edi
	vpextrd	$3, %xmm6, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %esi
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %edx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm5, %xmm5
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %r11d
	movslq	%r13d, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %r13d
	movslq	%ebp, %rbp
	vpaddw	%zmm0, %zmm0, %zmm0
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm2, %ebp
	movslq	%ebp, %rbp
	vpaddw	%zmm4, %zmm4, %zmm4
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm5, %ebp
	movzbl	(%rax,%rcx), %ecx
	vpaddw	%zmm4, %zmm0, %zmm6
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm5, %r10d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %edx
	vmovdqu64	1280(%rsp), %zmm2       # 64-byte Reload
	vpaddw	1152(%rsp), %zmm2, %zmm2        # 64-byte Folded Reload
	movslq	%edi, %rdi
	movslq	704(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1088(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	416(%rsp), %r9                  # 4-byte Folded Reload
	movslq	%r12d, %r12
	movslq	%r14d, %r14
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$4, (%rax,%rbx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	movslq	512(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$6, (%rax,%r8), %xmm0, %xmm0
	vpmovzxbw	%ymm20, %zmm8           # zmm8 = ymm20[0],zero,ymm20[1],zero,ymm20[2],zero,ymm20[3],zero,ymm20[4],zero,ymm20[5],zero,ymm20[6],zero,ymm20[7],zero,ymm20[8],zero,ymm20[9],zero,ymm20[10],zero,ymm20[11],zero,ymm20[12],zero,ymm20[13],zero,ymm20[14],zero,ymm20[15],zero,ymm20[16],zero,ymm20[17],zero,ymm20[18],zero,ymm20[19],zero,ymm20[20],zero,ymm20[21],zero,ymm20[22],zero,ymm20[23],zero,ymm20[24],zero,ymm20[25],zero,ymm20[26],zero,ymm20[27],zero,ymm20[28],zero,ymm20[29],zero,ymm20[30],zero,ymm20[31],zero
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r15), %xmm0, %xmm0
	vpmovzxbw	%ymm21, %zmm14          # zmm14 = ymm21[0],zero,ymm21[1],zero,ymm21[2],zero,ymm21[3],zero,ymm21[4],zero,ymm21[5],zero,ymm21[6],zero,ymm21[7],zero,ymm21[8],zero,ymm21[9],zero,ymm21[10],zero,ymm21[11],zero,ymm21[12],zero,ymm21[13],zero,ymm21[14],zero,ymm21[15],zero,ymm21[16],zero,ymm21[17],zero,ymm21[18],zero,ymm21[19],zero,ymm21[20],zero,ymm21[21],zero,ymm21[22],zero,ymm21[23],zero,ymm21[24],zero,ymm21[25],zero,ymm21[26],zero,ymm21[27],zero,ymm21[28],zero,ymm21[29],zero,ymm21[30],zero,ymm21[31],zero
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%r14), %xmm0, %xmm5
	vpmovzxbw	%ymm10, %zmm16          # zmm16 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	movslq	%r13d, %rdi
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm5
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm5
	vpaddw	%zmm3, %zmm7, %zmm10
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rsi), %xmm5, %xmm3
	vpinsrb	$14, (%rax,%rdi), %xmm3, %xmm3
	vpsllw	$2, %zmm8, %zmm11
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm3, %ymm13
	vpmovzxbw	%ymm15, %zmm3           # zmm3 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddd	2560(%rsp), %zmm26, %zmm7       # 64-byte Folded Reload
	vpaddw	%zmm6, %zmm2, %zmm1
	vpaddd	2496(%rsp), %zmm26, %zmm2       # 64-byte Folded Reload
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm2, %ecx
	vpsllw	$2, %zmm14, %zmm6
	movslq	%ecx, %rbp
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %r8d
	vpaddw	%zmm11, %zmm10, %zmm10
	movslq	%ecx, %rsi
	vextracti128	$1, %ymm2, %xmm5
	vmovd	%xmm5, %edi
	vpextrd	$1, %xmm5, %r13d
	vpaddw	%zmm6, %zmm1, %zmm1
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r12d
	vextracti32x4	$2, %zmm2, %xmm5
	vpaddw	%zmm16, %zmm16, %zmm11
	vmovd	%xmm5, %r14d
	vpextrd	$1, %xmm5, %r11d
	vpextrd	$2, %xmm5, %r10d
	vpcmpltuw	%zmm1, %zmm10, %k1
	vpextrd	$3, %xmm5, %r9d
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	vpsubw	%zmm1, %zmm10, %zmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpsubw	%zmm10, %zmm1, %zmm6 {%k1}
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpaddw	%zmm3, %zmm8, %zmm5
	vextracti128	$1, %ymm7, %xmm4
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %edx
	vpaddw	%zmm5, %zmm11, %zmm5
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm0
	vpextrd	$2, %xmm4, %ebx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$2, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm4, %esi
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r8d, %rbp
	vpextrd	$2, %xmm2, %r8d
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpextrd	$3, %xmm2, %edi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm2, %edx
	vextracti32x4	$2, %zmm7, %xmm2
	vpinsrb	$3, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %ebp
	movslq	%r13d, %r13
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%r13), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %r13d
	vpinsrb	$6, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm2, %ebx
	movslq	%r15d, %r15
	movslq	%r12d, %r12
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm7, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%r15), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %r15d
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %ecx
	vpinsrb	$7, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm2, %ebx
	vpextrd	$3, %xmm2, %r12d
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r13d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r9d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r15d, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpmovzxbw	%ymm13, %zmm2           # zmm2 = ymm13[0],zero,ymm13[1],zero,ymm13[2],zero,ymm13[3],zero,ymm13[4],zero,ymm13[5],zero,ymm13[6],zero,ymm13[7],zero,ymm13[8],zero,ymm13[9],zero,ymm13[10],zero,ymm13[11],zero,ymm13[12],zero,ymm13[13],zero,ymm13[14],zero,ymm13[15],zero,ymm13[16],zero,ymm13[17],zero,ymm13[18],zero,ymm13[19],zero,ymm13[20],zero,ymm13[21],zero,ymm13[22],zero,ymm13[23],zero,ymm13[24],zero,ymm13[25],zero,ymm13[26],zero,ymm13[27],zero,ymm13[28],zero,ymm13[29],zero,ymm13[30],zero,ymm13[31],zero
	vpmovzxbw	%ymm9, %zmm4            # zmm4 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpaddw	%zmm2, %zmm4, %zmm2
	vpaddw	%zmm2, %zmm5, %zmm2
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	vpaddw	%zmm4, %zmm4, %zmm1
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm1, %zmm0
	vpaddw	%zmm14, %zmm16, %zmm1
	vpaddw	%zmm3, %zmm1, %zmm1
	vpaddw	%zmm0, %zmm1, %zmm0
	vpcmpltuw	%zmm0, %zmm2, %k1
	vpsubw	%zmm0, %zmm2, %zmm1
	vpsubw	%zmm2, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm24, %zmm12, %zmm0
	vpaddw	%zmm6, %zmm1, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	2048(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, 1216(%rsp)                # 4-byte Folded Spill
	movl	960(%rsp), %r10d                # 4-byte Reload
	addl	%ecx, %r10d
	movl	1664(%rsp), %r14d               # 4-byte Reload
	addl	%ecx, %r14d
	movl	1472(%rsp), %r13d               # 4-byte Reload
	addl	%ecx, %r13d
	movl	1536(%rsp), %edx                # 4-byte Reload
	addl	%ecx, %edx
	movl	%edx, %ecx
	decq	288(%rsp)                       # 8-byte Folded Spill
	jne	.LBB214_13
# %bb.14:                               # %"end for output.s0.y.yi5"
                                        #   in Loop: Header=BB214_12 Depth=2
	movq	832(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	464(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	816(%rsp), %rdx                 # 8-byte Folded Reload
	jne	.LBB214_12
.LBB214_15:                             # %"end for output.s0.x.x2"
                                        #   in Loop: Header=BB214_10 Depth=1
	cmpl	$0, 632(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_20
# %bb.16:                               # %"for output.s0.x.x.rebased.preheader"
                                        #   in Loop: Header=BB214_10 Depth=1
	movslq	456(%rsp), %r15                 # 4-byte Folded Reload
	leaq	1(%r15), %rdx
	leaq	-1(%r15), %rsi
	leaq	2(%r15), %rdi
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movq	%rcx, %rbp
	imulq	%r15, %rbp
	addq	$-2, %r15
	imulq	%rcx, %rdx
	imulq	%rcx, %rsi
	imulq	%rcx, %rdi
	imulq	%rcx, %r15
	addq	%rax, %rbp
	movq	%rbp, 1024(%rsp)                # 8-byte Spill
	addq	%rax, %rdx
	movq	%rdx, 2048(%rsp)                # 8-byte Spill
	addq	%rax, %rsi
	movq	%rsi, 1984(%rsp)                # 8-byte Spill
	addq	%rax, %rdi
	movq	%rdi, 2176(%rsp)                # 8-byte Spill
	addq	%rax, %r15
	movq	568(%rsp), %rcx                 # 8-byte Reload
	movq	280(%rsp), %rdx                 # 8-byte Reload
	addq	%rdx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movl	304(%rsp), %r13d                # 4-byte Reload
	movq	784(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %r12d
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB214_17:                             # %"for output.s0.x.x.rebased"
                                        #   Parent Loop BB214_10 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_18 Depth 3
	movslq	%r12d, %rdi
	addq	1216(%rsp), %rdi                # 8-byte Folded Reload
	movslq	%r13d, %rbp
	movl	$16, %ecx
	movq	%r15, %rdx
	movq	2176(%rsp), %rbx                # 8-byte Reload
	movq	1984(%rsp), %r10                # 8-byte Reload
	movq	2048(%rsp), %r9                 # 8-byte Reload
	movq	1024(%rsp), %r14                # 8-byte Reload
	movq	40(%rsp), %r11                  # 8-byte Reload
	movq	200(%rsp), %r8                  # 8-byte Reload
	.p2align	4, 0x90
.LBB214_18:                             # %"for output.s0.y.yi7"
                                        #   Parent Loop BB214_10 Depth=1
                                        #     Parent Loop BB214_17 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vpmovzxbw	-2(%rdx,%rbp), %zmm7    # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%rdx,%rbp), %zmm8     # zmm8 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%rbx,%rbp), %zmm9    # zmm9 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%rbx,%rbp), %zmm10    # zmm10 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r10,%rbp), %zmm11   # zmm11 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r10,%rbp), %zmm5     # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r9,%rbp), %zmm12    # zmm12 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r9,%rbp), %zmm6      # zmm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r14,%rbp), %zmm3    # zmm3 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r14,%rbp), %zmm0     # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-1(%r14,%rbp), %zmm1    # zmm1 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	(%r14,%rbp), %zmm2      # zmm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	1(%r14,%rbp), %zmm4     # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm7, %zmm7, %zmm7
	vpaddw	%zmm8, %zmm8, %zmm8
	vpaddw	%zmm9, %zmm9, %zmm9
	vpaddw	%zmm10, %zmm10, %zmm10
	vpmovzxbw	(%rdx,%rbp), %zmm13     # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpsllw	$2, %zmm13, %zmm13
	vpmovzxbw	-1(%rdx,%rbp), %zmm14   # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	1(%rdx,%rbp), %zmm14    # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpaddw	%zmm7, %zmm8, %zmm15
	vpaddw	%zmm13, %zmm15, %zmm13
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	(%rbx,%rbp), %zmm14     # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpsllw	$2, %zmm14, %zmm14
	vpmovzxbw	-1(%rbx,%rbp), %zmm15   # zmm15 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm15, %zmm15, %zmm15
	vpmovzxbw	1(%rbx,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm15, %zmm14, %zmm14
	vpaddw	%zmm16, %zmm16, %zmm15
	vpaddw	%zmm9, %zmm10, %zmm16
	vpaddw	%zmm14, %zmm16, %zmm14
	vpaddw	%zmm15, %zmm14, %zmm14
	vpcmpltuw	%zmm14, %zmm13, %k1
	vpsubw	%zmm14, %zmm13, %zmm15
	vpsubw	%zmm13, %zmm14, %zmm15 {%k1}
	vpmovzxbw	(%r10,%rbp), %zmm13     # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-1(%r10,%rbp), %zmm14   # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm13, %zmm13, %zmm13
	vpmovzxbw	1(%r10,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm16, %zmm14, %zmm14
	vpaddw	%zmm11, %zmm5, %zmm16
	vpaddw	%zmm13, %zmm16, %zmm13
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	(%r9,%rbp), %zmm14      # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpmovzxbw	-1(%r9,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	1(%r9,%rbp), %zmm17     # zmm17 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm17, %zmm16, %zmm16
	vpaddw	%zmm12, %zmm6, %zmm17
	vpaddw	%zmm14, %zmm17, %zmm14
	vpaddw	%zmm16, %zmm14, %zmm14
	vpcmpltuw	%zmm14, %zmm13, %k1
	vpsubw	%zmm14, %zmm13, %zmm16
	vpsubw	%zmm13, %zmm14, %zmm16 {%k1}
	vpaddw	%zmm15, %zmm16, %zmm13
	vpmovuswb	%zmm13, %ymm13
	vpsllw	$2, %zmm3, %zmm14
	vpaddw	%zmm11, %zmm11, %zmm11
	vpaddw	%zmm12, %zmm12, %zmm12
	vpaddw	%zmm12, %zmm11, %zmm11
	vpaddw	%zmm7, %zmm9, %zmm7
	vpaddw	%zmm11, %zmm7, %zmm7
	vpaddw	%zmm14, %zmm7, %zmm7
	vpsllw	$2, %zmm0, %zmm9
	vpaddw	%zmm5, %zmm5, %zmm5
	vpaddw	%zmm6, %zmm6, %zmm6
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm8, %zmm10, %zmm6
	vpaddw	%zmm5, %zmm6, %zmm5
	vpaddw	%zmm9, %zmm5, %zmm5
	vpcmpltuw	%zmm5, %zmm7, %k1
	vpsubw	%zmm5, %zmm7, %zmm6
	vpsubw	%zmm7, %zmm5, %zmm6 {%k1}
	vpaddw	%zmm1, %zmm1, %zmm5
	vpmovzxbw	-3(%r14,%rbp), %zmm7    # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm2, %zmm3, %zmm3
	vpaddw	%zmm3, %zmm5, %zmm3
	vpaddw	%zmm7, %zmm4, %zmm5
	vpaddw	%zmm5, %zmm3, %zmm3
	vpaddw	%zmm4, %zmm4, %zmm4
	vpmovzxbw	3(%r14,%rbp), %zmm5     # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm5, %zmm4, %zmm4
	vpaddw	%zmm0, %zmm1, %zmm0
	vpaddw	%zmm2, %zmm0, %zmm0
	vpaddw	%zmm4, %zmm0, %zmm0
	vpcmpltuw	%zmm0, %zmm3, %k1
	vpsubw	%zmm0, %zmm3, %zmm1
	vpsubw	%zmm3, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm6, %zmm1, %zmm0
	vpmovuswb	%zmm0, %ymm0
	vpaddb	%ymm0, %ymm13, %ymm0
	vmovdqu	%ymm0, (%rdi)
	addq	%r8, %rdi
	addq	%r11, %r14
	addq	%r11, %r9
	addq	%r11, %r10
	addq	%r11, %rbx
	addq	%r11, %rdx
	decq	%rcx
	jne	.LBB214_18
# %bb.19:                               # %"end for output.s0.y.yi8"
                                        #   in Loop: Header=BB214_17 Depth=2
	incq	%rsi
	addl	$32, %r12d
	addl	$32, %r13d
	cmpq	632(%rsp), %rsi                 # 8-byte Folded Reload
	jne	.LBB214_17
.LBB214_20:                             # %"end for output.s0.x.x.rebased"
                                        #   in Loop: Header=BB214_10 Depth=1
	cmpl	$0, 448(%rsp)                   # 4-byte Folded Reload
	movq	280(%rsp), %rdi                 # 8-byte Reload
	jle	.LBB214_25
# %bb.21:                               # %"for output.s0.x.x.rebased10.preheader"
                                        #   in Loop: Header=BB214_10 Depth=1
	addq	792(%rsp), %rdi                 # 8-byte Folded Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_22:                             # %"for output.s0.x.x.rebased10"
                                        #   Parent Loop BB214_10 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_23 Depth 3
	movq	624(%rsp), %rcx                 # 8-byte Reload
	movq	%rdx, 464(%rsp)                 # 8-byte Spill
	addl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3136(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2432(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2368(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2048(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1984(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2304(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2240(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2944(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2880(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2816(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2752(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2688(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2624(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2560(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2496(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	116(%rsp), %ecx                 # 4-byte Reload
	movl	120(%rsp), %r13d                # 4-byte Reload
	movl	124(%rsp), %r14d                # 4-byte Reload
	movl	128(%rsp), %r11d                # 4-byte Reload
	movl	132(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 2176(%rsp)                # 4-byte Spill
	movq	%rdi, 280(%rsp)                 # 8-byte Spill
	movq	%rdi, 1024(%rsp)                # 8-byte Spill
	.p2align	4, 0x90
.LBB214_23:                             # %"for output.s0.y.yi13"
                                        #   Parent Loop BB214_10 Depth=1
                                        #     Parent Loop BB214_22 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 1216(%rsp)                # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	2048(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm0, %zmm1
	vmovdqa64	%zmm2, %zmm14
	vmovdqu64	1984(%rsp), %zmm25      # 64-byte Reload
	vpaddd	%zmm25, %zmm0, %zmm2
	vmovdqa64	%zmm0, %zmm6
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	movslq	%ecx, %rdi
	movslq	%edx, %rdx
	vpextrd	$3, %xmm2, %ecx
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$1, %xmm3, %r15d
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %r12d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$3, %xmm3, %r9d
	vpextrd	$1, %xmm0, %r10d
	vpextrd	$2, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 960(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %r8d
	vextracti32x4	$3, %zmm2, %xmm22
	vpextrd	$1, %xmm22, 416(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm22, 1152(%rsp)          # 4-byte Folded Spill
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm2
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %ebp
	vpinsrb	$2, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm22, 1280(%rsp)          # 4-byte Folded Spill
	movslq	%ebp, %rbx
	vextracti128	$1, %ymm1, %xmm3
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm4
	vpextrd	$1, %xmm3, %ebp
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm5
	vpextrd	$2, %xmm3, %edi
	vpinsrb	$3, (%rax,%rbx), %xmm2, %xmm4
	vmovd	%xmm3, %ebx
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %esi
	vpinsrb	$3, (%rax,%rcx), %xmm5, %xmm5
	vmovdqu64	2816(%rsp), %zmm21      # 64-byte Reload
	vmovdqa64	%zmm6, %zmm3
	vmovdqu64	%zmm6, 1408(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm6, %zmm20
	vmovd	%xmm20, %ecx
	movslq	%ecx, %rdx
	movslq	%r8d, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$1, %xmm20, %edx
	movslq	%edx, %rdx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm7
	vpextrd	$1, %xmm2, %r8d
	vpinsrb	$4, (%rax,%rbx), %xmm4, %xmm4
	movslq	%ebp, %rdx
	vpextrd	$2, %xmm2, %ebp
	vextracti32x4	$3, %zmm1, %xmm9
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	vmovdqu64	2752(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm3, %zmm6
	vpextrd	$2, %xmm20, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm20, %ebx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$3, %xmm2, 1088(%rsp)           # 4-byte Folded Spill
	movslq	%ebx, %rcx
	vextracti32x4	$1, %ymm20, %xmm1
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm1, %ecx
	movslq	%r15d, %rbx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm6, %ecx
	vpinsrb	$5, (%rax,%rbx), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %ebx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$1, %xmm9, %r15d
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %ecx
	movslq	%r12d, %rbx
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rdi), %xmm4, %xmm4
	vmovd	%xmm0, %ecx
	vpextrd	$3, %xmm6, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rbx), %xmm5, %xmm11
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %ebx
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm0, %edi
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%esi, %r12
	movslq	%edi, %rsi
	movslq	%ebx, %rdi
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rdi), %xmm7, %xmm12
	vmovd	%xmm2, %esi
	vpextrd	$2, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm3, %xmm2
	movl	%r13d, 1536(%rsp)               # 4-byte Spill
	vpbroadcastd	%r13d, %zmm3
	vpaddd	%zmm14, %zmm3, %zmm10
	vmovdqa64	%zmm14, %zmm26
	vmovdqa64	%zmm3, %zmm17
	vmovd	%xmm10, %edi
	movslq	%edi, %rbx
	vpinsrb	$7, (%rax,%r12), %xmm4, %xmm3
	vpextrd	$1, %xmm10, %edi
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpextrd	$2, %xmm9, %r13d
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$2, %xmm10, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm10, %edi
	movslq	%edi, %rdi
	movslq	%r9d, %rbx
	vpinsrb	$3, (%rax,%rdi), %xmm4, %xmm7
	vpextrd	$3, %xmm9, 512(%rsp)            # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rbx), %xmm11, %xmm5
	movslq	%esi, %rsi
	vpextrd	$3, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm12, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm0, %edi
	vextracti32x4	$2, %zmm20, %xmm23
	vpaddd	%zmm25, %zmm17, %zmm29
	vextracti128	$1, %ymm10, %xmm4
	vpinsrb	$8, (%rax,%rsi), %xmm3, %xmm11
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm7, %xmm3
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm4
	vmovd	%xmm29, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rcx), %xmm5, %xmm3
	vpextrd	$1, %xmm29, %ecx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm5
	vpextrd	$1, %xmm0, %esi
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm7
	movslq	%edi, %rcx
	vpextrd	$3, %xmm29, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm12
	vextracti32x4	$1, %ymm29, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$3, (%rax,%rdi), %xmm5, %xmm2
	vpextrd	$1, %xmm1, %edi
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edi
	vextracti32x4	$2, %zmm29, %xmm13
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm10, %xmm14
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	vmovd	%xmm14, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm4, %xmm16
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm15
	vmovdqu64	%zmm17, 896(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm17, %zmm30
	vmovd	%xmm30, %ecx
	vpextrd	$2, %xmm0, %edi
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%r10d, %r10
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%r8d, %rbx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$9, (%rax,%rbx), %xmm11, %xmm2
	vextracti32x4	$3, %zmm6, %xmm11
	vpextrd	$1, %xmm23, %ebx
	vextracti32x4	$1, %ymm30, %xmm0
	vpinsrb	$9, (%rax,%r10), %xmm3, %xmm3
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %edx
	vpaddd	%zmm8, %zmm17, %zmm6
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm30, %xmm27
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm27, %edx
	movslq	%edx, %rdx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rbx), %xmm7, %xmm4
	vpextrd	$1, %xmm6, %r8d
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$1, %xmm11, %ebx
	movslq	%r8d, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %edx
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$2, %xmm11, %r8d
	vpinsrb	$9, (%rax,%rsi), %xmm12, %xmm12
	vpextrd	$1, %xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm16, %xmm17
	vpextrd	$1, %xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm15, %xmm18
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm27, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm1, %xmm19
	vmovd	%xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm7, %xmm1
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm1, %xmm7
	vpextrd	$3, %xmm5, %edx
	movslq	704(%rsp), %rsi                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm6, %xmm1
	vpinsrb	$7, (%rax,%rdx), %xmm7, %xmm5
	vmovd	%xmm1, %edx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm24
	vpextrd	$3, %xmm11, 480(%rsp)           # 4-byte Folded Spill
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm15
	vpextrd	$2, %xmm23, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm20, %xmm31
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm16
	movl	%r14d, 1472(%rsp)               # 4-byte Spill
	vpbroadcastd	%r14d, %zmm0
	vpaddd	%zmm26, %zmm0, %zmm3
	vmovdqa64	%zmm0, %zmm20
	vmovd	%xmm3, %esi
	movslq	%esi, %rbp
	vpinsrb	$10, (%rax,%rdx), %xmm4, %xmm26
	vpextrd	$1, %xmm3, %esi
	movzbl	(%rax,%rbp), %edx
	vmovd	%edx, %xmm2
	vpextrd	$3, %xmm23, %ebp
	movslq	%esi, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm31, %r14d
	vpinsrb	$10, (%rax,%rdi), %xmm12, %xmm4
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm17, %xmm5
	vpextrd	$2, %xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm18, %xmm28
	vpextrd	$2, %xmm27, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm19, %xmm23
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%rdx), %xmm24, %xmm24
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm0
	vpextrd	$3, %xmm7, %edx
	vpaddd	%zmm25, %zmm20, %zmm12
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm3, %xmm2
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %edx
	movslq	1088(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm25
	vmovd	%xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdi), %xmm15, %xmm15
	vpextrd	$1, %xmm12, %edi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm31, %r10d
	movslq	%edi, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm12, %edx
	movslq	%edx, %rdx
	movslq	960(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm13, %esi
	vpinsrb	$11, (%rax,%rdi), %xmm16, %xmm19
	movslq	%ebp, %rdx
	vpextrd	$3, %xmm14, %edi
	vextracti128	$1, %ymm12, %xmm7
	vpinsrb	$11, (%rax,%rdx), %xmm26, %xmm17
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm7, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm18
	movslq	%edi, %rcx
	vpextrd	$3, %xmm27, %ebp
	vextracti32x4	$2, %zmm12, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm16
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm1
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm27
	vpinsrb	$11, (%rax,%rsi), %xmm28, %xmm14
	vmovd	%xmm22, %ecx
	vmovdqu64	%zmm20, 704(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm20, %zmm1
	vmovd	%xmm1, %esi
	movslq	%esi, %rdi
	movzbl	(%rax,%rdi), %esi
	movslq	%ebp, %rdi
	vmovd	%esi, %xmm4
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdi), %xmm23, %xmm23
	vpextrd	$2, %xmm1, %edi
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$3, %xmm2, %esi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm2
	vpextrd	$3, %xmm1, %edi
	movslq	%edi, %rdi
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %edi
	vpinsrb	$11, (%rax,%rdx), %xmm24, %xmm13
	vmovd	%xmm9, %edx
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm4, %edi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm5
	vpinsrb	$11, (%rax,%rsi), %xmm25, %xmm26
	vmovd	%xmm11, %esi
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm27, %xmm28
	vmovd	%xmm31, %edi
	vextracti32x4	$3, %zmm29, %xmm2
	vpaddd	%zmm8, %zmm20, %zmm4
	vmovdqa64	%zmm8, %zmm22
	vextracti32x4	$2, %zmm1, %xmm0
	vpinsrb	$12, (%rax,%rdx), %xmm15, %xmm9
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm24
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm19, %xmm15
	vpextrd	$1, %xmm4, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$3, %xmm31, 672(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movslq	%edi, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm2, %ecx
	vextracti128	$1, %ymm4, %xmm5
	vpinsrb	$12, (%rax,%rdx), %xmm17, %xmm17
	vmovd	%xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm5
	vpextrd	$1, %xmm2, %ebp
	vpinsrb	$12, (%rax,%rsi), %xmm18, %xmm19
	vextracti32x4	$3, %zmm10, %xmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$12, (%rax,%rdx), %xmm16, %xmm16
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm7, %edx
	vextracti32x4	$3, %zmm6, %xmm25
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm7, %edx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm30, %xmm8
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm30
	vpinsrb	$12, (%rax,%rcx), %xmm14, %xmm29
	vextracti32x4	$3, %zmm12, %xmm10
	movl	%r11d, 1664(%rsp)               # 4-byte Spill
	vpbroadcastd	%r11d, %zmm5
	vmovdqu64	2048(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm5, %zmm7
	vmovdqa64	%zmm5, %zmm14
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rdx
	vmovd	%xmm8, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm5
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm23, %xmm27
	movslq	%edx, %rcx
	vpextrd	$3, %xmm7, %edx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm25, %ecx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm7, %xmm6
	vpinsrb	$3, (%rax,%rdx), %xmm5, %xmm5
	vmovd	%xmm6, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm13, %xmm23
	vmovd	%xmm10, %ecx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm6, %edx
	vextracti32x4	$3, %zmm3, %xmm13
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm5, %xmm3
	vmovd	%xmm13, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm4, %xmm12
	vextracti32x4	$3, %zmm1, %xmm11
	vmovdqa64	%zmm14, %zmm5
	vmovdqu64	1984(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm14, %zmm1
	vextracti32x4	$2, %zmm7, %xmm4
	vpinsrb	$12, (%rax,%rdx), %xmm26, %xmm6
	vmovdqa	%xmm6, 1600(%rsp)               # 16-byte Spill
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm6
	vmovdqa	%xmm6, 176(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm14
	vmovd	%xmm11, %ecx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm24, %xmm4
	vmovdqa	%xmm4, 1856(%rsp)               # 16-byte Spill
	vmovd	%xmm12, %ecx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	vextracti32x4	$2, %zmm1, %xmm4
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm30, %xmm6
	vmovdqa	%xmm6, 160(%rsp)                # 16-byte Spill
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm3
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm1
	vmovd	%xmm3, %edx
	vextracti32x4	$3, %zmm7, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm14, %xmm6
	vmovdqa	%xmm6, 1728(%rsp)               # 16-byte Spill
	movslq	%edx, %rdx
	vpaddd	%zmm21, %zmm5, %zmm6
	vmovdqu64	%zmm5, 960(%rsp)        # 64-byte Spill
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm1
	vmovdqa	%xmm1, 1088(%rsp)               # 16-byte Spill
	vpextrd	$1, %xmm6, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$2, %xmm2, %r9d
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %edx
	movslq	%edx, %rdx
	movslq	%r15d, %rsi
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm2, 352(%rsp)            # 4-byte Folded Spill
	vpinsrb	$13, (%rax,%rsi), %xmm9, %xmm14
	movslq	416(%rsp), %rsi                 # 4-byte Folded Reload
	vpextrd	$1, %xmm0, %r15d
	vextracti128	$1, %ymm6, %xmm2
	vpinsrb	$13, (%rax,%rsi), %xmm15, %xmm31
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %esi
	movslq	%r14d, %rdi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm2
	vpextrd	$2, %xmm0, %r14d
	vpinsrb	$13, (%rax,%rdi), %xmm17, %xmm1
	movslq	%ebx, %rdi
	vpextrd	$3, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$13, (%rax,%rdi), %xmm19, %xmm30
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %edi
	vpaddd	%zmm22, %zmm5, %zmm15
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm6, %xmm9
	vpinsrb	$11, (%rax,%rdi), %xmm2, %xmm0
	vmovd	%xmm9, %edi
	movslq	%edi, %rdi
	movslq	%r15d, %rbx
	vpinsrb	$12, (%rax,%rdi), %xmm0, %xmm0
	vmovdqa	%xmm0, 1920(%rsp)               # 16-byte Spill
	vmovd	%xmm15, %edi
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rbx), %xmm16, %xmm2
	vpextrd	$1, %xmm15, %ebx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm0
	vpextrd	$1, %xmm25, %edi
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm15, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$3, %xmm15, %ebx
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm25, %r12d
	vpinsrb	$13, (%rax,%rbp), %xmm29, %xmm16
	vpextrd	$1, %xmm8, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm15, %xmm6
	vpinsrb	$13, (%rax,%rbp), %xmm27, %xmm17
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm15, %xmm6
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebp
	movslq	%r13d, %r15
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebp
	movslq	%edi, %rdi
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm28
	vpextrd	$3, %xmm25, %r13d
	vpinsrb	$13, (%rax,%rdi), %xmm23, %xmm25
	movl	2176(%rsp), %ecx                # 4-byte Reload
	vpbroadcastd	%ecx, %zmm19
	vpaddd	%zmm20, %zmm19, %zmm29
	vmovd	%xmm29, %edi
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%r15), %xmm14, %xmm23
	vpextrd	$1, %xmm29, %ebp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm0
	vpextrd	$2, %xmm8, %r15d
	movslq	%ebp, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm29, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm29, %edi
	movslq	%edi, %rdi
	movslq	1152(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm8, %ebx
	vpinsrb	$14, (%rax,%rbp), %xmm31, %xmm31
	movslq	%r10d, %rbp
	vpextrd	$1, %xmm10, 384(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm29, %xmm6
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm29, %xmm6
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebp
	movslq	%r8d, %r8
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm10, 416(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%r8), %xmm30, %xmm30
	movslq	%r14d, %rbp
	vpextrd	$3, %xmm6, %r8d
	vpaddd	%zmm18, %zmm19, %zmm8
	vpinsrb	$14, (%rax,%rbp), %xmm2, %xmm27
	vmovd	%xmm8, %ebp
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$3, %xmm10, 1152(%rsp)          # 4-byte Folded Spill
	movslq	%ebp, %rdx
	vextracti128	$1, %ymm8, %xmm0
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm8, %ebp
	vmovd	%xmm0, %r8d
	vpextrd	$1, %xmm0, %r11d
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %ecx
	vmovd	%edx, %xmm0
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r9d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm16, %xmm2
	vpextrd	$3, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r8d, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r11d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	movslq	512(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%r14d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm13, 320(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm13, 640(%rsp)           # 4-byte Folded Spill
	vpinsrb	$15, (%rax,%rdx), %xmm23, %xmm10
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpextrd	$3, %xmm13, 512(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm8, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm13
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinserti128	$1, %xmm10, %ymm13, %ymm5
	vmovdqu	%ymm5, 576(%rsp)                # 32-byte Spill
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm10
	vpaddd	%zmm21, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	vextracti32x4	$1, %ymm16, %xmm0
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm0, %r10d
	vpextrd	$2, %xmm0, %r9d
	vpextrd	$3, %xmm0, %r11d
	vmovd	%xmm0, %r14d
	vextracti32x4	$2, %zmm16, %xmm0
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm16, %ecx
	vmovd	%xmm0, %ebp
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %edi
	vpextrd	$3, %xmm0, %r8d
	vmovd	%edx, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	movslq	%edi, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm12, 48(%rsp)            # 4-byte Folded Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm14
	vpextrd	$2, %xmm12, 1792(%rsp)          # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm12, 1280(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm22, %zmm19, %zmm17
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm12
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, 208(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 256(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, 672(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm6
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm17, %ecx
	vextracti32x4	$1, %ymm17, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm0
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm6
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	movslq	%edi, %rdx
	vinserti32x4	$1, %xmm12, %ymm6, %ymm24
	vextracti32x4	$2, %zmm17, %xmm6
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	movslq	%ebx, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %edx
	vpinsrb	$15, (%rax,%rsi), %xmm13, %xmm11
	movslq	%edx, %rdx
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rcx), %xmm25, %xmm12
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqu	%ymm1, 480(%rsp)                # 32-byte Spill
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm23
	vmovdqu64	2432(%rsp), %zmm20      # 64-byte Reload
	vmovdqu64	1408(%rsp), %zmm22      # 64-byte Reload
	vpaddd	%zmm20, %zmm22, %zmm2
	vinserti128	$1, %xmm11, %ymm12, %ymm0
	vmovdqu	%ymm0, 352(%rsp)                # 32-byte Spill
	vmovdqu64	2368(%rsp), %zmm5       # 64-byte Reload
	vpaddd	%zmm5, %zmm22, %zmm1
	vextracti128	$1, %ymm1, %xmm0
	vmovd	%xmm0, 96(%rsp)                 # 4-byte Folded Spill
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vextracti32x4	$3, %zmm15, %xmm13
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm26
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti128	$1, %ymm2, %xmm6
	vpextrd	$1, %xmm6, %r10d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vmovd	%xmm6, %edi
	vextracti32x4	$3, %zmm29, %xmm18
	vmovd	%xmm18, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm7, %xmm6
	vmovdqa	%xmm6, 240(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm6
	vpextrd	$1, %xmm6, %ebp
	vpextrd	$2, %xmm6, %ebx
	vpextrd	$3, %xmm6, %esi
	vmovd	%xmm6, %edx
	vextracti32x4	$3, %zmm8, %xmm11
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm6
	vmovdqa	%xmm6, 144(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm1, %xmm6
	vpextrd	$1, %xmm6, %edi
	vpextrd	$2, %xmm6, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm6, %esi
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %ebx
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %ebp
	vextracti32x4	$3, %zmm1, %xmm1
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2240(%rsp), %zmm31      # 64-byte Reload
	vpaddd	%zmm31, %zmm22, %zmm1
	vmovd	%xmm2, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, -28(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm2
	vmovd	%xmm2, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2304(%rsp), %zmm21      # 64-byte Reload
	vpaddd	%zmm21, %zmm22, %zmm2
	vmovd	%xmm1, %r10d
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r15d
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, -32(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -36(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r13d
	vpextrd	$3, %xmm1, %r12d
	vmovd	%xmm1, %edx
	vmovd	%ecx, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm2, %xmm6
	vpextrd	$1, %xmm6, %ecx
	vpextrd	$2, %xmm6, %ebp
	vpextrd	$3, %xmm6, %r9d
	vmovd	%xmm6, %ebx
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r8d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r13d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r12d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %esi
	vmovd	%xmm2, %edi
	vextracti32x4	$3, %zmm16, %xmm28
	vmovd	%xmm28, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm14, %xmm16
	movslq	%r9d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm2
	movslq	%r10d, %rbp
	movslq	%r11d, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm0, %xmm0
	movslq	%r14d, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r15d, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	movslq	-28(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rdi), %xmm6, %xmm6
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	movslq	-48(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm6, %xmm6
	movslq	80(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm1, %xmm1
	movslq	-100(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rdi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	movslq	%edx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm0, %ymm1, %ymm1
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm0
	vpextrd	$1, %xmm3, -80(%rsp)            # 4-byte Folded Spill
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-36(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm2, %ymm0, %ymm0
	vpaddd	%zmm21, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdx
	vpextrd	$2, %xmm3, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$1, %xmm4, %r8d
	vpextrd	$2, %xmm4, 64(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$3, %xmm4, -60(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm4
	movzbl	(%rax,%rdx), %edx
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %ebp
	vpextrd	$3, %xmm3, %r14d
	vmovdqu64	2880(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm22, %zmm10
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %r9d
	vpextrd	$2, %xmm4, %r10d
	vpextrd	$3, %xmm4, %r11d
	vextracti128	$1, %ymm10, %xmm3
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm3, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm3
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	movslq	%edi, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebp, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$2, %zmm10, %xmm4
	vpextrd	$1, %xmm4, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -92(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, -100(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$3, %zmm17, %xmm15
	vmovd	%xmm15, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm23, %xmm12
	movslq	%r14d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	movslq	320(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	1600(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm25
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	176(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	vmovdqa	%xmm4, 80(%rsp)                 # 16-byte Spill
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm23
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm27
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm17
	movslq	%r8d, %rcx
	vmovdqa	1728(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	vmovdqa	%xmm3, 1856(%rsp)               # 16-byte Spill
	vextracti32x4	$3, %zmm2, %xmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm17, %xmm2
	vmovdqa	%xmm2, 1728(%rsp)               # 16-byte Spill
	vmovdqu64	2944(%rsp), %zmm30      # 64-byte Reload
	vpaddd	%zmm30, %zmm22, %zmm2
	vpextrd	$1, %xmm10, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm10, 320(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm10, 384(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm10, %r8d
	vextracti32x4	$3, %zmm10, %xmm7
	vpextrd	$1, %xmm7, 1408(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 1600(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm7, 160(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 176(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm7
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm7, %esi
	vpextrd	$2, %xmm7, %edi
	vpextrd	$3, %xmm7, %edx
	vmovd	%xmm7, -24(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm7
	vpextrd	$1, %xmm2, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 36(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -20(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm7, -12(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm7, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -40(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -36(%rsp)            # 4-byte Folded Spill
	vmovdqu64	896(%rsp), %zmm3        # 64-byte Reload
	vmovdqa64	%zmm5, %zmm29
	vpaddd	%zmm5, %zmm3, %zmm7
	vmovd	%xmm2, %ebx
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti128	$1, %ymm7, %xmm2
	vmovd	%xmm2, 208(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm2
	vmovd	%xmm2, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -32(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm3, %zmm2
	vmovd	%xmm7, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 8(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 16(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 20(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$3, %zmm7, %xmm7
	vmovd	%xmm7, 32(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -52(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm7
	vmovd	%xmm7, 4(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm7, (%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm7, %r13d
	vextracti32x4	$2, %zmm2, %xmm7
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm7, %r12d
	vpextrd	$2, %xmm7, %r11d
	vpextrd	$3, %xmm7, %r10d
	vmovd	%xmm7, %r9d
	vmovd	%ecx, %xmm7
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm2, %ebx
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, %edx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %r8d
	vmovd	%ecx, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	1408(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	movslq	%r14d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	movslq	8(%rsp), %rdx                   # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm5, %xmm5
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm5, %xmm5
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm5, %xmm5
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm5
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm5, %xmm5
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm5, %xmm5
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm5
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm5
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm5, %xmm5
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpsllw	$2, %zmm1, %zmm1
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm1, %zmm0
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm1
	vpextrd	$1, %xmm13, 320(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm7, %ymm2, %ymm2
	vpmovzxbw	576(%rsp), %zmm5        # 32-byte Folded Reload
                                        # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm5, %zmm5, %zmm6
	vmovdqu64	%zmm6, 1600(%rsp)       # 64-byte Spill
	vpmovzxbw	%ymm24, %zmm5           # zmm5 = ymm24[0],zero,ymm24[1],zero,ymm24[2],zero,ymm24[3],zero,ymm24[4],zero,ymm24[5],zero,ymm24[6],zero,ymm24[7],zero,ymm24[8],zero,ymm24[9],zero,ymm24[10],zero,ymm24[11],zero,ymm24[12],zero,ymm24[13],zero,ymm24[14],zero,ymm24[15],zero,ymm24[16],zero,ymm24[17],zero,ymm24[18],zero,ymm24[19],zero,ymm24[20],zero,ymm24[21],zero,ymm24[22],zero,ymm24[23],zero,ymm24[24],zero,ymm24[25],zero,ymm24[26],zero,ymm24[27],zero,ymm24[28],zero,ymm24[29],zero,ymm24[30],zero,ymm24[31],zero
	vpaddw	%zmm5, %zmm5, %zmm5
	vmovdqu64	%zmm5, 1408(%rsp)       # 64-byte Spill
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm0, %zmm5, %zmm0
	vpmovzxbw	%ymm2, %zmm2            # zmm2 = ymm2[0],zero,ymm2[1],zero,ymm2[2],zero,ymm2[3],zero,ymm2[4],zero,ymm2[5],zero,ymm2[6],zero,ymm2[7],zero,ymm2[8],zero,ymm2[9],zero,ymm2[10],zero,ymm2[11],zero,ymm2[12],zero,ymm2[13],zero,ymm2[14],zero,ymm2[15],zero,ymm2[16],zero,ymm2[17],zero,ymm2[18],zero,ymm2[19],zero,ymm2[20],zero,ymm2[21],zero,ymm2[22],zero,ymm2[23],zero,ymm2[24],zero,ymm2[25],zero,ymm2[26],zero,ymm2[27],zero,ymm2[28],zero,ymm2[29],zero,ymm2[30],zero,ymm2[31],zero
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vpaddw	%zmm2, %zmm2, %zmm2
	vpaddw	%zmm2, %zmm0, %zmm7
	vinserti128	$1, %xmm4, %ymm1, %ymm0
	vpaddd	%zmm21, %zmm3, %zmm4
	vpaddd	%zmm31, %zmm3, %zmm1
	vpextrd	$2, %xmm13, 160(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm13, 176(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, 576(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 384(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 208(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm4, %xmm2
	vmovd	%xmm2, %r12d
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$2, %zmm4, %xmm2
	vmovd	%xmm2, %ebp
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %r10d
	vpextrd	$3, %xmm2, %r11d
	vpaddd	%zmm31, %zmm19, %zmm2
	vmovd	%xmm2, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm4, %edx
	vpextrd	$2, %xmm4, %esi
	vpextrd	$3, %xmm4, %edi
	vmovd	%xmm4, %ecx
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, %r8d
	vpextrd	$3, %xmm4, -104(%rsp)           # 4-byte Folded Spill
	vmovd	%ebx, %xmm4
	vpextrd	$1, %xmm2, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm2, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm4, %xmm24
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rdx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r14d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %edx
	vpextrd	$3, %xmm5, %r8d
	vmovd	%xmm5, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm5
	vpinsrb	$1, (%rax,%rbx), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm5, %xmm5
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	1088(%rsp), %xmm6               # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm10
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	movslq	576(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	movslq	384(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	movslq	48(%rsp), %rbp                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm5
	movslq	208(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm5, %xmm5
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %ebp
	vmovd	%xmm1, %ebx
	vpextrd	$1, %xmm9, %esi
	movslq	%esi, %rsi
	vmovdqa	1920(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	vmovdqa	%xmm1, 384(%rsp)                # 16-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm1
	movslq	320(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm26, %xmm5
	vmovdqa	%xmm5, 320(%rsp)                # 16-byte Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	movslq	-108(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ebx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm1
	movslq	-96(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vpextrd	$2, %xmm9, 208(%rsp)            # 4-byte Folded Spill
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpsllw	$2, %zmm0, %zmm0
	vpaddw	%zmm1, %zmm1, %zmm1
	vpaddw	%zmm1, %zmm0, %zmm13
	vextracti128	$1, %ymm2, %xmm0
	vpextrd	$3, %xmm9, 576(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm1
	vpextrd	$1, %xmm11, -76(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm11, 1920(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm19, %zmm11
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vpaddd	%zmm29, %zmm19, %zmm0
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, -108(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm11, %xmm1
	vmovd	%xmm1, %r14d
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm11, %xmm1
	vmovd	%xmm1, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm3, %zmm1
	vextracti128	$1, %ymm1, %xmm4
	vpextrd	$1, %xmm4, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 1088(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm4, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm18, %r13d
	movslq	%r13d, %rbp
	vmovdqa	240(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm4, %xmm17
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm24, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$3, %zmm2, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm26
	vpaddd	%zmm30, %zmm3, %zmm2
	vextracti128	$1, %ymm2, %xmm4
	vpextrd	$1, %xmm4, %r13d
	vpextrd	$2, %xmm4, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -52(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, %r11d
	vmovd	%xmm11, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm4
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm3, %xmm22
	vpextrd	$3, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r14d, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r15d, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r12d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm4, %xmm4
	movslq	-68(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm4, %xmm4
	movslq	-56(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	movslq	-64(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm4, %xmm4
	vextracti32x4	$2, %zmm2, %xmm5
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r12d
	vmovd	%xmm5, %r8d
	movslq	-100(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm2, %edx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %ecx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 896(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %edi
	vpextrd	$1, %xmm28, %r10d
	movslq	%r10d, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm16, %xmm2
	vmovdqa	%xmm2, 144(%rsp)                # 16-byte Spill
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rbp), %xmm4, %xmm4
	vextracti32x4	$3, %zmm11, %xmm2
	vmovd	%xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm16
	movslq	%edx, %rdx
	movslq	%ebx, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm4
	vpinsrb	$1, (%rax,%rbp), %xmm4, %xmm4
	movslq	%esi, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vpextrd	$2, %xmm5, %r9d
	vpextrd	$3, %xmm5, %r8d
	vmovd	%xmm5, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm5
	vpinsrb	$1, (%rax,%rbx), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm5
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %ebx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm24
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm5
	movslq	%r10d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	80(%rsp), %xmm3                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm12
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm23, %xmm23
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm27, %xmm27
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vpmovzxbw	480(%rsp), %zmm4        # 32-byte Folded Reload
                                        # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm6
	vmovdqu64	%zmm6, 1088(%rsp)       # 64-byte Spill
	vpmovzxbw	352(%rsp), %zmm4        # 32-byte Folded Reload
                                        # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm3
	vmovdqu64	%zmm3, 896(%rsp)        # 64-byte Spill
	vpaddw	%zmm6, %zmm3, %zmm4
	vpaddw	%zmm13, %zmm4, %zmm4
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm18, -108(%rsp)          # 4-byte Folded Spill
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm1, %zmm1
	vpaddw	%zmm1, %zmm4, %zmm1
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm4
	vextracti32x4	$2, %zmm0, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm6, %ecx
	vpcmpltuw	%zmm1, %zmm7, %k1
	vpsubw	%zmm1, %zmm7, %zmm13
	vpsubw	%zmm7, %zmm1, %zmm13 {%k1}
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm1
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm25
	vpaddd	%zmm30, %zmm19, %zmm11
	vmovd	%xmm11, %ecx
	vpextrd	$3, %xmm18, -104(%rsp)          # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm11, %xmm1
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm11, %edx
	vmovd	%xmm1, %esi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm6
	vpextrd	$3, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm11, %xmm12
	vmovd	%xmm12, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm12, %ecx
	vinserti128	$1, %xmm5, %ymm6, %ymm3
	vmovdqu	%ymm3, 416(%rsp)                # 32-byte Spill
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vmovdqa	%xmm1, 1152(%rsp)               # 16-byte Spill
	vpextrd	$2, %xmm15, -112(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm15, -80(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm19, %zmm7
	vmovdqa64	%zmm8, %zmm18
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm28, -84(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm7, %edx
	vpextrd	$3, %xmm28, -96(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm9, %r14d
	vpextrd	$2, %xmm9, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm14, %r9d
	vpextrd	$2, %xmm14, %r15d
	vmovdqu64	704(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm20, %zmm4, %zmm5
	vpextrd	$3, %xmm14, 256(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm5, %xmm3
	vpextrd	$1, %xmm3, %r10d
	vmovd	%xmm3, %edi
	vpextrd	$2, %xmm3, %r11d
	vpextrd	$3, %xmm3, %r8d
	vextracti128	$1, %ymm7, %xmm3
	vmovd	%xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	672(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbp), %xmm23, %xmm6
	vpextrd	$2, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 512(%rsp)                # 16-byte Spill
	vextracti32x4	$2, %zmm5, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%xmm5, %r13d
	movslq	%r13d, %r13
	vpextrd	$1, %xmm5, %r12d
	movslq	%r12d, %r12
	movzbl	(%rax,%r13), %ebx
	vmovd	%ebx, %xmm1
	vpinsrb	$1, (%rax,%r12), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	1280(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbx), %xmm27, %xmm8
	vpextrd	$3, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r10d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r11d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm5, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %r8d
	vinserti128	$1, %xmm6, %ymm8, %ymm15
	vpextrd	$2, %xmm0, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 672(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm29, %zmm4, %zmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, 80(%rsp)             # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vpextrd	$3, %xmm2, 640(%rsp)            # 4-byte Folded Spill
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edx
	vextracti128	$1, %ymm0, %xmm6
	vpextrd	$1, %xmm6, %edi
	vpextrd	$2, %xmm6, %ebp
	vpextrd	$3, %xmm6, %ebx
	vmovd	%xmm6, %ecx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm6
	movslq	%r9d, %rdx
	vmovdqa	1728(%rsp), %xmm2               # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm23
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm26, %xmm14
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm28
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm25, %xmm25
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm3, 1280(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm3, %esi
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r8d
	vpaddd	%zmm21, %zmm4, %zmm3
	vextracti128	$1, %ymm3, %xmm2
	vpextrd	$1, %xmm2, %r10d
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r11d
	vmovd	%xmm2, %r12d
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm1, %xmm9
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm2
	movslq	64(%rsp), %rdx                  # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm6
	movslq	%r9d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm2, %xmm2
	movslq	-72(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm10, %xmm8
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	movslq	-60(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm10
	vextracti32x4	$3, %zmm0, %xmm6
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm16
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebx
	vmovd	%xmm3, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm8, %xmm8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm2
	vextracti32x4	$3, %zmm3, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm12, 480(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm10, %ymm8, %ymm10
	vpextrd	$3, %xmm12, 1856(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm31, %zmm4, %zmm8
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm6, %edx
	vpextrd	$2, %xmm6, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 1728(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm3
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm3, %r10d
	vpextrd	$2, %xmm3, %r8d
	vmovd	%xmm3, %ebp
	vpextrd	$3, %xmm3, %r9d
	vmovd	%ecx, %xmm3
	vpextrd	$1, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm26
	vpextrd	$2, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm6
	vextracti32x4	$2, %zmm8, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	vpextrd	$3, %xmm3, %edi
	vmovd	%xmm3, %ebx
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm2, %xmm3
	vpextrd	$3, %xmm8, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm2
	movslq	%ebp, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r10d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r8d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r9d, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	movslq	%ebx, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm2, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm8, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm5, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm6, %r8d
	vpextrd	$2, %xmm6, -88(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm30, %zmm4, %zmm8
	vpextrd	$3, %xmm6, 96(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm5
	vmovd	%xmm8, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r10d
	vpextrd	$3, %xmm5, %r11d
	vmovd	%xmm5, %r14d
	vextracti32x4	$2, %zmm8, %xmm5
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm8, %ecx
	vmovd	%xmm5, %esi
	vpextrd	$1, %xmm5, %edi
	vpextrd	$2, %xmm5, %ebp
	vpextrd	$3, %xmm5, %ebx
	vmovd	%edx, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm6
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm5
	vpextrd	$3, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm8, %xmm8
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm12
	vpextrd	$2, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm4, %zmm6
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %r11d
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %ebp
	vextracti32x4	$2, %zmm6, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm6, %ebx
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %r9d
	vpextrd	$3, %xmm0, %r10d
	vmovd	%ecx, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm2
	vextracti32x4	$3, %zmm6, %xmm0
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %r8d
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	960(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm20, %zmm4, %zmm0
	vextracti128	$1, %ymm0, %xmm6
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r12d
	vpextrd	$3, %xmm6, %r14d
	vmovd	%xmm6, %ebx
	vpextrd	$1, %xmm8, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm12, %xmm6
	movslq	%r9d, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	movslq	208(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	384(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm12
	movslq	%r10d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm2, %xmm2
	movslq	160(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	320(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm16
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	movslq	-108(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm17, %xmm17
	movslq	%r8d, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm2, %xmm20
	movslq	48(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm22, %xmm22
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm0, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm1
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %edi
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-112(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm24, %xmm24
	movslq	%r13d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r15d, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm23, %xmm27
	movslq	%r12d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	576(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm12, %xmm12
	movslq	%r14d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm16, %xmm16
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm17, %xmm23
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm22, %xmm22
	vinserti32x4	$1, %xmm12, %ymm16, %ymm12
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm17
	vpaddd	%zmm29, %zmm4, %zmm16
	vextracti32x4	$1, %ymm16, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %r14d
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm17, %xmm17
	vextracti32x4	$3, %zmm0, %xmm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm17, %xmm0
	movslq	-96(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm17
	vextracti32x4	$2, %zmm16, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %ebx
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %esi
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm24, %xmm29
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm16, %xmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm16, %ecx
	vinserti32x4	$1, %xmm23, %ymm22, %ymm22
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm24
	vpextrd	$2, %xmm8, 320(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm2, %ymm29, %ymm23
	vpextrd	$3, %xmm8, 176(%rsp)            # 4-byte Folded Spill
	vmovdqa64	%zmm4, %zmm2
	vpaddd	%zmm21, %zmm4, %zmm0
	vpextrd	$2, %xmm16, 576(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm16, 1920(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 384(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 160(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm31, %zmm4, %zmm8
	vmovdqa64	%zmm4, %zmm21
	vextracti128	$1, %ymm8, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vmovd	%xmm1, %r14d
	vpextrd	$3, %xmm1, %r13d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %edi
	movslq	%edi, %rbx
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm0, %ecx
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vmovd	%ebx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm14, %xmm2
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm29
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm28, %xmm28
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm8, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm14
	vpextrd	$2, %xmm8, %ecx
	vextracti32x4	$2, %zmm8, %xmm1
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edi
	vmovd	%xmm1, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm14, %xmm1
	vpextrd	$3, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm16
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm14
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm9, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm14, %xmm14
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm26, %xmm9
	movslq	%r12d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm14, %xmm14
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm14, %xmm25
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm14
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm25, %xmm5
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm26
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm5, %xmm5
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti32x4	$1, %xmm6, %ymm2, %ymm25
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm2
	vextracti32x4	$3, %zmm8, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm6
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm20, %xmm20
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm27
	vextracti32x4	$2, %zmm7, %xmm2
	vpextrd	$1, %xmm2, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 256(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %r10d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r11d
	vpaddd	%zmm30, %zmm4, %zmm8
	vmovd	%xmm8, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm0, %r12d
	vpextrd	$3, %xmm0, %r14d
	vextracti128	$1, %ymm8, %xmm0
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %ebx
	vmovd	%xmm0, %edx
	vpextrd	$3, %xmm0, %esi
	vmovd	%edi, %xmm0
	vpextrd	$1, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm8, %edi
	vextracti32x4	$2, %zmm8, %xmm2
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r9d
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	movslq	384(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm17, %xmm2
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	movslq	576(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm24, %xmm5
	movslq	%ebp, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r12d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm29, %xmm29
	movslq	%ebx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r15d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm27, %xmm17
	movslq	%esi, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	movslq	640(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm28, %xmm24
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm16, %xmm16
	movslq	%r8d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm27
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm8, %xmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpaddd	%zmm18, %zmm4, %zmm8
	vextracti128	$1, %ymm8, %xmm6
	vpextrd	$1, %xmm6, %r15d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vpextrd	$1, %xmm1, %edi
	vmovd	%xmm6, %ebp
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm0, %xmm28
	movslq	704(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm18
	vmovd	%xmm8, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm8, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm8, %edi
	vextracti32x4	$2, %zmm8, %xmm3
	vpextrd	$1, %xmm3, %ebx
	vpextrd	$2, %xmm3, %esi
	vpextrd	$3, %xmm3, %edx
	vmovd	%xmm3, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm3
	vpextrd	$3, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	movslq	96(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm14, %xmm6
	movslq	%ebp, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm26, %xmm0
	movslq	%r15d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm3, %xmm3
	movslq	-60(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm20, %xmm14
	movslq	%r9d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm3, %xmm3
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm20
	vextracti32x4	$3, %zmm7, %xmm7
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm7, %r13d
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm7, %r15d
	movslq	%ebx, %rcx
	vextracti32x4	$3, %zmm8, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %ecx
	movslq	1920(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ebp
	vpinsrb	$15, (%rax,%rdi), %xmm5, %xmm5
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm2
	vpextrd	$3, %xmm7, %r9d
	movslq	%r14d, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm29, %xmm3
	vmovd	%xmm7, %r8d
	vpextrd	$3, %xmm1, %ecx
	movslq	%r11d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm17, %xmm1
	vextracti32x4	$3, %zmm11, %xmm4
	vpextrd	$1, %xmm4, %ebx
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm7
	vmovd	%xmm4, %r11d
	movslq	%esi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %esi
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm8
	vpextrd	$3, %xmm4, %edx
	vinserti32x4	$1, %xmm24, %ymm16, %ymm16
	vinserti32x4	$1, %xmm27, %ymm9, %ymm4
	vinserti32x4	$1, %xmm18, %ymm6, %ymm6
	vinserti128	$1, %xmm0, %ymm14, %ymm0
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpmovzxbw	416(%rsp), %zmm14       # 32-byte Folded Reload
                                        # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpmovzxbw	%ymm6, %zmm6            # zmm6 = ymm6[0],zero,ymm6[1],zero,ymm6[2],zero,ymm6[3],zero,ymm6[4],zero,ymm6[5],zero,ymm6[6],zero,ymm6[7],zero,ymm6[8],zero,ymm6[9],zero,ymm6[10],zero,ymm6[11],zero,ymm6[12],zero,ymm6[13],zero,ymm6[14],zero,ymm6[15],zero,ymm6[16],zero,ymm6[17],zero,ymm6[18],zero,ymm6[19],zero,ymm6[20],zero,ymm6[21],zero,ymm6[22],zero,ymm6[23],zero,ymm6[24],zero,ymm6[25],zero,ymm6[26],zero,ymm6[27],zero,ymm6[28],zero,ymm6[29],zero,ymm6[30],zero,ymm6[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm6, %zmm6
	vpmovzxbw	%ymm15, %zmm15          # zmm15 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm14, %zmm15, %zmm9
	vpaddw	%zmm4, %zmm9, %zmm4
	vpaddw	%zmm6, %zmm4, %zmm6
	vinserti32x4	$1, %xmm20, %ymm5, %ymm4
	vinserti128	$1, %xmm3, %ymm1, %ymm1
	vinserti128	$1, %xmm7, %ymm8, %ymm3
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm5
	vpmovzxbw	%ymm10, %zmm7           # zmm7 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm3, %zmm3            # zmm3 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpmovzxbw	%ymm12, %zmm4           # zmm4 = ymm12[0],zero,ymm12[1],zero,ymm12[2],zero,ymm12[3],zero,ymm12[4],zero,ymm12[5],zero,ymm12[6],zero,ymm12[7],zero,ymm12[8],zero,ymm12[9],zero,ymm12[10],zero,ymm12[11],zero,ymm12[12],zero,ymm12[13],zero,ymm12[14],zero,ymm12[15],zero,ymm12[16],zero,ymm12[17],zero,ymm12[18],zero,ymm12[19],zero,ymm12[20],zero,ymm12[21],zero,ymm12[22],zero,ymm12[23],zero,ymm12[24],zero,ymm12[25],zero,ymm12[26],zero,ymm12[27],zero,ymm12[28],zero,ymm12[29],zero,ymm12[30],zero,ymm12[31],zero
	vpaddw	%zmm7, %zmm4, %zmm3
	vpaddw	%zmm5, %zmm3, %zmm3
	movslq	1280(%rsp), %rbp                # 4-byte Folded Reload
	movslq	480(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	1856(%rsp), %r14                # 4-byte Folded Reload
	vmovdqa	1152(%rsp), %xmm0               # 16-byte Reload
	vpinsrb	$10, (%rax,%rdi), %xmm0, %xmm2
	movslq	%r11d, %rdi
	vmovdqa	512(%rsp), %xmm0                # 16-byte Reload
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vpaddw	%zmm1, %zmm3, %zmm3
	vpcmpltuw	%zmm3, %zmm6, %k1
	vpsubw	%zmm3, %zmm6, %zmm17
	vpsubw	%zmm6, %zmm3, %zmm17 {%k1}
	vpaddd	2624(%rsp), %zmm19, %zmm5       # 64-byte Folded Reload
	vextracti128	$1, %ymm5, %xmm3
	vmovd	%xmm3, 960(%rsp)                # 4-byte Folded Spill
	vpinsrb	$11, (%rax,%r14), %xmm2, %xmm2
	vpextrd	$1, %xmm3, 704(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 1152(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, 1280(%rsp)           # 4-byte Folded Spill
	movslq	%r10d, %rdi
	movslq	1792(%rsp), %rbp                # 4-byte Folded Reload
	movslq	256(%rsp), %r10                 # 4-byte Folded Reload
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%r8d, %r8
	movslq	%r13d, %r13
	movslq	%r15d, %r15
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$9, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%r10), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%r8), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%r13), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%r15), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm2, %ymm0, %ymm9
	vextracti32x4	$2, %zmm5, %xmm0
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm0, %r14d
	vpaddw	%zmm14, %zmm14, %zmm2
	vmovd	%xmm0, %r15d
	vpextrd	$3, %xmm0, 416(%rsp)            # 4-byte Folded Spill
	vpaddd	2688(%rsp), %zmm19, %zmm0       # 64-byte Folded Reload
	vmovd	%xmm0, %edx
	vpaddw	%zmm7, %zmm7, %zmm3
	movslq	%edx, %rdx
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %esi
	vpaddw	%zmm3, %zmm2, %zmm2
	movslq	%ebx, %rbx
	movslq	%esi, %r13
	vpextrd	$3, %xmm0, %edi
	vextracti128	$1, %ymm0, %xmm3
	movslq	%edi, %rdi
	vmovd	%xmm3, %ebp
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %esi
	vmovdqu64	1088(%rsp), %zmm1       # 64-byte Reload
	vpaddw	1600(%rsp), %zmm1, %zmm6        # 64-byte Folded Reload
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm3, %edx
	vextracti32x4	$2, %zmm0, %xmm3
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$1, %xmm3, %ebx
	vpinsrb	$2, (%rax,%r13), %xmm7, %xmm7
	vpextrd	$2, %xmm3, %r13d
	vpinsrb	$3, (%rax,%rdi), %xmm7, %xmm7
	vpinsrb	$4, (%rax,%rbp), %xmm7, %xmm7
	vmovd	%xmm3, %edi
	vpextrd	$3, %xmm3, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm3
	vmovd	%xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %esi
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %edx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm5, %xmm5
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %r11d
	movslq	%r13d, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %r13d
	movslq	%ebp, %rbp
	vpaddw	%zmm15, %zmm15, %zmm7
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm0, %ebp
	movslq	%ebp, %rbp
	vpaddw	%zmm4, %zmm4, %zmm4
	vpinsrb	$12, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %ebp
	movzbl	(%rax,%rcx), %ecx
	vpaddw	%zmm4, %zmm7, %zmm10
	vmovd	%ecx, %xmm4
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm5, %r10d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm0, %esi
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm0, %edx
	vmovdqu64	896(%rsp), %zmm0        # 64-byte Reload
	vpaddw	1408(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	movslq	%edi, %rdi
	movslq	960(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1152(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	1280(%rsp), %r9                 # 4-byte Folded Reload
	movslq	%r12d, %r12
	movslq	%r14d, %r14
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm0
	vpinsrb	$3, (%rax,%rdi), %xmm4, %xmm3
	vpinsrb	$4, (%rax,%rbx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm5
	movslq	416(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm0
	vpinsrb	$6, (%rax,%r8), %xmm0, %xmm0
	vpmovzxbw	%ymm22, %zmm8           # zmm8 = ymm22[0],zero,ymm22[1],zero,ymm22[2],zero,ymm22[3],zero,ymm22[4],zero,ymm22[5],zero,ymm22[6],zero,ymm22[7],zero,ymm22[8],zero,ymm22[9],zero,ymm22[10],zero,ymm22[11],zero,ymm22[12],zero,ymm22[13],zero,ymm22[14],zero,ymm22[15],zero,ymm22[16],zero,ymm22[17],zero,ymm22[18],zero,ymm22[19],zero,ymm22[20],zero,ymm22[21],zero,ymm22[22],zero,ymm22[23],zero,ymm22[24],zero,ymm22[25],zero,ymm22[26],zero,ymm22[27],zero,ymm22[28],zero,ymm22[29],zero,ymm22[30],zero,ymm22[31],zero
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r15), %xmm0, %xmm0
	vpmovzxbw	%ymm23, %zmm18          # zmm18 = ymm23[0],zero,ymm23[1],zero,ymm23[2],zero,ymm23[3],zero,ymm23[4],zero,ymm23[5],zero,ymm23[6],zero,ymm23[7],zero,ymm23[8],zero,ymm23[9],zero,ymm23[10],zero,ymm23[11],zero,ymm23[12],zero,ymm23[13],zero,ymm23[14],zero,ymm23[15],zero,ymm23[16],zero,ymm23[17],zero,ymm23[18],zero,ymm23[19],zero,ymm23[20],zero,ymm23[21],zero,ymm23[22],zero,ymm23[23],zero,ymm23[24],zero,ymm23[25],zero,ymm23[26],zero,ymm23[27],zero,ymm23[28],zero,ymm23[29],zero,ymm23[30],zero,ymm23[31],zero
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%r14), %xmm0, %xmm3
	vpmovzxbw	%ymm25, %zmm0           # zmm0 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	movslq	%r13d, %rdi
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpaddw	%zmm2, %zmm6, %zmm6
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	vpsllw	$2, %zmm8, %zmm3
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm5, %ymm2, %ymm15
	vpmovzxbw	%ymm16, %zmm2           # zmm2 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddd	2560(%rsp), %zmm19, %zmm7       # 64-byte Folded Reload
	vpaddw	%zmm10, %zmm11, %zmm10
	vpaddd	2496(%rsp), %zmm19, %zmm11      # 64-byte Folded Reload
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm11, %ecx
	vpsllw	$2, %zmm18, %zmm12
	movslq	%ecx, %rbp
	vpextrd	$2, %xmm11, %ecx
	vpextrd	$3, %xmm11, %r8d
	vpaddw	%zmm3, %zmm6, %zmm14
	movslq	%ecx, %rsi
	vextracti128	$1, %ymm11, %xmm3
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, %r13d
	vpaddw	%zmm12, %zmm10, %zmm10
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r12d
	vextracti32x4	$2, %zmm11, %xmm3
	vpaddw	%zmm0, %zmm0, %zmm12
	vmovd	%xmm3, %r14d
	vpextrd	$1, %xmm3, %r11d
	vpextrd	$2, %xmm3, %r10d
	vpcmpltuw	%zmm10, %zmm14, %k1
	vpextrd	$3, %xmm3, %r9d
	vextracti32x4	$3, %zmm11, %xmm3
	vpextrd	$1, %xmm3, 960(%rsp)            # 4-byte Folded Spill
	vpsubw	%zmm10, %zmm14, %zmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpsubw	%zmm14, %zmm10, %zmm6 {%k1}
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpaddw	%zmm2, %zmm8, %zmm8
	vextracti128	$1, %ymm7, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %edx
	vpaddw	%zmm8, %zmm12, %zmm8
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpextrd	$2, %xmm1, %ebx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm4
	vmovd	%xmm1, %esi
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm1
	movslq	%r8d, %rbp
	vpextrd	$2, %xmm3, %r8d
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpextrd	$3, %xmm3, %edi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm3, %edx
	vextracti32x4	$2, %zmm7, %xmm3
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %ebp
	movslq	%r13d, %r13
	vpinsrb	$4, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %esi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%r13), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %r13d
	vpinsrb	$6, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm3, %ebx
	movslq	%r15d, %r15
	movslq	%r12d, %r12
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm7, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%r15), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %r15d
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm3, %ecx
	vpinsrb	$7, (%rax,%r12), %xmm4, %xmm4
	vpinsrb	$8, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm3, %ebx
	vpextrd	$3, %xmm3, %r12d
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	movslq	%r13d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r9d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	movslq	%r15d, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	960(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r12d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpmovzxbw	%ymm15, %zmm4           # zmm4 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpmovzxbw	%ymm9, %zmm5            # zmm5 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpaddw	%zmm4, %zmm5, %zmm4
	vpaddw	%zmm4, %zmm8, %zmm4
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm3, %ymm1
	vpaddw	%zmm5, %zmm5, %zmm3
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm3, %zmm1
	vpaddw	%zmm18, %zmm0, %zmm0
	vpaddw	%zmm2, %zmm0, %zmm0
	vpaddw	%zmm1, %zmm0, %zmm0
	vpcmpltuw	%zmm0, %zmm4, %k1
	vpsubw	%zmm0, %zmm4, %zmm1
	vpsubw	%zmm4, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm13, %zmm17, %zmm0
	vpaddw	%zmm6, %zmm1, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, 2176(%rsp)                # 4-byte Folded Spill
	movl	1664(%rsp), %r11d               # 4-byte Reload
	addl	%ecx, %r11d
	movl	1472(%rsp), %r14d               # 4-byte Reload
	addl	%ecx, %r14d
	movl	1536(%rsp), %r13d               # 4-byte Reload
	addl	%ecx, %r13d
	movl	1216(%rsp), %edx                # 4-byte Reload
	addl	%ecx, %edx
	movl	%edx, %ecx
	decq	288(%rsp)                       # 8-byte Folded Spill
	jne	.LBB214_23
# %bb.24:                               # %"end for output.s0.y.yi14"
                                        #   in Loop: Header=BB214_22 Depth=2
	movq	464(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movq	280(%rsp), %rdi                 # 8-byte Reload
	addq	$32, %rdi
	cmpq	448(%rsp), %rdx                 # 8-byte Folded Reload
	jne	.LBB214_22
.LBB214_25:                             # %"end for output.s0.x.x.rebased11"
                                        #   in Loop: Header=BB214_10 Depth=1
	movq	808(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	232(%rsp), %ebp                 # 4-byte Reload
	addl	$16, %ebp
	movl	308(%rsp), %ecx                 # 4-byte Reload
	addl	%ecx, 132(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 128(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 124(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 120(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 116(%rsp)                 # 4-byte Folded Spill
	movq	456(%rsp), %rcx                 # 8-byte Reload
	addl	$16, %ecx
	movq	%rcx, 456(%rsp)                 # 8-byte Spill
	cmpq	800(%rsp), %rdx                 # 8-byte Folded Reload
	movq	40(%rsp), %r14                  # 8-byte Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB214_10
.LBB214_26:                             # %"end for output.s0.y.y.rebased"
	movq	616(%rsp), %rdx                 # 8-byte Reload
	movl	2172(%rsp), %ecx                # 4-byte Reload
	subl	%edx, %ecx
	movq	136(%rsp), %r8                  # 8-byte Reload
	jle	.LBB214_34
# %bb.27:                               # %"for output.s0.y.y.rebased16.preheader"
	cmpl	$0, 768(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_34
# %bb.28:                               # %"for output.s0.y.y.rebased16.us.preheader"
	movq	608(%rsp), %rbp                 # 8-byte Reload
	decl	%ebp
	leal	1(%r8), %r9d
	leal	-1(%r8), %ebx
	movl	%ebx, 512(%rsp)                 # 4-byte Spill
	vpbroadcastd	%ebp, %zmm0
	vmovdqu64	%zmm0, 1792(%rsp)       # 64-byte Spill
	movl	%ecx, %ecx
	movq	%rcx, 480(%rsp)                 # 8-byte Spill
	shll	$4, %edx
	movl	112(%rsp), %ecx                 # 4-byte Reload
	addl	%ecx, %edx
	imull	%esi, %ecx
	negl	%ecx
	movl	%ecx, 112(%rsp)                 # 4-byte Spill
	movl	%edx, %ecx
	subl	776(%rsp), %ecx                 # 4-byte Folded Reload
	xorl	%ebp, %ebp
	movl	$2, %edi
	movl	%r9d, 416(%rsp)                 # 4-byte Spill
	.p2align	4, 0x90
.LBB214_29:                             # %"for output.s0.y.y.rebased16.us"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_30 Depth 2
                                        #       Child Loop BB214_31 Depth 3
	movq	%rbp, 256(%rsp)                 # 8-byte Spill
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	%rdx, 616(%rsp)                 # 8-byte Spill
	movslq	%edx, %rcx
	imulq	%rsi, %rcx
	addq	568(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1920(%rsp)                # 8-byte Spill
	movl	112(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_30:                             # %"for output.s0.x.x19.us"
                                        #   Parent Loop BB214_29 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_31 Depth 3
	movl	%ecx, 672(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	1920(%rsp), %rcx                # 8-byte Folded Reload
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	1792(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm17
	vpmaxsd	%zmm2, %zmm0, %zmm22
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm23
	vpmaxsd	%zmm2, %zmm0, %zmm26
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm8
	vpmaxsd	%zmm2, %zmm0, %zmm9
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm10
	vpmaxsd	%zmm2, %zmm0, %zmm11
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm12
	vpmaxsd	%zmm2, %zmm0, %zmm13
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1408(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1152(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1088(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1280(%rsp)       # 64-byte Spill
	movq	$-16, %rcx
	.p2align	4, 0x90
.LBB214_31:                             # %"for output.s0.y.yi22.us"
                                        #   Parent Loop BB214_29 Depth=1
                                        #     Parent Loop BB214_30 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	1984(%rsp), %rdx                # 8-byte Reload
	leal	(%rdx,%rcx), %esi
	addl	$16, %esi
	movl	%esi, 1536(%rsp)                # 4-byte Spill
	cmpl	%r8d, %esi
	movl	%esi, %ecx
	cmovgl	%r9d, %ecx
	movq	136(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %r11d
	cmovll	%esi, %r11d
	cmpl	$2, %ecx
	cmovlel	%edi, %ecx
	addl	$-2, %ecx
	imull	%r14d, %ecx
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm27
	vpaddd	%zmm23, %zmm27, %zmm1
	vpaddd	%zmm26, %zmm27, %zmm0
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm0, %ecx
	vpextrd	$3, %xmm0, %esi
	movslq	%ecx, %r15
	movslq	%esi, %r10
	vextracti128	$1, %ymm0, %xmm2
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %esi
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %edi
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %ebx
	movslq	%edi, %r9
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm1, %edi
	vpextrd	$1, %xmm1, %ecx
	movslq	%edi, %rbp
	movslq	%ecx, %rdi
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm3
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm3, %xmm2
	vpextrd	$3, %xmm1, %edi
	movslq	%edi, %rdi
	vextracti128	$1, %ymm1, %xmm3
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm1, %xmm3
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm2, %xmm2
	movslq	%ecx, %r8
	vpextrd	$1, %xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	movslq	%esi, %rsi
	vpextrd	$2, %xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %edi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$11, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %edi
	movslq	%edx, %rbp
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edi
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%edi, %rdi
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm1
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	movq	1216(%rsp), %rcx                # 8-byte Reload
	movzbl	(%rax,%rcx), %edx
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm0, %edx
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rdx
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%r15), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %r15d
	vpinsrb	$3, (%rax,%r10), %xmm2, %xmm0
	vpinsrb	$4, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$5, (%rax,%r13), %xmm0, %xmm0
	vpinsrb	$6, (%rax,%r14), %xmm0, %xmm0
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r8), %xmm0, %xmm0
	vpinsrb	$9, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%rbx), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$12, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r15d, %rdx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm15
	vpaddd	%zmm12, %zmm27, %zmm2
	vpaddd	%zmm13, %zmm27, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %r10
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %esi
	vmovd	%xmm3, %edi
	vpextrd	$3, %xmm3, %ebx
	vmovd	%xmm2, %edx
	movslq	%edx, %r8
	movzbl	(%rax,%r8), %edx
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	movslq	%edi, %r9
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %r8
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm3, %xmm2
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%r10), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %r10
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edi
	movq	1472(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ebp
	movslq	%edi, %rdi
	movslq	%ebp, %rbp
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r12), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r15), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r14), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$14, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rbp), %xmm1, %xmm1
	vinserti128	$1, %xmm2, %ymm1, %ymm14
	movq	2048(%rsp), %rcx                # 8-byte Reload
	movq	1984(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	addl	$19, %ecx
	cmpl	136(%rsp), %ecx                 # 4-byte Folded Reload
	cmovgel	136(%rsp), %ecx                 # 4-byte Folded Reload
	testl	%ecx, %ecx
	movl	$1, %edx
	cmovlel	%edx, %ecx
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm24
	vpaddd	%zmm23, %zmm24, %zmm2
	vpaddd	%zmm26, %zmm24, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rbp
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %r15
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r9
	vextracti32x4	$2, %zmm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %esi
	vpextrd	$2, %xmm3, %r14d
	vpextrd	$3, %xmm3, %ecx
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	movslq	%esi, %rsi
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm3, %xmm3
	movslq	%r14d, %r14
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm3
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movq	1024(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edi
	vpinsrb	$2, (%rax,%rbp), %xmm3, %xmm3
	movslq	%edi, %rdi
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%r15), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r12), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%r14), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$14, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rbp), %xmm1, %xmm1
	vinserti32x4	$1, %xmm2, %ymm1, %ymm29
	vpaddd	%zmm12, %zmm24, %zmm2
	vpaddd	%zmm13, %zmm24, %zmm1
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %r15
	movslq	%edx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %esi
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %edi
	movslq	%ecx, %r10
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %esi
	vpextrd	$2, %xmm3, %ebx
	movslq	%edi, %r14
	vpextrd	$3, %xmm3, %edx
	vmovd	%xmm2, %edi
	vpextrd	$1, %xmm2, %ecx
	movslq	%edi, %rbp
	movslq	%ecx, %rdi
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm4
	vmovd	%xmm3, %ecx
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm3, %xmm3
	movslq	%ecx, %r9
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm3, %xmm3
	movslq	%esi, %r8
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm2
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	movzbl	(%rax,%r15), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %r15
	movq	1472(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movq	1024(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r12), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r14), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%rbp), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%r15), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm2, %ymm1, %ymm31
	testl	%r11d, %r11d
	movl	$1, %ecx
	cmovlel	%ecx, %r11d
	decl	%r11d
	imull	40(%rsp), %r11d                 # 4-byte Folded Reload
	subl	-8(%rsp), %r11d                 # 4-byte Folded Reload
	vpbroadcastd	%r11d, %zmm21
	vpaddd	%zmm23, %zmm21, %zmm1
	vpaddd	%zmm26, %zmm21, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r10
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r15
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r14
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r9
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %r13
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rbx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$4, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ebx
	movslq	%edx, %rdx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$8, (%rax,%rbx), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$14, (%rax,%rbx), %xmm3, %xmm1
	vmovd	%xmm2, %ebx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%ebx, %rbx
	movslq	%edx, %rdx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%r10), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm28
	vpaddd	%zmm12, %zmm21, %zmm1
	vpaddd	%zmm13, %zmm21, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %r11
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r10
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r15
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vextracti128	$1, %ymm1, %xmm4
	vmovd	%xmm4, %edx
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%r11), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r11d
	vpinsrb	$2, (%rax,%r10), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm25
	movq	2048(%rsp), %rcx                # 8-byte Reload
	movq	1984(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	addl	$18, %ecx
	cmpl	136(%rsp), %ecx                 # 4-byte Folded Reload
	cmovgel	136(%rsp), %ecx                 # 4-byte Folded Reload
	testl	%ecx, %ecx
	movl	$1, %edx
	cmovlel	%edx, %ecx
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm19
	vpaddd	%zmm23, %zmm19, %zmm1
	vpaddd	%zmm26, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %edx
	movslq	%ecx, %rsi
	movslq	%edx, %r9
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %edx
	movslq	%ecx, %r10
	movslq	%edx, %r15
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %edx
	movslq	%ecx, %r13
	movslq	%edx, %r12
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %edx
	movslq	%ecx, %r14
	movslq	%edx, %r8
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %edx
	movslq	%ecx, %r11
	movslq	%edx, %rdi
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %edx
	movslq	%ecx, %rbx
	movslq	%edx, %rbp
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$1, %xmm2, %edx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%r9), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r9d
	vpinsrb	$2, (%rax,%r10), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vmovdqu	%ymm0, 1024(%rsp)               # 32-byte Spill
	vpaddd	%zmm12, %zmm19, %zmm1
	vpaddd	%zmm13, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r11
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r15
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm1
	vmovd	%xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%r11), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vmovdqu	%ymm0, 1216(%rsp)               # 32-byte Spill
	movl	512(%rsp), %ecx                 # 4-byte Reload
	movl	1536(%rsp), %edx                # 4-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%ecx, %edx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm18
	vpaddd	%zmm23, %zmm18, %zmm1
	vpaddd	%zmm26, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %esi
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$2, %xmm1, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	movq	1536(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm2
	movq	1472(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm2, %xmm2
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm20
	vpaddd	%zmm12, %zmm18, %zmm1
	vpaddd	%zmm13, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rbp
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %esi
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	movslq	%esi, %rdx
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%r9), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %r9d
	movq	1536(%rsp), %r10                # 8-byte Reload
	vpinsrb	$1, (%rax,%r10), %xmm3, %xmm2
	movq	1472(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	movq	1664(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm5
	vpaddd	%zmm8, %zmm18, %zmm1
	vpaddd	%zmm9, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	movslq	%ecx, %r13
	movslq	%edx, %r12
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	movslq	%ecx, %r8
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebx, %rdx
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm3, %xmm1
	vpextrd	$2, %xmm2, %ebx
	movzbl	(%rax,%r9), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %r9d
	movslq	%ebx, %rbx
	movslq	%r9d, %r9
	movq	1536(%rsp), %r10                # 8-byte Reload
	vpinsrb	$1, (%rax,%r10), %xmm3, %xmm2
	movq	1472(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	movq	1664(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%r9), %xmm2, %xmm2
	vpaddd	%zmm17, %zmm18, %zmm3
	vpaddd	%zmm22, %zmm18, %zmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm4, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r15
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %r12
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %r13
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %r14
	vextracti32x4	$2, %zmm4, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, %esi
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm3, %xmm6
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm3, %xmm6
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm3, %xmm3
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm3, %ecx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm4, %edx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdi), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	movq	1536(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	movq	1472(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm3, %xmm3
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vpinsrb	$4, (%rax,%r15), %xmm3, %xmm3
	vpinsrb	$5, (%rax,%r12), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%r13), %xmm3, %xmm3
	vpinsrb	$7, (%rax,%r14), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%r11), %xmm3, %xmm3
	vpinsrb	$9, (%rax,%r10), %xmm3, %xmm3
	vpinsrb	$10, (%rax,%r8), %xmm3, %xmm3
	vpinsrb	$11, (%rax,%rbx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm2, %ymm2
	vinserti32x4	$1, %xmm0, %ymm3, %ymm30
	vpaddd	%zmm11, %zmm18, %zmm3
	vextracti128	$1, %ymm3, %xmm0
	vpextrd	$1, %xmm0, %r11d
	vmovd	%xmm0, %r12d
	vpextrd	$2, %xmm0, 1536(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1472(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, 1664(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 960(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r14d
	vpmovzxbw	%ymm15, %zmm0           # zmm0 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm0, %zmm0, %zmm1
	vpmovzxbw	%ymm14, %zmm0           # zmm0 = ymm14[0],zero,ymm14[1],zero,ymm14[2],zero,ymm14[3],zero,ymm14[4],zero,ymm14[5],zero,ymm14[6],zero,ymm14[7],zero,ymm14[8],zero,ymm14[9],zero,ymm14[10],zero,ymm14[11],zero,ymm14[12],zero,ymm14[13],zero,ymm14[14],zero,ymm14[15],zero,ymm14[16],zero,ymm14[17],zero,ymm14[18],zero,ymm14[19],zero,ymm14[20],zero,ymm14[21],zero,ymm14[22],zero,ymm14[23],zero,ymm14[24],zero,ymm14[25],zero,ymm14[26],zero,ymm14[27],zero,ymm14[28],zero,ymm14[29],zero,ymm14[30],zero,ymm14[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddd	%zmm17, %zmm27, %zmm15
	vpaddd	%zmm22, %zmm27, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm14, %edx
	movslq	%edx, %r8
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %rdx
	vpextrd	$3, %xmm14, %esi
	movslq	%esi, %r15
	vextracti128	$1, %ymm14, %xmm4
	vpextrd	$1, %xmm4, %r10d
	vmovd	%xmm15, %esi
	vpextrd	$1, %xmm15, %ecx
	movslq	%esi, %rbp
	movslq	%ecx, %rsi
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm15, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm15, %xmm7
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm15, %xmm7
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vextracti32x4	$3, %zmm15, %xmm7
	vmovd	%xmm7, %ecx
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm6
	movslq	%r10d, %rsi
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r10
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm7
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm14, %xmm4
	vpinsrb	$1, (%rax,%r8), %xmm7, %xmm7
	vmovd	%xmm4, %ebx
	movslq	%ebx, %r8
	vpinsrb	$2, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%r15), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r15
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm14, %xmm4
	vpinsrb	$5, (%rax,%rsi), %xmm7, %xmm7
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %ebx
	vpinsrb	$7, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ebp
	vpinsrb	$9, (%rax,%rdx), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r15), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rdi), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpsllw	$2, %zmm4, %zmm14
	vpaddd	%zmm8, %zmm27, %zmm4
	vpaddd	%zmm9, %zmm27, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm15, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm15, %edx
	movslq	%edx, %rbx
	vpextrd	$3, %xmm15, %edx
	movslq	%edx, %r15
	vmovd	%xmm4, %edx
	movslq	%edx, %rsi
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rbp
	vpextrd	$2, %xmm4, %edx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	%esi, %rsi
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	vextracti32x4	$2, %zmm4, %xmm7
	vmovd	%xmm7, %edx
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edx
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	vextracti128	$1, %ymm15, %xmm7
	vmovd	%xmm7, %esi
	movslq	%esi, %r8
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %edx
	movslq	%ebp, %r10
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$2, %zmm15, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %r15
	vpinsrb	$4, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm15, %xmm7
	vpinsrb	$5, (%rax,%r10), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	movslq	%esi, %rdx
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r15), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm14, %zmm14
	vpaddd	%zmm10, %zmm27, %zmm16
	vpaddd	%zmm11, %zmm27, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm15, %edx
	movslq	%edx, %rbx
	vpextrd	$2, %xmm15, %edx
	movslq	%edx, %rdi
	vpextrd	$3, %xmm15, %edx
	movslq	%edx, %r15
	vextracti128	$1, %ymm15, %xmm4
	vpextrd	$1, %xmm4, %edx
	vmovd	%xmm16, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%edx, %r9
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r10
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %r8
	vextracti32x4	$2, %zmm15, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	vmovd	%xmm4, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	movslq	%ebx, %rdi
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$4, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %r15
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm15, %xmm4
	vpinsrb	$5, (%rax,%r9), %xmm7, %xmm7
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$8, (%rax,%rdi), %xmm7, %xmm7
	movslq	%edx, %rdx
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rbx), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r15), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rdi), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm1, %zmm0, %zmm6
	vpaddw	%zmm14, %zmm6, %zmm6
	vpaddw	%zmm4, %zmm6, %zmm15
	vpaddd	%zmm17, %zmm24, %zmm16
	vpaddd	%zmm22, %zmm24, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm14, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %r10
	vpextrd	$3, %xmm14, %edx
	movslq	%edx, %r15
	vextracti128	$1, %ymm14, %xmm4
	vpextrd	$1, %xmm4, %edx
	vmovd	%xmm16, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm16, %ebp
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$2, %xmm16, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ebp, %rsi
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	movslq	%esi, %r8
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%edx, %r9
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm4
	vextracti32x4	$2, %zmm14, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$2, (%rax,%r10), %xmm4, %xmm4
	vpextrd	$1, %xmm7, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %r10
	vpinsrb	$3, (%rax,%r15), %xmm4, %xmm4
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%r8), %xmm4, %xmm4
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm14, %xmm7
	vpinsrb	$5, (%rax,%r9), %xmm4, %xmm4
	vmovd	%xmm7, %ebx
	vpinsrb	$6, (%rax,%rbp), %xmm4, %xmm4
	movslq	%ebx, %rbp
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm7, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%r10), %xmm4, %xmm4
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rdi), %xmm4, %xmm4
	vinserti32x4	$1, %xmm6, %ymm4, %ymm16
	vpaddd	%zmm8, %zmm24, %zmm4
	vpaddd	%zmm9, %zmm24, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm14, %edx
	movslq	%edx, %r15
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %r10
	vpextrd	$3, %xmm14, %edx
	movslq	%edx, %r8
	vmovd	%xmm4, %edx
	movslq	%edx, %rbp
	vpextrd	$1, %xmm4, %edx
	vpextrd	$2, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	movslq	%esi, %rsi
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	vextracti128	$1, %ymm14, %xmm7
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r9d
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm6
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %edx
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %edx
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %esi
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vextracti32x4	$2, %zmm14, %xmm7
	vpinsrb	$1, (%rax,%r15), %xmm6, %xmm6
	vmovd	%xmm7, %r15d
	vpinsrb	$2, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r10d
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edx
	vpmovzxbw	%ymm29, %zmm7           # zmm7 = ymm29[0],zero,ymm29[1],zero,ymm29[2],zero,ymm29[3],zero,ymm29[4],zero,ymm29[5],zero,ymm29[6],zero,ymm29[7],zero,ymm29[8],zero,ymm29[9],zero,ymm29[10],zero,ymm29[11],zero,ymm29[12],zero,ymm29[13],zero,ymm29[14],zero,ymm29[15],zero,ymm29[16],zero,ymm29[17],zero,ymm29[18],zero,ymm29[19],zero,ymm29[20],zero,ymm29[21],zero,ymm29[22],zero,ymm29[23],zero,ymm29[24],zero,ymm29[25],zero,ymm29[26],zero,ymm29[27],zero,ymm29[28],zero,ymm29[29],zero,ymm29[30],zero,ymm29[31],zero
	vpaddw	%zmm7, %zmm7, %zmm29
	vpmovzxbw	%ymm31, %zmm7           # zmm7 = ymm31[0],zero,ymm31[1],zero,ymm31[2],zero,ymm31[3],zero,ymm31[4],zero,ymm31[5],zero,ymm31[6],zero,ymm31[7],zero,ymm31[8],zero,ymm31[9],zero,ymm31[10],zero,ymm31[11],zero,ymm31[12],zero,ymm31[13],zero,ymm31[14],zero,ymm31[15],zero,ymm31[16],zero,ymm31[17],zero,ymm31[18],zero,ymm31[19],zero,ymm31[20],zero,ymm31[21],zero,ymm31[22],zero,ymm31[23],zero,ymm31[24],zero,ymm31[25],zero,ymm31[26],zero,ymm31[27],zero,ymm31[28],zero,ymm31[29],zero,ymm31[30],zero,ymm31[31],zero
	vpaddw	%zmm7, %zmm7, %zmm27
	vpmovzxbw	%ymm16, %zmm7           # zmm7 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpsllw	$2, %zmm7, %zmm16
	movslq	%r9d, %rbx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	movslq	%r15d, %rdi
	movslq	%r10d, %r8
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm14, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edi
	vpinsrb	$9, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm16, %zmm14
	vpaddd	%zmm10, %zmm24, %zmm4
	vpaddd	%zmm11, %zmm24, %zmm16
	vmovd	%xmm16, %ecx
	vpextrd	$1, %xmm16, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rbx
	vpextrd	$2, %xmm16, %edx
	vpextrd	$3, %xmm16, %esi
	movslq	%edx, %rdi
	movslq	%esi, %r10
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rbp
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm7, %edx
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %r8
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %esi
	movslq	%ebp, %r9
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %r15
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edi
	vpinsrb	$3, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edi, %rdi
	movslq	%edx, %r10
	vpinsrb	$4, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$5, (%rax,%r9), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%r15), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	movslq	%edx, %rcx
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r10), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm6
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm29, %zmm27, %zmm6
	vpaddw	%zmm14, %zmm6, %zmm6
	vpaddw	%zmm4, %zmm6, %zmm4
	vpcmpltuw	%zmm4, %zmm15, %k1
	vpsubw	%zmm4, %zmm15, %zmm14
	vpsubw	%zmm15, %zmm4, %zmm14 {%k1}
	vpaddd	%zmm17, %zmm21, %zmm16
	vpaddd	%zmm22, %zmm21, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm15, %edx
	movslq	%edx, %rbx
	vpextrd	$2, %xmm15, %edx
	movslq	%edx, %rdi
	vpextrd	$3, %xmm15, %edx
	movslq	%edx, %r15
	vextracti128	$1, %ymm15, %xmm4
	vpextrd	$1, %xmm4, %edx
	vmovd	%xmm16, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%edx, %r8
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r9
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %r10
	vextracti32x4	$2, %zmm15, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	vmovd	%xmm4, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	movslq	%ebx, %rdi
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$4, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %r15
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm15, %xmm4
	vpinsrb	$5, (%rax,%r8), %xmm7, %xmm7
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r9), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$8, (%rax,%rdi), %xmm7, %xmm7
	movslq	%edx, %rdx
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rbx), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r15), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rdi), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm15
	vpmovzxbw	%ymm28, %zmm24          # zmm24 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddd	%zmm8, %zmm21, %zmm4
	vpaddd	%zmm9, %zmm21, %zmm28
	vmovd	%xmm28, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm28, %ecx
	vpextrd	$2, %xmm28, %edx
	movslq	%ecx, %rbx
	movslq	%edx, %r10
	vpextrd	$3, %xmm28, %ebp
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vextracti32x4	$1, %ymm28, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%ebp, %r8
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %r9
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	%edx, %r15
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %esi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm28, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %r10
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$4, (%rax,%r9), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%r15), %xmm6, %xmm6
	vextracti32x4	$3, %zmm28, %xmm7
	vmovd	%xmm7, %ebp
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%r10), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm28           # zmm28 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddd	%zmm10, %zmm21, %zmm4
	vpaddd	%zmm11, %zmm21, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm16, %ecx
	vpextrd	$2, %xmm16, %edx
	movslq	%ecx, %rbx
	movslq	%edx, %r10
	vpextrd	$3, %xmm16, %ebp
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%ebp, %r8
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %r9
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	%edx, %r15
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %esi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %r10
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$4, (%rax,%r9), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%r15), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vmovd	%xmm7, %ebp
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%r10), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm28, %zmm28
	vpmovzxbw	%ymm25, %zmm21          # zmm21 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	vpaddw	%zmm24, %zmm21, %zmm4
	vpaddw	%zmm15, %zmm4, %zmm15
	vpaddd	%zmm17, %zmm19, %zmm4
	vpaddd	%zmm22, %zmm19, %zmm25
	vmovd	%xmm25, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm25, %edx
	movslq	%edx, %rbx
	vpextrd	$2, %xmm25, %edx
	movslq	%edx, %rdi
	vpextrd	$3, %xmm25, %edx
	movslq	%edx, %r15
	vmovd	%xmm4, %edx
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %esi
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vextracti32x4	$1, %ymm25, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebp
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%edx, %r8
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %r9
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %r10
	vextracti32x4	$2, %zmm25, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	movslq	%ebx, %rdi
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%edx, %r15
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm25, %xmm7
	vpinsrb	$5, (%rax,%r8), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$3, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r15), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vpaddw	%zmm28, %zmm15, %zmm25
	vinserti128	$1, %xmm4, %ymm6, %ymm15
	vpaddd	%zmm8, %zmm19, %zmm4
	vpaddd	%zmm9, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm16, %edx
	movslq	%edx, %rbx
	vpextrd	$2, %xmm16, %edx
	movslq	%edx, %rdi
	vpextrd	$3, %xmm16, %edx
	movslq	%edx, %r15
	vmovd	%xmm4, %edx
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %esi
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebp
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%edx, %r8
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %r9
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %r10
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	movslq	%ebx, %rdi
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%edx, %r15
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$5, (%rax,%r8), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$3, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r15), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vinserti32x4	$1, %xmm4, %ymm6, %ymm28
	vpaddd	%zmm10, %zmm19, %zmm4
	vpaddd	%zmm11, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	vpextrd	$1, %xmm16, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %r15
	vpextrd	$2, %xmm16, %edx
	vpextrd	$3, %xmm16, %esi
	movslq	%edx, %rdi
	movslq	%esi, %r10
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm7, %edx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r8d
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebp
	movslq	%edx, %rdx
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%r15), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r9d
	vpinsrb	$3, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %r10d
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %r15d
	movslq	%r8d, %rdi
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %r8d
	movslq	%r9d, %rsi
	movslq	%r10d, %rdx
	movslq	%r15d, %rbx
	movslq	%edi, %rdi
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	movslq	%r8d, %rdx
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm15, %zmm6           # zmm6 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm6, %zmm6, %zmm6
	vpmovzxbw	1024(%rsp), %zmm7       # 32-byte Folded Reload
                                        # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm28, %zmm15          # zmm15 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm15, %zmm4
	vpmovzxbw	1216(%rsp), %zmm28      # 32-byte Folded Reload
                                        # zmm28 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm7, %zmm28, %zmm15
	vpaddw	%zmm6, %zmm15, %zmm6
	vpaddd	%zmm10, %zmm18, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm16, %edx
	movslq	%edx, %r8
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpextrd	$3, %xmm16, %edi
	movslq	%edi, %rdi
	vpaddw	%zmm4, %zmm6, %zmm4
	vpcmpltuw	%zmm4, %zmm25, %k1
	vpsubw	%zmm4, %zmm25, %zmm15
	vpsubw	%zmm25, %zmm4, %zmm15 {%k1}
	vextracti32x4	$1, %ymm16, %xmm4
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	vpextrd	$2, %xmm4, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm16, %xmm4
	vpinsrb	$1, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %r8d
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %esi
	vpinsrb	$3, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %edi
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebp
	vpextrd	$1, %xmm3, %r9d
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm4
	vmovd	%xmm3, %edx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %ecx
	movslq	%edx, %rdx
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %ebp
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%r8d, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm3, 1024(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm16, %xmm6
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm4
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$1, %xmm6, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm6, %esi
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm4
	vmovd	%xmm3, %r13d
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, 1216(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm3, %edx
	movslq	%r9d, %rbx
	movslq	%ecx, %rcx
	movq	%rcx, 896(%rsp)                 # 8-byte Spill
	movslq	%ebp, %rbp
	movslq	%r12d, %rdi
	movslq	%r11d, %rsi
	movslq	1536(%rsp), %r9                 # 4-byte Folded Reload
	movslq	1472(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r14d, %r14
	movslq	1664(%rsp), %r10                # 4-byte Folded Reload
	movslq	960(%rsp), %r11                 # 4-byte Folded Reload
	movslq	704(%rsp), %r15                 # 4-byte Folded Reload
	movslq	%r13d, %r13
	movslq	1024(%rsp), %r12                # 4-byte Folded Reload
	movslq	1216(%rsp), %rcx                # 4-byte Folded Reload
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm3
	movq	896(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%r9), %xmm3, %xmm3
	vpinsrb	$7, (%rax,%r8), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%r14), %xmm3, %xmm3
	vpinsrb	$9, (%rax,%r10), %xmm3, %xmm3
	vpinsrb	$10, (%rax,%r11), %xmm3, %xmm3
	vpinsrb	$11, (%rax,%r15), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%r13), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%r12), %xmm3, %xmm3
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	vinserti32x4	$1, %xmm4, %ymm3, %ymm16
	vpaddw	%zmm24, %zmm24, %zmm3
	vpaddw	%zmm7, %zmm7, %zmm4
	vpaddw	%zmm4, %zmm3, %zmm3
	vpaddw	%zmm1, %zmm29, %zmm1
	vpmovzxbw	%ymm20, %zmm19          # zmm19 = ymm20[0],zero,ymm20[1],zero,ymm20[2],zero,ymm20[3],zero,ymm20[4],zero,ymm20[5],zero,ymm20[6],zero,ymm20[7],zero,ymm20[8],zero,ymm20[9],zero,ymm20[10],zero,ymm20[11],zero,ymm20[12],zero,ymm20[13],zero,ymm20[14],zero,ymm20[15],zero,ymm20[16],zero,ymm20[17],zero,ymm20[18],zero,ymm20[19],zero,ymm20[20],zero,ymm20[21],zero,ymm20[22],zero,ymm20[23],zero,ymm20[24],zero,ymm20[25],zero,ymm20[26],zero,ymm20[27],zero,ymm20[28],zero,ymm20[29],zero,ymm20[30],zero,ymm20[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpsllw	$2, %zmm19, %zmm3
	vpaddw	%zmm3, %zmm1, %zmm3
	vpmovzxbw	%ymm5, %zmm1            # zmm1 = ymm5[0],zero,ymm5[1],zero,ymm5[2],zero,ymm5[3],zero,ymm5[4],zero,ymm5[5],zero,ymm5[6],zero,ymm5[7],zero,ymm5[8],zero,ymm5[9],zero,ymm5[10],zero,ymm5[11],zero,ymm5[12],zero,ymm5[13],zero,ymm5[14],zero,ymm5[15],zero,ymm5[16],zero,ymm5[17],zero,ymm5[18],zero,ymm5[19],zero,ymm5[20],zero,ymm5[21],zero,ymm5[22],zero,ymm5[23],zero,ymm5[24],zero,ymm5[25],zero,ymm5[26],zero,ymm5[27],zero,ymm5[28],zero,ymm5[29],zero,ymm5[30],zero,ymm5[31],zero
	vpaddw	%zmm21, %zmm21, %zmm4
	vpaddw	%zmm28, %zmm28, %zmm5
	vpaddw	%zmm5, %zmm4, %zmm4
	vpsllw	$2, %zmm1, %zmm5
	vpaddw	%zmm0, %zmm27, %zmm0
	vpaddw	%zmm4, %zmm0, %zmm0
	vpaddw	%zmm5, %zmm0, %zmm4
	vpcmpltuw	%zmm4, %zmm3, %k1
	vpsubw	%zmm4, %zmm3, %zmm0
	vpsubw	%zmm3, %zmm4, %zmm0 {%k1}
	vpmovzxbw	%ymm2, %zmm2            # zmm2 = ymm2[0],zero,ymm2[1],zero,ymm2[2],zero,ymm2[3],zero,ymm2[4],zero,ymm2[5],zero,ymm2[6],zero,ymm2[7],zero,ymm2[8],zero,ymm2[9],zero,ymm2[10],zero,ymm2[11],zero,ymm2[12],zero,ymm2[13],zero,ymm2[14],zero,ymm2[15],zero,ymm2[16],zero,ymm2[17],zero,ymm2[18],zero,ymm2[19],zero,ymm2[20],zero,ymm2[21],zero,ymm2[22],zero,ymm2[23],zero,ymm2[24],zero,ymm2[25],zero,ymm2[26],zero,ymm2[27],zero,ymm2[28],zero,ymm2[29],zero,ymm2[30],zero,ymm2[31],zero
	vpaddd	1408(%rsp), %zmm18, %zmm3       # 64-byte Folded Reload
	vpaddd	1152(%rsp), %zmm18, %zmm4       # 64-byte Folded Reload
	vextracti128	$1, %ymm4, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vpextrd	$2, %xmm5, 1024(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm5, 1216(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm5, %r15d
	vextracti32x4	$2, %zmm4, %xmm5
	vpextrd	$1, %xmm5, %r14d
	vpextrd	$2, %xmm5, %r12d
	vmovd	%xmm5, %r13d
	vpextrd	$3, %xmm5, 1536(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpextrd	$2, %xmm3, %ebx
	vpextrd	$3, %xmm3, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm3, %xmm5
	vmovd	%xmm5, %ecx
	vpextrd	$1, %xmm5, %edx
	vpextrd	$2, %xmm5, %r8d
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm5, %esi
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm3, %xmm5
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm5, %edi
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm5, %ebx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm5, %ebp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %ecx
	vpaddw	%zmm2, %zmm2, %zmm5
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm4, %edx
	movslq	%edx, %r11
	movslq	%r8d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	movq	%rdx, 1472(%rsp)                # 8-byte Spill
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rdx
	movq	%rdx, 1664(%rsp)                # 8-byte Spill
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rdx
	movq	%rdx, 960(%rsp)                 # 8-byte Spill
	movslq	%r15d, %r9
	movslq	%r10d, %r15
	movslq	1024(%rsp), %r10                # 4-byte Folded Reload
	movslq	1216(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r13d, %r13
	movslq	%r14d, %r14
	movslq	%r12d, %r12
	vextracti32x4	$3, %zmm4, %xmm4
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %edi
	movslq	1536(%rsp), %rdx                # 4-byte Folded Reload
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm3, %xmm3
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%edi, %rdi
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm3
	vpextrd	$2, %xmm4, %ebx
	movzbl	(%rax,%r11), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm4, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movq	1472(%rsp), %r11                # 8-byte Reload
	vpinsrb	$1, (%rax,%r11), %xmm6, %xmm4
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm4
	movq	960(%rsp), %rbp                 # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$4, (%rax,%r9), %xmm4, %xmm4
	vpinsrb	$5, (%rax,%r15), %xmm4, %xmm4
	vpinsrb	$6, (%rax,%r10), %xmm4, %xmm4
	vpinsrb	$7, (%rax,%r8), %xmm4, %xmm4
	vpinsrb	$8, (%rax,%r13), %xmm4, %xmm4
	vpinsrb	$9, (%rax,%r14), %xmm4, %xmm4
	vpinsrb	$10, (%rax,%r12), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpmovzxbw	%ymm3, %zmm4            # zmm4 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpmovzxbw	%ymm30, %zmm3           # zmm3 = ymm30[0],zero,ymm30[1],zero,ymm30[2],zero,ymm30[3],zero,ymm30[4],zero,ymm30[5],zero,ymm30[6],zero,ymm30[7],zero,ymm30[8],zero,ymm30[9],zero,ymm30[10],zero,ymm30[11],zero,ymm30[12],zero,ymm30[13],zero,ymm30[14],zero,ymm30[15],zero,ymm30[16],zero,ymm30[17],zero,ymm30[18],zero,ymm30[19],zero,ymm30[20],zero,ymm30[21],zero,ymm30[22],zero,ymm30[23],zero,ymm30[24],zero,ymm30[25],zero,ymm30[26],zero,ymm30[27],zero,ymm30[28],zero,ymm30[29],zero,ymm30[30],zero,ymm30[31],zero
	vpmovzxbw	%ymm16, %zmm20          # zmm20 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddw	%zmm3, %zmm19, %zmm6
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm4, %zmm20, %zmm4
	vpaddw	%zmm4, %zmm5, %zmm19
	vpaddd	1088(%rsp), %zmm18, %zmm4       # 64-byte Folded Reload
	vpaddd	1280(%rsp), %zmm18, %zmm16      # 64-byte Folded Reload
	vpextrd	$1, %xmm16, 1024(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm16, 1216(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm16, 1536(%rsp)          # 4-byte Folded Spill
	vextracti32x4	$1, %ymm16, %xmm5
	vmovd	%xmm5, %r14d
	vpextrd	$1, %xmm5, %r12d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r11d
	vextracti32x4	$2, %zmm16, %xmm5
	vpextrd	$1, %xmm5, %r13d
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm4, %xmm6
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm6, %edx
	vpextrd	$2, %xmm6, %ecx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm6, %esi
	vextracti32x4	$2, %zmm4, %xmm6
	vpinsrb	$1, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %r8d
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edi
	vpinsrb	$3, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %ebx
	vpinsrb	$4, (%rax,%rbp), %xmm7, %xmm7
	vmovd	%xmm6, %ebp
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm7, %xmm6
	vpextrd	$2, %xmm5, %r10d
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm16, %ecx
	vpextrd	$3, %xmm5, %r9d
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vpextrd	$1, %xmm7, %ebp
	movslq	%r8d, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %edi
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm5, %edi
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm5, %xmm4
	vmovd	%xmm7, %ebx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$3, %xmm7, %ecx
	movslq	1024(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rsi), %xmm5, %xmm5
	movslq	1216(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rsi), %xmm5, %xmm5
	movslq	1536(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r14d, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r12d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r15d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r11d, %rsi
	movq	136(%rsp), %r8                  # 8-byte Reload
	vpinsrb	$7, (%rax,%rsi), %xmm5, %xmm5
	movslq	%edi, %rsi
	movq	40(%rsp), %r14                  # 8-byte Reload
	vpinsrb	$8, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r13d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r9d, %rsi
	movl	$2, %edi
	movl	416(%rsp), %r9d                 # 4-byte Reload
	vpinsrb	$11, (%rax,%rsi), %xmm5, %xmm5
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm5, %xmm5
	movslq	%ebp, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm5, %xmm5
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm5, %xmm5
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpaddw	%zmm20, %zmm20, %zmm5
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm5, %zmm4
	vpaddw	%zmm1, %zmm2, %zmm1
	vpaddw	%zmm3, %zmm1, %zmm1
	vpaddw	%zmm4, %zmm1, %zmm1
	vpcmpltuw	%zmm1, %zmm19, %k1
	vpsubw	%zmm1, %zmm19, %zmm2
	vpsubw	%zmm19, %zmm1, %zmm2 {%k1}
	vpaddw	%zmm14, %zmm15, %zmm1
	vpaddw	%zmm0, %zmm2, %zmm0
	vpmovuswb	%zmm1, %ymm1
	vpmovuswb	%zmm0, %ymm0
	vpaddb	%ymm1, %ymm0, %ymm0
	movq	2176(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	movq	2048(%rsp), %rcx                # 8-byte Reload
	incq	%rcx
	jne	.LBB214_31
# %bb.32:                               # %"end for output.s0.y.yi23.us"
                                        #   in Loop: Header=BB214_30 Depth=2
	movq	640(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	672(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	1400(%rsp), %rdx                # 8-byte Folded Reload
	jne	.LBB214_30
# %bb.33:                               # %"end for output.s0.x.x20.loopexit.us"
                                        #   in Loop: Header=BB214_29 Depth=1
	movq	256(%rsp), %rbp                 # 8-byte Reload
	incq	%rbp
	movq	616(%rsp), %rdx                 # 8-byte Reload
	addl	$16, %edx
	movq	1984(%rsp), %rcx                # 8-byte Reload
	addq	$16, %rcx
	cmpq	480(%rsp), %rbp                 # 8-byte Folded Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB214_29
.LBB214_34:                             # %"end for output.s0.y.y.rebased17"
	movq	3064(%rsp), %rsi                # 8-byte Reload
	movl	(%rsi), %r14d
	movl	16(%rsi), %r9d
	movl	20(%rsi), %r8d
	movslq	24(%rsi), %r10
	movq	2120(%rsp), %rcx                # 8-byte Reload
	movl	(%rcx), %edx
	movq	%rdx, 312(%rsp)                 # 8-byte Spill
	movl	4(%rcx), %r11d
	movl	16(%rcx), %ebx
	movl	20(%rcx), %edi
	movslq	24(%rcx), %rcx
	movq	%rcx, 200(%rsp)                 # 8-byte Spill
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	addl	$17, %ecx
	movl	%ecx, %edx
	sarl	$4, %edx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %edx
	leal	15(%rdi), %ecx
	sarl	$4, %ecx
	cmpl	%ecx, %edx
	movl	%ecx, 2120(%rsp)                # 4-byte Spill
	cmovgel	%ecx, %edx
	movq	%r8, 136(%rsp)                  # 8-byte Spill
	movq	%r9, 776(%rsp)                  # 8-byte Spill
	leal	(%r8,%r9), %ecx
	movl	%ebx, 112(%rsp)                 # 4-byte Spill
	subl	%ebx, %ecx
	addl	$17, %edi
	cmpl	%edi, %ecx
	cmovll	%ecx, %edi
	addl	$-2, %edi
	sarl	$4, %edi
	cmpl	%edi, %edx
	cmovgl	%edx, %edi
	movq	%rdi, 616(%rsp)                 # 8-byte Spill
	movq	%r11, 768(%rsp)                 # 8-byte Spill
	leal	31(%r11), %edi
	sarl	$5, %edi
	movl	4(%rsi), %ecx
	addl	%r14d, %ecx
	movq	%rcx, 608(%rsp)                 # 8-byte Spill
	vpbroadcastd	%r14d, %zmm0
	vmovdqu64	%zmm0, 3072(%rsp)       # 64-byte Spill
	movq	%rdi, 448(%rsp)                 # 8-byte Spill
	movl	%edi, %ecx
	movq	%rcx, 1400(%rsp)                # 8-byte Spill
	movl	%edx, 232(%rsp)                 # 4-byte Spill
	testl	%edx, %edx
	movl	%r14d, -8(%rsp)                 # 4-byte Spill
	movq	%r10, 40(%rsp)                  # 8-byte Spill
	jle	.LBB214_42
# %bb.35:                               # %"for output.s0.y.y45.preheader"
	cmpl	$0, 768(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_42
# %bb.36:                               # %"for output.s0.y.y45.us.preheader"
	movq	608(%rsp), %rcx                 # 8-byte Reload
	decl	%ecx
	movq	136(%rsp), %rdx                 # 8-byte Reload
	leal	1(%rdx), %edi
	decl	%edx
	movl	%edx, 856(%rsp)                 # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	%zmm0, 3200(%rsp)       # 64-byte Spill
	movl	232(%rsp), %ecx                 # 4-byte Reload
	movq	%rcx, 2128(%rsp)                # 8-byte Spill
	movq	200(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %edx
	movl	112(%rsp), %ecx                 # 4-byte Reload
	imull	%ecx, %edx
	negl	%edx
	movl	%edx, 844(%rsp)                 # 4-byte Spill
	movl	%ecx, %edx
	subl	776(%rsp), %edx                 # 4-byte Folded Reload
	movl	%edx, 476(%rsp)                 # 4-byte Spill
	xorl	%edx, %edx
	movl	$2, %ebp
	movl	%edi, 860(%rsp)                 # 4-byte Spill
	.p2align	4, 0x90
.LBB214_37:                             # %"for output.s0.y.y45.us"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_38 Depth 2
                                        #       Child Loop BB214_39 Depth 3
	movq	%rdx, 2136(%rsp)                # 8-byte Spill
	movl	%ecx, 848(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	imulq	200(%rsp), %rcx                 # 8-byte Folded Reload
	addq	568(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2144(%rsp)                # 8-byte Spill
	movl	844(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_38:                             # %"for output.s0.x.x51.us"
                                        #   Parent Loop BB214_37 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_39 Depth 3
	movl	%ecx, 852(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	2144(%rsp), %rcx                # 8-byte Folded Reload
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	%rdx, 2152(%rsp)                # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3200(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2176(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1024(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3776(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3712(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1216(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1536(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3648(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1472(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3584(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3520(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3456(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3392(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 3328(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 3264(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 2160(%rsp)                # 8-byte Spill
	movl	476(%rsp), %ecx                 # 4-byte Reload
                                        # kill: def $ecx killed $ecx def $rcx
	.p2align	4, 0x90
.LBB214_39:                             # %"for output.s0.y.yi57.us"
                                        #   Parent Loop BB214_37 Depth=1
                                        #     Parent Loop BB214_38 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	136(%rsp), %rsi                 # 8-byte Reload
	cmpl	%esi, %ecx
	movl	%ecx, %edx
                                        # kill: def $esi killed $esi killed $rsi
	cmovgl	%edi, %edx
	cmovll	%ecx, %esi
	movl	%esi, 1664(%rsp)                # 4-byte Spill
	cmpl	$2, %edx
	cmovlel	%ebp, %edx
	addl	$-2, %edx
	imull	%r10d, %edx
	subl	%r14d, %edx
	vpbroadcastd	%edx, %zmm22
	vmovdqu64	3776(%rsp), %zmm19      # 64-byte Reload
	vpaddd	%zmm19, %zmm22, %zmm0
	vmovdqu64	3712(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm22, %zmm2
	vmovd	%xmm2, %edx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %ebx
	vextracti128	$1, %ymm2, %xmm3
	movslq	%edx, %r14
	vpextrd	$1, %xmm3, %edx
	vpextrd	$2, %xmm3, %r13d
	vpextrd	$3, %xmm3, %r10d
	movslq	%esi, %r9
	vextracti32x4	$2, %zmm2, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r8d
	movslq	%edi, %rdi
	vpextrd	$3, %xmm1, 960(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	movslq	%ebx, %rsi
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm2, 896(%rsp)            # 4-byte Folded Spill
	movslq	%ebx, %rbx
	movslq	%r12d, %r12
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpinsrb	$1, (%rax,%r12), %xmm4, %xmm4
	vmovd	%xmm3, %ebx
	vpextrd	$2, %xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm3
	vpextrd	$3, %xmm0, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm0, %xmm4
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r12d
	vextracti32x4	$2, %zmm0, %xmm4
	movslq	%edx, %rdx
	movzbl	(%rax,%r14), %ebx
	vmovd	%ebx, %xmm5
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm1, %ebx
	vpinsrb	$1, (%rax,%r9), %xmm5, %xmm1
	vpextrd	$1, %xmm4, %ebp
	movslq	%r13d, %r9
	movslq	%ebx, %r13
	vpinsrb	$2, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %edi
	movslq	%r10d, %rbx
	vpinsrb	$3, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %r10d
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %r14d
	vpinsrb	$9, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ebp
	movslq	%r15d, %rdx
	movslq	%ecx, %rcx
	movq	%rcx, 1408(%rsp)                # 8-byte Spill
	vpinsrb	$6, (%rax,%r9), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %r15d
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rdx), %xmm1, %xmm2
	vmovd	%xmm0, %edx
	vmovdqu64	3584(%rsp), %zmm26      # 64-byte Reload
	vpaddd	%zmm26, %zmm22, %zmm0
	vmovdqu64	3520(%rsp), %zmm27      # 64-byte Reload
	vpaddd	%zmm27, %zmm22, %zmm1
	vmovd	%xmm1, %edi
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edi, %r11
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vmovd	%xmm0, %ebx
	movslq	%edx, %r13
	movslq	%ebx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm4
	vpextrd	$1, %xmm0, %edx
	movslq	%r8d, %rcx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$2, %xmm0, %edx
	movslq	960(%rsp), %rsi                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm0, %ebx
	movslq	704(%rsp), %rdx                 # 4-byte Folded Reload
	movq	%rdx, 1088(%rsp)                # 8-byte Spill
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm0, %xmm5
	vpinsrb	$3, (%rax,%rbx), %xmm4, %xmm4
	vmovd	%xmm5, %ebx
	movslq	896(%rsp), %rdx                 # 4-byte Folded Reload
	movq	%rdx, 704(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ebx
	movslq	%r12d, %rdx
	movq	%rdx, 960(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm5, %ebx
	movslq	%r10d, %r10
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$3, %xmm5, %r12d
	movslq	%r14d, %rbx
	movslq	%r12d, %r14
	vextracti32x4	$2, %zmm0, %xmm5
	vpinsrb	$7, (%rax,%r14), %xmm4, %xmm4
	vmovd	%xmm5, %r14d
	movslq	%ebp, %r12
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %r14
	vpextrd	$2, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rcx
	vextracti128	$1, %ymm1, %xmm6
	vpinsrb	$11, (%rax,%r10), %xmm3, %xmm3
	vmovd	%xmm6, %r10d
	movzbl	(%rax,%r11), %ebp
	vmovd	%ebp, %xmm7
	vpextrd	$1, %xmm6, %r11d
	movslq	%r15d, %r9
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %r15d
	vpinsrb	$11, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r10d, %r10
	vpextrd	$3, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm4, %xmm4
	movslq	%r11d, %r11
	vpextrd	$3, %xmm6, %esi
	vextracti32x4	$2, %zmm1, %xmm5
	vpinsrb	$12, (%rax,%r13), %xmm3, %xmm3
	vmovd	%xmm5, %edi
	vpinsrb	$2, (%rax,%r14), %xmm7, %xmm6
	vpextrd	$1, %xmm5, %r14d
	movslq	%r15d, %r15
	movslq	%edi, %r13
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%r14d, %r10
	vpinsrb	$5, (%rax,%r11), %xmm6, %xmm5
	vpextrd	$1, %xmm1, %r11d
	movq	1408(%rsp), %rdx                # 8-byte Reload
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	vmovd	%xmm1, %r14d
	vpinsrb	$6, (%rax,%r15), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %r15d
	movslq	%ecx, %rdx
	movslq	%r14d, %r8
	vpinsrb	$7, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %r14d
	movslq	%edi, %rcx
	movslq	%r11d, %r11
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$8, (%rax,%r13), %xmm5, %xmm1
	vmovd	%xmm0, %esi
	movslq	%r15d, %rdi
	movslq	%esi, %r15
	vpinsrb	$9, (%rax,%r10), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %esi
	movslq	%r14d, %rbp
	movq	%rbp, 1152(%rsp)                # 8-byte Spill
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$12, (%rax,%r15), %xmm4, %xmm0
	movslq	%ecx, %r14
	vpinsrb	$12, (%rax,%r8), %xmm1, %xmm4
	movq	2048(%rsp), %rcx                # 8-byte Reload
	addl	$3, %ecx
	vpinsrb	$13, (%rax,%rbx), %xmm3, %xmm3
	movq	136(%rsp), %rbx                 # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovgel	%ebx, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovlel	%ebx, %ecx
	movq	1088(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm2, %xmm2
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm6
	vpinsrb	$13, (%rax,%rsi), %xmm0, %xmm0
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm5
	vmovdqa64	%zmm6, %zmm12
	vmovd	%xmm5, %ecx
	movslq	%ecx, %r10
	vpinsrb	$13, (%rax,%r11), %xmm4, %xmm4
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %esi
	vpinsrb	$14, (%rax,%r12), %xmm3, %xmm3
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$3, %xmm5, %ecx
	movq	704(%rsp), %rsi                 # 8-byte Reload
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm6
	movslq	%ecx, %r15
	vextracti128	$1, %ymm5, %xmm2
	vpextrd	$1, %xmm2, %r11d
	vpinsrb	$14, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %ebx
	vpinsrb	$14, (%rax,%rdi), %xmm4, %xmm0
	vmovd	%xmm2, %esi
	vextracti32x4	$2, %zmm5, %xmm4
	vpextrd	$1, %xmm4, %edi
	vpinsrb	$15, (%rax,%r9), %xmm3, %xmm8
	vpextrd	$2, %xmm4, %r8d
	vextracti32x4	$3, %zmm5, %xmm2
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	movq	960(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm11
	vpextrd	$2, %xmm1, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm4, 960(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm5
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm5, %ecx
	movslq	%r11d, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %ecx
	movslq	%edx, %r11
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	vpinsrb	$15, (%rax,%r14), %xmm7, %xmm10
	vmovd	%xmm4, %ecx
	movzbl	(%rax,%r10), %edx
	vmovd	%edx, %xmm4
	vpextrd	$2, %xmm2, 1408(%rsp)           # 4-byte Folded Spill
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%r12), %xmm4, %xmm4
	vpextrd	$3, %xmm2, 896(%rsp)            # 4-byte Folded Spill
	movq	1152(%rsp), %rbx                # 8-byte Reload
	vpinsrb	$15, (%rax,%rbx), %xmm0, %xmm9
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbx), %xmm3, %xmm0
	vmovd	%xmm2, %r12d
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%r13), %xmm4, %xmm3
	vmovd	%xmm2, %ebx
	movslq	%edi, %r13
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r15), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%r8d, %r8
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ebp
	vextracti32x4	$3, %zmm1, %xmm5
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r11), %xmm3, %xmm1
	vpextrd	$1, %xmm5, %r11d
	vpinsrb	$8, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, 1152(%rsp)           # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm5, %r9d
	vmovdqu64	%zmm12, 1728(%rsp)      # 64-byte Spill
	vpaddd	%zmm26, %zmm12, %zmm2
	vpaddd	%zmm27, %zmm12, %zmm3
	vmovd	%xmm3, %esi
	vpinsrb	$10, (%rax,%r8), %xmm1, %xmm1
	vpextrd	$1, %xmm3, %edx
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm4
	movslq	%esi, %rcx
	movq	%rcx, 1088(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm3, %esi
	vmovd	%xmm2, %ebp
	vpextrd	$3, %xmm5, %r13d
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpextrd	$1, %xmm2, %ebp
	movslq	%edx, %rdi
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%ebx, %r14
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %edx
	movslq	%esi, %rcx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm5
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm5, %esi
	movslq	%r12d, %r12
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %esi
	movslq	%r9d, %r8
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %esi
	movslq	960(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %ebp
	movslq	704(%rsp), %r15                 # 4-byte Folded Reload
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm2, %xmm5
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vmovd	%xmm5, %ebp
	movslq	1408(%rsp), %rsi                # 4-byte Folded Reload
	movq	%rsi, 1408(%rsp)                # 8-byte Spill
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %ebp
	movslq	896(%rsp), %rsi                 # 4-byte Folded Reload
	movq	%rsi, 960(%rsp)                 # 8-byte Spill
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %r9d
	movslq	%r11d, %r10
	vextracti128	$1, %ymm3, %xmm6
	movslq	%r9d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm6, %esi
	movslq	1152(%rsp), %r11                # 4-byte Folded Reload
	movslq	%esi, %rsi
	movq	1088(%rsp), %rbp                # 8-byte Reload
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm7
	vpextrd	$1, %xmm6, %ebp
	movslq	%r13d, %rbx
	movq	%rbx, 704(%rsp)                 # 8-byte Spill
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edi
	movslq	%edi, %r13
	vpinsrb	$2, (%rax,%r14), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %edi
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm3, %xmm6
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm5
	vpextrd	$1, %xmm6, %ecx
	movslq	%edi, %rdi
	movslq	%edx, %r14
	vpinsrb	$4, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %esi
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$3, %xmm6, %ebp
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%r13), %xmm5, %xmm5
	vpextrd	$1, %xmm3, %edx
	vpinsrb	$12, (%rax,%r8), %xmm4, %xmm4
	vmovd	%xmm3, %ebx
	vpinsrb	$7, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %edi
	movslq	%ebp, %rbp
	movslq	%ebx, %r13
	vpinsrb	$8, (%rax,%r14), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %ebx
	movslq	%edx, %rdx
	movslq	%edi, %r9
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm3
	vmovd	%xmm2, %ecx
	movslq	%ebx, %rdi
	movq	%rdi, 896(%rsp)                 # 8-byte Spill
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edi
	vpinsrb	$12, (%rax,%r12), %xmm1, %xmm1
	movslq	%edi, %r12
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	movq	%rdi, 1088(%rsp)                # 8-byte Spill
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm2
	vpinsrb	$12, (%rax,%r13), %xmm3, %xmm3
	movl	1664(%rsp), %ecx                # 4-byte Reload
	testl	%ecx, %ecx
	movl	$1, %edi
	cmovlel	%edi, %ecx
	vpinsrb	$13, (%rax,%r10), %xmm4, %xmm4
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm6
	vpinsrb	$13, (%rax,%r15), %xmm1, %xmm5
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm0
	vmovdqa64	%zmm6, %zmm14
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1280(%rsp)                # 8-byte Spill
	vpinsrb	$13, (%rax,%rsi), %xmm2, %xmm6
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %ecx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm7
	movslq	%esi, %r13
	vpextrd	$3, %xmm0, %ebp
	vextracti128	$1, %ymm0, %xmm2
	vpinsrb	$14, (%rax,%r11), %xmm4, %xmm13
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$2, %xmm2, %r14d
	movq	1408(%rsp), %rdx                # 8-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm5, %xmm12
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$2, %zmm0, %xmm3
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm3, %ebx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm5
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %r10d
	vpinsrb	$14, (%rax,%r12), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%r9), %xmm7, %xmm15
	vpextrd	$3, %xmm1, %r11d
	vpinsrb	$2, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$3, %xmm3, 1152(%rsp)           # 4-byte Folded Spill
	movslq	%ebp, %rsi
	movslq	%r11d, %rbp
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$2, %xmm4, %ebp
	movq	704(%rsp), %rdx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rdx), %xmm13, %xmm7
	vmovdqa	%xmm7, 1408(%rsp)               # 16-byte Spill
	vmovd	%xmm2, %r9d
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm2
	vpextrd	$3, %xmm4, %ebp
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$7, (%rax,%rbp), %xmm2, %xmm2
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %r12d
	movslq	%r9d, %rdi
	movq	1280(%rsp), %rdx                # 8-byte Reload
	movzbl	(%rax,%rdx), %ebp
	vmovd	%ebp, %xmm5
	vpextrd	$2, %xmm0, %ebp
	movq	960(%rsp), %rdx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rdx), %xmm12, %xmm30
	vmovd	%xmm3, %r9d
	vpinsrb	$1, (%rax,%r13), %xmm5, %xmm3
	vpextrd	$3, %xmm0, %r13d
	movslq	%r8d, %r8
	movslq	%r9d, %rdx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ecx
	movslq	%r14d, %r14
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %r9d
	movq	1088(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm17
	vmovd	%xmm0, %esi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm0
	vpextrd	$3, %xmm4, %r11d
	movslq	%r15d, %r15
	movslq	%esi, %rsi
	movq	%rsi, 960(%rsp)                 # 8-byte Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$5, (%rax,%r8), %xmm0, %xmm0
	vmovd	%xmm1, %esi
	movslq	%ebx, %rdi
	movq	%rdi, 1088(%rsp)                # 8-byte Spill
	movslq	%esi, %rsi
	movq	%rsi, 704(%rsp)                 # 8-byte Spill
	vpinsrb	$6, (%rax,%r14), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %ebx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%r15), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %r15d
	movq	896(%rsp), %rsi                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rsi), %xmm15, %xmm13
	vpextrd	$3, %xmm1, %r14d
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm4
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm1
	vmovdqu64	%zmm14, 1664(%rsp)      # 64-byte Spill
	vpaddd	%zmm26, %zmm14, %zmm2
	vpaddd	%zmm27, %zmm14, %zmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rdi
	vmovd	%xmm2, %ecx
	movslq	%r10d, %r10
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %esi
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%r12d, %r12
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %edx
	movslq	%ebp, %rsi
	movq	%rsi, 896(%rsp)                 # 8-byte Spill
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm5
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm5, %edx
	movslq	%r13d, %rsi
	movq	%rsi, 512(%rsp)                 # 8-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %edx
	movslq	%r9d, %rbp
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %edx
	movslq	%r11d, %r9
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm2, %xmm5
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm5, %edx
	movslq	%edx, %rdx
	movslq	%ebx, %r8
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm3, %edx
	movq	1088(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$9, (%rax,%rsi), %xmm4, %xmm4
	movslq	%edx, %rdx
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdi), %edi
	movslq	%r15d, %r13
	vmovd	%edi, %xmm6
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %edi
	movslq	%edi, %r11
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm7, %ebp
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%r14d, %r14
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %r15d
	vpinsrb	$10, (%rax,%r10), %xmm4, %xmm4
	movslq	%ebp, %r10
	vpextrd	$2, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm0, %xmm0
	movslq	%edx, %rdx
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$2, %zmm3, %xmm7
	vpinsrb	$11, (%rax,%r9), %xmm1, %xmm1
	vmovd	%xmm7, %edi
	vpinsrb	$3, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%r15d, %rbp
	movslq	%edi, %r9
	vpinsrb	$4, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edx
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm3, %ebp
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %r10
	vpextrd	$3, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm0, %xmm0
	vmovd	%xmm3, %edi
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm5
	vpextrd	$2, %xmm3, %ebx
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%r9), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %ecx
	movslq	%ebp, %r11
	movslq	%ebx, %r9
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$9, (%rax,%rsi), %xmm5, %xmm3
	vmovd	%xmm2, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 1280(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%r10), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edx
	movq	704(%rsp), %rbp                 # 8-byte Reload
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm5
	movslq	%edx, %r10
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	movq	%rdx, 416(%rsp)                 # 8-byte Spill
	movq	960(%rsp), %rdx                 # 8-byte Reload
	vpinsrb	$12, (%rax,%rdx), %xmm4, %xmm2
	movq	2048(%rsp), %r15                # 8-byte Reload
	leal	2(%r15), %edx
	movq	136(%rsp), %rbx                 # 8-byte Reload
	cmpl	%ebx, %edx
	cmovgel	%ebx, %edx
	testl	%edx, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movl	$1, %ecx
	cmovlel	%ecx, %edx
	decl	%edx
	imull	40(%rsp), %edx                  # 4-byte Folded Reload
	subl	-8(%rsp), %edx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rdi), %xmm3, %xmm4
	vpbroadcastd	%edx, %zmm6
	vpaddd	%zmm19, %zmm6, %zmm1
	vpaddd	%zmm20, %zmm6, %zmm3
	vmovdqa64	%zmm6, %zmm24
	vmovd	%xmm3, %ecx
	vpinsrb	$13, (%rax,%r8), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm3, %edx
	movslq	%edx, %rdi
	vpinsrb	$13, (%rax,%r12), %xmm2, %xmm15
	vpextrd	$2, %xmm3, %ebx
	vpextrd	$3, %xmm3, %edx
	vpinsrb	$13, (%rax,%rsi), %xmm0, %xmm14
	vextracti128	$1, %ymm3, %xmm2
	vpextrd	$1, %xmm2, %r12d
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%r11), %xmm4, %xmm16
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm2, %esi
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$3, %xmm2, %r8d
	vpinsrb	$14, (%rax,%r13), %xmm5, %xmm5
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, %r13d
	vextracti128	$1, %ymm1, %xmm6
	movq	896(%rsp), %rbp                 # 8-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm15, %xmm4
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$2, %xmm0, 672(%rsp)            # 4-byte Folded Spill
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm3, %xmm23
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$3, %xmm0, 1088(%rsp)           # 4-byte Folded Spill
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm23, 1152(%rsp)          # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%r10), %xmm14, %xmm14
	vmovd	%xmm2, %ecx
	vpinsrb	$2, (%rax,%rbx), %xmm3, %xmm2
	vpextrd	$2, %xmm23, 896(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movslq	%r12d, %rbp
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm23, 704(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm6, %ecx
	vpinsrb	$14, (%rax,%r9), %xmm16, %xmm15
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm6, %edi
	vextracti32x4	$2, %zmm1, %xmm3
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm18
	vpextrd	$1, %xmm3, %ebx
	vpinsrb	$5, (%rax,%rbp), %xmm2, %xmm6
	vpinsrb	$15, (%rax,%r14), %xmm5, %xmm25
	vmovd	%xmm0, %edx
	vpextrd	$2, %xmm3, %r9d
	vextracti32x4	$3, %zmm1, %xmm2
	movq	512(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm12
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm1
	vmovdqu64	%zmm24, 960(%rsp)       # 64-byte Spill
	vpaddd	%zmm26, %zmm24, %zmm29
	vmovd	%xmm29, %ecx
	vpextrd	$3, %xmm3, 640(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, 512(%rsp)            # 4-byte Folded Spill
	movq	416(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm14, %xmm14
	vpaddd	%zmm27, %zmm24, %zmm16
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	movq	1280(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$15, (%rax,%rsi), %xmm15, %xmm15
	vpextrd	$3, %xmm29, %esi
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, 416(%rsp)            # 4-byte Folded Spill
	movslq	%esi, %rcx
	vextracti32x4	$1, %ymm29, %xmm5
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm5, %ecx
	movslq	%edi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %ecx
	vpinsrb	$7, (%rax,%rsi), %xmm18, %xmm6
	vmovd	%xmm16, %esi
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, 1280(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpextrd	$1, %xmm16, %ecx
	movslq	%r8d, %rsi
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm3, %ecx
	vpextrd	$3, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm5
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$1, %ymm16, %xmm3
	vpinsrb	$8, (%rax,%rdx), %xmm1, %xmm6
	vmovd	%xmm3, %edx
	vpinsrb	$2, (%rax,%rsi), %xmm7, %xmm1
	vpextrd	$1, %xmm3, %esi
	vinserti128	$1, %xmm8, %ymm11, %ymm4
	vmovdqu	%ymm4, 1920(%rsp)               # 32-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm3, %ecx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm3, %edx
	vextracti32x4	$2, %zmm16, %xmm7
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm7, %esi
	vextracti32x4	$2, %zmm29, %xmm11
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm8
	vmovd	%xmm7, %ecx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm0
	vpextrd	$2, %xmm7, 1792(%rsp)           # 4-byte Folded Spill
	movslq	%ebx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movl	856(%rsp), %ecx                 # 4-byte Reload
	cmpl	%ecx, %r15d
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm4
	cmovll	%r15d, %ecx
	movl	%ecx, %edx
	sarl	$31, %edx
	andnl	%ecx, %edx, %ecx
	vpextrd	$3, %xmm7, 256(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm10, %ymm9, %ymm28
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm1
	vpextrd	$1, %xmm11, %edx
	movslq	%r13d, %rdi
	movslq	%esi, %rcx
	movslq	%edx, %rdx
	vpaddd	%zmm19, %zmm1, %zmm24
	vpaddd	%zmm20, %zmm1, %zmm31
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm5
	vmovd	%xmm24, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$1, %xmm24, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm31, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm31, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdx), %xmm8, %xmm9
	vpextrd	$2, %xmm31, %edx
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm8
	vmovd	%xmm23, 1856(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm24, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm2, 160(%rsp)                # 4-byte Folded Spill
	movslq	%edx, %rcx
	vmovdqa64	%zmm1, %zmm19
	vpaddd	%zmm26, %zmm1, %zmm23
	vpaddd	%zmm27, %zmm1, %zmm27
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm23, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm10
	vmovd	%xmm27, %edx
	vmovdqu64	2176(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm22, %zmm2
	vmovdqu64	1024(%rsp), %zmm3       # 64-byte Reload
	vpaddd	%zmm3, %zmm22, %zmm0
	movslq	%edx, %rsi
	vextracti32x4	$1, %ymm0, %xmm26
	vmovd	%xmm26, 480(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm26, 1600(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 176(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm26, 576(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm26
	vmovd	%xmm26, 384(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm26, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm26, -72(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm2, %xmm26
	movzbl	(%rax,%rsi), %esi
	vpextrd	$1, %xmm26, -60(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm26, 64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm26, %r13d
	vmovd	%xmm26, %r15d
	vextracti32x4	$2, %zmm2, %xmm26
	vpextrd	$1, %xmm26, %r12d
	vpextrd	$2, %xmm26, %r14d
	vmovd	%xmm26, %ebx
	vpextrd	$3, %xmm26, %ebp
	vmovd	%esi, %xmm26
	vpextrd	$1, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm26, %xmm26
	movslq	%r9d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm1
	vmovdqa	%xmm1, 320(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm26, %xmm26
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %edi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	movzbl	(%rax,%rsi), %esi
	vmovd	%xmm2, %r8d
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %r10d
	vpextrd	$3, %xmm2, %r11d
	vmovd	%esi, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r15d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vextracti32x4	$3, %zmm0, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r9d
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %ebx
	vmovd	%ecx, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	vinserti32x4	$1, 1408(%rsp), %ymm30, %ymm2 # 16-byte Folded Reload
	vmovdqu	%ymm2, 576(%rsp)                # 32-byte Spill
	vmovdqu64	1216(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm19, %zmm30
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm30, %ecx
	vinserti32x4	$1, %xmm17, %ymm13, %ymm4
	vmovdqu	%ymm4, 384(%rsp)                # 32-byte Spill
	movslq	%ecx, %rdi
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm13
	vinserti32x4	$1, %xmm25, %ymm12, %ymm0
	vmovdqu	%ymm0, 1408(%rsp)               # 32-byte Spill
	vpaddd	%zmm2, %zmm22, %zmm1
	vmovdqu64	1536(%rsp), %zmm0       # 64-byte Reload
	vpaddd	%zmm0, %zmm22, %zmm4
	vmovdqa64	%zmm0, %zmm2
	vextracti128	$1, %ymm4, %xmm0
	vmovd	%xmm0, 480(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 176(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 352(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm0
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, %r12d
	vmovd	%xmm0, %r15d
	vextracti32x4	$2, %zmm1, %xmm0
	vpextrd	$1, %xmm0, %r13d
	vpextrd	$2, %xmm0, %r14d
	vmovd	%xmm0, %r11d
	vpextrd	$3, %xmm0, %ebx
	vmovd	%edi, %xmm0
	vpextrd	$1, %xmm30, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm30, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm0, %xmm0
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %esi
	vmovd	%edi, %xmm1
	movslq	%ebp, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm25
	vextracti32x4	$2, %zmm4, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edi
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	480(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	1600(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	176(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	352(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	movslq	%esi, %rdx
	vextracti32x4	$3, %zmm4, %xmm4
	movslq	672(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$10, (%rax,%rsi), %xmm5, %xmm12
	vpextrd	$2, %xmm4, %esi
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %edx
	vmovd	%xmm4, %edi
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm9, %xmm9
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	vextracti32x4	$3, %zmm16, %xmm4
	movslq	%ecx, %rcx
	movslq	1792(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm31, %ecx
	vpinsrb	$10, (%rax,%rdi), %xmm8, %xmm21
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm24, %edi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm25, %ymm1, %ymm25
	vinserti128	$1, %xmm14, %ymm15, %ymm1
	vmovdqu	%ymm1, 672(%rsp)                # 32-byte Spill
	vpaddd	1472(%rsp), %zmm22, %zmm1       # 64-byte Folded Reload
	vextracti128	$1, %ymm1, %xmm5
	vpextrd	$1, %xmm5, 1792(%rsp)           # 4-byte Folded Spill
	vpinsrb	$3, (%rax,%rdi), %xmm6, %xmm15
	vpextrd	$2, %xmm5, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, 1600(%rsp)           # 4-byte Folded Spill
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm16
	vmovd	%xmm5, 176(%rsp)                # 4-byte Folded Spill
	vpaddd	%zmm2, %zmm19, %zmm14
	vmovdqa64	%zmm2, %zmm8
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm4, -96(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm4, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm4, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 144(%rsp)            # 4-byte Folded Spill
	vmovd	%ecx, %xmm4
	vpextrd	$3, %xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm10, %xmm10
	vpextrd	$1, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm5, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -60(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm5, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm27, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm26, %xmm5
	vpextrd	$2, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm6
	vmovdqu64	3648(%rsp), %zmm26      # 64-byte Reload
	vpaddd	%zmm26, %zmm22, %zmm7
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -84(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 208(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, 80(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 240(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm7, %xmm1
	vpextrd	$1, %xmm1, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -100(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %r12d
	vextracti32x4	$2, %zmm7, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %ebx
	vmovd	%xmm1, %r11d
	vpaddd	%zmm18, %zmm19, %zmm4
	vmovd	%xmm4, %r13d
	movslq	%r13d, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%xmm7, %r13d
	vpextrd	$1, %xmm7, %ecx
	vpextrd	$2, %xmm7, %edx
	vpextrd	$3, %xmm7, %esi
	vextracti32x4	$3, %zmm7, %xmm1
	vmovd	%xmm1, %edi
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%ebp, %xmm1
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r13d, %rbp
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm7
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r12d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vpmovzxbw	%ymm13, %zmm13          # zmm13 = ymm13[0],zero,ymm13[1],zero,ymm13[2],zero,ymm13[3],zero,ymm13[4],zero,ymm13[5],zero,ymm13[6],zero,ymm13[7],zero,ymm13[8],zero,ymm13[9],zero,ymm13[10],zero,ymm13[11],zero,ymm13[12],zero,ymm13[13],zero,ymm13[14],zero,ymm13[15],zero,ymm13[16],zero,ymm13[17],zero,ymm13[18],zero,ymm13[19],zero,ymm13[20],zero,ymm13[21],zero,ymm13[22],zero,ymm13[23],zero,ymm13[24],zero,ymm13[25],zero,ymm13[26],zero,ymm13[27],zero,ymm13[28],zero,ymm13[29],zero,ymm13[30],zero,ymm13[31],zero
	vpsllw	$2, %zmm13, %zmm13
	vpmovzxbw	%ymm25, %zmm22          # zmm22 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	vpaddw	%zmm22, %zmm22, %zmm22
	vpaddw	%zmm22, %zmm13, %zmm25
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm13
	vinserti128	$1, %xmm7, %ymm2, %ymm0
	vpmovzxbw	1920(%rsp), %zmm2       # 32-byte Folded Reload
                                        # zmm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm2, %zmm2, %zmm7
	vmovdqu64	%zmm7, 1792(%rsp)       # 64-byte Spill
	vpmovzxbw	%ymm28, %zmm2           # zmm2 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddw	%zmm2, %zmm2, %zmm2
	vmovdqu64	%zmm2, 1920(%rsp)       # 64-byte Spill
	vpaddw	%zmm7, %zmm2, %zmm2
	vpaddw	%zmm25, %zmm2, %zmm2
	vpextrd	$3, %xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm28
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm6
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm2, %zmm20
	vpaddd	%zmm3, %zmm19, %zmm17
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, %r9d
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm25
	vextracti32x4	$3, %zmm29, %xmm0
	vmovd	%xmm0, -12(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm31, %xmm0
	vmovd	%xmm0, 16(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm31, %xmm0
	vextracti32x4	$3, %zmm31, %xmm2
	vpextrd	$1, %xmm0, 208(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 64(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm0, 80(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm24, %xmm0
	vpextrd	$1, %xmm2, 176(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 352(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 480(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm24, %xmm2
	vpextrd	$1, %xmm0, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 4(%rsp)                  # 4-byte Folded Spill
	vextracti32x4	$1, %ymm27, %xmm0
	vpextrd	$1, %xmm2, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 48(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm2, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -60(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm23, %xmm2
	vpextrd	$1, %xmm0, 36(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm0, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm14, %xmm0
	vpextrd	$1, %xmm2, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, (%rsp)                   # 4-byte Folded Spill
	vextracti32x4	$1, %ymm30, %xmm2
	vpextrd	$1, %xmm0, 32(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1728(%rsp), %zmm1       # 64-byte Reload
	vpaddd	%zmm3, %zmm1, %zmm0
	vpextrd	$1, %xmm2, 24(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm2, -4(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -52(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$1, %ymm17, %xmm2
	vpextrd	$1, %xmm2, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, -100(%rsp)               # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm2
	vpextrd	$1, %xmm2, 2624(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2752(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, 2496(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2816(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, 2880(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm2, 2688(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2368(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm1, %zmm2
	vpextrd	$1, %xmm0, 132(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 280(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 288(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r10d
	vextracti32x4	$3, %zmm0, %xmm0
	vpextrd	$1, %xmm0, 2944(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 2304(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm0, 2560(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 2432(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	vpextrd	$1, %xmm0, 884(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 304(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %r11d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$1, %xmm2, %r15d
	vpextrd	$2, %xmm2, %r12d
	vpextrd	$3, %xmm2, %r13d
	vmovd	%xmm2, %ecx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm0, 888(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 300(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r14d
	vpextrd	$3, %xmm0, 892(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm1, %zmm0
	vpextrd	$1, %xmm2, %edi
	vmovd	%xmm2, %edx
	vpextrd	$2, %xmm2, %r8d
	vpextrd	$3, %xmm2, %ebx
	vextracti128	$1, %ymm0, %xmm2
	vpextrd	$1, %xmm2, 128(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 124(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 120(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 808(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, 3136(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 464(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, 816(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 832(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm4, %xmm2
	movslq	%ecx, %rcx
	vmovd	%xmm2, 864(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 880(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 236(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 116(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1216(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm1, %zmm2
	vmovd	%xmm0, 792(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 784(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 308(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 800(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, 456(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 624(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 824(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 632(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, 876(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 872(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 868(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %ebp
	vmovd	%ecx, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	304(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	884(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	888(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	300(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	892(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm3
	vpextrd	$1, %xmm3, %r12d
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r14d
	vmovd	%xmm3, %r13d
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm29
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	320(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm31
	movslq	%edi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm12, %xmm12
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm9, %xmm9
	movslq	%ebx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm11
	movslq	%r10d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %r8d
	vextracti32x4	$3, %zmm2, %xmm0
	vmovd	%xmm0, %r9d
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %r11d
	vmovd	%ecx, %xmm0
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm21, %xmm2
	movslq	132(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm15, %xmm3
	movslq	280(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm16, %xmm6
	movslq	288(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm10, %xmm8
	movslq	2496(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	movslq	2624(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm13, %xmm10
	movslq	2752(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm28, %xmm13
	movslq	2816(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	864(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm29, %xmm15
	movslq	2688(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm31, %xmm16
	movslq	2880(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	1856(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm12, %xmm12
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm9, %xmm9
	movslq	2368(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm29
	movslq	2560(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm31
	movslq	2944(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm8, %xmm8
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm28
	movslq	%esi, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm5, %xmm5
	movslq	%edi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm10, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm13, %xmm3
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	880(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm15, %xmm10
	movslq	876(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm13
	movslq	872(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm12
	movslq	868(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r13d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm29, %xmm15
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm31, %xmm16
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm29
	movslq	%r14d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm8, %xmm8
	movslq	%r9d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm5, %xmm5
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r11d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	792(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	784(%rsp), %rdx                 # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rdx), %xmm7, %xmm7
	movslq	236(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm10, %xmm6
	movslq	308(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm10
	movslq	800(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm12, %xmm12
	movslq	808(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm9, %xmm9
	movslq	128(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm15, %xmm15
	movslq	124(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm16, %xmm31
	movslq	120(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm13
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm29, %xmm7
	movslq	816(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm13, %xmm13
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm8, %xmm29
	movslq	3136(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm13, %xmm8
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm13
	movslq	464(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm8, %xmm5
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm16
	movslq	832(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm5
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm2
	movslq	456(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm3
	movslq	116(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm8
	movslq	624(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm10, %xmm5
	movslq	824(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm6
	vinserti32x4	$1, %xmm11, %ymm28, %ymm10
	movslq	632(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm17, %ecx
	vinserti128	$1, %xmm0, %ymm3, %ymm11
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm25, %xmm3
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm9, %xmm0
	vmovdqa	%xmm0, 96(%rsp)                 # 16-byte Spill
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm15, %xmm28
	vextracti32x4	$3, %zmm24, %xmm24
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm24, 1856(%rsp)              # 4-byte Folded Spill
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm15
	vpextrd	$1, %xmm24, 416(%rsp)           # 4-byte Folded Spill
	vpmovzxbw	%ymm10, %zmm9           # zmm9 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpsllw	$2, %zmm9, %zmm9
	vpmovzxbw	%ymm11, %zmm10          # zmm10 = ymm11[0],zero,ymm11[1],zero,ymm11[2],zero,ymm11[3],zero,ymm11[4],zero,ymm11[5],zero,ymm11[6],zero,ymm11[7],zero,ymm11[8],zero,ymm11[9],zero,ymm11[10],zero,ymm11[11],zero,ymm11[12],zero,ymm11[13],zero,ymm11[14],zero,ymm11[15],zero,ymm11[16],zero,ymm11[17],zero,ymm11[18],zero,ymm11[19],zero,ymm11[20],zero,ymm11[21],zero,ymm11[22],zero,ymm11[23],zero,ymm11[24],zero,ymm11[25],zero,ymm11[26],zero,ymm11[27],zero,ymm11[28],zero,ymm11[29],zero,ymm11[30],zero,ymm11[31],zero
	vpaddw	%zmm10, %zmm10, %zmm10
	vpaddw	%zmm10, %zmm9, %zmm11
	vextracti32x4	$2, %zmm27, %xmm3
	vextracti32x4	$3, %zmm27, %xmm0
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	vmovdqu	%ymm5, 1152(%rsp)               # 32-byte Spill
	vextracti32x4	$2, %zmm23, %xmm5
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm15, %xmm10
	vpextrd	$2, %xmm24, 1280(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm24, 896(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm14, %xmm6
	vmovd	%xmm3, %r8d
	vpextrd	$1, %xmm3, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 640(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm30, %xmm3
	vmovd	%xmm0, 160(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 512(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 1088(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm17, %xmm0
	vmovd	%xmm5, %r9d
	vpextrd	$1, %xmm5, -12(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm5, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -108(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm4, %xmm9
	vmovd	%xmm6, %r10d
	vpextrd	$1, %xmm6, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 256(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm26, %zmm19, %zmm15
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, 36(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -112(%rsp)           # 4-byte Folded Spill
	vmovdqu64	1472(%rsp), %zmm22      # 64-byte Reload
	vpaddd	%zmm22, %zmm19, %zmm5
	vmovdqa64	%zmm19, %zmm25
	vmovd	%xmm0, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm5, %xmm0
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vmovd	%xmm9, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm9, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm9, -56(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, -84(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm15, %xmm3
	vmovd	%xmm0, %r14d
	vpextrd	$1, %xmm0, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 144(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm15, %xmm0
	vmovd	%xmm3, %ebp
	vpextrd	$1, %xmm3, %ebx
	vpextrd	$2, %xmm3, %edx
	vpextrd	$3, %xmm3, %esi
	vpaddd	%zmm22, %zmm1, %zmm3
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 4(%rsp)                  # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm0
	vpextrd	$1, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, 32(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm3, %xmm6
	vpextrd	$1, %xmm6, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm15, %ecx
	vmovd	%xmm6, 28(%rsp)                 # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm31, %xmm24
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm27
	movslq	%ebx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm29, %xmm9
	movslq	%edx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm13, %xmm13
	movslq	%esi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm16, %xmm16
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpaddd	%zmm26, %zmm1, %zmm21
	vmovd	%xmm3, 2432(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, 2368(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, (%rsp)               # 4-byte Folded Spill
	vextracti32x4	$3, %zmm3, %xmm3
	movzbl	(%rax,%rcx), %edx
	vpextrd	$1, %xmm3, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 1728(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm3, 16(%rsp)                 # 4-byte Folded Spill
	vextracti32x4	$1, %ymm21, %xmm3
	vpextrd	$1, %xmm3, %ebp
	vpextrd	$2, %xmm3, %r15d
	vmovd	%xmm3, %ebx
	vpextrd	$3, %xmm3, %r13d
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$2, %zmm21, %xmm6
	vpextrd	$1, %xmm6, %r12d
	vpextrd	$2, %xmm6, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 2304(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm5, %ecx
	vmovd	%xmm6, %r11d
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r10d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm29
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm31
	vmovd	%xmm21, %ecx
	vpextrd	$1, %xmm21, %r14d
	vpextrd	$2, %xmm21, %edx
	vpextrd	$3, %xmm21, %esi
	vextracti32x4	$3, %zmm21, %xmm3
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, %r8d
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r10d
	vmovd	%ecx, %xmm3
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm8, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm10, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm24, %xmm6
	movslq	%ebx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm27, %xmm8
	movslq	%ebp, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r15d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm13, %xmm10
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm16, %xmm13
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm29, %xmm16
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm27
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm24
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm29
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm8, %xmm8
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm21
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	movslq	2368(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm10, %xmm3
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm13, %xmm13
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm16, %xmm2
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm27, %xmm6
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm24, %xmm16
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm29, %xmm24
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm8, %xmm10
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm9, %xmm8
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm13, %xmm9
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm13
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm1
	vmovdqa	%xmm1, 256(%rsp)                # 16-byte Spill
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm16, %xmm1
	vmovdqa	%xmm1, 640(%rsp)                # 16-byte Spill
	vinserti32x4	$1, %xmm21, %ymm0, %ymm7
	vpmovzxbw	576(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm29
	vpmovzxbw	384(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm27
	vpaddw	%zmm29, %zmm27, %zmm0
	vpaddw	%zmm11, %zmm0, %zmm11
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1856(%rsp), %rdx                # 4-byte Folded Reload
	movslq	160(%rsp), %rsi                 # 4-byte Folded Reload
	vextracti32x4	$3, %zmm23, %xmm0
	movslq	-80(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm31, %xmm2
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	movslq	320(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	144(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rdx), %xmm24, %xmm1
	vmovdqa	%xmm1, 1856(%rsp)               # 16-byte Spill
	vpmovzxbw	%ymm7, %zmm7            # zmm7 = ymm7[0],zero,ymm7[1],zero,ymm7[2],zero,ymm7[3],zero,ymm7[4],zero,ymm7[5],zero,ymm7[6],zero,ymm7[7],zero,ymm7[8],zero,ymm7[9],zero,ymm7[10],zero,ymm7[11],zero,ymm7[12],zero,ymm7[13],zero,ymm7[14],zero,ymm7[15],zero,ymm7[16],zero,ymm7[17],zero,ymm7[18],zero,ymm7[19],zero,ymm7[20],zero,ymm7[21],zero,ymm7[22],zero,ymm7[23],zero,ymm7[24],zero,ymm7[25],zero,ymm7[26],zero,ymm7[27],zero,ymm7[28],zero,ymm7[29],zero,ymm7[30],zero,ymm7[31],zero
	vpaddw	%zmm7, %zmm7, %zmm7
	vpaddw	%zmm7, %zmm11, %zmm7
	vpcmpltuw	%zmm7, %zmm20, %k1
	vpsubw	%zmm7, %zmm20, %zmm31
	vpsubw	%zmm20, %zmm7, %zmm31 {%k1}
	vmovdqu64	1664(%rsp), %zmm19      # 64-byte Reload
	vmovdqu64	1024(%rsp), %zmm12      # 64-byte Reload
	vpaddd	%zmm12, %zmm19, %zmm7
	vextracti128	$1, %ymm7, %xmm11
	vpextrd	$1, %xmm11, 384(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm1
	vmovdqa	%xmm1, 160(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm11, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm11, -72(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm8, %xmm21
	vmovd	%xmm11, 144(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm8
	vpextrd	$1, %xmm8, -60(%rsp)            # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm23
	vmovd	%xmm8, 208(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm8, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm8, 48(%rsp)             # 4-byte Folded Spill
	vmovdqu64	2176(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm19, %zmm3
	vpextrd	$1, %xmm7, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, %r8d
	vextracti32x4	$3, %zmm7, %xmm7
	vpextrd	$1, %xmm7, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 80(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm7, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm7
	vpextrd	$1, %xmm7, %r9d
	vmovd	%xmm7, %esi
	vpextrd	$2, %xmm7, %r11d
	vpextrd	$3, %xmm7, %r10d
	vextracti32x4	$2, %zmm3, %xmm7
	vpextrd	$1, %xmm3, -12(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -16(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %ecx
	vextracti32x4	$3, %zmm3, %xmm3
	vpextrd	$1, %xmm7, %ebp
	vpextrd	$2, %xmm7, %edi
	vmovd	%xmm7, 36(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm7, %ebx
	vmovdqu64	1536(%rsp), %zmm11      # 64-byte Reload
	vpaddd	%zmm11, %zmm19, %zmm7
	vpextrd	$1, %xmm3, 20(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm3, -4(%rsp)                 # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 16(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 32(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm7, %xmm3
	vpextrd	$1, %xmm3, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -92(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, -68(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm3
	vpextrd	$1, %xmm3, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -76(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm19, %zmm3
	vmovdqa64	%zmm18, %zmm20
	vpextrd	$1, %xmm7, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, -44(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -32(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm7, %xmm7
	vpextrd	$1, %xmm7, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -24(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm7, -28(%rsp)                # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm7
	vpextrd	$1, %xmm7, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 24(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm7, 2432(%rsp)               # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 12(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm7
	vpextrd	$1, %xmm7, 8(%rsp)              # 4-byte Folded Spill
	vmovd	%xmm7, 2368(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 4(%rsp)              # 4-byte Folded Spill
	vpextrd	$3, %xmm7, (%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm5, %xmm7
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm2, %xmm24
	vmovd	%xmm3, %edx
	vpextrd	$1, %xmm3, 2944(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 2304(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$3, %zmm3, %xmm2
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 576(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1728(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm26, %zmm19, %zmm3
	vmovd	%xmm2, 2688(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 2752(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 2816(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 2880(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm0
	vmovd	%xmm0, 2624(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 2560(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 2496(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 288(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm0, %r13d
	vpextrd	$3, %xmm0, %r15d
	vmovd	%xmm0, %r14d
	vmovd	%ecx, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm18
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %r8d
	vpextrd	$3, %xmm3, %r9d
	vextracti32x4	$3, %zmm3, %xmm2
	vmovd	%xmm2, %r10d
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %r11d
	vpextrd	$3, %xmm2, %ebp
	vmovd	%ecx, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm10
	movslq	%edx, %rcx
	movslq	2944(%rsp), %rdx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	2240(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	2304(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	2368(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	2688(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	2752(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	2816(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	2880(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-40(%rsp), %rdx                 # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm0
	movslq	%esi, %rcx
	movslq	%edi, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	2624(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	2560(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	2496(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	288(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm14, %xmm3
	movslq	%ebx, %rcx
	movslq	%r11d, %rdx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm3, 208(%rsp)            # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm3, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 384(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %ecx
	vextracti32x4	$3, %zmm30, %xmm14
	vmovd	%xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm9, %xmm16
	movslq	%ebp, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm30
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm13, %xmm2
	vinserti32x4	$1, 96(%rsp), %ymm28, %ymm28 # 16-byte Folded Reload
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm3
	vinserti32x4	$1, %xmm18, %ymm10, %ymm10
	vinserti32x4	$1, %xmm1, %ymm0, %ymm18
	vpaddd	%zmm22, %zmm19, %zmm0
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm6
	movzbl	(%rax,%rcx), %r12d
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -92(%rsp)                # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %ecx
	vmovdqu64	960(%rsp), %zmm9        # 64-byte Reload
	vpaddd	%zmm12, %zmm9, %zmm0
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r14d
	vpextrd	$3, %xmm6, %r15d
	vmovd	%xmm6, %ebx
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -60(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 64(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, 48(%rsp)             # 4-byte Folded Spill
	vmovd	%xmm1, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 80(%rsp)             # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm9, %zmm1
	vpextrd	$1, %xmm14, 240(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm14, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm14, 1664(%rsp)          # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, -64(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm6, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, %r10d
	vpextrd	$3, %xmm6, %r11d
	vextracti32x4	$2, %zmm1, %xmm6
	vmovd	%xmm6, %r8d
	vpextrd	$1, %xmm6, %r9d
	vpextrd	$2, %xmm6, %esi
	vpextrd	$3, %xmm6, %edi
	vmovd	%r12d, %xmm6
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	movslq	-100(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	-108(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	movslq	-96(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edx, %rbp
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ecx, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r13d, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r14d, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r15d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm12
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r15d
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm1, %r12d
	vpextrd	$2, %xmm1, %r13d
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%ebx, %xmm1
	movslq	%ebp, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r14d, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r15d, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	-64(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbx), %xmm1, %xmm1
	movslq	-68(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbx), %xmm1, %xmm1
	movslq	%r10d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r11d, %rbp
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r8d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r9d, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm19
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm0, %ebp
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %r8d
	vpextrd	$3, %xmm0, %r9d
	vmovd	%ecx, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm13
	vextracti32x4	$3, %zmm17, %xmm0
	vmovd	%xmm0, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm4, 80(%rsp)             # 4-byte Folded Spill
	vmovdqa64	%zmm9, %zmm6
	vpaddd	%zmm11, %zmm9, %zmm0
	vpextrd	$2, %xmm4, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -60(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -100(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm9, %zmm1
	vpextrd	$1, %xmm0, -24(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -112(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -52(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vpextrd	$1, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -48(%rsp)                # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm0
	vpextrd	$1, %xmm0, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -20(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r13d
	vpextrd	$3, %xmm0, -12(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	vmovd	%xmm1, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %ebx
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm0, %r11d
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vpaddd	%zmm22, %zmm9, %zmm4
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vextracti128	$1, %ymm4, %xmm0
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm0, -40(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, -44(%rsp)                # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %esi
	vpaddd	%zmm26, %zmm9, %zmm1
	vpextrd	$2, %xmm7, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 960(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, %r12d
	vpextrd	$1, %xmm6, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, %r11d
	vextracti32x4	$2, %zmm1, %xmm6
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r14d
	vpextrd	$3, %xmm6, %r15d
	vmovd	%esi, %xmm6
	movslq	-24(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	movslq	-28(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	movslq	-52(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	-100(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rsi), %xmm6, %xmm6
	movslq	-104(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rsi), %xmm6, %xmm6
	movslq	-96(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	movslq	-108(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	movslq	-76(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	movslq	-84(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	movslq	-88(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	movslq	-92(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	movslq	-48(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rsi), %xmm6, %xmm6
	movslq	-64(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm6, %xmm6
	movslq	-68(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	movslq	-56(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm20
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$3, %zmm1, %xmm1
	movzbl	(%rax,%rsi), %esi
	vpextrd	$1, %xmm1, %ebx
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r8d
	vmovd	%esi, %xmm1
	movslq	%edi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm6
	vextracti32x4	$2, %zmm4, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %edi
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm7
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	256(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm17
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm7, %xmm1
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, %ebp
	vpextrd	$2, %xmm4, %ebx
	vpextrd	$3, %xmm4, %r10d
	vmovd	%xmm4, %edx
	vextracti32x4	$3, %zmm15, %xmm22
	vmovd	%xmm22, %esi
	movslq	%esi, %rsi
	vmovdqa	640(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$12, (%rax,%rsi), %xmm4, %xmm14
	movslq	-44(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	416(%rsp), %rsi                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm4, %xmm4
	movslq	-40(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	movslq	176(%rsp), %rsi                 # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm7                # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm7, %xmm7
	movslq	-36(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	movslq	320(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm21, %xmm21
	movslq	-32(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	movslq	512(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm23, %xmm11
	movslq	%edi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm1, %xmm1
	movslq	240(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rsi), %xmm16, %xmm9
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm16
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm15
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm21, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm11, %xmm3
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm30, %ymm12, %ymm8
	vpmovzxbw	%ymm10, %zmm10          # zmm10 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpaddw	%zmm10, %zmm10, %zmm11
	vpmovzxbw	1408(%rsp), %zmm10      # 32-byte Folded Reload
                                        # zmm10 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm18, %zmm12          # zmm12 = ymm18[0],zero,ymm18[1],zero,ymm18[2],zero,ymm18[3],zero,ymm18[4],zero,ymm18[5],zero,ymm18[6],zero,ymm18[7],zero,ymm18[8],zero,ymm18[9],zero,ymm18[10],zero,ymm18[11],zero,ymm18[12],zero,ymm18[13],zero,ymm18[14],zero,ymm18[15],zero,ymm18[16],zero,ymm18[17],zero,ymm18[18],zero,ymm18[19],zero,ymm18[20],zero,ymm18[21],zero,ymm18[22],zero,ymm18[23],zero,ymm18[24],zero,ymm18[25],zero,ymm18[26],zero,ymm18[27],zero,ymm18[28],zero,ymm18[29],zero,ymm18[30],zero,ymm18[31],zero
	vpmovzxbw	%ymm8, %zmm8            # zmm8 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpaddw	%zmm8, %zmm12, %zmm12
	vpmovzxbw	672(%rsp), %zmm8        # 32-byte Folded Reload
                                        # zmm8 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm10, %zmm8, %zmm18
	vpaddw	%zmm11, %zmm18, %zmm11
	vpaddw	%zmm12, %zmm11, %zmm18
	vinserti32x4	$1, %xmm19, %ymm13, %ymm11
	vinserti32x4	$1, %xmm0, %ymm20, %ymm0
	vinserti128	$1, %xmm6, %ymm1, %ymm1
	vpmovzxbw	%ymm11, %zmm6           # zmm6 = ymm11[0],zero,ymm11[1],zero,ymm11[2],zero,ymm11[3],zero,ymm11[4],zero,ymm11[5],zero,ymm11[6],zero,ymm11[7],zero,ymm11[8],zero,ymm11[9],zero,ymm11[10],zero,ymm11[11],zero,ymm11[12],zero,ymm11[13],zero,ymm11[14],zero,ymm11[15],zero,ymm11[16],zero,ymm11[17],zero,ymm11[18],zero,ymm11[19],zero,ymm11[20],zero,ymm11[21],zero,ymm11[22],zero,ymm11[23],zero,ymm11[24],zero,ymm11[25],zero,ymm11[26],zero,ymm11[27],zero,ymm11[28],zero,ymm11[29],zero,ymm11[30],zero,ymm11[31],zero
	vpaddw	%zmm6, %zmm6, %zmm6
	vpmovzxbw	1152(%rsp), %zmm12      # 32-byte Folded Reload
                                        # zmm12 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm28, %zmm11          # zmm11 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddw	%zmm1, %zmm0, %zmm0
	vpaddw	%zmm12, %zmm11, %zmm1
	vpaddw	%zmm6, %zmm1, %zmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vextracti32x4	$3, %zmm5, %xmm6
	movslq	960(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rdx), %xmm24, %xmm5
	vpextrd	$1, %xmm6, %esi
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vmovd	%xmm6, %edx
	vpinsrb	$11, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %r11d
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm5, %xmm20
	vpaddw	%zmm0, %zmm1, %zmm1
	vpcmpltuw	%zmm1, %zmm18, %k1
	vpsubw	%zmm1, %zmm18, %zmm19
	vpsubw	%zmm18, %zmm1, %zmm19 {%k1}
	vpaddd	3456(%rsp), %zmm25, %zmm13      # 64-byte Folded Reload
	vmovd	%xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm1
	vpextrd	$1, %xmm13, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm6, 960(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm6
	vpextrd	$2, %xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm13, %ecx
	movslq	%ecx, %rcx
	movslq	1728(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm22, %edi
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	movslq	704(%rsp), %rdx                 # 4-byte Folded Reload
	vpextrd	$2, %xmm22, %r14d
	vextracti128	$1, %ymm13, %xmm7
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm22, 704(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm2, %ymm3, %ymm3
	vpmovzxbw	%ymm1, %zmm18           # zmm18 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm10, %zmm10, %zmm1
	vpaddw	1792(%rsp), %zmm1, %zmm1        # 64-byte Folded Reload
	vpsllw	$2, %zmm18, %zmm6
	vpaddw	%zmm12, %zmm12, %zmm10
	vpaddw	%zmm6, %zmm10, %zmm6
	vpaddw	%zmm1, %zmm29, %zmm10
	vpaddw	%zmm8, %zmm8, %zmm1
	vpaddw	1920(%rsp), %zmm1, %zmm8        # 64-byte Folded Reload
	movslq	144(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm17, %xmm0
	vpmovzxbw	%ymm3, %zmm12           # zmm12 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpextrd	$3, %xmm7, %edx
	vpsllw	$2, %zmm12, %zmm3
	vpaddw	%zmm11, %zmm11, %zmm7
	vpaddw	%zmm3, %zmm7, %zmm3
	vpaddw	%zmm8, %zmm27, %zmm7
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm13, %xmm1
	vpinsrb	$7, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$1, %xmm1, %edx
	movslq	352(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm1, %edx
	vpaddd	3392(%rsp), %zmm25, %zmm1       # 64-byte Folded Reload
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm13, %xmm2
	vpinsrb	$11, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm2, %edx
	movslq	%edi, %rdi
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm1, %edx
	vpinsrb	$13, (%rax,%rdi), %xmm14, %xmm5
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	movzbl	(%rax,%rdx), %edx
	vpaddw	%zmm6, %zmm10, %zmm8
	vmovd	%edx, %xmm6
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rsi), %xmm20, %xmm11
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vpaddw	%zmm3, %zmm7, %zmm13
	vextracti128	$1, %ymm1, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm10
	vmovd	%xmm3, %edi
	vpinsrb	$2, (%rax,%rdx), %xmm6, %xmm4
	vpextrd	$1, %xmm3, %edx
	movslq	96(%rsp), %rbx                  # 4-byte Folded Reload
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %esi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %edi
	vextracti32x4	$2, %zmm1, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %edx
	vpinsrb	$14, (%rax,%rbx), %xmm9, %xmm6
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$6, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rbp), %xmm16, %xmm9
	vmovd	%xmm3, %esi
	vpextrd	$3, %xmm3, %ecx
	movslq	48(%rsp), %rbp                  # 4-byte Folded Reload
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$7, (%rax,%rdi), %xmm4, %xmm3
	vpextrd	$1, %xmm1, %r12d
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %r15d
	vpinsrb	$14, (%rax,%rbp), %xmm15, %xmm7
	vmovd	%xmm1, %esi
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, 896(%rsp)            # 4-byte Folded Spill
	movslq	384(%rsp), %r13                 # 4-byte Folded Reload
	movslq	1664(%rsp), %rdx                # 4-byte Folded Reload
	movslq	64(%rsp), %rbp                  # 4-byte Folded Reload
	movslq	%r14d, %r14
	movslq	%ebx, %rdi
	vpinsrb	$14, (%rax,%rbp), %xmm0, %xmm1
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm2, %ebx
	vpinsrb	$10, (%rax,%rdi), %xmm3, %xmm0
	vpcmpltuw	%zmm13, %zmm8, %k1
	movslq	%esi, %rsi
	vpextrd	$3, %xmm2, %r10d
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpsubw	%zmm13, %zmm8, %zmm3
	movslq	%r12d, %rcx
	vpinsrb	$12, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm14
	vpsubw	%zmm8, %zmm13, %zmm3 {%k1}
	vpaddd	3328(%rsp), %zmm25, %zmm8       # 64-byte Folded Reload
	vpaddd	3264(%rsp), %zmm25, %zmm4       # 64-byte Folded Reload
	vextracti128	$1, %ymm4, %xmm2
	vmovd	%xmm2, %r12d
	vpinsrb	$14, (%rax,%r14), %xmm5, %xmm13
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, %r9d
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$3, %xmm2, %r8d
	vmovd	%xmm8, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%r13), %xmm9, %xmm9
	vpextrd	$2, %xmm8, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vinserti128	$1, %xmm6, %ymm9, %ymm9
	vpextrd	$3, %xmm8, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm8, %xmm0
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm0, %r13d
	vpinsrb	$2, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %ebp
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %esi
	movslq	%r11d, %rcx
	movslq	%r13d, %r13
	movslq	%ebp, %rbp
	vinserti128	$1, %xmm7, %ymm1, %ymm15
	vextracti32x4	$2, %zmm8, %xmm1
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %r13d
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$14, (%rax,%rcx), %xmm11, %xmm7
	vmovd	%xmm1, %ecx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$2, %zmm4, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %esi
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%r13d, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %r13d
	vpinsrb	$14, (%rax,%rbx), %xmm10, %xmm5
	vmovd	%xmm1, %ebx
	movslq	704(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm8, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %ebp
	vpinsrb	$14, (%rax,%r15), %xmm14, %xmm6
	vpextrd	$2, %xmm1, %r15d
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm13, %xmm0
	vmovd	%xmm1, %edx
	vpextrd	$3, %xmm1, %edi
	movslq	960(%rsp), %r11                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%r11), %xmm7, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r10d, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm5, %xmm5
	movslq	%ebp, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	movslq	%r15d, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %ebp
	movzbl	(%rax,%rdx), %edx
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovd	%edx, %xmm1
	movslq	%ebp, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r12d, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r14d, %rdx
	movq	40(%rsp), %r10                  # 8-byte Reload
	movl	-8(%rsp), %r14d                 # 4-byte Reload
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r9d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r8d, %rdx
	movl	$2, %ebp
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %ecx
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	movl	860(%rsp), %edi                 # 4-byte Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vpmovzxbw	%ymm9, %zmm6            # zmm6 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpextrd	$3, %xmm4, %ecx
	vpaddw	%zmm6, %zmm6, %zmm4
	vpmovzxbw	%ymm15, %zmm7           # zmm7 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm7, %zmm18, %zmm8
	vpaddw	%zmm8, %zmm4, %zmm4
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpmovzxbw	%ymm5, %zmm2            # zmm2 = ymm5[0],zero,ymm5[1],zero,ymm5[2],zero,ymm5[3],zero,ymm5[4],zero,ymm5[5],zero,ymm5[6],zero,ymm5[7],zero,ymm5[8],zero,ymm5[9],zero,ymm5[10],zero,ymm5[11],zero,ymm5[12],zero,ymm5[13],zero,ymm5[14],zero,ymm5[15],zero,ymm5[16],zero,ymm5[17],zero,ymm5[18],zero,ymm5[19],zero,ymm5[20],zero,ymm5[21],zero,ymm5[22],zero,ymm5[23],zero,ymm5[24],zero,ymm5[25],zero,ymm5[26],zero,ymm5[27],zero,ymm5[28],zero,ymm5[29],zero,ymm5[30],zero,ymm5[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm2, %zmm0, %zmm2
	vpaddw	%zmm0, %zmm0, %zmm0
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm0, %zmm0
	vpaddw	%zmm2, %zmm4, %zmm1
	vpaddw	%zmm12, %zmm6, %zmm2
	vpaddw	%zmm7, %zmm2, %zmm2
	vpaddw	%zmm0, %zmm2, %zmm0
	vpcmpltuw	%zmm0, %zmm1, %k1
	vpsubw	%zmm0, %zmm1, %zmm2
	vpsubw	%zmm1, %zmm0, %zmm2 {%k1}
	vpaddw	%zmm31, %zmm19, %zmm0
	vpaddw	%zmm3, %zmm2, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	1984(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	2048(%rsp), %rcx                # 8-byte Reload
	incl	%ecx
	decq	2160(%rsp)                      # 8-byte Folded Spill
	jne	.LBB214_39
# %bb.40:                               # %"end for output.s0.y.yi58.us"
                                        #   in Loop: Header=BB214_38 Depth=2
	movq	2152(%rsp), %rdx                # 8-byte Reload
	incq	%rdx
	movl	852(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	1400(%rsp), %rdx                # 8-byte Folded Reload
	jne	.LBB214_38
# %bb.41:                               # %"end for output.s0.x.x52.loopexit.us"
                                        #   in Loop: Header=BB214_37 Depth=1
	movq	2136(%rsp), %rdx                # 8-byte Reload
	incq	%rdx
	movl	848(%rsp), %ecx                 # 4-byte Reload
	addl	$16, %ecx
	addl	$16, 476(%rsp)                  # 4-byte Folded Spill
	cmpq	2128(%rsp), %rdx                # 8-byte Folded Reload
	jne	.LBB214_37
.LBB214_42:                             # %"end for output.s0.y.y46"
	movl	%r14d, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	subl	%esi, %ecx
	addl	$34, %ecx
	movl	%ecx, %edx
	sarl	$5, %edx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %edx
	movq	448(%rsp), %rcx                 # 8-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%ecx, %edx
	movq	608(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %edi
	subl	%esi, %edi
	movq	768(%rsp), %rcx                 # 8-byte Reload
	addl	$34, %ecx
	cmpl	%ecx, %edi
	cmovgel	%ecx, %edi
	addl	$-3, %edi
	sarl	$5, %edi
	cmpl	%edx, %edi
	movl	%edx, 236(%rsp)                 # 4-byte Spill
	cmovlel	%edx, %edi
	movq	%rdi, 624(%rsp)                 # 8-byte Spill
	movq	616(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	movl	232(%rsp), %edi                 # 4-byte Reload
	subl	%edi, %ecx
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jle	.LBB214_60
# %bb.43:                               # %"for output.s0.y.y.rebased92.preheader"
	movq	608(%rsp), %rdx                 # 8-byte Reload
	decl	%edx
	vpbroadcastd	%edx, %zmm0
	vmovdqu64	%zmm0, 3136(%rsp)       # 64-byte Spill
	movq	624(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %edx
	movl	236(%rsp), %ebp                 # 4-byte Reload
	subl	%ebp, %edx
	movq	%rdx, 632(%rsp)                 # 8-byte Spill
	movq	448(%rsp), %rdx                 # 8-byte Reload
	subl	%ebx, %edx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	movl	%ebp, %edx
	movq	%rdx, 816(%rsp)                 # 8-byte Spill
	movslq	%ebx, %r9
	movl	%ecx, %ecx
	movq	%rcx, 800(%rsp)                 # 8-byte Spill
	shll	$4, %edi
	movl	112(%rsp), %edx                 # 4-byte Reload
	addl	%edx, %edi
	movl	%esi, %ecx
	imull	%edx, %ecx
                                        # kill: def $ebp killed $ebp def $rbp
	shll	$5, %ebp
	movq	312(%rsp), %rdx                 # 8-byte Reload
	addl	%ebp, %edx
	subl	%ecx, %ebp
	movq	%rbp, 784(%rsp)                 # 8-byte Spill
	movslq	%ecx, %r8
	negl	%ecx
	movl	%ecx, 300(%rsp)                 # 4-byte Spill
	movl	%edi, %ebp
	subl	776(%rsp), %ebp                 # 4-byte Folded Reload
	movl	%r10d, %ebx
	imull	%ebp, %ebx
	movl	-8(%rsp), %ecx                  # 4-byte Reload
	subl	%ecx, %ebx
	movl	%ebx, 132(%rsp)                 # 4-byte Spill
	movl	%r10d, %ebx
	shll	$4, %ebx
	movl	%ebx, 308(%rsp)                 # 4-byte Spill
	leal	1(%rbp), %ebx
	imull	%r10d, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 128(%rsp)                 # 4-byte Spill
	leal	-1(%rbp), %ebx
	imull	%r10d, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 124(%rsp)                 # 4-byte Spill
	leal	2(%rbp), %ebx
	imull	%r10d, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 120(%rsp)                 # 4-byte Spill
	movq	%rbp, 456(%rsp)                 # 8-byte Spill
	addl	$-2, %ebp
	imull	%r10d, %ebp
	subl	%ecx, %ebp
	movl	%ebp, 116(%rsp)                 # 4-byte Spill
	subl	%ecx, %edx
	movl	%edx, 304(%rsp)                 # 4-byte Spill
	shlq	$5, %r9
	subq	%r8, %r9
	addq	568(%rsp), %r9                  # 8-byte Folded Reload
	movq	%r9, 792(%rsp)                  # 8-byte Spill
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_44:                             # %"for output.s0.y.y.rebased92"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_46 Depth 2
                                        #       Child Loop BB214_47 Depth 3
                                        #     Child Loop BB214_51 Depth 2
                                        #       Child Loop BB214_52 Depth 3
                                        #     Child Loop BB214_56 Depth 2
                                        #       Child Loop BB214_57 Depth 3
	movq	%rdx, 808(%rsp)                 # 8-byte Spill
	movl	%edi, 232(%rsp)                 # 4-byte Spill
	movslq	%edi, %rcx
	imulq	%rsi, %rcx
	movq	%rcx, 280(%rsp)                 # 8-byte Spill
	cmpl	$0, 236(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_49
# %bb.45:                               # %"for output.s0.x.x97.preheader"
                                        #   in Loop: Header=BB214_44 Depth=1
	movq	568(%rsp), %rcx                 # 8-byte Reload
	movq	280(%rsp), %rdx                 # 8-byte Reload
	addq	%rdx, %rcx
	movq	%rcx, 824(%rsp)                 # 8-byte Spill
	movl	300(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_46:                             # %"for output.s0.x.x97"
                                        #   Parent Loop BB214_44 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_47 Depth 3
	movl	%ecx, 464(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	824(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	%rdx, 832(%rsp)                 # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3136(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2368(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2304(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1984(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2176(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2240(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1024(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2944(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2880(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2816(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2752(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2688(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2624(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2560(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2496(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	116(%rsp), %ecx                 # 4-byte Reload
	movl	120(%rsp), %r13d                # 4-byte Reload
	movl	124(%rsp), %r14d                # 4-byte Reload
	movl	128(%rsp), %r11d                # 4-byte Reload
	movl	132(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 1216(%rsp)                # 4-byte Spill
	.p2align	4, 0x90
.LBB214_47:                             # %"for output.s0.y.yi106"
                                        #   Parent Loop BB214_44 Depth=1
                                        #     Parent Loop BB214_46 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 1536(%rsp)                # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	1984(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm0, %zmm1
	vmovdqa64	%zmm2, %zmm14
	vmovdqu64	2176(%rsp), %zmm3       # 64-byte Reload
	vpaddd	%zmm3, %zmm0, %zmm2
	vmovdqa64	%zmm3, %zmm13
	vmovdqa64	%zmm0, %zmm6
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	movslq	%ecx, %rbx
	movslq	%edx, %rdx
	vpextrd	$3, %xmm2, %ecx
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$1, %xmm3, %r15d
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %r12d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$3, %xmm3, %r9d
	vpextrd	$1, %xmm0, 1280(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 896(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %r8d
	vextracti32x4	$3, %zmm2, %xmm22
	vpextrd	$1, %xmm22, 512(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm22, 1088(%rsp)          # 4-byte Folded Spill
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm2
	vpextrd	$2, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rbp), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %ebp
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm22, 416(%rsp)           # 4-byte Folded Spill
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm1, %xmm3
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm4
	vpextrd	$1, %xmm3, %edi
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm5
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$3, (%rax,%rbp), %xmm2, %xmm4
	vmovd	%xmm3, %ebp
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %r10d
	vpinsrb	$3, (%rax,%rcx), %xmm5, %xmm5
	vmovdqu64	2816(%rsp), %zmm21      # 64-byte Reload
	vmovdqa64	%zmm6, %zmm3
	vmovdqu64	%zmm6, 1152(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm6, %zmm20
	vmovd	%xmm20, %ecx
	movslq	%ecx, %rdx
	movslq	%r8d, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$1, %xmm20, %edx
	movslq	%edx, %rdx
	movslq	%ebp, %rsi
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm7
	vpextrd	$1, %xmm2, %ebp
	vpinsrb	$4, (%rax,%rsi), %xmm4, %xmm4
	movslq	%edi, %rdx
	vpextrd	$2, %xmm2, %edi
	vextracti32x4	$3, %zmm1, %xmm9
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	vmovdqu64	2752(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm3, %zmm6
	vpextrd	$2, %xmm20, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm20, %edx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$3, %xmm2, %r8d
	movslq	%edx, %rcx
	vextracti32x4	$1, %ymm20, %xmm1
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm1, %ecx
	movslq	%r15d, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm6, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$1, %xmm9, %r15d
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %ecx
	movslq	%r12d, %rsi
	movslq	%ebx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rdx), %xmm4, %xmm4
	vmovd	%xmm0, %edx
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rsi), %xmm5, %xmm11
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %esi
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm0, %ecx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%r10d, %rsi
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rbx), %xmm7, %xmm12
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm0, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm2
	movl	%r13d, 1472(%rsp)               # 4-byte Spill
	vpbroadcastd	%r13d, %zmm3
	vpaddd	%zmm14, %zmm3, %zmm10
	vmovdqa64	%zmm14, %zmm25
	vmovdqa64	%zmm3, %zmm17
	vmovd	%xmm10, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rsi), %xmm4, %xmm3
	vpextrd	$1, %xmm10, %esi
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpextrd	$2, %xmm9, %r12d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm10, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$3, %xmm10, %esi
	movslq	%esi, %rsi
	movslq	%r9d, %rbx
	vpinsrb	$3, (%rax,%rsi), %xmm4, %xmm7
	vpextrd	$3, %xmm9, 672(%rsp)            # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rbx), %xmm11, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm12, %xmm1
	vpextrd	$3, %xmm0, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	vmovd	%xmm0, %esi
	vextracti32x4	$2, %zmm20, %xmm24
	vpaddd	%zmm13, %zmm17, %zmm29
	vmovdqa64	%zmm13, %zmm26
	vextracti128	$1, %ymm10, %xmm4
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm11
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm3
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm4
	vmovd	%xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm3
	vpextrd	$1, %xmm29, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$1, %xmm0, %ebx
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm24, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm7
	movslq	%esi, %rcx
	vpextrd	$3, %xmm29, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm12
	vextracti32x4	$1, %ymm29, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm5, %xmm2
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$2, %zmm29, %xmm13
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm10, %xmm14
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm4, %xmm16
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm15
	vmovdqu64	%zmm17, 1408(%rsp)      # 64-byte Spill
	vpaddd	%zmm21, %zmm17, %zmm30
	vmovd	%xmm30, %ecx
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	1280(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$9, (%rax,%rbp), %xmm11, %xmm2
	vextracti32x4	$3, %zmm6, %xmm11
	vpextrd	$1, %xmm24, %ebp
	vextracti32x4	$1, %ymm30, %xmm0
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %esi
	vpaddd	%zmm8, %zmm17, %zmm6
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm30, %xmm27
	vpinsrb	$7, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm27, %esi
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rsi), %xmm1, %xmm1
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rbp), %xmm7, %xmm4
	vpextrd	$1, %xmm6, %ebp
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$1, %xmm11, %r10d
	movslq	%ebp, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm11, %r9d
	vpinsrb	$9, (%rax,%rbx), %xmm12, %xmm12
	vpextrd	$1, %xmm14, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm16, %xmm18
	vpextrd	$1, %xmm13, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm15, %xmm15
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm1, %xmm19
	vmovd	%xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm7, %xmm1
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm7
	vpextrd	$3, %xmm5, %esi
	movslq	896(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm6, %xmm1
	vpinsrb	$7, (%rax,%rsi), %xmm7, %xmm5
	vmovd	%xmm1, %esi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm5, %xmm23
	vpextrd	$3, %xmm11, 1600(%rsp)          # 4-byte Folded Spill
	vpinsrb	$10, (%rax,%rdi), %xmm2, %xmm17
	vpextrd	$2, %xmm24, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm20, %xmm31
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm16
	movl	%r14d, 1664(%rsp)               # 4-byte Spill
	vpbroadcastd	%r14d, %zmm0
	vpaddd	%zmm25, %zmm0, %zmm3
	vmovdqa64	%zmm0, %zmm20
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm25
	vpextrd	$1, %xmm3, %esi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm2
	vpextrd	$3, %xmm24, %edi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	movslq	%edx, %rbx
	vpinsrb	$3, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm31, %edx
	vpinsrb	$10, (%rax,%rbx), %xmm12, %xmm4
	vpextrd	$2, %xmm14, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm18, %xmm5
	vpextrd	$2, %xmm13, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm15, %xmm28
	vpextrd	$2, %xmm27, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm19, %xmm24
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%rsi), %xmm23, %xmm15
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm2, %xmm0
	vpextrd	$3, %xmm7, %esi
	vpaddd	%zmm26, %zmm20, %zmm12
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm3, %xmm2
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %esi
	movslq	%r8d, %rbp
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm26
	vmovd	%xmm12, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rbp), %xmm17, %xmm18
	vpextrd	$1, %xmm12, %ebx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm0
	vpextrd	$2, %xmm31, %r8d
	movslq	%ebx, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm12, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm12, %esi
	movslq	%esi, %rsi
	movslq	704(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm13, %ebp
	vpinsrb	$11, (%rax,%rbx), %xmm16, %xmm19
	movslq	%edi, %rsi
	vpextrd	$3, %xmm14, %edi
	vextracti128	$1, %ymm12, %xmm7
	vpinsrb	$11, (%rax,%rsi), %xmm25, %xmm17
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm7, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm1, %esi
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm16
	movslq	%edi, %rcx
	vpextrd	$3, %xmm27, %edi
	vextracti32x4	$2, %zmm12, %xmm1
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm27
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %ecx
	movslq	%ebp, %rbx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rbx), %xmm28, %xmm14
	vmovd	%xmm22, %ecx
	vmovdqu64	%zmm20, 896(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm20, %zmm23
	vmovd	%xmm23, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbx), %ebx
	movslq	%edi, %rdi
	vmovd	%ebx, %xmm4
	vpextrd	$1, %xmm23, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rdi), %xmm24, %xmm25
	vpextrd	$2, %xmm23, %edi
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$3, %xmm2, %ebx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm2
	vpextrd	$3, %xmm23, %edi
	movslq	%edi, %rdi
	vextracti32x4	$1, %ymm23, %xmm4
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm4, %edi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %edi
	vpinsrb	$11, (%rax,%rsi), %xmm15, %xmm13
	vmovd	%xmm9, %esi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm4, %edi
	movslq	%esi, %rsi
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm26, %xmm26
	vmovd	%xmm11, %edi
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbx), %xmm0, %xmm28
	vmovd	%xmm31, %ebx
	vextracti32x4	$3, %zmm29, %xmm1
	vpaddd	%zmm8, %zmm20, %zmm4
	vmovdqa64	%zmm8, %zmm22
	vextracti32x4	$2, %zmm23, %xmm0
	vpinsrb	$12, (%rax,%rsi), %xmm18, %xmm9
	vmovd	%xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm2, %xmm24
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rcx), %xmm19, %xmm15
	vpextrd	$1, %xmm4, %ecx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm31, 640(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movslq	%ebx, %rsi
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm1, %ecx
	vextracti128	$1, %ymm4, %xmm2
	vpinsrb	$12, (%rax,%rsi), %xmm17, %xmm17
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpinsrb	$12, (%rax,%rdi), %xmm16, %xmm19
	vextracti32x4	$3, %zmm10, %xmm2
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm4, %xmm5
	vpinsrb	$12, (%rax,%rdi), %xmm27, %xmm16
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm0, %xmm7
	vpextrd	$1, %xmm5, %edi
	vextracti32x4	$3, %zmm6, %xmm0
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm7, %xmm6
	vpextrd	$2, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %edi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm30, %xmm8
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm6, %xmm29
	vpinsrb	$12, (%rax,%rcx), %xmm14, %xmm14
	vextracti32x4	$3, %zmm12, %xmm10
	movl	%r11d, 960(%rsp)                # 4-byte Spill
	vpbroadcastd	%r11d, %zmm5
	vmovdqu64	1984(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm5, %zmm30
	vmovdqa64	%zmm5, %zmm6
	vmovd	%xmm30, %ecx
	movslq	%ecx, %rdi
	vmovd	%xmm8, %ecx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm7
	vpextrd	$1, %xmm30, %edi
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm30, %edi
	vpinsrb	$12, (%rax,%rcx), %xmm25, %xmm27
	movslq	%edi, %rcx
	vpextrd	$3, %xmm30, %edi
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	vmovd	%xmm0, %ecx
	movslq	%edi, %rdi
	vextracti32x4	$1, %ymm30, %xmm5
	vpinsrb	$3, (%rax,%rdi), %xmm7, %xmm7
	vmovd	%xmm5, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm5, %edi
	vpinsrb	$12, (%rax,%rcx), %xmm13, %xmm25
	vmovd	%xmm10, %ecx
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm3, %xmm13
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm7, %xmm3
	vmovd	%xmm13, %edi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm4, %xmm12
	vextracti32x4	$3, %zmm23, %xmm11
	vmovdqa64	%zmm6, %zmm4
	vmovdqu64	2176(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm6, %zmm7
	vextracti32x4	$2, %zmm30, %xmm5
	vpinsrb	$12, (%rax,%rdi), %xmm26, %xmm6
	vmovdqa	%xmm6, 144(%rsp)                # 16-byte Spill
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm7, %edi
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm6
	vmovdqa	%xmm6, 1856(%rsp)               # 16-byte Spill
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm28
	vmovd	%xmm11, %ecx
	movslq	%edi, %rdi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm5
	vpextrd	$1, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %edi
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$3, %xmm7, %edi
	vpinsrb	$12, (%rax,%rcx), %xmm24, %xmm3
	vmovdqa	%xmm3, 160(%rsp)                # 16-byte Spill
	vmovd	%xmm12, %ecx
	movslq	%edi, %rdi
	vextracti128	$1, %ymm7, %xmm3
	vpinsrb	$3, (%rax,%rdi), %xmm5, %xmm5
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$1, %xmm3, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$2, %xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %edi
	vextracti32x4	$2, %zmm7, %xmm3
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm5, %xmm6
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rcx), %xmm29, %xmm5
	vpextrd	$1, %xmm3, %ecx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$2, %xmm3, %edi
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm3, %ecx
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm7, %xmm24
	vpinsrb	$10, (%rax,%rdi), %xmm6, %xmm3
	vmovd	%xmm24, %edi
	vextracti32x4	$3, %zmm30, %xmm23
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm7
	movslq	%edi, %rcx
	vpaddd	%zmm21, %zmm4, %zmm6
	vmovdqu64	%zmm4, 704(%rsp)        # 64-byte Spill
	vmovd	%xmm6, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vmovdqa	%xmm3, 1280(%rsp)               # 16-byte Spill
	vpextrd	$1, %xmm6, %ecx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$2, %xmm1, %edi
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	movslq	%r15d, %rbx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm28
	vpextrd	$3, %xmm1, 1728(%rsp)           # 4-byte Folded Spill
	vpinsrb	$13, (%rax,%rbx), %xmm9, %xmm3
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$1, %xmm2, %ebx
	vextracti128	$1, %ymm6, %xmm1
	vpinsrb	$13, (%rax,%rcx), %xmm15, %xmm31
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm28, %xmm9
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm9, %xmm9
	vpextrd	$2, %xmm1, %ecx
	movslq	%edx, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm9, %xmm9
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm9, %xmm9
	vpextrd	$2, %xmm2, %edx
	vpinsrb	$13, (%rax,%rbp), %xmm17, %xmm1
	movslq	%r10d, %rcx
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm2
	vpinsrb	$13, (%rax,%rcx), %xmm19, %xmm30
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm9, %xmm9
	vpextrd	$1, %xmm2, %ecx
	vpaddd	%zmm22, %zmm4, %zmm15
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm9, %xmm9
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm9, %xmm17
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm6, %xmm4
	vmovdqa	%xmm4, 176(%rsp)                # 16-byte Spill
	vpinsrb	$11, (%rax,%rcx), %xmm17, %xmm2
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vmovdqa	%xmm2, 1792(%rsp)               # 16-byte Spill
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rbx), %xmm16, %xmm2
	vpextrd	$1, %xmm15, %ebx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$1, %xmm0, %ecx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm15, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm15, %ebx
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rbx), %xmm6, %xmm19
	vpextrd	$2, %xmm0, %r15d
	vpinsrb	$13, (%rax,%rsi), %xmm14, %xmm16
	vpextrd	$1, %xmm8, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm15, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm27, %xmm17
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm19, %xmm14
	vpextrd	$1, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm14, %xmm14
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm14, %xmm14
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm15, %xmm6
	vpinsrb	$7, (%rax,%rsi), %xmm14, %xmm14
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm14, %xmm14
	vpextrd	$1, %xmm6, %esi
	movslq	%r12d, %rbx
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm14, %xmm14
	vpextrd	$2, %xmm6, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm14, %xmm14
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm14, %xmm28
	vpextrd	$3, %xmm0, %r13d
	vpinsrb	$13, (%rax,%rcx), %xmm25, %xmm25
	movl	1216(%rsp), %ecx                # 4-byte Reload
	vpbroadcastd	%ecx, %zmm26
	vpaddd	%zmm20, %zmm26, %zmm29
	vmovd	%xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rbx), %xmm3, %xmm0
	vpextrd	$1, %xmm29, %ebx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$2, %xmm8, %r12d
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm29, %ecx
	movslq	%ecx, %rcx
	movslq	1088(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm14
	vpextrd	$3, %xmm8, %r14d
	vpinsrb	$14, (%rax,%rbx), %xmm31, %xmm31
	movslq	%r8d, %rbx
	vpextrd	$1, %xmm10, 576(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm29, %xmm6
	vpinsrb	$14, (%rax,%rbx), %xmm1, %xmm3
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm14, %xmm1
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm29, %xmm6
	vpinsrb	$7, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %ebx
	movslq	%r9d, %r8
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm10, 512(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%r8), %xmm30, %xmm27
	movslq	%edx, %rdx
	vpextrd	$3, %xmm6, %ebx
	vpaddd	%zmm18, %zmm26, %zmm8
	vpinsrb	$14, (%rax,%rdx), %xmm2, %xmm30
	vmovd	%xmm8, %edx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbx), %xmm1, %xmm4
	vpextrd	$3, %xmm10, 1088(%rsp)          # 4-byte Folded Spill
	movslq	%edx, %rdx
	vextracti128	$1, %ymm8, %xmm1
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm8, %ebx
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %esi
	vmovd	%edx, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	movslq	%edi, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm16, %xmm2
	vpextrd	$3, %xmm8, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r8d, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r9d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	movslq	672(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm13, 384(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm13, 1920(%rsp)          # 4-byte Folded Spill
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm13, 672(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm8, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm10
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinserti128	$1, %xmm0, %ymm10, %ymm6
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm10
	vpaddd	%zmm21, %zmm26, %zmm16
	vmovd	%xmm16, %ecx
	vextracti32x4	$1, %ymm16, %xmm0
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r11d
	vpextrd	$3, %xmm0, %r10d
	vmovd	%xmm0, %ebx
	vextracti32x4	$2, %zmm16, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm16, %edx
	vmovd	%xmm0, %esi
	vpextrd	$1, %xmm0, %edi
	vpextrd	$2, %xmm0, %r8d
	vpextrd	$3, %xmm0, %r9d
	vmovd	%ecx, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	movslq	%r8d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm12, 64(%rsp)            # 4-byte Folded Spill
	movslq	%r9d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm31
	vpextrd	$2, %xmm12, 256(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm12, 416(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm22, %zmm26, %zmm17
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm12
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 480(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, 640(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm17, %ecx
	vextracti32x4	$1, %ymm17, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm3
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	movslq	%edi, %rdx
	vinserti128	$1, %xmm12, %ymm1, %ymm1
	vmovdqu	%ymm1, 1728(%rsp)               # 32-byte Spill
	vextracti32x4	$2, %zmm17, %xmm1
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movslq	%r14d, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpinsrb	$15, (%rax,%rsi), %xmm13, %xmm11
	movslq	%edx, %rdx
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rcx), %xmm25, %xmm12
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm3, %ymm2, %ymm1
	vmovdqu	%ymm1, 1600(%rsp)               # 32-byte Spill
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm30
	vmovdqu64	2368(%rsp), %zmm27      # 64-byte Reload
	vmovdqu64	1152(%rsp), %zmm0       # 64-byte Reload
	vpaddd	%zmm27, %zmm0, %zmm1
	vinserti128	$1, %xmm11, %ymm12, %ymm2
	vmovdqu	%ymm2, 320(%rsp)                # 32-byte Spill
	vmovdqu64	2304(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm0, %zmm2
	vmovdqa64	%zmm0, %zmm12
	vextracti128	$1, %ymm2, %xmm0
	vmovd	%xmm0, 352(%rsp)                # 4-byte Folded Spill
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vextracti32x4	$3, %zmm15, %xmm13
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm18
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti128	$1, %ymm1, %xmm3
	vpextrd	$1, %xmm3, %r10d
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r8d
	vmovd	%xmm3, %edi
	vextracti32x4	$3, %zmm29, %xmm19
	vmovd	%xmm19, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm3
	vmovdqa	%xmm3, 240(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ebp
	vpextrd	$2, %xmm3, %ebx
	vpextrd	$3, %xmm3, %esi
	vmovd	%xmm3, %edx
	vextracti32x4	$3, %zmm8, %xmm11
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm3
	vmovdqa	%xmm3, 208(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm8
	vextracti32x4	$2, %zmm2, %xmm3
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %esi
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %ebx
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm2, %ebp
	vextracti32x4	$3, %zmm2, %xmm2
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm2, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -60(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -72(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1024(%rsp), %zmm9       # 64-byte Reload
	vpaddd	%zmm9, %zmm12, %zmm2
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, -28(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm1
	vmovd	%xmm1, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2240(%rsp), %zmm29      # 64-byte Reload
	vpaddd	%zmm29, %zmm12, %zmm1
	vmovd	%xmm2, %r10d
	vpextrd	$1, %xmm2, %r11d
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, -32(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -36(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r12d
	vmovd	%xmm2, %edx
	vmovd	%ecx, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm10
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %ebp
	vpextrd	$3, %xmm3, %r9d
	vmovd	%xmm3, %ebx
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r8d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r13d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r12d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm0
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edi
	vextracti32x4	$3, %zmm16, %xmm28
	vmovd	%xmm28, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm31, %xmm25
	movslq	%r9d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm16
	movslq	%r10d, %rbp
	movslq	%r11d, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm8, %xmm2
	movslq	%r14d, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm10, %xmm0
	movslq	%r15d, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm16, %xmm8
	movslq	-28(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm2, %xmm2
	movslq	-48(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	80(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	movslq	-100(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm8, %xmm8
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm8, %xmm8
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm8, %xmm10
	vinserti128	$1, %xmm2, %ymm0, %ymm8
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$1, %xmm24, 80(%rsp)            # 4-byte Folded Spill
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-36(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti32x4	$1, %xmm10, %ymm0, %ymm16
	vpaddd	%zmm29, %zmm26, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rdx
	vpextrd	$2, %xmm24, 96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm24, 352(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm23, %r8d
	vpextrd	$2, %xmm23, -60(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$3, %xmm23, -72(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	movzbl	(%rax,%rdx), %edx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edi
	vpextrd	$2, %xmm2, %ebp
	vpextrd	$3, %xmm2, %r14d
	vmovdqu64	2880(%rsp), %zmm4       # 64-byte Reload
	vpaddd	%zmm4, %zmm12, %zmm23
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %r9d
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %r11d
	vextracti32x4	$1, %ymm23, %xmm0
	vpextrd	$1, %xmm1, %ebx
	vmovd	%xmm0, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm0
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	movslq	%esi, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	movslq	%edi, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	movslq	%ebp, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm23, %xmm2
	vpextrd	$1, %xmm2, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -88(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, -92(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$3, %zmm17, %xmm10
	vmovd	%xmm10, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm30, %xmm30
	movslq	%r14d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	movslq	384(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	576(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm15
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm24
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm5, %xmm31
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm3
	vmovdqa	%xmm3, 576(%rsp)                # 16-byte Spill
	vextracti32x4	$3, %zmm1, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)                # 16-byte Spill
	vmovdqu64	2944(%rsp), %zmm14      # 64-byte Reload
	vpaddd	%zmm14, %zmm12, %zmm0
	vpextrd	$1, %xmm23, -100(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm23, 144(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm23, 48(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm23, %r8d
	vextracti32x4	$3, %zmm23, %xmm1
	vpextrd	$1, %xmm1, 1152(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 1856(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 160(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, -28(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm0, 20(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 32(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -12(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm1, 36(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -20(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -40(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1408(%rsp), %zmm21      # 64-byte Reload
	vpaddd	%zmm20, %zmm21, %zmm1
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vextracti128	$1, %ymm1, %xmm0
	vmovd	%xmm0, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm0
	vmovd	%xmm0, -48(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm0, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -32(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -36(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm27, %zmm21, %zmm0
	vmovd	%xmm1, 8(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 4(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 12(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 16(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, 28(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -24(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, (%rsp)                   # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 2432(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, %r13d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm1, %r12d
	vpextrd	$2, %xmm1, %r11d
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vmovd	%ecx, %xmm1
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm22
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %r14d
	vpextrd	$3, %xmm0, %r15d
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edx
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %edi
	vpextrd	$3, %xmm0, %r8d
	vmovd	%ecx, %xmm0
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	144(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	1856(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	2432(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r11d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r10d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm1
	movslq	8(%rsp), %rcx                   # 4-byte Folded Reload
	movslq	4(%rsp), %rdx                   # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm7
	vpinsrb	$1, (%rax,%rdx), %xmm7, %xmm7
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	vpmovzxbw	%ymm8, %zmm8            # zmm8 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpsllw	$2, %zmm8, %zmm8
	vpmovzxbw	%ymm16, %zmm16          # zmm16 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddw	%zmm16, %zmm16, %zmm16
	vpaddw	%zmm16, %zmm8, %zmm8
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$1, %xmm13, 48(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm22, %ymm0, %ymm0
	vpmovzxbw	%ymm6, %zmm6            # zmm6 = ymm6[0],zero,ymm6[1],zero,ymm6[2],zero,ymm6[3],zero,ymm6[4],zero,ymm6[5],zero,ymm6[6],zero,ymm6[7],zero,ymm6[8],zero,ymm6[9],zero,ymm6[10],zero,ymm6[11],zero,ymm6[12],zero,ymm6[13],zero,ymm6[14],zero,ymm6[15],zero,ymm6[16],zero,ymm6[17],zero,ymm6[18],zero,ymm6[19],zero,ymm6[20],zero,ymm6[21],zero,ymm6[22],zero,ymm6[23],zero,ymm6[24],zero,ymm6[25],zero,ymm6[26],zero,ymm6[27],zero,ymm6[28],zero,ymm6[29],zero,ymm6[30],zero,ymm6[31],zero
	vpaddw	%zmm6, %zmm6, %zmm16
	vmovdqu64	%zmm16, 1856(%rsp)      # 64-byte Spill
	vpmovzxbw	1728(%rsp), %zmm6       # 32-byte Folded Reload
                                        # zmm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm6, %zmm6, %zmm3
	vmovdqu64	%zmm3, 1152(%rsp)       # 64-byte Spill
	vpaddw	%zmm16, %zmm3, %zmm6
	vpaddw	%zmm8, %zmm6, %zmm6
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm8
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm6, %zmm7
	vinserti32x4	$1, %xmm1, %ymm8, %ymm16
	vpaddd	%zmm29, %zmm21, %zmm8
	vpaddd	%zmm9, %zmm21, %zmm1
	vpextrd	$2, %xmm13, 1728(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm13, 160(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm6
	vmovd	%xmm6, 64(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm6, 144(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm6, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm6
	vmovd	%xmm6, %r12d
	vpextrd	$1, %xmm6, %r14d
	vpextrd	$2, %xmm6, %r13d
	vpextrd	$3, %xmm6, %r15d
	vextracti32x4	$2, %zmm8, %xmm6
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %r9d
	vpextrd	$2, %xmm6, %r10d
	vpextrd	$3, %xmm6, %r11d
	vpaddd	%zmm9, %zmm26, %zmm6
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm8, %edx
	vpextrd	$2, %xmm8, %esi
	vpextrd	$3, %xmm8, %edi
	vmovd	%xmm8, %ecx
	vextracti32x4	$3, %zmm8, %xmm0
	vpextrd	$1, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm0, %r8d
	vpextrd	$3, %xmm0, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%ebx, %xmm0
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm0, %xmm13
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm22
	vextracti32x4	$2, %zmm1, %xmm0
	vpextrd	$1, %xmm0, %ecx
	vpextrd	$2, %xmm0, %edx
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpinsrb	$1, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	movslq	80(%rsp), %rbp                  # 4-byte Folded Reload
	vmovdqa	1280(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm3, %xmm23
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm0, %xmm0
	movslq	64(%rsp), %rbp                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm0, %xmm0
	movslq	144(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm0, %xmm0
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm0, %xmm0
	movslq	-104(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %ebp
	vmovd	%xmm1, %ebx
	vmovaps	176(%rsp), %xmm3                # 16-byte Reload
	vextractps	$1, %xmm3, %esi
	movslq	%esi, %rsi
	vmovdqa	1792(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	vmovdqa	%xmm1, 64(%rsp)                 # 16-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	movslq	48(%rsp), %rdx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm18, %xmm1
	vmovdqa	%xmm1, 48(%rsp)                 # 16-byte Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm0
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm22, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	movslq	-108(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	vextractps	$2, %xmm3, 80(%rsp)     # 4-byte Folded Spill
	vpmovzxbw	%ymm16, %zmm1           # zmm1 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpsllw	$2, %zmm1, %zmm1
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm1, %zmm16
	vextracti128	$1, %ymm6, %xmm0
	vextractps	$3, %xmm3, 176(%rsp)    # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm1
	vpextrd	$1, %xmm11, -76(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 144(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm11, 1792(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm27, %zmm26, %zmm11
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vpaddd	%zmm20, %zmm26, %zmm0
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, -108(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm11, %xmm1
	vmovd	%xmm1, %r14d
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm11, %xmm1
	vmovd	%xmm1, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vmovdqa64	%zmm4, %zmm18
	vpaddd	%zmm4, %zmm21, %zmm22
	vextracti32x4	$1, %ymm22, %xmm1
	vpextrd	$1, %xmm1, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 1280(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm19, %r13d
	movslq	%r13d, %rbp
	vmovdqa	240(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 240(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm13, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm6, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm17
	vmovdqa64	%zmm14, %zmm3
	vpaddd	%zmm14, %zmm21, %zmm13
	vextracti128	$1, %ymm13, %xmm1
	vpextrd	$1, %xmm1, %r13d
	vpextrd	$2, %xmm1, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -52(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm1, %r11d
	vmovd	%xmm11, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	208(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm4, %xmm4
	vmovdqa	%xmm4, 208(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r15d, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r12d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	movslq	-68(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	movslq	-56(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	movslq	-64(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm14
	vextracti32x4	$2, %zmm13, %xmm1
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r15d
	vpextrd	$3, %xmm1, %r12d
	vmovd	%xmm1, %r8d
	movslq	-100(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbx), %xmm14, %xmm14
	vpextrd	$1, %xmm13, %ebx
	vmovd	%xmm13, %edx
	vpextrd	$2, %xmm13, %esi
	vpextrd	$3, %xmm13, %ecx
	vextracti32x4	$3, %zmm13, %xmm1
	vpextrd	$1, %xmm1, %r14d
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 1408(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm1, %edi
	vpextrd	$1, %xmm28, %r10d
	movslq	%r10d, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm25, %xmm21
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rbp), %xmm14, %xmm1
	vextracti32x4	$3, %zmm11, %xmm14
	vmovd	%xmm14, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm12
	movslq	%edx, %rdx
	movslq	%ebx, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm1
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r13d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm13
	vextracti32x4	$2, %zmm22, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %edi
	vmovd	%xmm22, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm22, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm22, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm22, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vextracti32x4	$3, %zmm22, %xmm11
	vpextrd	$1, %xmm11, %ebp
	vpextrd	$2, %xmm11, %ebx
	vpextrd	$3, %xmm11, %esi
	vmovd	%xmm11, %edx
	vpextrd	$1, %xmm10, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm30, %xmm11
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm13, %xmm13
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm15, %xmm30
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm4
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm24, %xmm8
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm31, %xmm31
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm13
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	1408(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm13, %xmm13
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vinserti128	$1, %xmm13, %ymm4, %ymm4
	vpmovzxbw	1600(%rsp), %zmm13      # 32-byte Folded Reload
                                        # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm13, %zmm13, %zmm1
	vmovdqu64	%zmm1, 1408(%rsp)       # 64-byte Spill
	vpmovzxbw	320(%rsp), %zmm22       # 32-byte Folded Reload
                                        # zmm22 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm22, %zmm22, %zmm6
	vmovdqu64	%zmm6, 1280(%rsp)       # 64-byte Spill
	vpaddw	%zmm1, %zmm6, %zmm24
	vpaddw	%zmm16, %zmm24, %zmm16
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm1
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm19, -84(%rsp)           # 4-byte Folded Spill
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm16, %zmm4
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$2, %zmm0, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm6, %ecx
	vpcmpltuw	%zmm4, %zmm7, %k1
	vpsubw	%zmm4, %zmm7, %zmm24
	vpsubw	%zmm7, %zmm4, %zmm24 {%k1}
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm25
	vpaddd	%zmm3, %zmm26, %zmm15
	vmovdqa64	%zmm3, %zmm16
	vmovd	%xmm15, %ecx
	vpextrd	$3, %xmm19, -108(%rsp)          # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm15, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm15, %edx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %edi
	vpextrd	$2, %xmm2, %ebp
	vpextrd	$3, %xmm2, %ebx
	vmovd	%ecx, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm6
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm7
	vextracti32x4	$2, %zmm15, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$1, %xmm2, %ecx
	vinserti128	$1, %xmm1, %ymm6, %ymm1
	vmovdqu	%ymm1, 512(%rsp)                # 32-byte Spill
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm1
	vmovdqa	%xmm1, 1088(%rsp)               # 16-byte Spill
	vpextrd	$2, %xmm10, -88(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm10, -96(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm26, %zmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm28, -92(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm7, %edx
	vpextrd	$3, %xmm28, -112(%rsp)          # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm9, %r14d
	vpextrd	$2, %xmm9, 320(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r15d
	vmovdqu64	896(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm27, %zmm4, %zmm6
	vpextrd	$3, %xmm5, 1600(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vmovd	%xmm5, %edi
	vpextrd	$2, %xmm5, %r11d
	vpextrd	$3, %xmm5, %r8d
	vextracti128	$1, %ymm7, %xmm5
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	640(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbp), %xmm8, %xmm9
	vpextrd	$2, %xmm5, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 672(%rsp)                # 16-byte Spill
	vextracti32x4	$2, %zmm6, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%xmm6, %r13d
	movslq	%r13d, %r13
	vpextrd	$1, %xmm6, %r12d
	movslq	%r12d, %r12
	movzbl	(%rax,%r13), %ebx
	vmovd	%ebx, %xmm1
	vpinsrb	$1, (%rax,%r12), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	416(%rsp), %rbx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbx), %xmm31, %xmm10
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r10d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r11d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm6, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm28
	vpextrd	$1, %xmm0, %r8d
	vinserti128	$1, %xmm9, %ymm10, %ymm1
	vmovdqu	%ymm1, 640(%rsp)                # 32-byte Spill
	vpextrd	$2, %xmm0, -80(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1920(%rsp)           # 4-byte Folded Spill
	vmovdqa64	%zmm20, %zmm19
	vpaddd	%zmm20, %zmm4, %zmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm14, %esi
	vpextrd	$2, %xmm14, -104(%rsp)          # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vpextrd	$3, %xmm14, 256(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm6
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %edx
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%xmm1, %ecx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm6, %xmm1
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm1, %xmm1
	movslq	%r9d, %rdx
	vmovdqa	384(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm14
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm17, %xmm10
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm13
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm25, %xmm22
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm5, 416(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm8
	vpextrd	$1, %xmm5, %esi
	vpextrd	$2, %xmm5, %r9d
	vpextrd	$3, %xmm5, %r8d
	vpaddd	%zmm29, %zmm4, %zmm5
	vextracti128	$1, %ymm5, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r11d
	vmovd	%xmm1, %r12d
	vpextrd	$1, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm28, %xmm28
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm8, %xmm1
	movslq	-60(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	576(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm8
	movslq	%r9d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	movslq	96(%rsp), %rdx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm23, %xmm12
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	movslq	-72(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm8, %xmm8
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm17
	vextracti32x4	$2, %zmm5, %xmm1
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edi
	vmovd	%xmm1, %ebx
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm1
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm12
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm5, %xmm25
	vmovd	%xmm25, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, 576(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm8, %ymm12, %ymm30
	vpextrd	$3, %xmm2, 384(%rsp)            # 4-byte Folded Spill
	vmovdqu64	1024(%rsp), %zmm9       # 64-byte Reload
	vpaddd	%zmm9, %zmm4, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 352(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %r10d
	vpextrd	$2, %xmm0, %r8d
	vmovd	%xmm0, %ebp
	vpextrd	$3, %xmm0, %r9d
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm17, %xmm31
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm5
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %edx
	vpextrd	$3, %xmm5, %edi
	vmovd	%xmm5, %ebx
	vpextrd	$1, %xmm25, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm17
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebp, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r10d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r8d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r9d, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebx, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, -76(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm16, %zmm4, %zmm2
	vpextrd	$3, %xmm1, 96(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vmovd	%xmm1, %r14d
	vextracti32x4	$2, %zmm2, %xmm1
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm2, %ecx
	vmovd	%xmm1, %esi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%edx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm5
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm8
	vpextrd	$2, %xmm25, -68(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm4, %zmm0
	vpextrd	$3, %xmm25, 896(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm2
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %r11d
	vpextrd	$3, %xmm2, %r8d
	vmovd	%xmm2, %ebp
	vextracti32x4	$2, %zmm0, %xmm2
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm0, %ebx
	vmovd	%xmm2, %edi
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %r9d
	vpextrd	$3, %xmm2, %r10d
	vmovd	%ecx, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %r8d
	vpextrd	$2, %xmm0, -56(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	704(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm27, %zmm4, %zmm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %r13d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, %r14d
	vmovd	%xmm1, %ebx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm8, %xmm25
	movslq	%r9d, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm1
	movslq	80(%rsp), %rbp                  # 4-byte Folded Reload
	vmovdqa	64(%rsp), %xmm2                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm2, %xmm2
	movslq	%r10d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm1, %xmm1
	movslq	1728(%rsp), %rbp                # 4-byte Folded Reload
	vmovdqa	48(%rsp), %xmm6                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm8
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	240(%rsp), %xmm6                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm6, %xmm12
	movslq	%r8d, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm1, %xmm23
	movslq	144(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	208(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm1, %xmm20
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm0, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %edi
	vextracti32x4	$2, %zmm0, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	movslq	-92(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm21, %xmm21
	movslq	%ebx, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-88(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm11, %xmm11
	movslq	%r13d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r15d, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm14, %xmm27
	movslq	%r12d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	movslq	%r14d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm8, %xmm8
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm14
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm20, %xmm20
	vinserti128	$1, %xmm2, %ymm8, %ymm12
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	vpaddd	%zmm19, %zmm4, %zmm2
	vextracti128	$1, %ymm2, %xmm6
	vpextrd	$1, %xmm6, %r10d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vmovd	%xmm6, %r14d
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm6
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm0
	movslq	-112(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm21, %xmm1
	vpextrd	$1, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm8
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %ebx
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %esi
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm11, %xmm11
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %ecx
	vinserti32x4	$1, %xmm14, %ymm20, %ymm20
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm14
	vpextrd	$2, %xmm3, 48(%rsp)             # 4-byte Folded Spill
	vinserti32x4	$1, %xmm1, %ymm11, %ymm21
	vpextrd	$3, %xmm3, 176(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm29, %zmm4, %zmm0
	vpextrd	$2, %xmm2, 1728(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm6, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 160(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm9, %zmm4, %zmm11
	vextracti128	$1, %ymm11, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vmovd	%xmm1, %r14d
	vpextrd	$3, %xmm1, %r13d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %edi
	movslq	%edi, %rbx
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm0, %ecx
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vmovd	%ebx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm10, %xmm3
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm0
	vpextrd	$1, %xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm13, %xmm29
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm11, %ecx
	vextracti32x4	$2, %zmm11, %xmm2
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edi
	vmovd	%xmm2, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm2
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm22, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm6
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm28, %xmm28
	movslq	%r15d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm31, %xmm31
	movslq	%r12d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm17
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm5
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm5, %ymm3, %ymm10
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm11, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm23, %xmm23
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm5
	vextracti32x4	$2, %zmm7, %xmm2
	vpextrd	$1, %xmm2, 480(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 1600(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %r10d
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r11d
	vpaddd	%zmm16, %zmm4, %zmm11
	vmovd	%xmm11, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm9, %r12d
	vpextrd	$3, %xmm9, %r14d
	vextracti128	$1, %ymm11, %xmm2
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %ebx
	vmovd	%xmm2, %edx
	vpextrd	$3, %xmm2, %esi
	vmovd	%edi, %xmm2
	vpextrd	$1, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$2, %xmm11, %edi
	vextracti32x4	$2, %zmm11, %xmm3
	vpextrd	$1, %xmm3, %r8d
	vpextrd	$2, %xmm3, %r13d
	vpextrd	$3, %xmm3, %r9d
	vmovd	%xmm3, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	movslq	64(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm8, %xmm3
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm2, %xmm2
	movslq	1728(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm14, %xmm6
	movslq	%ebp, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r12d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm1
	movslq	%ebx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r15d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm5, %xmm5
	movslq	%esi, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm2, %xmm2
	movslq	256(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm29, %xmm8
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm9
	movslq	%r8d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm27
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm14
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm11, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm28
	vpaddd	%zmm18, %zmm4, %zmm11
	vextracti128	$1, %ymm11, %xmm2
	vpextrd	$1, %xmm2, %r15d
	vpextrd	$2, %xmm2, %r9d
	vpextrd	$3, %xmm2, %r8d
	vpextrd	$1, %xmm0, %edi
	vmovd	%xmm2, %ebp
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm28, %xmm2
	vpextrd	$2, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm28
	movslq	896(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm13, %xmm13
	vmovd	%xmm11, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm11, %edi
	vextracti32x4	$2, %zmm11, %xmm2
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm2
	vpextrd	$3, %xmm11, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	movslq	96(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm17, %xmm17
	movslq	%ebp, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm2, %xmm2
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm25, %xmm4
	movslq	%r15d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm2, %xmm2
	movslq	-60(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm23, %xmm23
	movslq	%r9d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm2, %xmm2
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm25
	vextracti32x4	$3, %zmm7, %xmm7
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %r13d
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %r15d
	movslq	%ebx, %rcx
	vextracti32x4	$3, %zmm11, %xmm3
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$1, %xmm3, %ecx
	movslq	1792(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %esi
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %ebp
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vmovd	%xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm7, %r9d
	movslq	%r14d, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm7, %r8d
	vpextrd	$3, %xmm0, %ecx
	movslq	%r11d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm5, %xmm3
	vextracti32x4	$3, %zmm15, %xmm0
	vpextrd	$1, %xmm0, %ebx
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm5
	vmovd	%xmm0, %r11d
	movslq	%esi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %esi
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm7
	vpextrd	$3, %xmm0, %edx
	vinserti128	$1, %xmm8, %ymm9, %ymm15
	vinserti32x4	$1, %xmm27, %ymm14, %ymm0
	vinserti32x4	$1, %xmm13, %ymm17, %ymm8
	vinserti32x4	$1, %xmm4, %ymm23, %ymm4
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	512(%rsp), %zmm11       # 32-byte Folded Reload
                                        # zmm11 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm0, %zmm0, %zmm9
	vpmovzxbw	%ymm8, %zmm0            # zmm0 = ymm8[0],zero,ymm8[1],zero,ymm8[2],zero,ymm8[3],zero,ymm8[4],zero,ymm8[5],zero,ymm8[6],zero,ymm8[7],zero,ymm8[8],zero,ymm8[9],zero,ymm8[10],zero,ymm8[11],zero,ymm8[12],zero,ymm8[13],zero,ymm8[14],zero,ymm8[15],zero,ymm8[16],zero,ymm8[17],zero,ymm8[18],zero,ymm8[19],zero,ymm8[20],zero,ymm8[21],zero,ymm8[22],zero,ymm8[23],zero,ymm8[24],zero,ymm8[25],zero,ymm8[26],zero,ymm8[27],zero,ymm8[28],zero,ymm8[29],zero,ymm8[30],zero,ymm8[31],zero
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm0, %zmm4
	vpmovzxbw	640(%rsp), %zmm0        # 32-byte Folded Reload
                                        # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm11, %zmm0, %zmm8
	vpaddw	%zmm9, %zmm8, %zmm8
	vpaddw	%zmm4, %zmm8, %zmm8
	vinserti32x4	$1, %xmm25, %ymm6, %ymm4
	vinserti128	$1, %xmm1, %ymm3, %ymm1
	vinserti128	$1, %xmm5, %ymm7, %ymm3
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm5
	vpmovzxbw	%ymm30, %zmm6           # zmm6 = ymm30[0],zero,ymm30[1],zero,ymm30[2],zero,ymm30[3],zero,ymm30[4],zero,ymm30[5],zero,ymm30[6],zero,ymm30[7],zero,ymm30[8],zero,ymm30[9],zero,ymm30[10],zero,ymm30[11],zero,ymm30[12],zero,ymm30[13],zero,ymm30[14],zero,ymm30[15],zero,ymm30[16],zero,ymm30[17],zero,ymm30[18],zero,ymm30[19],zero,ymm30[20],zero,ymm30[21],zero,ymm30[22],zero,ymm30[23],zero,ymm30[24],zero,ymm30[25],zero,ymm30[26],zero,ymm30[27],zero,ymm30[28],zero,ymm30[29],zero,ymm30[30],zero,ymm30[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm3, %zmm3            # zmm3 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpmovzxbw	%ymm12, %zmm4           # zmm4 = ymm12[0],zero,ymm12[1],zero,ymm12[2],zero,ymm12[3],zero,ymm12[4],zero,ymm12[5],zero,ymm12[6],zero,ymm12[7],zero,ymm12[8],zero,ymm12[9],zero,ymm12[10],zero,ymm12[11],zero,ymm12[12],zero,ymm12[13],zero,ymm12[14],zero,ymm12[15],zero,ymm12[16],zero,ymm12[17],zero,ymm12[18],zero,ymm12[19],zero,ymm12[20],zero,ymm12[21],zero,ymm12[22],zero,ymm12[23],zero,ymm12[24],zero,ymm12[25],zero,ymm12[26],zero,ymm12[27],zero,ymm12[28],zero,ymm12[29],zero,ymm12[30],zero,ymm12[31],zero
	vpaddw	%zmm6, %zmm4, %zmm3
	vpaddw	%zmm5, %zmm3, %zmm3
	movslq	416(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	576(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	384(%rsp), %r14                 # 4-byte Folded Reload
	vmovdqa	1088(%rsp), %xmm2               # 16-byte Reload
	vpinsrb	$10, (%rax,%rdi), %xmm2, %xmm7
	movslq	%r11d, %rdi
	vmovdqa	672(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$7, (%rax,%rbp), %xmm2, %xmm2
	vpaddw	%zmm1, %zmm3, %zmm3
	vpcmpltuw	%zmm3, %zmm8, %k1
	vpsubw	%zmm3, %zmm8, %zmm12
	vpsubw	%zmm8, %zmm3, %zmm12 {%k1}
	vpaddd	2624(%rsp), %zmm26, %zmm5       # 64-byte Folded Reload
	vextracti128	$1, %ymm5, %xmm3
	vmovd	%xmm3, 704(%rsp)                # 4-byte Folded Spill
	vpinsrb	$11, (%rax,%r14), %xmm7, %xmm7
	vpextrd	$1, %xmm3, 896(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 1088(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$3, %xmm3, 416(%rsp)            # 4-byte Folded Spill
	movslq	%r10d, %rdi
	movslq	480(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	1600(%rsp), %r10                # 4-byte Folded Reload
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%r8d, %r8
	movslq	%r13d, %r13
	movslq	%r15d, %r15
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rbx), %xmm7, %xmm3
	vpinsrb	$10, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm3, %ymm2, %ymm9
	vextracti32x4	$2, %zmm5, %xmm2
	vpextrd	$1, %xmm2, %r12d
	vpextrd	$2, %xmm2, %r14d
	vpaddw	%zmm11, %zmm11, %zmm3
	vmovd	%xmm2, %r15d
	vpextrd	$3, %xmm2, 512(%rsp)            # 4-byte Folded Spill
	vpaddd	2688(%rsp), %zmm26, %zmm2       # 64-byte Folded Reload
	vmovd	%xmm2, %edx
	vpaddw	%zmm6, %zmm6, %zmm6
	movslq	%edx, %rdx
	vpextrd	$1, %xmm2, %ebx
	vpextrd	$2, %xmm2, %esi
	vpaddw	%zmm6, %zmm3, %zmm3
	movslq	%ebx, %rbx
	movslq	%esi, %r13
	vpextrd	$3, %xmm2, %edi
	vextracti128	$1, %ymm2, %xmm6
	movslq	%edi, %rdi
	vmovd	%xmm6, %ebp
	vpextrd	$1, %xmm6, %ecx
	vpextrd	$2, %xmm6, %esi
	vmovdqu64	1408(%rsp), %zmm1       # 64-byte Reload
	vpaddw	1856(%rsp), %zmm1, %zmm7        # 64-byte Folded Reload
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm1
	vpextrd	$3, %xmm6, %edx
	vextracti32x4	$2, %zmm2, %xmm6
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$1, %xmm6, %ebx
	vpinsrb	$2, (%rax,%r13), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %r13d
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm6, %edi
	vpextrd	$3, %xmm6, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %esi
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %edx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm5, %xmm5
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$1, %xmm5, %r11d
	movslq	%r13d, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %r13d
	movslq	%ebp, %rbp
	vpaddw	%zmm0, %zmm0, %zmm0
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm2, %ebp
	movslq	%ebp, %rbp
	vpaddw	%zmm4, %zmm4, %zmm4
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm5, %ebp
	movzbl	(%rax,%rcx), %ecx
	vpaddw	%zmm4, %zmm0, %zmm6
	vmovd	%ecx, %xmm0
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm5, %r10d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %edx
	vmovdqu64	1280(%rsp), %zmm2       # 64-byte Reload
	vpaddw	1152(%rsp), %zmm2, %zmm2        # 64-byte Folded Reload
	movslq	%edi, %rdi
	movslq	704(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1088(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	416(%rsp), %r9                  # 4-byte Folded Reload
	movslq	%r12d, %r12
	movslq	%r14d, %r14
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$4, (%rax,%rbx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	movslq	512(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$6, (%rax,%r8), %xmm0, %xmm0
	vpmovzxbw	%ymm20, %zmm8           # zmm8 = ymm20[0],zero,ymm20[1],zero,ymm20[2],zero,ymm20[3],zero,ymm20[4],zero,ymm20[5],zero,ymm20[6],zero,ymm20[7],zero,ymm20[8],zero,ymm20[9],zero,ymm20[10],zero,ymm20[11],zero,ymm20[12],zero,ymm20[13],zero,ymm20[14],zero,ymm20[15],zero,ymm20[16],zero,ymm20[17],zero,ymm20[18],zero,ymm20[19],zero,ymm20[20],zero,ymm20[21],zero,ymm20[22],zero,ymm20[23],zero,ymm20[24],zero,ymm20[25],zero,ymm20[26],zero,ymm20[27],zero,ymm20[28],zero,ymm20[29],zero,ymm20[30],zero,ymm20[31],zero
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r15), %xmm0, %xmm0
	vpmovzxbw	%ymm21, %zmm14          # zmm14 = ymm21[0],zero,ymm21[1],zero,ymm21[2],zero,ymm21[3],zero,ymm21[4],zero,ymm21[5],zero,ymm21[6],zero,ymm21[7],zero,ymm21[8],zero,ymm21[9],zero,ymm21[10],zero,ymm21[11],zero,ymm21[12],zero,ymm21[13],zero,ymm21[14],zero,ymm21[15],zero,ymm21[16],zero,ymm21[17],zero,ymm21[18],zero,ymm21[19],zero,ymm21[20],zero,ymm21[21],zero,ymm21[22],zero,ymm21[23],zero,ymm21[24],zero,ymm21[25],zero,ymm21[26],zero,ymm21[27],zero,ymm21[28],zero,ymm21[29],zero,ymm21[30],zero,ymm21[31],zero
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%r14), %xmm0, %xmm5
	vpmovzxbw	%ymm10, %zmm16          # zmm16 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	movslq	%r13d, %rdi
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm5
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm5
	vpaddw	%zmm3, %zmm7, %zmm10
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rsi), %xmm5, %xmm3
	vpinsrb	$14, (%rax,%rdi), %xmm3, %xmm3
	vpsllw	$2, %zmm8, %zmm11
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm3, %ymm13
	vpmovzxbw	%ymm15, %zmm3           # zmm3 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddd	2560(%rsp), %zmm26, %zmm7       # 64-byte Folded Reload
	vpaddw	%zmm6, %zmm2, %zmm1
	vpaddd	2496(%rsp), %zmm26, %zmm2       # 64-byte Folded Reload
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm2, %ecx
	vpsllw	$2, %zmm14, %zmm6
	movslq	%ecx, %rbp
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %r8d
	vpaddw	%zmm11, %zmm10, %zmm10
	movslq	%ecx, %rsi
	vextracti128	$1, %ymm2, %xmm5
	vmovd	%xmm5, %edi
	vpextrd	$1, %xmm5, %r13d
	vpaddw	%zmm6, %zmm1, %zmm1
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r12d
	vextracti32x4	$2, %zmm2, %xmm5
	vpaddw	%zmm16, %zmm16, %zmm11
	vmovd	%xmm5, %r14d
	vpextrd	$1, %xmm5, %r11d
	vpextrd	$2, %xmm5, %r10d
	vpcmpltuw	%zmm1, %zmm10, %k1
	vpextrd	$3, %xmm5, %r9d
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, 704(%rsp)            # 4-byte Folded Spill
	vpsubw	%zmm1, %zmm10, %zmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpsubw	%zmm10, %zmm1, %zmm6 {%k1}
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpaddw	%zmm3, %zmm8, %zmm5
	vextracti128	$1, %ymm7, %xmm4
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm4, %edx
	vpaddw	%zmm5, %zmm11, %zmm5
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm0
	vpextrd	$2, %xmm4, %ebx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm4, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$2, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm4, %esi
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r8d, %rbp
	vpextrd	$2, %xmm2, %r8d
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpextrd	$3, %xmm2, %edi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm2, %edx
	vextracti32x4	$2, %zmm7, %xmm2
	vpinsrb	$3, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %ebp
	movslq	%r13d, %r13
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %esi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%r13), %xmm0, %xmm0
	vpextrd	$3, %xmm2, %r13d
	vpinsrb	$6, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm2, %ebx
	movslq	%r15d, %r15
	movslq	%r12d, %r12
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm7, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%r15), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %r15d
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %ecx
	vpinsrb	$7, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm2, %ebx
	vpextrd	$3, %xmm2, %r12d
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm0, %xmm0
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r13d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r9d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm0, %xmm0
	movslq	%r15d, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpmovzxbw	%ymm13, %zmm2           # zmm2 = ymm13[0],zero,ymm13[1],zero,ymm13[2],zero,ymm13[3],zero,ymm13[4],zero,ymm13[5],zero,ymm13[6],zero,ymm13[7],zero,ymm13[8],zero,ymm13[9],zero,ymm13[10],zero,ymm13[11],zero,ymm13[12],zero,ymm13[13],zero,ymm13[14],zero,ymm13[15],zero,ymm13[16],zero,ymm13[17],zero,ymm13[18],zero,ymm13[19],zero,ymm13[20],zero,ymm13[21],zero,ymm13[22],zero,ymm13[23],zero,ymm13[24],zero,ymm13[25],zero,ymm13[26],zero,ymm13[27],zero,ymm13[28],zero,ymm13[29],zero,ymm13[30],zero,ymm13[31],zero
	vpmovzxbw	%ymm9, %zmm4            # zmm4 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpaddw	%zmm2, %zmm4, %zmm2
	vpaddw	%zmm2, %zmm5, %zmm2
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	vpaddw	%zmm4, %zmm4, %zmm1
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm1, %zmm0
	vpaddw	%zmm14, %zmm16, %zmm1
	vpaddw	%zmm3, %zmm1, %zmm1
	vpaddw	%zmm0, %zmm1, %zmm0
	vpcmpltuw	%zmm0, %zmm2, %k1
	vpsubw	%zmm0, %zmm2, %zmm1
	vpsubw	%zmm2, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm24, %zmm12, %zmm0
	vpaddw	%zmm6, %zmm1, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	2048(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, 1216(%rsp)                # 4-byte Folded Spill
	movl	960(%rsp), %r11d                # 4-byte Reload
	addl	%ecx, %r11d
	movl	1664(%rsp), %r14d               # 4-byte Reload
	addl	%ecx, %r14d
	movl	1472(%rsp), %r13d               # 4-byte Reload
	addl	%ecx, %r13d
	movl	1536(%rsp), %edx                # 4-byte Reload
	addl	%ecx, %edx
	movl	%edx, %ecx
	decq	288(%rsp)                       # 8-byte Folded Spill
	jne	.LBB214_47
# %bb.48:                               # %"end for output.s0.y.yi107"
                                        #   in Loop: Header=BB214_46 Depth=2
	movq	832(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	464(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	816(%rsp), %rdx                 # 8-byte Folded Reload
	jne	.LBB214_46
.LBB214_49:                             # %"end for output.s0.x.x98"
                                        #   in Loop: Header=BB214_44 Depth=1
	cmpl	$0, 632(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_54
# %bb.50:                               # %"for output.s0.x.x.rebased142.preheader"
                                        #   in Loop: Header=BB214_44 Depth=1
	movslq	456(%rsp), %r15                 # 4-byte Folded Reload
	leaq	1(%r15), %rdx
	leaq	-1(%r15), %rsi
	leaq	2(%r15), %rdi
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movq	%rcx, %rbp
	imulq	%r15, %rbp
	addq	$-2, %r15
	imulq	%rcx, %rdx
	imulq	%rcx, %rsi
	imulq	%rcx, %rdi
	imulq	%rcx, %r15
	addq	%rax, %rbp
	movq	%rbp, 1024(%rsp)                # 8-byte Spill
	addq	%rax, %rdx
	movq	%rdx, 2048(%rsp)                # 8-byte Spill
	addq	%rax, %rsi
	movq	%rsi, 1984(%rsp)                # 8-byte Spill
	addq	%rax, %rdi
	movq	%rdi, 2176(%rsp)                # 8-byte Spill
	addq	%rax, %r15
	movq	568(%rsp), %rcx                 # 8-byte Reload
	movq	280(%rsp), %rdx                 # 8-byte Reload
	addq	%rdx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movl	304(%rsp), %r13d                # 4-byte Reload
	movq	784(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %r12d
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB214_51:                             # %"for output.s0.x.x.rebased142"
                                        #   Parent Loop BB214_44 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_52 Depth 3
	movslq	%r12d, %rdi
	addq	1216(%rsp), %rdi                # 8-byte Folded Reload
	movslq	%r13d, %rbp
	movl	$16, %ecx
	movq	%r15, %rdx
	movq	2176(%rsp), %rbx                # 8-byte Reload
	movq	1984(%rsp), %r10                # 8-byte Reload
	movq	2048(%rsp), %r9                 # 8-byte Reload
	movq	1024(%rsp), %r14                # 8-byte Reload
	movq	40(%rsp), %r11                  # 8-byte Reload
	movq	200(%rsp), %r8                  # 8-byte Reload
	.p2align	4, 0x90
.LBB214_52:                             # %"for output.s0.y.yi148"
                                        #   Parent Loop BB214_44 Depth=1
                                        #     Parent Loop BB214_51 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vpmovzxbw	-2(%rdx,%rbp), %zmm7    # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%rdx,%rbp), %zmm8     # zmm8 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%rbx,%rbp), %zmm9    # zmm9 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%rbx,%rbp), %zmm10    # zmm10 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r10,%rbp), %zmm11   # zmm11 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r10,%rbp), %zmm5     # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r9,%rbp), %zmm12    # zmm12 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r9,%rbp), %zmm6      # zmm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-2(%r14,%rbp), %zmm3    # zmm3 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	2(%r14,%rbp), %zmm0     # zmm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-1(%r14,%rbp), %zmm1    # zmm1 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	(%r14,%rbp), %zmm2      # zmm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	1(%r14,%rbp), %zmm4     # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm7, %zmm7, %zmm7
	vpaddw	%zmm8, %zmm8, %zmm8
	vpaddw	%zmm9, %zmm9, %zmm9
	vpaddw	%zmm10, %zmm10, %zmm10
	vpmovzxbw	(%rdx,%rbp), %zmm13     # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpsllw	$2, %zmm13, %zmm13
	vpmovzxbw	-1(%rdx,%rbp), %zmm14   # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	1(%rdx,%rbp), %zmm14    # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpaddw	%zmm7, %zmm8, %zmm15
	vpaddw	%zmm13, %zmm15, %zmm13
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	(%rbx,%rbp), %zmm14     # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpsllw	$2, %zmm14, %zmm14
	vpmovzxbw	-1(%rbx,%rbp), %zmm15   # zmm15 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm15, %zmm15, %zmm15
	vpmovzxbw	1(%rbx,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm15, %zmm14, %zmm14
	vpaddw	%zmm16, %zmm16, %zmm15
	vpaddw	%zmm9, %zmm10, %zmm16
	vpaddw	%zmm14, %zmm16, %zmm14
	vpaddw	%zmm15, %zmm14, %zmm14
	vpcmpltuw	%zmm14, %zmm13, %k1
	vpsubw	%zmm14, %zmm13, %zmm15
	vpsubw	%zmm13, %zmm14, %zmm15 {%k1}
	vpmovzxbw	(%r10,%rbp), %zmm13     # zmm13 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	-1(%r10,%rbp), %zmm14   # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm13, %zmm13, %zmm13
	vpmovzxbw	1(%r10,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm16, %zmm14, %zmm14
	vpaddw	%zmm11, %zmm5, %zmm16
	vpaddw	%zmm13, %zmm16, %zmm13
	vpaddw	%zmm14, %zmm13, %zmm13
	vpmovzxbw	(%r9,%rbp), %zmm14      # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm14, %zmm14, %zmm14
	vpmovzxbw	-1(%r9,%rbp), %zmm16    # zmm16 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	1(%r9,%rbp), %zmm17     # zmm17 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm17, %zmm16, %zmm16
	vpaddw	%zmm12, %zmm6, %zmm17
	vpaddw	%zmm14, %zmm17, %zmm14
	vpaddw	%zmm16, %zmm14, %zmm14
	vpcmpltuw	%zmm14, %zmm13, %k1
	vpsubw	%zmm14, %zmm13, %zmm16
	vpsubw	%zmm13, %zmm14, %zmm16 {%k1}
	vpaddw	%zmm15, %zmm16, %zmm13
	vpmovuswb	%zmm13, %ymm13
	vpsllw	$2, %zmm3, %zmm14
	vpaddw	%zmm11, %zmm11, %zmm11
	vpaddw	%zmm12, %zmm12, %zmm12
	vpaddw	%zmm12, %zmm11, %zmm11
	vpaddw	%zmm7, %zmm9, %zmm7
	vpaddw	%zmm11, %zmm7, %zmm7
	vpaddw	%zmm14, %zmm7, %zmm7
	vpsllw	$2, %zmm0, %zmm9
	vpaddw	%zmm5, %zmm5, %zmm5
	vpaddw	%zmm6, %zmm6, %zmm6
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm8, %zmm10, %zmm6
	vpaddw	%zmm5, %zmm6, %zmm5
	vpaddw	%zmm9, %zmm5, %zmm5
	vpcmpltuw	%zmm5, %zmm7, %k1
	vpsubw	%zmm5, %zmm7, %zmm6
	vpsubw	%zmm7, %zmm5, %zmm6 {%k1}
	vpaddw	%zmm1, %zmm1, %zmm5
	vpmovzxbw	-3(%r14,%rbp), %zmm7    # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm2, %zmm3, %zmm3
	vpaddw	%zmm3, %zmm5, %zmm3
	vpaddw	%zmm7, %zmm4, %zmm5
	vpaddw	%zmm5, %zmm3, %zmm3
	vpaddw	%zmm4, %zmm4, %zmm4
	vpmovzxbw	3(%r14,%rbp), %zmm5     # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm5, %zmm4, %zmm4
	vpaddw	%zmm0, %zmm1, %zmm0
	vpaddw	%zmm2, %zmm0, %zmm0
	vpaddw	%zmm4, %zmm0, %zmm0
	vpcmpltuw	%zmm0, %zmm3, %k1
	vpsubw	%zmm0, %zmm3, %zmm1
	vpsubw	%zmm3, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm6, %zmm1, %zmm0
	vpmovuswb	%zmm0, %ymm0
	vpaddb	%ymm0, %ymm13, %ymm0
	vmovdqu	%ymm0, (%rdi)
	addq	%r8, %rdi
	addq	%r11, %r14
	addq	%r11, %r9
	addq	%r11, %r10
	addq	%r11, %rbx
	addq	%r11, %rdx
	decq	%rcx
	jne	.LBB214_52
# %bb.53:                               # %"end for output.s0.y.yi149"
                                        #   in Loop: Header=BB214_51 Depth=2
	incq	%rsi
	addl	$32, %r12d
	addl	$32, %r13d
	cmpq	632(%rsp), %rsi                 # 8-byte Folded Reload
	jne	.LBB214_51
.LBB214_54:                             # %"end for output.s0.x.x.rebased143"
                                        #   in Loop: Header=BB214_44 Depth=1
	cmpl	$0, 448(%rsp)                   # 4-byte Folded Reload
	movq	280(%rsp), %rdi                 # 8-byte Reload
	jle	.LBB214_59
# %bb.55:                               # %"for output.s0.x.x.rebased176.preheader"
                                        #   in Loop: Header=BB214_44 Depth=1
	addq	792(%rsp), %rdi                 # 8-byte Folded Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_56:                             # %"for output.s0.x.x.rebased176"
                                        #   Parent Loop BB214_44 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_57 Depth 3
	movq	624(%rsp), %rcx                 # 8-byte Reload
	movq	%rdx, 464(%rsp)                 # 8-byte Spill
	addl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	3136(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2432(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2368(%rsp)       # 64-byte Spill
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2048(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1984(%rsp)       # 64-byte Spill
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2304(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2240(%rsp)       # 64-byte Spill
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2944(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2880(%rsp)       # 64-byte Spill
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2816(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2752(%rsp)       # 64-byte Spill
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2688(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2624(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 2560(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 2496(%rsp)       # 64-byte Spill
	movl	$16, %ecx
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	116(%rsp), %ecx                 # 4-byte Reload
	movl	120(%rsp), %r13d                # 4-byte Reload
	movl	124(%rsp), %r14d                # 4-byte Reload
	movl	128(%rsp), %r10d                # 4-byte Reload
	movl	132(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 2176(%rsp)                # 4-byte Spill
	movq	%rdi, 280(%rsp)                 # 8-byte Spill
	movq	%rdi, 1024(%rsp)                # 8-byte Spill
	.p2align	4, 0x90
.LBB214_57:                             # %"for output.s0.y.yi184"
                                        #   Parent Loop BB214_44 Depth=1
                                        #     Parent Loop BB214_56 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 1216(%rsp)                # 4-byte Spill
	vpbroadcastd	%ecx, %zmm0
	vmovdqu64	2048(%rsp), %zmm2       # 64-byte Reload
	vpaddd	%zmm2, %zmm0, %zmm1
	vmovdqa64	%zmm2, %zmm14
	vmovdqu64	1984(%rsp), %zmm25      # 64-byte Reload
	vpaddd	%zmm25, %zmm0, %zmm2
	vmovdqa64	%zmm0, %zmm6
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r8
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rbp
	movslq	%esi, %rdx
	vpextrd	$3, %xmm2, %esi
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$1, %xmm3, %r15d
	movslq	%esi, %rsi
	vpextrd	$2, %xmm3, %r12d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$3, %xmm3, %r11d
	vpextrd	$1, %xmm0, 1088(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 960(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm3, %r9d
	vextracti32x4	$3, %zmm2, %xmm22
	vpextrd	$1, %xmm22, 416(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm22, 1152(%rsp)          # 4-byte Folded Spill
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm2
	vpextrd	$2, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %ebx
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm22, 1280(%rsp)          # 4-byte Folded Spill
	movslq	%ebx, %rcx
	vextracti128	$1, %ymm1, %xmm3
	movzbl	(%rax,%r8), %edi
	vmovd	%edi, %xmm4
	vpextrd	$1, %xmm3, %edi
	vpinsrb	$1, (%rax,%rbp), %xmm4, %xmm5
	vpextrd	$2, %xmm3, %ebx
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm4
	vmovd	%xmm3, %ebp
	vextracti32x4	$2, %zmm1, %xmm2
	vpinsrb	$2, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm3, %ecx
	vpinsrb	$3, (%rax,%rsi), %xmm5, %xmm5
	vmovdqu64	2816(%rsp), %zmm21      # 64-byte Reload
	vmovdqa64	%zmm6, %zmm3
	vmovdqu64	%zmm6, 1408(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm6, %zmm20
	vmovd	%xmm20, %edx
	movslq	%edx, %rsi
	movslq	%r9d, %rdx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$1, %xmm20, %esi
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm7
	vpextrd	$1, %xmm2, %r8d
	vpinsrb	$4, (%rax,%rbp), %xmm4, %xmm4
	movslq	%edi, %rdi
	vpextrd	$2, %xmm2, %esi
	vextracti32x4	$3, %zmm1, %xmm9
	vpinsrb	$4, (%rax,%rdx), %xmm5, %xmm5
	vmovdqu64	2752(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm3, %zmm6
	vpextrd	$2, %xmm20, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm20, %edi
	vpinsrb	$2, (%rax,%rdx), %xmm7, %xmm7
	vpextrd	$3, %xmm2, %r9d
	movslq	%edi, %rdx
	vextracti32x4	$1, %ymm20, %xmm1
	vpinsrb	$3, (%rax,%rdx), %xmm7, %xmm7
	vmovd	%xmm1, %edx
	movslq	%r15d, %rdi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm7, %xmm7
	vmovd	%xmm6, %edx
	vpinsrb	$5, (%rax,%rdi), %xmm5, %xmm5
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %edi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm9, %r15d
	movslq	%edi, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm6, %edx
	movslq	%r12d, %rdi
	movslq	%ebx, %rbx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rbx), %xmm4, %xmm4
	vmovd	%xmm0, %edx
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rdi), %xmm5, %xmm11
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %edi
	vpinsrb	$3, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$1, %xmm0, %ebx
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebp
	movslq	%ecx, %rdi
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%rbx), %xmm7, %xmm12
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm0, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm2
	movl	%r13d, 1536(%rsp)               # 4-byte Spill
	vpbroadcastd	%r13d, %zmm3
	vpaddd	%zmm14, %zmm3, %zmm10
	vmovdqa64	%zmm14, %zmm26
	vmovdqa64	%zmm3, %zmm17
	vmovd	%xmm10, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rdi), %xmm4, %xmm3
	vpextrd	$1, %xmm10, %edi
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpextrd	$2, %xmm9, %r12d
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$2, %xmm10, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm10, %edi
	movslq	%edi, %rdi
	movslq	%r11d, %rbx
	vpinsrb	$3, (%rax,%rdi), %xmm4, %xmm7
	vpextrd	$3, %xmm9, 512(%rsp)            # 4-byte Folded Spill
	vpinsrb	$7, (%rax,%rbx), %xmm11, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm12, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$7, (%rax,%rdi), %xmm2, %xmm2
	vmovd	%xmm0, %ebx
	vextracti32x4	$2, %zmm20, %xmm23
	vpaddd	%zmm25, %zmm17, %zmm29
	vextracti128	$1, %ymm10, %xmm4
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm11
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm3
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm4
	vmovd	%xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm3
	vpextrd	$1, %xmm29, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$1, %xmm0, %edi
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm29, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm23, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm7
	movslq	%ebx, %rcx
	vpextrd	$3, %xmm29, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm12
	vextracti32x4	$1, %ymm29, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm5, %xmm2
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ecx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %edx
	vextracti32x4	$2, %zmm29, %xmm13
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm1
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm10, %xmm14
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm4, %xmm16
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm15
	vmovdqu64	%zmm17, 896(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm17, %zmm30
	vmovd	%xmm30, %ecx
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm30, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	1088(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm30, %ecx
	movslq	%ecx, %rcx
	movslq	%r8d, %rbx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ecx
	vpinsrb	$9, (%rax,%rbx), %xmm11, %xmm2
	vextracti32x4	$3, %zmm6, %xmm11
	vpextrd	$1, %xmm23, %ebx
	vextracti32x4	$1, %ymm30, %xmm0
	vpinsrb	$9, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %ebp
	vpaddd	%zmm8, %zmm17, %zmm6
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm30, %xmm27
	vpinsrb	$7, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm27, %ebp
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbp), %xmm1, %xmm1
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbx), %xmm7, %xmm4
	vpextrd	$1, %xmm6, %ebx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm7
	vpextrd	$1, %xmm11, %r11d
	movslq	%ebx, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %ebp
	movslq	%ebp, %rbp
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rbp), %xmm7, %xmm7
	vpextrd	$2, %xmm11, %ebp
	vpinsrb	$9, (%rax,%rdi), %xmm12, %xmm12
	vpextrd	$1, %xmm14, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm16, %xmm17
	vpextrd	$1, %xmm13, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm15, %xmm18
	vextracti128	$1, %ymm6, %xmm5
	vpextrd	$1, %xmm27, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm1, %xmm19
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm7, %xmm1
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %edi
	movslq	%edi, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm7
	vpextrd	$3, %xmm5, %edi
	movslq	704(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	%edi, %rdi
	vextracti32x4	$2, %zmm6, %xmm1
	vpinsrb	$7, (%rax,%rdi), %xmm7, %xmm5
	vmovd	%xmm1, %edi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm5, %xmm24
	vpextrd	$3, %xmm11, 480(%rsp)           # 4-byte Folded Spill
	vpinsrb	$10, (%rax,%rsi), %xmm2, %xmm15
	vpextrd	$2, %xmm23, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm20, %xmm31
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm16
	movl	%r14d, 1472(%rsp)               # 4-byte Spill
	vpbroadcastd	%r14d, %zmm0
	vpaddd	%zmm26, %zmm0, %zmm3
	vmovdqa64	%zmm0, %zmm20
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm26
	vpextrd	$1, %xmm3, %esi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm2
	vpextrd	$3, %xmm23, %ebx
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm31, %r14d
	vpinsrb	$10, (%rax,%rdx), %xmm12, %xmm4
	vpextrd	$2, %xmm14, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm17, %xmm5
	vpextrd	$2, %xmm13, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm18, %xmm28
	vpextrd	$2, %xmm27, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm19, %xmm23
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm3, %xmm7
	vpinsrb	$10, (%rax,%rdx), %xmm24, %xmm24
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm2, %xmm0
	vpextrd	$3, %xmm7, %edx
	vpaddd	%zmm25, %zmm20, %zmm12
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm3, %xmm2
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm2, %edx
	movslq	%r9d, %rsi
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm25
	vmovd	%xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rsi), %xmm15, %xmm15
	vpextrd	$1, %xmm12, %esi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm31, %r9d
	movslq	%esi, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm12, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm12, %edx
	movslq	%edx, %rdx
	movslq	960(%rsp), %rsi                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm13, %edi
	vpinsrb	$11, (%rax,%rsi), %xmm16, %xmm19
	movslq	%ebx, %rdx
	vpextrd	$3, %xmm14, %esi
	vextracti128	$1, %ymm12, %xmm7
	vpinsrb	$11, (%rax,%rdx), %xmm26, %xmm17
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm7, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm7
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm18
	movslq	%esi, %rcx
	vpextrd	$3, %xmm27, %esi
	vextracti32x4	$2, %zmm12, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm16
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm1
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm27
	vpinsrb	$11, (%rax,%rdi), %xmm28, %xmm14
	vmovd	%xmm22, %ecx
	vmovdqu64	%zmm20, 704(%rsp)       # 64-byte Spill
	vpaddd	%zmm21, %zmm20, %zmm1
	vmovd	%xmm1, %edi
	movslq	%edi, %rbx
	movzbl	(%rax,%rbx), %edi
	movslq	%esi, %rsi
	vmovd	%edi, %xmm4
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rsi), %xmm23, %xmm23
	vpextrd	$2, %xmm1, %esi
	vpinsrb	$1, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm2, %edi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm2
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rsi), %xmm2, %xmm2
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$1, %xmm4, %esi
	vpinsrb	$11, (%rax,%rdx), %xmm24, %xmm13
	vmovd	%xmm9, %edx
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm2, %xmm2
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm5
	vpinsrb	$11, (%rax,%rdi), %xmm25, %xmm26
	vmovd	%xmm11, %esi
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$11, (%rax,%rdi), %xmm27, %xmm28
	vmovd	%xmm31, %edi
	vextracti32x4	$3, %zmm29, %xmm2
	vpaddd	%zmm8, %zmm20, %zmm4
	vmovdqa64	%zmm8, %zmm22
	vextracti32x4	$2, %zmm1, %xmm0
	vpinsrb	$12, (%rax,%rdx), %xmm15, %xmm9
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm24
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm19, %xmm15
	vpextrd	$1, %xmm4, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$3, %xmm31, 672(%rsp)           # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movslq	%edi, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vmovd	%xmm2, %ecx
	vextracti128	$1, %ymm4, %xmm5
	vpinsrb	$12, (%rax,%rdx), %xmm17, %xmm17
	vmovd	%xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm5, %edx
	movslq	%esi, %rdi
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$3, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm5
	vpextrd	$1, %xmm2, %esi
	vpinsrb	$12, (%rax,%rdi), %xmm18, %xmm19
	vextracti32x4	$3, %zmm10, %xmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$12, (%rax,%rdx), %xmm16, %xmm16
	vmovd	%xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm7, %edx
	vextracti32x4	$3, %zmm6, %xmm25
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm7, %edx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm30, %xmm8
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm5, %xmm30
	vpinsrb	$12, (%rax,%rcx), %xmm14, %xmm29
	vextracti32x4	$3, %zmm12, %xmm10
	movl	%r10d, 1664(%rsp)               # 4-byte Spill
	vpbroadcastd	%r10d, %zmm5
	vmovdqu64	2048(%rsp), %zmm20      # 64-byte Reload
	vpaddd	%zmm20, %zmm5, %zmm7
	vmovdqa64	%zmm5, %zmm14
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rdx
	vmovd	%xmm8, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm5
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm23, %xmm27
	movslq	%edx, %rcx
	vpextrd	$3, %xmm7, %edx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vmovd	%xmm25, %ecx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm7, %xmm6
	vpinsrb	$3, (%rax,%rdx), %xmm5, %xmm5
	vmovd	%xmm6, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$2, %xmm6, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm13, %xmm23
	vmovd	%xmm10, %ecx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$3, %xmm6, %edx
	vextracti32x4	$3, %zmm3, %xmm13
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm5, %xmm3
	vmovd	%xmm13, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm4, %xmm12
	vextracti32x4	$3, %zmm1, %xmm11
	vmovdqa64	%zmm14, %zmm5
	vmovdqu64	1984(%rsp), %zmm18      # 64-byte Reload
	vpaddd	%zmm18, %zmm14, %zmm1
	vextracti32x4	$2, %zmm7, %xmm4
	vpinsrb	$12, (%rax,%rdx), %xmm26, %xmm6
	vmovdqa	%xmm6, 1600(%rsp)               # 16-byte Spill
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm6
	vmovdqa	%xmm6, 176(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm14
	vmovd	%xmm11, %ecx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$12, (%rax,%rcx), %xmm24, %xmm4
	vmovdqa	%xmm4, 1856(%rsp)               # 16-byte Spill
	vmovd	%xmm12, %ecx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	vextracti32x4	$2, %zmm1, %xmm4
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm30, %xmm6
	vmovdqa	%xmm6, 160(%rsp)                # 16-byte Spill
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm3
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm1
	vmovd	%xmm3, %edx
	vextracti32x4	$3, %zmm7, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm14, %xmm6
	vmovdqa	%xmm6, 1728(%rsp)               # 16-byte Spill
	movslq	%edx, %rcx
	vpaddd	%zmm21, %zmm5, %zmm6
	vmovdqu64	%zmm5, 960(%rsp)        # 64-byte Spill
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vmovdqa	%xmm1, 1088(%rsp)               # 16-byte Spill
	vpextrd	$1, %xmm6, %ecx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm1
	vpextrd	$2, %xmm2, %r8d
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	movslq	%r15d, %rdi
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm2, 352(%rsp)            # 4-byte Folded Spill
	vpinsrb	$13, (%rax,%rdi), %xmm9, %xmm14
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$1, %xmm0, %edi
	vextracti128	$1, %ymm6, %xmm2
	vpinsrb	$13, (%rax,%rcx), %xmm15, %xmm31
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm2, %ecx
	movslq	%r14d, %rbx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm2
	vpextrd	$2, %xmm0, %ecx
	vpinsrb	$13, (%rax,%rbx), %xmm17, %xmm1
	movslq	%r11d, %rbx
	vpextrd	$3, %xmm0, 96(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$2, %zmm6, %xmm0
	vpinsrb	$13, (%rax,%rbx), %xmm19, %xmm30
	vmovd	%xmm0, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$1, %xmm0, %ebx
	vpaddd	%zmm22, %zmm5, %zmm15
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm6, %xmm9
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm0
	vmovd	%xmm9, %ebx
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rbx), %xmm0, %xmm0
	vmovdqa	%xmm0, 1920(%rsp)               # 16-byte Spill
	vmovd	%xmm15, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rdi), %xmm16, %xmm2
	vpextrd	$1, %xmm15, %edi
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm0
	vpextrd	$1, %xmm25, %ebx
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm15, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm15, %edi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm25, %r15d
	vpinsrb	$13, (%rax,%rsi), %xmm29, %xmm16
	vpextrd	$1, %xmm8, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm15, %xmm6
	vpinsrb	$13, (%rax,%rsi), %xmm27, %xmm17
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm15, %xmm6
	vpinsrb	$7, (%rax,%rsi), %xmm0, %xmm0
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %esi
	movslq	%r12d, %r12
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm0, %xmm28
	vpextrd	$3, %xmm25, %r13d
	vpinsrb	$13, (%rax,%rbx), %xmm23, %xmm25
	movl	2176(%rsp), %edx                # 4-byte Reload
	vpbroadcastd	%edx, %zmm19
	vpaddd	%zmm20, %zmm19, %zmm29
	vmovd	%xmm29, %esi
	movslq	%esi, %rbx
	vpinsrb	$14, (%rax,%r12), %xmm14, %xmm23
	vpextrd	$1, %xmm29, %esi
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm0
	vpextrd	$2, %xmm8, %r12d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$2, %xmm29, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm29, %esi
	movslq	%esi, %rsi
	movslq	1152(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm0, %xmm0
	vpextrd	$3, %xmm8, %r14d
	vpinsrb	$14, (%rax,%rbx), %xmm31, %xmm31
	movslq	%r9d, %rbx
	vpextrd	$1, %xmm10, 384(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$1, %ymm29, %xmm6
	vpinsrb	$14, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm29, %xmm6
	vpinsrb	$7, (%rax,%rbx), %xmm0, %xmm0
	vmovd	%xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm0, %xmm0
	vpextrd	$2, %xmm10, 416(%rsp)           # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rbp), %xmm30, %xmm30
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm6, %ebp
	vpaddd	%zmm18, %zmm19, %zmm8
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm27
	vmovd	%xmm8, %ecx
	movslq	%ebp, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm7
	vpextrd	$3, %xmm10, 1152(%rsp)          # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm8, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm8, %ebp
	vmovd	%xmm0, %ebx
	vpextrd	$1, %xmm0, %r9d
	vpextrd	$2, %xmm0, %edx
	vpextrd	$3, %xmm0, %esi
	vmovd	%ecx, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm16, %xmm2
	vpextrd	$3, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm13, 320(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm13, 640(%rsp)           # 4-byte Folded Spill
	vpinsrb	$15, (%rax,%rcx), %xmm23, %xmm10
	movslq	1280(%rsp), %rcx                # 4-byte Folded Reload
	vpextrd	$3, %xmm13, 512(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm8, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm31, %xmm13
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinserti128	$1, %xmm10, %ymm13, %ymm5
	vmovdqu	%ymm5, 576(%rsp)                # 32-byte Spill
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm10
	vpaddd	%zmm21, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	vextracti32x4	$1, %ymm16, %xmm0
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm0, %r11d
	vpextrd	$2, %xmm0, %r10d
	vpextrd	$3, %xmm0, %ebp
	vmovd	%xmm0, %ebx
	vextracti32x4	$2, %zmm16, %xmm0
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm16, %ecx
	vmovd	%xmm0, %esi
	vpextrd	$1, %xmm0, %edi
	vpextrd	$2, %xmm0, %r8d
	vpextrd	$3, %xmm0, %r9d
	vmovd	%edx, %xmm0
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm17, %xmm13
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r15d, %rcx
	movslq	%r8d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm12, 48(%rsp)            # 4-byte Folded Spill
	movslq	%r9d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm0, %xmm14
	vpextrd	$2, %xmm12, 1792(%rsp)          # 4-byte Folded Spill
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm25
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpextrd	$3, %xmm12, 1280(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm22, %zmm19, %zmm17
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm12
	vmovd	%xmm17, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm11, 208(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 256(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$3, %xmm11, 672(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm6
	vpextrd	$1, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm17, %ecx
	vextracti32x4	$1, %ymm17, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm0
	vpextrd	$3, %xmm17, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	480(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm30, %xmm6
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r13d, %rcx
	movslq	%edi, %rdx
	vinserti32x4	$1, %xmm12, %ymm6, %ymm24
	vextracti32x4	$2, %zmm17, %xmm6
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	movslq	%r14d, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %edx
	vpinsrb	$15, (%rax,%rsi), %xmm13, %xmm11
	movslq	%edx, %rdx
	vpextrd	$2, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rcx), %xmm25, %xmm12
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqu	%ymm1, 480(%rsp)                # 32-byte Spill
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm23
	vmovdqu64	2432(%rsp), %zmm20      # 64-byte Reload
	vmovdqu64	1408(%rsp), %zmm22      # 64-byte Reload
	vpaddd	%zmm20, %zmm22, %zmm2
	vinserti128	$1, %xmm11, %ymm12, %ymm0
	vmovdqu	%ymm0, 352(%rsp)                # 32-byte Spill
	vmovdqu64	2368(%rsp), %zmm5       # 64-byte Reload
	vpaddd	%zmm5, %zmm22, %zmm1
	vextracti128	$1, %ymm1, %xmm0
	vmovd	%xmm0, 96(%rsp)                 # 4-byte Folded Spill
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm0, -72(%rsp)            # 4-byte Folded Spill
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm0, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -108(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm0
	vextracti32x4	$3, %zmm15, %xmm13
	vmovd	%xmm13, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm28, %xmm26
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vextracti128	$1, %ymm2, %xmm6
	vpextrd	$1, %xmm6, %r10d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vmovd	%xmm6, %edi
	vextracti32x4	$3, %zmm29, %xmm18
	vmovd	%xmm18, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm7, %xmm6
	vmovdqa	%xmm6, 240(%rsp)                # 16-byte Spill
	vpextrd	$2, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm0, %xmm0
	vextracti32x4	$2, %zmm2, %xmm6
	vpextrd	$1, %xmm6, %ebp
	vpextrd	$2, %xmm6, %ebx
	vpextrd	$3, %xmm6, %esi
	vmovd	%xmm6, %edx
	vextracti32x4	$3, %zmm8, %xmm11
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm10, %xmm6
	vmovdqa	%xmm6, 144(%rsp)                # 16-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$2, %zmm1, %xmm6
	vpextrd	$1, %xmm6, %edi
	vpextrd	$2, %xmm6, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm6, -40(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm6, %esi
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %ebx
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %ebp
	vextracti32x4	$3, %zmm1, %xmm1
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, -80(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 80(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2240(%rsp), %zmm31      # 64-byte Reload
	vpaddd	%zmm31, %zmm22, %zmm1
	vmovd	%xmm2, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -104(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, -28(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -92(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm1, %xmm2
	vmovd	%xmm2, -76(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -68(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -56(%rsp)            # 4-byte Folded Spill
	vmovdqu64	2304(%rsp), %zmm21      # 64-byte Reload
	vpaddd	%zmm21, %zmm22, %zmm2
	vmovd	%xmm1, %r10d
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %r14d
	vpextrd	$3, %xmm1, %r15d
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, -32(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -52(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -24(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -36(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm1
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$2, %xmm1, %r13d
	vpextrd	$3, %xmm1, %r12d
	vmovd	%xmm1, %edx
	vmovd	%ecx, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm2, %xmm6
	vpextrd	$1, %xmm6, %ecx
	vpextrd	$2, %xmm6, %ebp
	vpextrd	$3, %xmm6, %r9d
	vmovd	%xmm6, %ebx
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r8d, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r13d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	movslq	%r12d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	movslq	%ebx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	vpextrd	$3, %xmm2, %esi
	vmovd	%xmm2, %edi
	vextracti32x4	$3, %zmm16, %xmm28
	vmovd	%xmm28, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm14, %xmm16
	movslq	%r9d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm2
	movslq	%r10d, %rbp
	movslq	%r11d, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm0, %xmm0
	movslq	%r14d, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r15d, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	movslq	-28(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rdi), %xmm6, %xmm6
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	movslq	-48(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rdi), %xmm6, %xmm6
	movslq	80(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdi), %xmm1, %xmm1
	movslq	-100(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rdi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	movslq	%edx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	movslq	-60(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm0, %ymm1, %ymm1
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm0
	vpextrd	$1, %xmm3, -80(%rsp)            # 4-byte Folded Spill
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	-36(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm2, %ymm0, %ymm0
	vpaddd	%zmm21, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdx
	vpextrd	$2, %xmm3, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 96(%rsp)             # 4-byte Folded Spill
	vpextrd	$1, %xmm4, %r8d
	vpextrd	$2, %xmm4, 64(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$3, %xmm4, -60(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm4
	movzbl	(%rax,%rdx), %edx
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %ebp
	vpextrd	$3, %xmm3, %r14d
	vmovdqu64	2880(%rsp), %zmm8       # 64-byte Reload
	vpaddd	%zmm8, %zmm22, %zmm10
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %r9d
	vpextrd	$2, %xmm4, %r10d
	vpextrd	$3, %xmm4, %r11d
	vextracti128	$1, %ymm10, %xmm3
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm3, -104(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, -112(%rsp)           # 4-byte Folded Spill
	vmovd	%edx, %xmm3
	movslq	%ebx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	movslq	%edi, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebp, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$2, %zmm10, %xmm4
	vpextrd	$1, %xmm4, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -88(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -92(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, -100(%rsp)               # 4-byte Folded Spill
	vextracti32x4	$3, %zmm17, %xmm15
	vmovd	%xmm15, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm23, %xmm12
	movslq	%r14d, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	movslq	320(%rsp), %rdx                 # 4-byte Folded Reload
	vmovdqa	1600(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm25
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	176(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	vmovdqa	%xmm4, 80(%rsp)                 # 16-byte Spill
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm4               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm23
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vmovdqa	160(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm27
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm17
	movslq	%r8d, %rcx
	vmovdqa	1728(%rsp), %xmm3               # 16-byte Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	vmovdqa	%xmm3, 1856(%rsp)               # 16-byte Spill
	vextracti32x4	$3, %zmm2, %xmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm17, %xmm2
	vmovdqa	%xmm2, 1728(%rsp)               # 16-byte Spill
	vmovdqu64	2944(%rsp), %zmm30      # 64-byte Reload
	vpaddd	%zmm30, %zmm22, %zmm2
	vpextrd	$1, %xmm10, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm10, 320(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm10, 384(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm10, %r8d
	vextracti32x4	$3, %zmm10, %xmm7
	vpextrd	$1, %xmm7, 1408(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 1600(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm7, 160(%rsp)                # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 176(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm7
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm7, %esi
	vpextrd	$2, %xmm7, %edi
	vpextrd	$3, %xmm7, %edx
	vmovd	%xmm7, -24(%rsp)                # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm7
	vpextrd	$1, %xmm2, 24(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 36(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -20(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm7, -12(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm7, -16(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -40(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -36(%rsp)            # 4-byte Folded Spill
	vmovdqu64	896(%rsp), %zmm3        # 64-byte Reload
	vmovdqa64	%zmm5, %zmm29
	vpaddd	%zmm5, %zmm3, %zmm7
	vmovd	%xmm2, %ebx
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti128	$1, %ymm7, %xmm2
	vmovd	%xmm2, 208(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm2
	vmovd	%xmm2, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, -28(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, -32(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm3, %zmm2
	vmovd	%xmm7, 12(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 8(%rsp)              # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 16(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 20(%rsp)             # 4-byte Folded Spill
	vextracti32x4	$3, %zmm7, %xmm7
	vmovd	%xmm7, 32(%rsp)                 # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 28(%rsp)             # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -44(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, -52(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm7
	vmovd	%xmm7, 4(%rsp)                  # 4-byte Folded Spill
	vpextrd	$1, %xmm7, (%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm7, -4(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm7, %r13d
	vextracti32x4	$2, %zmm2, %xmm7
	movzbl	(%rax,%r12), %ecx
	vpextrd	$1, %xmm7, %r12d
	vpextrd	$2, %xmm7, %r11d
	vpextrd	$3, %xmm7, %r10d
	vmovd	%xmm7, %r9d
	vmovd	%ecx, %xmm7
	movslq	24(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm7, %xmm7
	movslq	36(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm7, %xmm7
	movslq	-20(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm7, %xmm7
	movslq	-24(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	movslq	%esi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm7, %xmm7
	movslq	%edx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm7, %xmm7
	movslq	-12(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm7, %xmm7
	movslq	-16(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm7, %xmm7
	movslq	-40(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm7, %xmm7
	movslq	-36(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm7, %xmm7
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r14d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r15d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm7, %xmm7
	movslq	%r8d, %rcx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%xmm2, %ebx
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm2, %edx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, %edi
	vpextrd	$3, %xmm2, %r8d
	vmovd	%ecx, %xmm2
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm2, %xmm2
	movslq	384(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm2, %xmm2
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	-112(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	-84(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	movslq	160(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	movslq	1408(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	movslq	1600(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	movslq	176(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	movslq	%ebp, %rbx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	movslq	%r14d, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	4(%rsp), %rcx                   # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	(%rsp), %rcx                    # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	-4(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	12(%rsp), %rcx                  # 4-byte Folded Reload
	movslq	8(%rsp), %rdx                   # 4-byte Folded Reload
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	movslq	16(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	movslq	20(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rcx), %xmm5, %xmm5
	movslq	208(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm5, %xmm5
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm5, %xmm5
	movslq	-64(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm5, %xmm5
	movslq	-68(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm5
	movslq	-56(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rcx), %xmm5, %xmm5
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	movslq	-28(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rcx), %xmm5, %xmm5
	movslq	-32(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm5
	movslq	32(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$12, (%rax,%rcx), %xmm5, %xmm5
	movslq	28(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm5, %xmm5
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpsllw	$2, %zmm1, %zmm1
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddw	%zmm0, %zmm1, %zmm0
	movslq	-44(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm1
	vpextrd	$1, %xmm13, 320(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm7, %ymm2, %ymm2
	vpmovzxbw	576(%rsp), %zmm5        # 32-byte Folded Reload
                                        # zmm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm5, %zmm5, %zmm6
	vmovdqu64	%zmm6, 1600(%rsp)       # 64-byte Spill
	vpmovzxbw	%ymm24, %zmm5           # zmm5 = ymm24[0],zero,ymm24[1],zero,ymm24[2],zero,ymm24[3],zero,ymm24[4],zero,ymm24[5],zero,ymm24[6],zero,ymm24[7],zero,ymm24[8],zero,ymm24[9],zero,ymm24[10],zero,ymm24[11],zero,ymm24[12],zero,ymm24[13],zero,ymm24[14],zero,ymm24[15],zero,ymm24[16],zero,ymm24[17],zero,ymm24[18],zero,ymm24[19],zero,ymm24[20],zero,ymm24[21],zero,ymm24[22],zero,ymm24[23],zero,ymm24[24],zero,ymm24[25],zero,ymm24[26],zero,ymm24[27],zero,ymm24[28],zero,ymm24[29],zero,ymm24[30],zero,ymm24[31],zero
	vpaddw	%zmm5, %zmm5, %zmm5
	vmovdqu64	%zmm5, 1408(%rsp)       # 64-byte Spill
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm0, %zmm5, %zmm0
	vpmovzxbw	%ymm2, %zmm2            # zmm2 = ymm2[0],zero,ymm2[1],zero,ymm2[2],zero,ymm2[3],zero,ymm2[4],zero,ymm2[5],zero,ymm2[6],zero,ymm2[7],zero,ymm2[8],zero,ymm2[9],zero,ymm2[10],zero,ymm2[11],zero,ymm2[12],zero,ymm2[13],zero,ymm2[14],zero,ymm2[15],zero,ymm2[16],zero,ymm2[17],zero,ymm2[18],zero,ymm2[19],zero,ymm2[20],zero,ymm2[21],zero,ymm2[22],zero,ymm2[23],zero,ymm2[24],zero,ymm2[25],zero,ymm2[26],zero,ymm2[27],zero,ymm2[28],zero,ymm2[29],zero,ymm2[30],zero,ymm2[31],zero
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vpaddw	%zmm2, %zmm2, %zmm2
	vpaddw	%zmm2, %zmm0, %zmm7
	vinserti128	$1, %xmm4, %ymm1, %ymm0
	vpaddd	%zmm21, %zmm3, %zmm4
	vpaddd	%zmm31, %zmm3, %zmm1
	vpextrd	$2, %xmm13, 160(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm13, 176(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, 576(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm2, 384(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 48(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 208(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm4, %xmm2
	vmovd	%xmm2, %r12d
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r15d
	vextracti32x4	$2, %zmm4, %xmm2
	vmovd	%xmm2, %ebp
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$2, %xmm2, %r10d
	vpextrd	$3, %xmm2, %r11d
	vpaddd	%zmm31, %zmm19, %zmm2
	vmovd	%xmm2, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm4, %edx
	vpextrd	$2, %xmm4, %esi
	vpextrd	$3, %xmm4, %edi
	vmovd	%xmm4, %ecx
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, -108(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -96(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, %r8d
	vpextrd	$3, %xmm4, -104(%rsp)           # 4-byte Folded Spill
	vmovd	%ebx, %xmm4
	vpextrd	$1, %xmm2, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm2, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm4, %xmm24
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rdx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r14d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$2, %xmm5, %edx
	vpextrd	$3, %xmm5, %r8d
	vmovd	%xmm5, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm5
	vpinsrb	$1, (%rax,%rbx), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm5, %xmm5
	movslq	-80(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	1088(%rsp), %xmm6               # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm10
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	movslq	576(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	movslq	384(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	movslq	48(%rsp), %rbp                  # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm5
	movslq	208(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm5, %xmm5
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %ebp
	vmovd	%xmm1, %ebx
	vpextrd	$1, %xmm9, %esi
	movslq	%esi, %rsi
	vmovdqa	1920(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	vmovdqa	%xmm1, 384(%rsp)                # 16-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm5, %xmm1
	movslq	320(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm26, %xmm5
	vmovdqa	%xmm5, 320(%rsp)                # 16-byte Spill
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	movslq	-108(%rsp), %rdx                # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ebx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm1
	movslq	-96(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vpextrd	$2, %xmm9, 208(%rsp)            # 4-byte Folded Spill
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpsllw	$2, %zmm0, %zmm0
	vpaddw	%zmm1, %zmm1, %zmm1
	vpaddw	%zmm1, %zmm0, %zmm13
	vextracti128	$1, %ymm2, %xmm0
	vpextrd	$3, %xmm9, 576(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm2, %xmm1
	vpextrd	$1, %xmm11, -76(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm11, 48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm11, 1920(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm20, %zmm19, %zmm11
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vpaddd	%zmm29, %zmm19, %zmm0
	vmovd	%xmm1, %r8d
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r11d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, -108(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -96(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -104(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -80(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm11, %xmm1
	vmovd	%xmm1, %r14d
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vpextrd	$3, %xmm1, -68(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm11, %xmm1
	vmovd	%xmm1, -56(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm1, -64(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm1, -100(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm1, -92(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm3, %zmm1
	vextracti128	$1, %ymm1, %xmm4
	vpextrd	$1, %xmm4, -112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm4, -84(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 1088(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm4, -88(%rsp)                # 4-byte Folded Spill
	vpextrd	$1, %xmm18, %r13d
	movslq	%r13d, %rbp
	vmovdqa	240(%rsp), %xmm4                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm4, %xmm17
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm24, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	%esi, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r10d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$3, %zmm2, %xmm9
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm26
	vpaddd	%zmm30, %zmm3, %zmm2
	vextracti128	$1, %ymm2, %xmm4
	vpextrd	$1, %xmm4, %r13d
	vpextrd	$2, %xmm4, -48(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm4, -52(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, %r11d
	vmovd	%xmm11, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm11, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm4
	vpinsrb	$1, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$2, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm4
	movslq	-76(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm3                # 16-byte Reload
	vpinsrb	$13, (%rax,%rbp), %xmm3, %xmm22
	vpextrd	$3, %xmm11, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r14d, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r15d, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm4, %xmm4
	movslq	%r12d, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm4, %xmm4
	movslq	-68(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rbp), %xmm4, %xmm4
	movslq	-56(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	movslq	-64(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$9, (%rax,%rbp), %xmm4, %xmm4
	vextracti32x4	$2, %zmm2, %xmm5
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r12d
	vmovd	%xmm5, %r8d
	movslq	-100(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$10, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm2, %edx
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %ecx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %r14d
	vpextrd	$2, %xmm2, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 896(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %edi
	vpextrd	$1, %xmm28, %r10d
	movslq	%r10d, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm16, %xmm2
	vmovdqa	%xmm2, 144(%rsp)                # 16-byte Spill
	movslq	-92(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$11, (%rax,%rbp), %xmm4, %xmm4
	vextracti32x4	$3, %zmm11, %xmm2
	vmovd	%xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm16
	movslq	%edx, %rdx
	movslq	%ebx, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm4
	vpinsrb	$1, (%rax,%rbp), %xmm4, %xmm4
	movslq	%esi, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r11d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r13d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm4, %xmm4
	movslq	-48(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm4, %xmm4
	movslq	-52(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r8d, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r9d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r15d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm4
	movslq	%r12d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vextracti32x4	$2, %zmm1, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vpextrd	$2, %xmm5, %r9d
	vpextrd	$3, %xmm5, %r8d
	vmovd	%xmm5, %edi
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm5
	vpinsrb	$1, (%rax,%rbx), %xmm5, %xmm5
	vpextrd	$2, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm5
	movslq	-88(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rbp), %xmm5, %xmm5
	movslq	-112(%rsp), %rbp                # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rbp), %xmm5, %xmm5
	movslq	-84(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rbp), %xmm5, %xmm5
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %ebx
	vpextrd	$3, %xmm1, %esi
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm12, %xmm24
	movslq	1088(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm5, %xmm1
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	640(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm5
	movslq	%r10d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	416(%rsp), %rcx                 # 4-byte Folded Reload
	vmovdqa	80(%rsp), %xmm3                 # 16-byte Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm12
	movslq	%r9d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm23, %xmm23
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm27, %xmm27
	movslq	%edx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm1
	movslq	896(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	movslq	%ebx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vpmovzxbw	480(%rsp), %zmm4        # 32-byte Folded Reload
                                        # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm6
	vmovdqu64	%zmm6, 1088(%rsp)       # 64-byte Spill
	vpmovzxbw	352(%rsp), %zmm4        # 32-byte Folded Reload
                                        # zmm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm3
	vmovdqu64	%zmm3, 896(%rsp)        # 64-byte Spill
	vpaddw	%zmm6, %zmm3, %zmm4
	vpaddw	%zmm13, %zmm4, %zmm4
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpinsrb	$1, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm6
	movslq	-108(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	movslq	-96(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm18, -108(%rsp)          # 4-byte Folded Spill
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm1, %zmm1
	vpaddw	%zmm1, %zmm4, %zmm1
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm4
	vextracti32x4	$2, %zmm0, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm6, %ecx
	vpcmpltuw	%zmm1, %zmm7, %k1
	vpsubw	%zmm1, %zmm7, %zmm13
	vpsubw	%zmm7, %zmm1, %zmm13 {%k1}
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm4, %xmm1
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm25
	vpaddd	%zmm30, %zmm19, %zmm11
	vmovd	%xmm11, %ecx
	vpextrd	$3, %xmm18, -104(%rsp)          # 4-byte Folded Spill
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm11, %xmm1
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm11, %edx
	vmovd	%xmm1, %esi
	vpextrd	$1, %xmm1, %edi
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %ebx
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	1152(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm12, %xmm6
	vpextrd	$3, %xmm11, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$2, %zmm11, %xmm12
	vmovd	%xmm12, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm12, %ecx
	vinserti128	$1, %xmm5, %ymm6, %ymm3
	vmovdqu	%ymm3, 416(%rsp)                # 32-byte Spill
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	vmovdqa	%xmm1, 1152(%rsp)               # 16-byte Spill
	vpextrd	$2, %xmm15, -112(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm15, -80(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm8, %zmm19, %zmm7
	vmovdqa64	%zmm8, %zmm18
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$2, %xmm28, -84(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm7, %edx
	vpextrd	$3, %xmm28, -96(%rsp)           # 4-byte Folded Spill
	vmovd	%ecx, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm9, %r14d
	vpextrd	$2, %xmm9, 352(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm9, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$1, %xmm14, %r9d
	vpextrd	$2, %xmm14, %r15d
	vmovdqu64	704(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm20, %zmm4, %zmm5
	vpextrd	$3, %xmm14, 256(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm5, %xmm3
	vpextrd	$1, %xmm3, %r10d
	vmovd	%xmm3, %edi
	vpextrd	$2, %xmm3, %r11d
	vpextrd	$3, %xmm3, %r8d
	vextracti128	$1, %ymm7, %xmm3
	vmovd	%xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm1, %xmm1
	movslq	672(%rsp), %rbp                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbp), %xmm23, %xmm6
	vpextrd	$2, %xmm3, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rbp), %xmm1, %xmm1
	vmovdqa	%xmm1, 512(%rsp)                # 16-byte Spill
	vextracti32x4	$2, %zmm5, %xmm1
	vpextrd	$1, %xmm1, %ebp
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edx
	vmovd	%xmm1, %ecx
	vmovd	%xmm5, %r13d
	movslq	%r13d, %r13
	vpextrd	$1, %xmm5, %r12d
	movslq	%r12d, %r12
	movzbl	(%rax,%r13), %ebx
	vmovd	%ebx, %xmm1
	vpinsrb	$1, (%rax,%r12), %xmm1, %xmm1
	vpextrd	$2, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm1, %xmm1
	movslq	1280(%rsp), %rbx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rbx), %xmm27, %xmm8
	vpextrd	$3, %xmm5, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbx), %xmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r10d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r11d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm5, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %r8d
	vinserti128	$1, %xmm6, %ymm8, %ymm15
	vpextrd	$2, %xmm0, 240(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 672(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm29, %zmm4, %zmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm2, %esi
	vpextrd	$2, %xmm2, 80(%rsp)             # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vpextrd	$3, %xmm2, 640(%rsp)            # 4-byte Folded Spill
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$2, %xmm0, %edx
	vextracti128	$1, %ymm0, %xmm6
	vpextrd	$1, %xmm6, %edi
	vpextrd	$2, %xmm6, %ebp
	vpextrd	$3, %xmm6, %ebx
	vmovd	%xmm6, %ecx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rdx), %xmm2, %xmm6
	movslq	%r9d, %rdx
	vmovdqa	1728(%rsp), %xmm2               # 16-byte Reload
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm23
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	movslq	%r14d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm26, %xmm14
	movslq	%edi, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm28
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm25, %xmm25
	movslq	%ebx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm3, 1280(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm0, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm3, %esi
	vpextrd	$2, %xmm3, %r9d
	vpextrd	$3, %xmm3, %r8d
	vpaddd	%zmm21, %zmm4, %zmm3
	vextracti128	$1, %ymm3, %xmm2
	vpextrd	$1, %xmm2, %r10d
	vpextrd	$2, %xmm2, %r14d
	vpextrd	$3, %xmm2, %r11d
	vmovd	%xmm2, %r12d
	vpextrd	$1, %xmm5, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm1, %xmm9
	movslq	%esi, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm2
	movslq	64(%rsp), %rdx                  # 4-byte Folded Reload
	vmovdqa	1856(%rsp), %xmm1               # 16-byte Reload
	vpinsrb	$14, (%rax,%rdx), %xmm1, %xmm6
	movslq	%r9d, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm2, %xmm2
	movslq	-72(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm10, %xmm8
	movslq	%r8d, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm2, %xmm2
	movslq	-60(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm6, %xmm10
	vextracti32x4	$3, %zmm0, %xmm6
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm2, %xmm16
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %esi
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %ebx
	vmovd	%xmm3, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm0
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	96(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm8, %xmm8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r12d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm2
	vextracti32x4	$3, %zmm3, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm12, 480(%rsp)           # 4-byte Folded Spill
	vinserti128	$1, %xmm10, %ymm8, %ymm10
	vpextrd	$3, %xmm12, 1856(%rsp)          # 4-byte Folded Spill
	vpaddd	%zmm31, %zmm4, %zmm8
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm6, %edx
	vpextrd	$2, %xmm6, 64(%rsp)             # 4-byte Folded Spill
	vpextrd	$3, %xmm6, 1728(%rsp)           # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm3
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm3, %r10d
	vpextrd	$2, %xmm3, %r8d
	vmovd	%xmm3, %ebp
	vpextrd	$3, %xmm3, %r9d
	vmovd	%ecx, %xmm3
	vpextrd	$1, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm16, %xmm26
	vpextrd	$2, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm6
	vextracti32x4	$2, %zmm8, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	vpextrd	$3, %xmm3, %edi
	vmovd	%xmm3, %ebx
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm2, %xmm3
	vpextrd	$3, %xmm8, %esi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm2
	movslq	%ebp, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r10d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r8d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r9d, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm2, %xmm2
	movslq	%ebx, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm2, %xmm2
	movslq	%ecx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm8, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm5, -92(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm5, -72(%rsp)            # 4-byte Folded Spill
	vpextrd	$1, %xmm6, %r8d
	vpextrd	$2, %xmm6, -88(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm30, %zmm4, %zmm8
	vpextrd	$3, %xmm6, 96(%rsp)             # 4-byte Folded Spill
	vextracti128	$1, %ymm8, %xmm5
	vmovd	%xmm8, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm5, %r9d
	vpextrd	$2, %xmm5, %r10d
	vpextrd	$3, %xmm5, %r11d
	vmovd	%xmm5, %r14d
	vextracti32x4	$2, %zmm8, %xmm5
	movzbl	(%rax,%rdx), %edx
	vpextrd	$1, %xmm8, %ecx
	vmovd	%xmm5, %esi
	vpextrd	$1, %xmm5, %edi
	vpextrd	$2, %xmm5, %ebp
	vpextrd	$3, %xmm5, %ebx
	vmovd	%edx, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$2, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm6
	movslq	%r8d, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm5
	vpextrd	$3, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm6, %xmm2
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm2, %xmm2
	movslq	%r11d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm2, %xmm2
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebp, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm2, %xmm2
	movslq	%ebx, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm2, %xmm2
	vextracti32x4	$3, %zmm8, %xmm8
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm12
	vpextrd	$2, %xmm0, -100(%rsp)           # 4-byte Folded Spill
	vpaddd	%zmm18, %zmm4, %zmm6
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %r11d
	vpextrd	$3, %xmm0, %r8d
	vmovd	%xmm0, %ebp
	vextracti32x4	$2, %zmm6, %xmm0
	movzbl	(%rax,%rcx), %ecx
	vpextrd	$1, %xmm6, %ebx
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %r9d
	vpextrd	$3, %xmm0, %r10d
	vmovd	%ecx, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebp, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r11d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm2
	vextracti32x4	$3, %zmm6, %xmm0
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %r8d
	vpextrd	$2, %xmm0, -76(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, -60(%rsp)            # 4-byte Folded Spill
	vmovdqu64	960(%rsp), %zmm4        # 64-byte Reload
	vpaddd	%zmm20, %zmm4, %zmm0
	vextracti128	$1, %ymm0, %xmm6
	vpextrd	$1, %xmm6, %r13d
	vpextrd	$2, %xmm6, %r12d
	vpextrd	$3, %xmm6, %r14d
	vmovd	%xmm6, %ebx
	vpextrd	$1, %xmm8, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm12, %xmm6
	movslq	%r9d, %rbp
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	movslq	208(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	384(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm12
	movslq	%r10d, %rbp
	vpinsrb	$11, (%rax,%rbp), %xmm2, %xmm2
	movslq	160(%rsp), %rbp                 # 4-byte Folded Reload
	vmovdqa	320(%rsp), %xmm1                # 16-byte Reload
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm16
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	movslq	-108(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm17, %xmm17
	movslq	%r8d, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm2, %xmm20
	movslq	48(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm22, %xmm22
	vmovd	%xmm0, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm0, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm1
	vpinsrb	$1, (%rax,%rbp), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %edi
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, %ebp
	vpextrd	$2, %xmm2, %esi
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm1, %xmm1
	vpextrd	$3, %xmm0, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm1, %xmm1
	movslq	-84(%rsp), %rdi                 # 4-byte Folded Reload
	vmovdqa	144(%rsp), %xmm2                # 16-byte Reload
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm1, %xmm1
	movslq	-112(%rsp), %rdi                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm24, %xmm24
	movslq	%r13d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm1, %xmm1
	movslq	%r15d, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm23, %xmm27
	movslq	%r12d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm1, %xmm1
	movslq	576(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm12, %xmm12
	movslq	%r14d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm1, %xmm1
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm16, %xmm16
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	-104(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm17, %xmm23
	movslq	%ebp, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	1920(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm22, %xmm22
	vinserti32x4	$1, %xmm12, %ymm16, %ymm12
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm17
	vpaddd	%zmm29, %zmm4, %zmm16
	vextracti32x4	$1, %ymm16, %xmm1
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r9d
	vpextrd	$3, %xmm1, %r8d
	vmovd	%xmm1, %r14d
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm17, %xmm17
	vextracti32x4	$3, %zmm0, %xmm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm17, %xmm0
	movslq	-96(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm17
	vextracti32x4	$2, %zmm16, %xmm0
	vpextrd	$1, %xmm0, %edx
	vpextrd	$2, %xmm0, %ebx
	vpextrd	$3, %xmm0, %edi
	vmovd	%xmm0, %esi
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm0
	vpinsrb	$1, (%rax,%rbp), %xmm0, %xmm0
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm0, %xmm0
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm24, %xmm29
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r10d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r9d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm0, %xmm0
	movslq	%r8d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	movslq	%esi, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	%ebx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm16, %xmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm16, %ecx
	vinserti32x4	$1, %xmm23, %ymm22, %ymm22
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm24
	vpextrd	$2, %xmm8, 320(%rsp)            # 4-byte Folded Spill
	vinserti32x4	$1, %xmm2, %ymm29, %ymm23
	vpextrd	$3, %xmm8, 176(%rsp)            # 4-byte Folded Spill
	vmovdqa64	%zmm4, %zmm2
	vpaddd	%zmm21, %zmm4, %zmm0
	vpextrd	$2, %xmm16, 576(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm16, 1920(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm1, 384(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm1, 160(%rsp)            # 4-byte Folded Spill
	vpaddd	%zmm31, %zmm4, %zmm8
	vmovdqa64	%zmm4, %zmm21
	vextracti128	$1, %ymm8, %xmm1
	vpextrd	$1, %xmm1, %r15d
	vpextrd	$2, %xmm1, %r12d
	vmovd	%xmm1, %r14d
	vpextrd	$3, %xmm1, %r13d
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm0, %edi
	movslq	%edi, %rbx
	vpextrd	$1, %xmm1, %r11d
	vpextrd	$2, %xmm1, %ebp
	vpextrd	$3, %xmm1, %r10d
	vmovd	%xmm1, %r9d
	vextracti32x4	$2, %zmm0, %xmm1
	movzbl	(%rax,%rbx), %ebx
	vpextrd	$1, %xmm0, %ecx
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$2, %xmm1, %edi
	vpextrd	$3, %xmm1, %r8d
	vmovd	%ebx, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm1, %xmm1
	movslq	352(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm14, %xmm2
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r9d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r11d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm1, %xmm1
	movslq	%ebp, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm1, %xmm1
	movslq	%esi, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm1, %xmm1
	movslq	%edi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vextracti32x4	$3, %zmm0, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm1, %xmm29
	movslq	80(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm28, %xmm28
	vmovd	%xmm8, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm8, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm1, %xmm14
	vpextrd	$2, %xmm8, %ecx
	vextracti32x4	$2, %zmm8, %xmm1
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %esi
	vpextrd	$3, %xmm1, %edi
	vmovd	%xmm1, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rcx), %xmm14, %xmm1
	vpextrd	$3, %xmm8, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rcx), %xmm1, %xmm1
	movslq	240(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm25, %xmm16
	movslq	%r14d, %rcx
	vpinsrb	$4, (%rax,%rcx), %xmm1, %xmm14
	movslq	-92(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm9, %xmm1
	movslq	%r15d, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm14, %xmm14
	movslq	64(%rsp), %rcx                  # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm26, %xmm9
	movslq	%r12d, %rcx
	vpinsrb	$6, (%rax,%rcx), %xmm14, %xmm14
	movslq	-100(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r13d, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm14, %xmm25
	movslq	-88(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm5, %xmm14
	movslq	%ebp, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm25, %xmm5
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm26
	movslq	%edx, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm5, %xmm5
	movslq	256(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm27, %xmm6
	movslq	%esi, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm5, %xmm5
	movslq	1792(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti32x4	$1, %xmm6, %ymm2, %ymm25
	movslq	%edi, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm5, %xmm2
	vextracti32x4	$3, %zmm8, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm6
	movslq	-76(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rcx), %xmm20, %xmm20
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm27
	vextracti32x4	$2, %zmm7, %xmm2
	vpextrd	$1, %xmm2, 1792(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm2, 256(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm2, 320(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm2, %r10d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r11d
	vpaddd	%zmm30, %zmm4, %zmm8
	vmovd	%xmm8, %edx
	movslq	%edx, %rdi
	vpextrd	$2, %xmm0, %r12d
	vpextrd	$3, %xmm0, %r14d
	vextracti128	$1, %ymm8, %xmm0
	movzbl	(%rax,%rdi), %edi
	vpextrd	$1, %xmm0, %ebp
	vpextrd	$2, %xmm0, %ebx
	vmovd	%xmm0, %edx
	vpextrd	$3, %xmm0, %esi
	vmovd	%edi, %xmm0
	vpextrd	$1, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm8, %edi
	vextracti32x4	$2, %zmm8, %xmm2
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$2, %xmm2, %r13d
	vpextrd	$3, %xmm2, %r9d
	vmovd	%xmm2, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$3, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm0, %xmm0
	movslq	384(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdi), %xmm17, %xmm2
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	movslq	576(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$14, (%rax,%rdx), %xmm24, %xmm5
	movslq	%ebp, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r12d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm29, %xmm29
	movslq	%ebx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r15d, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm27, %xmm17
	movslq	%esi, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm0, %xmm0
	movslq	640(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdx), %xmm28, %xmm24
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm0, %xmm0
	movslq	672(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm16, %xmm16
	movslq	%r8d, %rcx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	movslq	-72(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm27
	movslq	%r13d, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm0, %xmm0
	movslq	1728(%rsp), %rcx                # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rcx), %xmm9, %xmm9
	movslq	%r9d, %rcx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vextracti32x4	$3, %zmm8, %xmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rcx), %xmm0, %xmm0
	vpaddd	%zmm18, %zmm4, %zmm8
	vextracti128	$1, %ymm8, %xmm6
	vpextrd	$1, %xmm6, %r15d
	vpextrd	$2, %xmm6, %r9d
	vpextrd	$3, %xmm6, %r8d
	vpextrd	$1, %xmm1, %edi
	vmovd	%xmm6, %ebp
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm0, %xmm0
	vpextrd	$2, %xmm1, %edi
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm0, %xmm28
	movslq	704(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm18
	vmovd	%xmm8, %edi
	movslq	%edi, %rdi
	vpextrd	$1, %xmm8, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm8, %edi
	vextracti32x4	$2, %zmm8, %xmm3
	vpextrd	$1, %xmm3, %ebx
	vpextrd	$2, %xmm3, %esi
	vpextrd	$3, %xmm3, %edx
	vmovd	%xmm3, %ecx
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm3
	vpextrd	$3, %xmm8, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%rdi), %xmm3, %xmm3
	movslq	96(%rsp), %rdi                  # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm14, %xmm6
	movslq	%ebp, %rdi
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	movslq	176(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm26, %xmm0
	movslq	%r15d, %rdi
	vpinsrb	$5, (%rax,%rdi), %xmm3, %xmm3
	movslq	-60(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm20, %xmm14
	movslq	%r9d, %rdi
	vpinsrb	$6, (%rax,%rdi), %xmm3, %xmm3
	movslq	160(%rsp), %rdi                 # 4-byte Folded Reload
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm20
	vextracti32x4	$3, %zmm7, %xmm7
	movslq	%r8d, %rdi
	vpinsrb	$7, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm7, %r13d
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm7, %r15d
	movslq	%ebx, %rcx
	vextracti32x4	$3, %zmm8, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %ecx
	movslq	1920(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ebp
	vpinsrb	$15, (%rax,%rdi), %xmm5, %xmm5
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm2
	vpextrd	$3, %xmm7, %r9d
	movslq	%r14d, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdx), %xmm29, %xmm3
	vmovd	%xmm7, %r8d
	vpextrd	$3, %xmm1, %ecx
	movslq	%r11d, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm17, %xmm1
	vextracti32x4	$3, %zmm11, %xmm4
	vpextrd	$1, %xmm4, %ebx
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm28, %xmm7
	vmovd	%xmm4, %r11d
	movslq	%esi, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm2, %xmm2
	vpextrd	$2, %xmm4, %esi
	movslq	%ebp, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm8
	vpextrd	$3, %xmm4, %edx
	vinserti32x4	$1, %xmm24, %ymm16, %ymm16
	vinserti32x4	$1, %xmm27, %ymm9, %ymm4
	vinserti32x4	$1, %xmm18, %ymm6, %ymm6
	vinserti128	$1, %xmm0, %ymm14, %ymm0
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpmovzxbw	416(%rsp), %zmm14       # 32-byte Folded Reload
                                        # zmm14 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpmovzxbw	%ymm6, %zmm6            # zmm6 = ymm6[0],zero,ymm6[1],zero,ymm6[2],zero,ymm6[3],zero,ymm6[4],zero,ymm6[5],zero,ymm6[6],zero,ymm6[7],zero,ymm6[8],zero,ymm6[9],zero,ymm6[10],zero,ymm6[11],zero,ymm6[12],zero,ymm6[13],zero,ymm6[14],zero,ymm6[15],zero,ymm6[16],zero,ymm6[17],zero,ymm6[18],zero,ymm6[19],zero,ymm6[20],zero,ymm6[21],zero,ymm6[22],zero,ymm6[23],zero,ymm6[24],zero,ymm6[25],zero,ymm6[26],zero,ymm6[27],zero,ymm6[28],zero,ymm6[29],zero,ymm6[30],zero,ymm6[31],zero
	vpmovzxbw	%ymm0, %zmm0            # zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero,ymm0[16],zero,ymm0[17],zero,ymm0[18],zero,ymm0[19],zero,ymm0[20],zero,ymm0[21],zero,ymm0[22],zero,ymm0[23],zero,ymm0[24],zero,ymm0[25],zero,ymm0[26],zero,ymm0[27],zero,ymm0[28],zero,ymm0[29],zero,ymm0[30],zero,ymm0[31],zero
	vpaddw	%zmm0, %zmm6, %zmm6
	vpmovzxbw	%ymm15, %zmm15          # zmm15 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm14, %zmm15, %zmm9
	vpaddw	%zmm4, %zmm9, %zmm4
	vpaddw	%zmm6, %zmm4, %zmm6
	vinserti32x4	$1, %xmm20, %ymm5, %ymm4
	vinserti128	$1, %xmm3, %ymm1, %ymm1
	vinserti128	$1, %xmm7, %ymm8, %ymm3
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm5
	vpmovzxbw	%ymm10, %zmm7           # zmm7 = ymm10[0],zero,ymm10[1],zero,ymm10[2],zero,ymm10[3],zero,ymm10[4],zero,ymm10[5],zero,ymm10[6],zero,ymm10[7],zero,ymm10[8],zero,ymm10[9],zero,ymm10[10],zero,ymm10[11],zero,ymm10[12],zero,ymm10[13],zero,ymm10[14],zero,ymm10[15],zero,ymm10[16],zero,ymm10[17],zero,ymm10[18],zero,ymm10[19],zero,ymm10[20],zero,ymm10[21],zero,ymm10[22],zero,ymm10[23],zero,ymm10[24],zero,ymm10[25],zero,ymm10[26],zero,ymm10[27],zero,ymm10[28],zero,ymm10[29],zero,ymm10[30],zero,ymm10[31],zero
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpmovzxbw	%ymm3, %zmm3            # zmm3 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpmovzxbw	%ymm12, %zmm4           # zmm4 = ymm12[0],zero,ymm12[1],zero,ymm12[2],zero,ymm12[3],zero,ymm12[4],zero,ymm12[5],zero,ymm12[6],zero,ymm12[7],zero,ymm12[8],zero,ymm12[9],zero,ymm12[10],zero,ymm12[11],zero,ymm12[12],zero,ymm12[13],zero,ymm12[14],zero,ymm12[15],zero,ymm12[16],zero,ymm12[17],zero,ymm12[18],zero,ymm12[19],zero,ymm12[20],zero,ymm12[21],zero,ymm12[22],zero,ymm12[23],zero,ymm12[24],zero,ymm12[25],zero,ymm12[26],zero,ymm12[27],zero,ymm12[28],zero,ymm12[29],zero,ymm12[30],zero,ymm12[31],zero
	vpaddw	%zmm7, %zmm4, %zmm3
	vpaddw	%zmm5, %zmm3, %zmm3
	movslq	1280(%rsp), %rbp                # 4-byte Folded Reload
	movslq	480(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	1856(%rsp), %r14                # 4-byte Folded Reload
	vmovdqa	1152(%rsp), %xmm0               # 16-byte Reload
	vpinsrb	$10, (%rax,%rdi), %xmm0, %xmm2
	movslq	%r11d, %rdi
	vmovdqa	512(%rsp), %xmm0                # 16-byte Reload
	vpinsrb	$7, (%rax,%rbp), %xmm0, %xmm0
	vpaddw	%zmm1, %zmm3, %zmm3
	vpcmpltuw	%zmm3, %zmm6, %k1
	vpsubw	%zmm3, %zmm6, %zmm17
	vpsubw	%zmm6, %zmm3, %zmm17 {%k1}
	vpaddd	2624(%rsp), %zmm19, %zmm5       # 64-byte Folded Reload
	vextracti128	$1, %ymm5, %xmm3
	vmovd	%xmm3, 960(%rsp)                # 4-byte Folded Spill
	vpinsrb	$11, (%rax,%r14), %xmm2, %xmm2
	vpextrd	$1, %xmm3, 704(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 1152(%rsp)           # 4-byte Folded Spill
	vpinsrb	$12, (%rax,%rdi), %xmm2, %xmm2
	vpextrd	$3, %xmm3, 1280(%rsp)           # 4-byte Folded Spill
	movslq	%r10d, %rdi
	movslq	1792(%rsp), %rbp                # 4-byte Folded Reload
	movslq	256(%rsp), %r10                 # 4-byte Folded Reload
	movslq	320(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%r8d, %r8
	movslq	%r13d, %r13
	movslq	%r15d, %r15
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$9, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%r10), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%r8), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%r13), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%r15), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vinserti128	$1, %xmm2, %ymm0, %ymm9
	vextracti32x4	$2, %zmm5, %xmm0
	vpextrd	$1, %xmm0, %r12d
	vpextrd	$2, %xmm0, %r14d
	vpaddw	%zmm14, %zmm14, %zmm2
	vmovd	%xmm0, %r15d
	vpextrd	$3, %xmm0, 416(%rsp)            # 4-byte Folded Spill
	vpaddd	2688(%rsp), %zmm19, %zmm0       # 64-byte Folded Reload
	vmovd	%xmm0, %edx
	vpaddw	%zmm7, %zmm7, %zmm3
	movslq	%edx, %rdx
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %esi
	vpaddw	%zmm3, %zmm2, %zmm2
	movslq	%ebx, %rbx
	movslq	%esi, %r13
	vpextrd	$3, %xmm0, %edi
	vextracti128	$1, %ymm0, %xmm3
	movslq	%edi, %rdi
	vmovd	%xmm3, %ebp
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %esi
	vmovdqu64	1088(%rsp), %zmm1       # 64-byte Reload
	vpaddw	1600(%rsp), %zmm1, %zmm6        # 64-byte Folded Reload
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm3, %edx
	vextracti32x4	$2, %zmm0, %xmm3
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$1, %xmm3, %ebx
	vpinsrb	$2, (%rax,%r13), %xmm7, %xmm7
	vpextrd	$2, %xmm3, %r13d
	vpinsrb	$3, (%rax,%rdi), %xmm7, %xmm7
	vpinsrb	$4, (%rax,%rbp), %xmm7, %xmm7
	vmovd	%xmm3, %edi
	vpextrd	$3, %xmm3, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$5, (%rax,%rcx), %xmm7, %xmm3
	vmovd	%xmm5, %ecx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %esi
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %edx
	movslq	%edi, %rdi
	vpinsrb	$8, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %edi
	vextracti32x4	$3, %zmm5, %xmm5
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$1, %xmm5, %r11d
	movslq	%r13d, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm5, %r13d
	movslq	%ebp, %rbp
	vpaddw	%zmm15, %zmm15, %zmm7
	vextracti32x4	$3, %zmm0, %xmm0
	vpinsrb	$11, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm0, %ebp
	movslq	%ebp, %rbp
	vpaddw	%zmm4, %zmm4, %zmm4
	vpinsrb	$12, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm5, %ebp
	movzbl	(%rax,%rcx), %ecx
	vpaddw	%zmm4, %zmm7, %zmm10
	vmovd	%ecx, %xmm4
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm5, %r10d
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm0, %esi
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm4, %xmm4
	vpextrd	$3, %xmm0, %edx
	vmovdqu64	896(%rsp), %zmm0        # 64-byte Reload
	vpaddw	1408(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	movslq	%edi, %rdi
	movslq	960(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	704(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	1152(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	1280(%rsp), %r9                 # 4-byte Folded Reload
	movslq	%r12d, %r12
	movslq	%r14d, %r14
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm0
	vpinsrb	$3, (%rax,%rdi), %xmm4, %xmm3
	vpinsrb	$4, (%rax,%rbx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm5
	movslq	416(%rsp), %rdx                 # 4-byte Folded Reload
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm0
	vpinsrb	$6, (%rax,%r8), %xmm0, %xmm0
	vpmovzxbw	%ymm22, %zmm8           # zmm8 = ymm22[0],zero,ymm22[1],zero,ymm22[2],zero,ymm22[3],zero,ymm22[4],zero,ymm22[5],zero,ymm22[6],zero,ymm22[7],zero,ymm22[8],zero,ymm22[9],zero,ymm22[10],zero,ymm22[11],zero,ymm22[12],zero,ymm22[13],zero,ymm22[14],zero,ymm22[15],zero,ymm22[16],zero,ymm22[17],zero,ymm22[18],zero,ymm22[19],zero,ymm22[20],zero,ymm22[21],zero,ymm22[22],zero,ymm22[23],zero,ymm22[24],zero,ymm22[25],zero,ymm22[26],zero,ymm22[27],zero,ymm22[28],zero,ymm22[29],zero,ymm22[30],zero,ymm22[31],zero
	movslq	%r10d, %rcx
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r15), %xmm0, %xmm0
	vpmovzxbw	%ymm23, %zmm18          # zmm18 = ymm23[0],zero,ymm23[1],zero,ymm23[2],zero,ymm23[3],zero,ymm23[4],zero,ymm23[5],zero,ymm23[6],zero,ymm23[7],zero,ymm23[8],zero,ymm23[9],zero,ymm23[10],zero,ymm23[11],zero,ymm23[12],zero,ymm23[13],zero,ymm23[14],zero,ymm23[15],zero,ymm23[16],zero,ymm23[17],zero,ymm23[18],zero,ymm23[19],zero,ymm23[20],zero,ymm23[21],zero,ymm23[22],zero,ymm23[23],zero,ymm23[24],zero,ymm23[25],zero,ymm23[26],zero,ymm23[27],zero,ymm23[28],zero,ymm23[29],zero,ymm23[30],zero,ymm23[31],zero
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%r14), %xmm0, %xmm3
	vpmovzxbw	%ymm25, %zmm0           # zmm0 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	movslq	%r13d, %rdi
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpaddw	%zmm2, %zmm6, %zmm6
	movslq	%ebp, %rcx
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	vpsllw	$2, %zmm8, %zmm3
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm5, %ymm2, %ymm15
	vpmovzxbw	%ymm16, %zmm2           # zmm2 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddd	2560(%rsp), %zmm19, %zmm7       # 64-byte Folded Reload
	vpaddw	%zmm10, %zmm11, %zmm10
	vpaddd	2496(%rsp), %zmm19, %zmm11      # 64-byte Folded Reload
	vmovd	%xmm11, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm11, %ecx
	vpsllw	$2, %zmm18, %zmm12
	movslq	%ecx, %rbp
	vpextrd	$2, %xmm11, %ecx
	vpextrd	$3, %xmm11, %r8d
	vpaddw	%zmm3, %zmm6, %zmm14
	movslq	%ecx, %rsi
	vextracti128	$1, %ymm11, %xmm3
	vmovd	%xmm3, %edi
	vpextrd	$1, %xmm3, %r13d
	vpaddw	%zmm12, %zmm10, %zmm10
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %r12d
	vextracti32x4	$2, %zmm11, %xmm3
	vpaddw	%zmm0, %zmm0, %zmm12
	vmovd	%xmm3, %r14d
	vpextrd	$1, %xmm3, %r11d
	vpextrd	$2, %xmm3, %r10d
	vpcmpltuw	%zmm10, %zmm14, %k1
	vpextrd	$3, %xmm3, %r9d
	vextracti32x4	$3, %zmm11, %xmm3
	vpextrd	$1, %xmm3, 960(%rsp)            # 4-byte Folded Spill
	vpsubw	%zmm10, %zmm14, %zmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpsubw	%zmm14, %zmm10, %zmm6 {%k1}
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpaddw	%zmm2, %zmm8, %zmm8
	vextracti128	$1, %ymm7, %xmm1
	vpinsrb	$1, (%rax,%rdx), %xmm5, %xmm5
	vpextrd	$1, %xmm1, %edx
	vpaddw	%zmm8, %zmm12, %zmm8
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm4
	vpextrd	$2, %xmm1, %ebx
	vpinsrb	$2, (%rax,%rcx), %xmm5, %xmm5
	vpextrd	$3, %xmm1, %ecx
	vpinsrb	$1, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm4
	vmovd	%xmm1, %esi
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbp), %xmm5, %xmm1
	movslq	%r8d, %rbp
	vpextrd	$2, %xmm3, %r8d
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edi, %rsi
	vpextrd	$3, %xmm3, %edi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm1, %xmm1
	vmovd	%xmm3, %edx
	vextracti32x4	$2, %zmm7, %xmm3
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %ebp
	movslq	%r13d, %r13
	vpinsrb	$4, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %esi
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%r13), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %r13d
	vpinsrb	$6, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm3, %ebx
	movslq	%r15d, %r15
	movslq	%r12d, %r12
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm7, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%r15), %xmm4, %xmm4
	vpextrd	$1, %xmm3, %r15d
	vpinsrb	$7, (%rax,%rcx), %xmm1, %xmm1
	vpextrd	$2, %xmm3, %ecx
	vpinsrb	$7, (%rax,%r12), %xmm4, %xmm4
	vpinsrb	$8, (%rax,%rbx), %xmm1, %xmm1
	vmovd	%xmm3, %ebx
	vpextrd	$3, %xmm3, %r12d
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm1, %xmm1
	movslq	%r14d, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm3
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r11d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	movslq	%r13d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	movslq	%r9d, %rsi
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	movslq	%r15d, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm1, %xmm1
	movslq	960(%rsp), %rcx                 # 4-byte Folded Reload
	vpinsrb	$13, (%rax,%rcx), %xmm3, %xmm3
	movslq	%r12d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	movslq	%r8d, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpmovzxbw	%ymm15, %zmm4           # zmm4 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpmovzxbw	%ymm9, %zmm5            # zmm5 = ymm9[0],zero,ymm9[1],zero,ymm9[2],zero,ymm9[3],zero,ymm9[4],zero,ymm9[5],zero,ymm9[6],zero,ymm9[7],zero,ymm9[8],zero,ymm9[9],zero,ymm9[10],zero,ymm9[11],zero,ymm9[12],zero,ymm9[13],zero,ymm9[14],zero,ymm9[15],zero,ymm9[16],zero,ymm9[17],zero,ymm9[18],zero,ymm9[19],zero,ymm9[20],zero,ymm9[21],zero,ymm9[22],zero,ymm9[23],zero,ymm9[24],zero,ymm9[25],zero,ymm9[26],zero,ymm9[27],zero,ymm9[28],zero,ymm9[29],zero,ymm9[30],zero,ymm9[31],zero
	vpaddw	%zmm4, %zmm5, %zmm4
	vpaddw	%zmm4, %zmm8, %zmm4
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm3, %ymm1
	vpaddw	%zmm5, %zmm5, %zmm3
	vpmovzxbw	%ymm1, %zmm1            # zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero,ymm1[8],zero,ymm1[9],zero,ymm1[10],zero,ymm1[11],zero,ymm1[12],zero,ymm1[13],zero,ymm1[14],zero,ymm1[15],zero,ymm1[16],zero,ymm1[17],zero,ymm1[18],zero,ymm1[19],zero,ymm1[20],zero,ymm1[21],zero,ymm1[22],zero,ymm1[23],zero,ymm1[24],zero,ymm1[25],zero,ymm1[26],zero,ymm1[27],zero,ymm1[28],zero,ymm1[29],zero,ymm1[30],zero,ymm1[31],zero
	vpaddw	%zmm1, %zmm3, %zmm1
	vpaddw	%zmm18, %zmm0, %zmm0
	vpaddw	%zmm2, %zmm0, %zmm0
	vpaddw	%zmm1, %zmm0, %zmm0
	vpcmpltuw	%zmm0, %zmm4, %k1
	vpsubw	%zmm0, %zmm4, %zmm1
	vpsubw	%zmm4, %zmm0, %zmm1 {%k1}
	vpaddw	%zmm13, %zmm17, %zmm0
	vpaddw	%zmm6, %zmm1, %zmm1
	vpmovuswb	%zmm0, %ymm0
	vpmovuswb	%zmm1, %ymm1
	vpaddb	%ymm0, %ymm1, %ymm0
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, 2176(%rsp)                # 4-byte Folded Spill
	movl	1664(%rsp), %r10d               # 4-byte Reload
	addl	%ecx, %r10d
	movl	1472(%rsp), %r14d               # 4-byte Reload
	addl	%ecx, %r14d
	movl	1536(%rsp), %r13d               # 4-byte Reload
	addl	%ecx, %r13d
	movl	1216(%rsp), %edx                # 4-byte Reload
	addl	%ecx, %edx
	movl	%edx, %ecx
	decq	288(%rsp)                       # 8-byte Folded Spill
	jne	.LBB214_57
# %bb.58:                               # %"end for output.s0.y.yi185"
                                        #   in Loop: Header=BB214_56 Depth=2
	movq	464(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movq	280(%rsp), %rdi                 # 8-byte Reload
	addq	$32, %rdi
	cmpq	448(%rsp), %rdx                 # 8-byte Folded Reload
	jne	.LBB214_56
.LBB214_59:                             # %"end for output.s0.x.x.rebased177"
                                        #   in Loop: Header=BB214_44 Depth=1
	movq	808(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	232(%rsp), %edi                 # 4-byte Reload
	addl	$16, %edi
	movl	308(%rsp), %ecx                 # 4-byte Reload
	addl	%ecx, 132(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 128(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 124(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 120(%rsp)                 # 4-byte Folded Spill
	addl	%ecx, 116(%rsp)                 # 4-byte Folded Spill
	movq	456(%rsp), %rcx                 # 8-byte Reload
	addl	$16, %ecx
	movq	%rcx, 456(%rsp)                 # 8-byte Spill
	cmpq	800(%rsp), %rdx                 # 8-byte Folded Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB214_44
.LBB214_60:                             # %"end for output.s0.y.y.rebased93"
	movq	616(%rsp), %rdx                 # 8-byte Reload
	movl	2120(%rsp), %ecx                # 4-byte Reload
	subl	%edx, %ecx
	movq	136(%rsp), %rdi                 # 8-byte Reload
	jle	.LBB214_68
# %bb.61:                               # %"for output.s0.y.y.rebased221.preheader"
	cmpl	$0, 768(%rsp)                   # 4-byte Folded Reload
	jle	.LBB214_68
# %bb.62:                               # %"for output.s0.y.y.rebased221.us.preheader"
	movq	608(%rsp), %rbp                 # 8-byte Reload
	decl	%ebp
	leal	1(%rdi), %r8d
	leal	-1(%rdi), %ebx
	movl	%ebx, 512(%rsp)                 # 4-byte Spill
	vpbroadcastd	%ebp, %zmm0
	vmovdqu64	%zmm0, 1792(%rsp)       # 64-byte Spill
	movl	%ecx, %ecx
	movq	%rcx, 480(%rsp)                 # 8-byte Spill
	shll	$4, %edx
	movl	112(%rsp), %ecx                 # 4-byte Reload
	addl	%ecx, %edx
	imull	%esi, %ecx
	negl	%ecx
	movl	%ecx, 112(%rsp)                 # 4-byte Spill
	movl	%edx, %ecx
	subl	776(%rsp), %ecx                 # 4-byte Folded Reload
	xorl	%ebp, %ebp
	movl	$2, %r9d
	movl	%r8d, 416(%rsp)                 # 4-byte Spill
	.p2align	4, 0x90
.LBB214_63:                             # %"for output.s0.y.y.rebased221.us"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB214_64 Depth 2
                                        #       Child Loop BB214_65 Depth 3
	movq	%rbp, 256(%rsp)                 # 8-byte Spill
	movq	%rcx, 1984(%rsp)                # 8-byte Spill
	movq	%rdx, 616(%rsp)                 # 8-byte Spill
	movslq	%edx, %rcx
	imulq	%rsi, %rcx
	addq	568(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 1920(%rsp)                # 8-byte Spill
	movl	112(%rsp), %ecx                 # 4-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB214_64:                             # %"for output.s0.x.x227.us"
                                        #   Parent Loop BB214_63 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB214_65 Depth 3
	movl	%ecx, 672(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	addq	1920(%rsp), %rcx                # 8-byte Folded Reload
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movq	312(%rsp), %rsi                 # 8-byte Reload
	leal	(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vmovdqa64	.LCPI214_0(%rip), %zmm4 # zmm4 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpaddd	%zmm4, %zmm0, %zmm1
	vmovdqa64	.LCPI214_1(%rip), %zmm5 # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	vpaddd	%zmm5, %zmm0, %zmm0
	vmovdqu64	1792(%rsp), %zmm3       # 64-byte Reload
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vmovdqu64	3072(%rsp), %zmm2       # 64-byte Reload
	vpmaxsd	%zmm2, %zmm1, %zmm17
	vpmaxsd	%zmm2, %zmm0, %zmm22
	leal	-2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm23
	vpmaxsd	%zmm2, %zmm0, %zmm26
	leal	-1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm8
	vpmaxsd	%zmm2, %zmm0, %zmm9
	leal	1(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm10
	vpmaxsd	%zmm2, %zmm0, %zmm11
	leal	2(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm12
	vpmaxsd	%zmm2, %zmm0, %zmm13
	leal	-3(%rcx,%rsi), %edx
	vpbroadcastd	%edx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1408(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1152(%rsp)       # 64-byte Spill
	leal	3(%rcx,%rsi), %ecx
	vpbroadcastd	%ecx, %zmm0
	vpaddd	%zmm4, %zmm0, %zmm1
	vpaddd	%zmm5, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm0, %zmm0
	vpminsd	%zmm3, %zmm1, %zmm1
	vpmaxsd	%zmm2, %zmm1, %zmm1
	vmovdqu64	%zmm1, 1088(%rsp)       # 64-byte Spill
	vpmaxsd	%zmm2, %zmm0, %zmm0
	vmovdqu64	%zmm0, 1280(%rsp)       # 64-byte Spill
	movq	$-16, %rcx
	.p2align	4, 0x90
.LBB214_65:                             # %"for output.s0.y.yi233.us"
                                        #   Parent Loop BB214_63 Depth=1
                                        #     Parent Loop BB214_64 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, 2048(%rsp)                # 8-byte Spill
	movq	1984(%rsp), %rdx                # 8-byte Reload
	addl	%ecx, %edx
	addl	$16, %edx
	movl	%edx, 1536(%rsp)                # 4-byte Spill
	cmpl	%edi, %edx
	movl	%edx, %ecx
	cmovgl	%r8d, %ecx
	movl	%edi, %r14d
	cmovll	%edx, %r14d
	cmpl	$2, %ecx
	cmovlel	%r9d, %ecx
	addl	$-2, %ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm27
	vpaddd	%zmm23, %zmm27, %zmm1
	vpaddd	%zmm26, %zmm27, %zmm0
	vmovd	%xmm0, %ecx
	vpextrd	$1, %xmm0, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm0, %ecx
	vpextrd	$3, %xmm0, %esi
	movslq	%ecx, %r11
	movslq	%esi, %r10
	vextracti128	$1, %ymm0, %xmm2
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %esi
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm0, %xmm2
	vpextrd	$1, %xmm2, %edi
	vpextrd	$2, %xmm2, %esi
	movslq	%ebp, %r9
	vpextrd	$3, %xmm2, %edx
	vmovd	%xmm1, %ebx
	vpextrd	$1, %xmm1, %ecx
	movslq	%ebx, %rbp
	movslq	%ecx, %rbx
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm3
	vpinsrb	$1, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbx), %xmm3, %xmm2
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm1, %xmm3
	vpinsrb	$3, (%rax,%rbx), %xmm2, %xmm2
	vmovd	%xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$1, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$2, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm1, %xmm3
	vpinsrb	$7, (%rax,%rbx), %xmm2, %xmm2
	vmovd	%xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm2, %xmm2
	movslq	%ecx, %r8
	vpextrd	$1, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm2, %xmm2
	movslq	%edi, %rdi
	vpextrd	$2, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$1, %xmm1, %ebx
	movslq	%esi, %rsi
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$2, %xmm1, %ebx
	movslq	%edx, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm2, %xmm2
	vpextrd	$3, %xmm1, %ebx
	vextracti32x4	$3, %zmm0, %xmm0
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm2, %xmm1
	vmovd	%xmm0, %ebx
	movslq	%ebx, %rbx
	movq	1216(%rsp), %rcx                # 8-byte Reload
	movzbl	(%rax,%rcx), %edx
	vmovd	%edx, %xmm2
	vpextrd	$1, %xmm0, %edx
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm2, %xmm2
	movslq	%edx, %rdx
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%r11), %xmm2, %xmm2
	vpextrd	$3, %xmm0, %r11d
	vpinsrb	$3, (%rax,%r10), %xmm2, %xmm0
	vpinsrb	$4, (%rax,%r12), %xmm0, %xmm0
	vpinsrb	$5, (%rax,%r13), %xmm0, %xmm0
	vpinsrb	$6, (%rax,%r15), %xmm0, %xmm0
	vpinsrb	$7, (%rax,%r9), %xmm0, %xmm0
	vpinsrb	$8, (%rax,%r8), %xmm0, %xmm0
	vpinsrb	$9, (%rax,%rdi), %xmm0, %xmm0
	vpinsrb	$10, (%rax,%rsi), %xmm0, %xmm0
	vpinsrb	$11, (%rax,%rbp), %xmm0, %xmm0
	vpinsrb	$12, (%rax,%rbx), %xmm0, %xmm0
	vpinsrb	$13, (%rax,%rdx), %xmm0, %xmm0
	movslq	%r11d, %rdx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpinsrb	$15, (%rax,%rdx), %xmm0, %xmm0
	vinserti128	$1, %xmm1, %ymm0, %ymm15
	vpaddd	%zmm12, %zmm27, %zmm2
	vpaddd	%zmm13, %zmm27, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r10
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %esi
	vmovd	%xmm3, %edi
	vpextrd	$3, %xmm3, %ebx
	vmovd	%xmm2, %edx
	movslq	%edx, %r8
	movzbl	(%rax,%r8), %edx
	vmovd	%edx, %xmm3
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	movslq	%edi, %r9
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %r8
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %esi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm3, %xmm2
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%r11), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movslq	%ebp, %r11
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edi
	movq	1472(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ebp
	movslq	%edi, %rdi
	movslq	%ebp, %rbp
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r12), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r15), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%r11), %xmm1, %xmm1
	vpinsrb	$14, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rbp), %xmm1, %xmm1
	vinserti128	$1, %xmm2, %ymm1, %ymm14
	movq	2048(%rsp), %rcx                # 8-byte Reload
	movq	1984(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	addl	$19, %ecx
	cmpl	136(%rsp), %ecx                 # 4-byte Folded Reload
	cmovgel	136(%rsp), %ecx                 # 4-byte Folded Reload
	testl	%ecx, %ecx
	movl	$1, %edx
	cmovlel	%edx, %ecx
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm24
	vpaddd	%zmm23, %zmm24, %zmm2
	vpaddd	%zmm26, %zmm24, %zmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rbx
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %r12
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r9
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r8
	vextracti32x4	$2, %zmm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %r15d
	vpextrd	$3, %xmm3, %ecx
	vmovd	%xmm2, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbp), %xmm3, %xmm3
	movslq	%edi, %rdi
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%rbp), %xmm3, %xmm3
	movslq	%r15d, %r15
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm3
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movq	1024(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebp
	vpinsrb	$2, (%rax,%rbx), %xmm3, %xmm3
	movslq	%ebp, %rbp
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%r12), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r11), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%r15), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rcx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%rsi), %xmm1, %xmm1
	vpinsrb	$14, (%rax,%rbp), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rbx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm2, %ymm1, %ymm29
	vpaddd	%zmm12, %zmm24, %zmm2
	vpaddd	%zmm13, %zmm24, %zmm1
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %r10
	movslq	%edx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm1, %ecx
	vpextrd	$3, %xmm1, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %esi
	movslq	%ecx, %r12
	movslq	%esi, %r13
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %esi
	movslq	%ecx, %r11
	vextracti32x4	$2, %zmm1, %xmm3
	vpextrd	$1, %xmm3, %edi
	vpextrd	$2, %xmm3, %ebx
	movslq	%esi, %r15
	vpextrd	$3, %xmm3, %edx
	vmovd	%xmm2, %esi
	vpextrd	$1, %xmm2, %ecx
	movslq	%esi, %rbp
	movslq	%ecx, %rsi
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm4
	vpinsrb	$1, (%rax,%rsi), %xmm4, %xmm4
	vmovd	%xmm3, %ecx
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm3
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm2, %xmm4
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm2, %xmm4
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm3, %xmm3
	movslq	%ecx, %r9
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edi, %r8
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$10, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm2, %xmm2
	vpinsrb	$11, (%rax,%rdi), %xmm3, %xmm3
	vmovd	%xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$1, %xmm2, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %rdi
	vpinsrb	$13, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$14, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edi
	vextracti32x4	$3, %zmm1, %xmm1
	movslq	%edi, %rdi
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm2
	vmovd	%xmm1, %edi
	movslq	%edi, %rdi
	movzbl	(%rax,%r10), %ebp
	vmovd	%ebp, %xmm3
	vpextrd	$1, %xmm1, %ebp
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %r10
	movq	1472(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movq	1024(%rsp), %rsi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm1
	vpinsrb	$4, (%rax,%r12), %xmm1, %xmm1
	vpinsrb	$5, (%rax,%r13), %xmm1, %xmm1
	vpinsrb	$6, (%rax,%r11), %xmm1, %xmm1
	vpinsrb	$7, (%rax,%r15), %xmm1, %xmm1
	vpinsrb	$8, (%rax,%r9), %xmm1, %xmm1
	vpinsrb	$9, (%rax,%r8), %xmm1, %xmm1
	vpinsrb	$10, (%rax,%rbx), %xmm1, %xmm1
	vpinsrb	$11, (%rax,%rdx), %xmm1, %xmm1
	vpinsrb	$12, (%rax,%rdi), %xmm1, %xmm1
	vpinsrb	$13, (%rax,%rbp), %xmm1, %xmm1
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%r10), %xmm1, %xmm1
	vpinsrb	$15, (%rax,%rcx), %xmm1, %xmm1
	vinserti32x4	$1, %xmm2, %ymm1, %ymm31
	testl	%r14d, %r14d
	movl	$1, %ecx
	cmovlel	%ecx, %r14d
	decl	%r14d
	imull	40(%rsp), %r14d                 # 4-byte Folded Reload
	subl	-8(%rsp), %r14d                 # 4-byte Folded Reload
	vpbroadcastd	%r14d, %zmm21
	vpaddd	%zmm23, %zmm21, %zmm1
	vpaddd	%zmm26, %zmm21, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r14
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r12
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r15
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r9
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %r11
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rbx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$4, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ebx
	movslq	%edx, %rdx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$8, (%rax,%rbx), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rbx), %xmm3, %xmm3
	vmovd	%xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rbx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$14, (%rax,%rbx), %xmm3, %xmm1
	vmovd	%xmm2, %ebx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%ebx, %rbx
	movslq	%edx, %rdx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%rdi), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$2, (%rax,%r14), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm28
	vpaddd	%zmm12, %zmm21, %zmm1
	vpaddd	%zmm13, %zmm21, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r14
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r12
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vextracti128	$1, %ymm1, %xmm4
	vmovd	%xmm4, %edx
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm3
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	vpinsrb	$9, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	vpinsrb	$1, (%rax,%r10), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r10d
	vpinsrb	$2, (%rax,%r14), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	movslq	%r10d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm25
	movq	2048(%rsp), %rcx                # 8-byte Reload
	movq	1984(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	addl	$18, %ecx
	cmpl	136(%rsp), %ecx                 # 4-byte Folded Reload
	cmovgel	136(%rsp), %ecx                 # 4-byte Folded Reload
	testl	%ecx, %ecx
	movl	$1, %edx
	cmovlel	%edx, %ecx
	decl	%ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm19
	vpaddd	%zmm23, %zmm19, %zmm1
	vpaddd	%zmm26, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %edx
	movslq	%ecx, %rsi
	movslq	%edx, %r9
	vpextrd	$2, %xmm2, %ecx
	vpextrd	$3, %xmm2, %edx
	movslq	%ecx, %r14
	movslq	%edx, %r12
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %edx
	movslq	%ecx, %r13
	movslq	%edx, %r11
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %edx
	movslq	%ecx, %r15
	movslq	%edx, %r8
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %edx
	movslq	%ecx, %r10
	movslq	%edx, %rdi
	vpextrd	$2, %xmm3, %ecx
	vpextrd	$3, %xmm3, %edx
	movslq	%ecx, %rbx
	movslq	%edx, %rbp
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	vpextrd	$1, %xmm1, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$1, %xmm2, %edx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm3
	vpextrd	$2, %xmm2, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$1, (%rax,%r9), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %r9d
	vpinsrb	$2, (%rax,%r14), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vmovdqu	%ymm0, 1024(%rsp)               # 32-byte Spill
	vpaddd	%zmm12, %zmm19, %zmm1
	vpaddd	%zmm13, %zmm19, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %r14
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %r12
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %edx
	movslq	%edx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %esi
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%esi, %rdx
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rsi), %xmm3, %xmm3
	vmovd	%xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rsi), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %esi
	movslq	%esi, %rsi
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	vpinsrb	$14, (%rax,%rsi), %xmm3, %xmm1
	vmovd	%xmm2, %esi
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm1, %xmm1
	vpextrd	$1, %xmm2, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm3
	vpextrd	$2, %xmm2, %edi
	movslq	%edi, %rdi
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%r14), %xmm3, %xmm2
	vpinsrb	$3, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r9), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vmovdqu	%ymm0, 1216(%rsp)               # 32-byte Spill
	movl	512(%rsp), %ecx                 # 4-byte Reload
	movl	1536(%rsp), %edx                # 4-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%ecx, %edx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	imull	40(%rsp), %ecx                  # 4-byte Folded Reload
	subl	-8(%rsp), %ecx                  # 4-byte Folded Reload
	vpbroadcastd	%ecx, %zmm18
	vpaddd	%zmm23, %zmm18, %zmm1
	vpaddd	%zmm26, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r14
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %esi
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$2, %xmm1, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	movslq	%edx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdi), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	movq	1536(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm2
	movq	1472(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm2, %xmm2
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r10), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%rdi), %xmm2, %xmm2
	vinserti32x4	$1, %xmm1, %ymm2, %ymm20
	vpaddd	%zmm12, %zmm18, %zmm1
	vpaddd	%zmm13, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r14
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %r13
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rbp
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %esi
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm3
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	vpinsrb	$3, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$5, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rcx), %xmm3, %xmm3
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$9, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$10, (%rax,%rcx), %xmm3, %xmm3
	vextracti32x4	$3, %zmm1, %xmm1
	vmovd	%xmm1, %ecx
	vpinsrb	$11, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$1, %xmm1, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$12, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm1
	movslq	%esi, %rdx
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	movzbl	(%rax,%r9), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %r9d
	movq	1536(%rsp), %r10                # 8-byte Reload
	vpinsrb	$1, (%rax,%r10), %xmm3, %xmm2
	movq	1472(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	movq	1664(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rsi), %xmm2, %xmm2
	movslq	%r9d, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm5
	vpaddd	%zmm8, %zmm18, %zmm1
	vpaddd	%zmm9, %zmm18, %zmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm2, %ecx
	vpextrd	$2, %xmm2, %edx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	movslq	%ecx, %r13
	movslq	%edx, %r14
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm2, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm3, %ecx
	vpextrd	$2, %xmm3, %edx
	movslq	%ecx, %r8
	movslq	%edx, %rbp
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rsi
	vextracti32x4	$3, %zmm2, %xmm2
	vpextrd	$1, %xmm2, %ebx
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm3
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm1, %xmm4
	vpinsrb	$2, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$4, (%rax,%rdx), %xmm3, %xmm3
	vpextrd	$2, %xmm4, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm1, %xmm4
	vpinsrb	$6, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$1, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rcx), %xmm3, %xmm3
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm1, %xmm1
	vpinsrb	$10, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$13, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm3, %xmm3
	movslq	%ebx, %rdx
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm3, %xmm1
	vpextrd	$2, %xmm2, %ebx
	movzbl	(%rax,%r9), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm2, %r9d
	movslq	%ebx, %rbx
	movslq	%r9d, %r9
	movq	1536(%rsp), %r10                # 8-byte Reload
	vpinsrb	$1, (%rax,%r10), %xmm3, %xmm2
	movq	1472(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$2, (%rax,%rdi), %xmm2, %xmm2
	movq	1664(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$3, (%rax,%rdi), %xmm2, %xmm2
	vpinsrb	$4, (%rax,%r12), %xmm2, %xmm2
	vpinsrb	$5, (%rax,%r13), %xmm2, %xmm2
	vpinsrb	$6, (%rax,%r14), %xmm2, %xmm2
	vpinsrb	$7, (%rax,%r15), %xmm2, %xmm2
	vpinsrb	$8, (%rax,%r11), %xmm2, %xmm2
	vpinsrb	$9, (%rax,%r8), %xmm2, %xmm2
	vpinsrb	$10, (%rax,%rbp), %xmm2, %xmm2
	vpinsrb	$11, (%rax,%rsi), %xmm2, %xmm2
	vpinsrb	$12, (%rax,%rcx), %xmm2, %xmm2
	vpinsrb	$13, (%rax,%rdx), %xmm2, %xmm2
	vpinsrb	$14, (%rax,%rbx), %xmm2, %xmm2
	vpinsrb	$15, (%rax,%r9), %xmm2, %xmm2
	vpaddd	%zmm17, %zmm18, %zmm3
	vpaddd	%zmm22, %zmm18, %zmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	vextracti128	$1, %ymm4, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r12
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %r14
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %r13
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %r15
	vextracti32x4	$2, %zmm4, %xmm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %r11
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %r8
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpextrd	$1, %xmm4, %esi
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm3, %xmm6
	vpinsrb	$2, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$3, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$4, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm3, %xmm6
	vpinsrb	$6, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm6, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm6, %ecx
	vpinsrb	$8, (%rax,%rdx), %xmm0, %xmm0
	vpextrd	$2, %xmm6, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm3, %xmm3
	vpinsrb	$10, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm3, %edx
	movslq	%edx, %rdx
	vpinsrb	$11, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$1, %xmm3, %ecx
	vpinsrb	$12, (%rax,%rdx), %xmm0, %xmm0
	vmovd	%xmm4, %edx
	movslq	%ecx, %rcx
	vpinsrb	$13, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm3, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$3, %xmm3, %ecx
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm0, %xmm0
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	movzbl	(%rax,%rdi), %r9d
	vmovd	%r9d, %xmm3
	vpextrd	$3, %xmm4, %edi
	movslq	%edi, %rdi
	movq	1536(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$1, (%rax,%rbp), %xmm3, %xmm3
	movq	1472(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm3, %xmm3
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vpinsrb	$4, (%rax,%r12), %xmm3, %xmm3
	vpinsrb	$5, (%rax,%r14), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%r13), %xmm3, %xmm3
	vpinsrb	$7, (%rax,%r15), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%r11), %xmm3, %xmm3
	vpinsrb	$9, (%rax,%r10), %xmm3, %xmm3
	vpinsrb	$10, (%rax,%r8), %xmm3, %xmm3
	vpinsrb	$11, (%rax,%rbx), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%rdx), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdi), %xmm3, %xmm3
	vinserti128	$1, %xmm1, %ymm2, %ymm2
	vinserti32x4	$1, %xmm0, %ymm3, %ymm30
	vpaddd	%zmm11, %zmm18, %zmm3
	vextracti128	$1, %ymm3, %xmm0
	vpextrd	$1, %xmm0, %r14d
	vmovd	%xmm0, %r12d
	vpextrd	$2, %xmm0, 1536(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 1472(%rsp)           # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm0
	vpextrd	$1, %xmm0, 1664(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm0, 960(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 704(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm0, %r15d
	vpmovzxbw	%ymm15, %zmm0           # zmm0 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm0, %zmm0, %zmm1
	vpmovzxbw	%ymm14, %zmm0           # zmm0 = ymm14[0],zero,ymm14[1],zero,ymm14[2],zero,ymm14[3],zero,ymm14[4],zero,ymm14[5],zero,ymm14[6],zero,ymm14[7],zero,ymm14[8],zero,ymm14[9],zero,ymm14[10],zero,ymm14[11],zero,ymm14[12],zero,ymm14[13],zero,ymm14[14],zero,ymm14[15],zero,ymm14[16],zero,ymm14[17],zero,ymm14[18],zero,ymm14[19],zero,ymm14[20],zero,ymm14[21],zero,ymm14[22],zero,ymm14[23],zero,ymm14[24],zero,ymm14[25],zero,ymm14[26],zero,ymm14[27],zero,ymm14[28],zero,ymm14[29],zero,ymm14[30],zero,ymm14[31],zero
	vpaddw	%zmm0, %zmm0, %zmm0
	vpaddd	%zmm17, %zmm27, %zmm15
	vpaddd	%zmm22, %zmm27, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm14, %ecx
	movslq	%ecx, %r10
	vpextrd	$2, %xmm14, %ecx
	movslq	%ecx, %rdi
	vpextrd	$3, %xmm14, %ecx
	movslq	%ecx, %r9
	vextracti128	$1, %ymm14, %xmm4
	vpextrd	$1, %xmm4, %edx
	vmovd	%xmm15, %ebx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ebx, %rbp
	movslq	%ecx, %rbx
	movzbl	(%rax,%rbp), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm15, %ebx
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm15, %xmm7
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpinsrb	$3, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm15, %xmm7
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$9, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vextracti32x4	$3, %zmm15, %xmm7
	vmovd	%xmm7, %ecx
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %r8
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti32x4	$2, %zmm14, %xmm4
	vpinsrb	$1, (%rax,%r10), %xmm7, %xmm7
	vmovd	%xmm4, %ebp
	movslq	%ebp, %r10
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%r9), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %r9
	vpinsrb	$4, (%rax,%rcx), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm14, %xmm4
	vpinsrb	$5, (%rax,%rdx), %xmm7, %xmm7
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %ebp
	vpinsrb	$7, (%rax,%rsi), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %esi
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ebx
	vpinsrb	$9, (%rax,%rdi), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r9), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rsi), %xmm4, %xmm4
	movslq	%ebx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpsllw	$2, %zmm4, %zmm14
	vpaddd	%zmm8, %zmm27, %zmm4
	vpaddd	%zmm9, %zmm27, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rdi
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rsi
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %r8
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %ebp
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm6
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	movslq	%ebx, %rcx
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vextracti32x4	$2, %zmm4, %xmm7
	vmovd	%xmm7, %ebp
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	vextracti128	$1, %ymm15, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %r9
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebx, %r10
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$2, %zmm15, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %r8
	vpinsrb	$4, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm15, %xmm7
	vpinsrb	$5, (%rax,%r10), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rdi), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm14, %zmm14
	vpaddd	%zmm10, %zmm27, %zmm16
	vpaddd	%zmm11, %zmm27, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rdi
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %r11
	vextracti128	$1, %ymm15, %xmm4
	vpextrd	$1, %xmm4, %ecx
	vmovd	%xmm16, %ebp
	movslq	%ebp, %rbx
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm6
	vpextrd	$2, %xmm16, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ecx, %r9
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r10
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %r8
	vextracti32x4	$2, %zmm15, %xmm4
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vmovd	%xmm4, %esi
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	movslq	%esi, %rsi
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%r11), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$4, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ebx
	movslq	%ecx, %r11
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm15, %xmm4
	vpinsrb	$5, (%rax,%r9), %xmm7, %xmm7
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rsi), %xmm7, %xmm7
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdi), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r11), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm1, %zmm0, %zmm6
	vpaddw	%zmm14, %zmm6, %zmm6
	vpaddw	%zmm4, %zmm6, %zmm15
	vpaddd	%zmm17, %zmm24, %zmm16
	vpaddd	%zmm22, %zmm24, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm14, %ecx
	movslq	%ecx, %rdi
	vpextrd	$2, %xmm14, %ecx
	movslq	%ecx, %rsi
	vpextrd	$3, %xmm14, %ecx
	movslq	%ecx, %r8
	vextracti128	$1, %ymm14, %xmm4
	vpextrd	$1, %xmm4, %ecx
	vmovd	%xmm16, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm16, %ebx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$2, %xmm16, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$3, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ebx, %rbp
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebp
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ebp, %r9
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ecx, %r10
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %r11
	vpinsrb	$1, (%rax,%rdi), %xmm7, %xmm4
	vextracti32x4	$2, %zmm14, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$2, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$1, %xmm7, %esi
	movslq	%edi, %rdi
	movslq	%esi, %rsi
	vpinsrb	$3, (%rax,%r8), %xmm4, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %r8
	vpinsrb	$4, (%rax,%r9), %xmm4, %xmm4
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm14, %xmm7
	vpinsrb	$5, (%rax,%r10), %xmm4, %xmm4
	vmovd	%xmm7, %edx
	vpinsrb	$6, (%rax,%rbx), %xmm4, %xmm4
	movslq	%edx, %rdx
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%r11), %xmm4, %xmm4
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$3, %xmm7, %edi
	movslq	%ebp, %rbp
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rsi), %xmm4, %xmm4
	vpinsrb	$10, (%rax,%r8), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rdi), %xmm4, %xmm4
	vinserti32x4	$1, %xmm6, %ymm4, %ymm16
	vpaddd	%zmm8, %zmm24, %zmm4
	vpaddd	%zmm9, %zmm24, %zmm14
	vmovd	%xmm14, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm14, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm14, %ecx
	movslq	%ecx, %r9
	vpextrd	$3, %xmm14, %ecx
	movslq	%ecx, %r10
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rbx
	vpextrd	$1, %xmm4, %ecx
	vpextrd	$2, %xmm4, %esi
	movslq	%ecx, %rbp
	movslq	%esi, %rcx
	movzbl	(%rax,%rbx), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$2, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	vpinsrb	$3, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$6, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vextracti128	$1, %ymm14, %xmm7
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r11d
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %ebp
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ecx
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vpinsrb	$12, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	vextracti32x4	$2, %zmm14, %xmm7
	vpinsrb	$1, (%rax,%r8), %xmm6, %xmm6
	vmovd	%xmm7, %r8d
	vpinsrb	$2, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r9d
	vpinsrb	$3, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %esi
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vpmovzxbw	%ymm29, %zmm7           # zmm7 = ymm29[0],zero,ymm29[1],zero,ymm29[2],zero,ymm29[3],zero,ymm29[4],zero,ymm29[5],zero,ymm29[6],zero,ymm29[7],zero,ymm29[8],zero,ymm29[9],zero,ymm29[10],zero,ymm29[11],zero,ymm29[12],zero,ymm29[13],zero,ymm29[14],zero,ymm29[15],zero,ymm29[16],zero,ymm29[17],zero,ymm29[18],zero,ymm29[19],zero,ymm29[20],zero,ymm29[21],zero,ymm29[22],zero,ymm29[23],zero,ymm29[24],zero,ymm29[25],zero,ymm29[26],zero,ymm29[27],zero,ymm29[28],zero,ymm29[29],zero,ymm29[30],zero,ymm29[31],zero
	vpaddw	%zmm7, %zmm7, %zmm29
	vpmovzxbw	%ymm31, %zmm7           # zmm7 = ymm31[0],zero,ymm31[1],zero,ymm31[2],zero,ymm31[3],zero,ymm31[4],zero,ymm31[5],zero,ymm31[6],zero,ymm31[7],zero,ymm31[8],zero,ymm31[9],zero,ymm31[10],zero,ymm31[11],zero,ymm31[12],zero,ymm31[13],zero,ymm31[14],zero,ymm31[15],zero,ymm31[16],zero,ymm31[17],zero,ymm31[18],zero,ymm31[19],zero,ymm31[20],zero,ymm31[21],zero,ymm31[22],zero,ymm31[23],zero,ymm31[24],zero,ymm31[25],zero,ymm31[26],zero,ymm31[27],zero,ymm31[28],zero,ymm31[29],zero,ymm31[30],zero,ymm31[31],zero
	vpaddw	%zmm7, %zmm7, %zmm27
	vpmovzxbw	%ymm16, %zmm7           # zmm7 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpsllw	$2, %zmm7, %zmm16
	movslq	%r11d, %rbx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	movslq	%r8d, %rdi
	movslq	%r9d, %r8
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm14, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vpinsrb	$8, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %edi
	vpinsrb	$9, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	movslq	%edi, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm4, %zmm16, %zmm14
	vpaddd	%zmm10, %zmm24, %zmm4
	vpaddd	%zmm11, %zmm24, %zmm16
	vmovd	%xmm16, %ecx
	vpextrd	$1, %xmm16, %esi
	movslq	%ecx, %rdx
	movslq	%esi, %r8
	vpextrd	$2, %xmm16, %ecx
	vpextrd	$3, %xmm16, %esi
	movslq	%ecx, %rdi
	movslq	%esi, %r9
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rbp
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebx
	movslq	%ecx, %r10
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %r11
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%r8), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %r8
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edi
	vpinsrb	$3, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%edi, %rdi
	movslq	%ecx, %r9
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$5, (%rax,%r10), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %esi
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	movslq	%esi, %rdx
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$8, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%r9), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpinsrb	$14, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm4
	vpaddw	%zmm29, %zmm27, %zmm6
	vpaddw	%zmm14, %zmm6, %zmm6
	vpaddw	%zmm4, %zmm6, %zmm4
	vpcmpltuw	%zmm4, %zmm15, %k1
	vpsubw	%zmm4, %zmm15, %zmm14
	vpsubw	%zmm15, %zmm4, %zmm14 {%k1}
	vpaddd	%zmm17, %zmm21, %zmm16
	vpaddd	%zmm22, %zmm21, %zmm15
	vmovd	%xmm15, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm15, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm15, %ecx
	movslq	%ecx, %rdi
	vpextrd	$3, %xmm15, %ecx
	movslq	%ecx, %r8
	vextracti128	$1, %ymm15, %xmm4
	vpextrd	$1, %xmm4, %ecx
	vmovd	%xmm16, %ebp
	movslq	%ebp, %rbx
	vpextrd	$1, %xmm16, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rbx), %ebx
	vmovd	%ebx, %xmm6
	vpextrd	$2, %xmm16, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$1, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm16, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$1, %ymm16, %xmm7
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebp, %rbp
	vpinsrb	$13, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vpinsrb	$14, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	movslq	%ecx, %r9
	movslq	%ebp, %rbp
	vpinsrb	$15, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %r10
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm7
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %r11
	vextracti32x4	$2, %zmm15, %xmm4
	vpinsrb	$1, (%rax,%rsi), %xmm7, %xmm7
	vmovd	%xmm4, %esi
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	movslq	%esi, %rsi
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$4, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm4, %ebx
	movslq	%ecx, %r8
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm15, %xmm4
	vpinsrb	$5, (%rax,%r9), %xmm7, %xmm7
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%r10), %xmm7, %xmm7
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%r11), %xmm7, %xmm7
	vpextrd	$2, %xmm4, %ecx
	vpinsrb	$8, (%rax,%rsi), %xmm7, %xmm7
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdi), %xmm7, %xmm4
	vpinsrb	$10, (%rax,%r8), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vinserti128	$1, %xmm6, %ymm4, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm4, %zmm15
	vpmovzxbw	%ymm28, %zmm24          # zmm24 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpaddd	%zmm8, %zmm21, %zmm4
	vpaddd	%zmm9, %zmm21, %zmm28
	vmovd	%xmm28, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm28, %ecx
	vpextrd	$2, %xmm28, %edx
	movslq	%ecx, %rsi
	movslq	%edx, %r8
	vpextrd	$3, %xmm28, %ecx
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbx
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vextracti32x4	$1, %ymm28, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%ecx, %r9
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%edx, %r10
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %r11
	vextracti32x4	$2, %zmm28, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %r8
	vpinsrb	$3, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%r10), %xmm6, %xmm6
	vextracti32x4	$3, %zmm28, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm28           # zmm28 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddd	%zmm10, %zmm21, %zmm4
	vpaddd	%zmm11, %zmm21, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rdi
	vpextrd	$1, %xmm16, %ecx
	vpextrd	$2, %xmm16, %edx
	movslq	%ecx, %rsi
	movslq	%edx, %r8
	vpextrd	$3, %xmm16, %ecx
	vmovd	%xmm4, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm4, %ebp
	movslq	%ebp, %rbx
	vpextrd	$2, %xmm4, %ebp
	movslq	%ebp, %rbp
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$2, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$3, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vpinsrb	$4, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	vpinsrb	$7, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebx
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebp
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%ecx, %r9
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%edx, %r10
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movzbl	(%rax,%rdi), %edi
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm7, %edi
	movslq	%ebx, %rbx
	movslq	%edi, %r11
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$2, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %r8
	vpinsrb	$3, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	movslq	%edx, %rdx
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$5, (%rax,%r10), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$7, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm28, %zmm28
	vpmovzxbw	%ymm25, %zmm21          # zmm21 = ymm25[0],zero,ymm25[1],zero,ymm25[2],zero,ymm25[3],zero,ymm25[4],zero,ymm25[5],zero,ymm25[6],zero,ymm25[7],zero,ymm25[8],zero,ymm25[9],zero,ymm25[10],zero,ymm25[11],zero,ymm25[12],zero,ymm25[13],zero,ymm25[14],zero,ymm25[15],zero,ymm25[16],zero,ymm25[17],zero,ymm25[18],zero,ymm25[19],zero,ymm25[20],zero,ymm25[21],zero,ymm25[22],zero,ymm25[23],zero,ymm25[24],zero,ymm25[25],zero,ymm25[26],zero,ymm25[27],zero,ymm25[28],zero,ymm25[29],zero,ymm25[30],zero,ymm25[31],zero
	vpaddw	%zmm24, %zmm21, %zmm4
	vpaddw	%zmm15, %zmm4, %zmm15
	vpaddd	%zmm17, %zmm19, %zmm4
	vpaddd	%zmm22, %zmm19, %zmm25
	vmovd	%xmm25, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm25, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm25, %ecx
	movslq	%ecx, %rdi
	vpextrd	$3, %xmm25, %ecx
	movslq	%ecx, %r8
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %ebx
	movslq	%ecx, %rbp
	movslq	%ebx, %rcx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %ecx
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$1, %ymm25, %xmm7
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebp, %r9
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %r10
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %r11
	vextracti32x4	$2, %zmm25, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	movslq	%esi, %rsi
	vpextrd	$1, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ebp, %rbp
	movslq	%ecx, %r8
	vextracti32x4	$3, %zmm25, %xmm7
	vpinsrb	$5, (%rax,%r9), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm6
	vpaddw	%zmm28, %zmm15, %zmm25
	vinserti128	$1, %xmm4, %ymm6, %ymm15
	vpaddd	%zmm8, %zmm19, %zmm4
	vpaddd	%zmm9, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rdx
	vpextrd	$1, %xmm16, %ecx
	movslq	%ecx, %rsi
	vpextrd	$2, %xmm16, %ecx
	movslq	%ecx, %rdi
	vpextrd	$3, %xmm16, %ecx
	movslq	%ecx, %r8
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %ebx
	movslq	%ecx, %rbp
	movslq	%ebx, %rcx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %ecx
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	movslq	%ebx, %rbx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	vpinsrb	$12, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebx
	movslq	%ecx, %rcx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebp, %r9
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movslq	%ebx, %r10
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %r11
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	movslq	%esi, %rsi
	vpextrd	$1, %xmm7, %edi
	movslq	%edi, %rdi
	vpinsrb	$3, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ebp, %rbp
	movslq	%ecx, %r8
	vextracti32x4	$3, %zmm16, %xmm7
	vpinsrb	$5, (%rax,%r9), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%r10), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vpinsrb	$7, (%rax,%r11), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%r8), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$14, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rsi), %xmm6, %xmm6
	vinserti32x4	$1, %xmm4, %ymm6, %ymm28
	vpaddd	%zmm10, %zmm19, %zmm4
	vpaddd	%zmm11, %zmm19, %zmm16
	vmovd	%xmm16, %ecx
	vpextrd	$1, %xmm16, %esi
	movslq	%ecx, %rdx
	movslq	%esi, %r8
	vpextrd	$2, %xmm16, %ecx
	vpextrd	$3, %xmm16, %esi
	movslq	%ecx, %rdi
	movslq	%esi, %r9
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rbp
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rax,%rbp), %ebp
	vmovd	%ebp, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm4, %xmm7
	vpinsrb	$1, (%rax,%rcx), %xmm6, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm4, %xmm7
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm7, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$6, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebp
	vpinsrb	$7, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %ecx
	vpinsrb	$8, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %ebx
	vextracti32x4	$1, %ymm16, %xmm7
	movslq	%ebp, %rbp
	vpinsrb	$9, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm7, %ebp
	movslq	%ecx, %rcx
	vpinsrb	$10, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r10d
	movslq	%ebx, %rbx
	vextracti32x4	$3, %zmm4, %xmm4
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm4
	vpextrd	$2, %xmm7, %ebx
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm7, %edx
	vextracti32x4	$2, %zmm16, %xmm7
	vpinsrb	$1, (%rax,%r8), %xmm6, %xmm6
	vmovd	%xmm7, %esi
	vpinsrb	$2, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %r8d
	vpinsrb	$3, (%rax,%r9), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %r9d
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %r11d
	movslq	%r10d, %rdi
	movslq	%ebx, %rbx
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpinsrb	$5, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vmovd	%xmm7, %edi
	vpinsrb	$6, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm7, %ebx
	vpinsrb	$7, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	vpinsrb	$8, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$3, %xmm7, %r10d
	movslq	%r8d, %rcx
	movslq	%r9d, %rbp
	movslq	%r11d, %rsi
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	movslq	%edx, %rdx
	vpinsrb	$9, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$10, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$11, (%rax,%rsi), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%r10d, %rcx
	vpinsrb	$14, (%rax,%rdx), %xmm6, %xmm6
	vpinsrb	$15, (%rax,%rcx), %xmm6, %xmm6
	vinserti128	$1, %xmm4, %ymm6, %ymm4
	vpmovzxbw	%ymm15, %zmm6           # zmm6 = ymm15[0],zero,ymm15[1],zero,ymm15[2],zero,ymm15[3],zero,ymm15[4],zero,ymm15[5],zero,ymm15[6],zero,ymm15[7],zero,ymm15[8],zero,ymm15[9],zero,ymm15[10],zero,ymm15[11],zero,ymm15[12],zero,ymm15[13],zero,ymm15[14],zero,ymm15[15],zero,ymm15[16],zero,ymm15[17],zero,ymm15[18],zero,ymm15[19],zero,ymm15[20],zero,ymm15[21],zero,ymm15[22],zero,ymm15[23],zero,ymm15[24],zero,ymm15[25],zero,ymm15[26],zero,ymm15[27],zero,ymm15[28],zero,ymm15[29],zero,ymm15[30],zero,ymm15[31],zero
	vpaddw	%zmm6, %zmm6, %zmm6
	vpmovzxbw	1024(%rsp), %zmm7       # 32-byte Folded Reload
                                        # zmm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpmovzxbw	%ymm28, %zmm15          # zmm15 = ymm28[0],zero,ymm28[1],zero,ymm28[2],zero,ymm28[3],zero,ymm28[4],zero,ymm28[5],zero,ymm28[6],zero,ymm28[7],zero,ymm28[8],zero,ymm28[9],zero,ymm28[10],zero,ymm28[11],zero,ymm28[12],zero,ymm28[13],zero,ymm28[14],zero,ymm28[15],zero,ymm28[16],zero,ymm28[17],zero,ymm28[18],zero,ymm28[19],zero,ymm28[20],zero,ymm28[21],zero,ymm28[22],zero,ymm28[23],zero,ymm28[24],zero,ymm28[25],zero,ymm28[26],zero,ymm28[27],zero,ymm28[28],zero,ymm28[29],zero,ymm28[30],zero,ymm28[31],zero
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm15, %zmm4
	vpmovzxbw	1216(%rsp), %zmm28      # 32-byte Folded Reload
                                        # zmm28 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero,mem[16],zero,mem[17],zero,mem[18],zero,mem[19],zero,mem[20],zero,mem[21],zero,mem[22],zero,mem[23],zero,mem[24],zero,mem[25],zero,mem[26],zero,mem[27],zero,mem[28],zero,mem[29],zero,mem[30],zero,mem[31],zero
	vpaddw	%zmm7, %zmm28, %zmm15
	vpaddw	%zmm6, %zmm15, %zmm6
	vpaddd	%zmm10, %zmm18, %zmm16
	vmovd	%xmm16, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm16, %edx
	movslq	%edx, %r8
	vpextrd	$2, %xmm16, %esi
	movslq	%esi, %rsi
	vpextrd	$3, %xmm16, %edi
	movslq	%edi, %rdi
	vpaddw	%zmm4, %zmm6, %zmm4
	vpcmpltuw	%zmm4, %zmm25, %k1
	vpsubw	%zmm4, %zmm25, %zmm15
	vpsubw	%zmm25, %zmm4, %zmm15 {%k1}
	vextracti32x4	$1, %ymm16, %xmm4
	vmovd	%xmm4, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm4, %ebx
	vpextrd	$2, %xmm4, %edx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%ebx, %rbx
	vextracti32x4	$2, %zmm16, %xmm4
	vpinsrb	$1, (%rax,%r8), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %r8d
	vpinsrb	$2, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %esi
	vpinsrb	$3, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %edi
	vpinsrb	$4, (%rax,%rbp), %xmm6, %xmm6
	vpinsrb	$5, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ebp
	vpextrd	$1, %xmm3, %r9d
	movslq	%edx, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm4
	vmovd	%xmm3, %edx
	movslq	%ecx, %rcx
	vpinsrb	$7, (%rax,%rcx), %xmm4, %xmm4
	vpextrd	$2, %xmm3, %ecx
	movslq	%edx, %rdx
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm4, %xmm4
	vpextrd	$3, %xmm3, %ebp
	vextracti32x4	$3, %zmm3, %xmm3
	movslq	%r8d, %rbx
	vpinsrb	$9, (%rax,%rbx), %xmm4, %xmm4
	vpextrd	$1, %xmm3, 1024(%rsp)           # 4-byte Folded Spill
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vextracti32x4	$3, %zmm16, %xmm6
	vpinsrb	$10, (%rax,%rsi), %xmm4, %xmm4
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$11, (%rax,%rdi), %xmm4, %xmm4
	vpextrd	$1, %xmm6, %edi
	movslq	%edi, %rdi
	vpinsrb	$12, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm6, %esi
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm4
	vmovd	%xmm3, %r13d
	movslq	%esi, %rsi
	vpinsrb	$14, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vpextrd	$2, %xmm3, 1216(%rsp)           # 4-byte Folded Spill
	movzbl	(%rax,%rdx), %edx
	vmovd	%edx, %xmm6
	vpextrd	$3, %xmm3, %edx
	movslq	%r9d, %rbx
	movslq	%ecx, %rcx
	movq	%rcx, 896(%rsp)                 # 8-byte Spill
	movslq	%ebp, %rbp
	movslq	%r12d, %rdi
	movslq	%r14d, %rsi
	movslq	1536(%rsp), %r9                 # 4-byte Folded Reload
	movslq	1472(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r15d, %r15
	movslq	1664(%rsp), %r10                # 4-byte Folded Reload
	movslq	960(%rsp), %r11                 # 4-byte Folded Reload
	movslq	704(%rsp), %r14                 # 4-byte Folded Reload
	movslq	%r13d, %r13
	movslq	1024(%rsp), %r12                # 4-byte Folded Reload
	movslq	1216(%rsp), %rcx                # 4-byte Folded Reload
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	movslq	%edx, %rdx
	vpinsrb	$1, (%rax,%rbx), %xmm6, %xmm3
	movq	896(%rsp), %rcx                 # 8-byte Reload
	vpinsrb	$2, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$3, (%rax,%rbp), %xmm3, %xmm3
	vpinsrb	$4, (%rax,%rdi), %xmm3, %xmm3
	vpinsrb	$5, (%rax,%rsi), %xmm3, %xmm3
	vpinsrb	$6, (%rax,%r9), %xmm3, %xmm3
	vpinsrb	$7, (%rax,%r8), %xmm3, %xmm3
	vpinsrb	$8, (%rax,%r15), %xmm3, %xmm3
	vpinsrb	$9, (%rax,%r10), %xmm3, %xmm3
	vpinsrb	$10, (%rax,%r11), %xmm3, %xmm3
	vpinsrb	$11, (%rax,%r14), %xmm3, %xmm3
	vpinsrb	$12, (%rax,%r13), %xmm3, %xmm3
	vpinsrb	$13, (%rax,%r12), %xmm3, %xmm3
	movq	1024(%rsp), %rcx                # 8-byte Reload
	vpinsrb	$14, (%rax,%rcx), %xmm3, %xmm3
	vpinsrb	$15, (%rax,%rdx), %xmm3, %xmm3
	vinserti32x4	$1, %xmm4, %ymm3, %ymm16
	vpaddw	%zmm24, %zmm24, %zmm3
	vpaddw	%zmm7, %zmm7, %zmm4
	vpaddw	%zmm4, %zmm3, %zmm3
	vpaddw	%zmm1, %zmm29, %zmm1
	vpmovzxbw	%ymm20, %zmm19          # zmm19 = ymm20[0],zero,ymm20[1],zero,ymm20[2],zero,ymm20[3],zero,ymm20[4],zero,ymm20[5],zero,ymm20[6],zero,ymm20[7],zero,ymm20[8],zero,ymm20[9],zero,ymm20[10],zero,ymm20[11],zero,ymm20[12],zero,ymm20[13],zero,ymm20[14],zero,ymm20[15],zero,ymm20[16],zero,ymm20[17],zero,ymm20[18],zero,ymm20[19],zero,ymm20[20],zero,ymm20[21],zero,ymm20[22],zero,ymm20[23],zero,ymm20[24],zero,ymm20[25],zero,ymm20[26],zero,ymm20[27],zero,ymm20[28],zero,ymm20[29],zero,ymm20[30],zero,ymm20[31],zero
	vpaddw	%zmm3, %zmm1, %zmm1
	vpsllw	$2, %zmm19, %zmm3
	vpaddw	%zmm3, %zmm1, %zmm3
	vpmovzxbw	%ymm5, %zmm1            # zmm1 = ymm5[0],zero,ymm5[1],zero,ymm5[2],zero,ymm5[3],zero,ymm5[4],zero,ymm5[5],zero,ymm5[6],zero,ymm5[7],zero,ymm5[8],zero,ymm5[9],zero,ymm5[10],zero,ymm5[11],zero,ymm5[12],zero,ymm5[13],zero,ymm5[14],zero,ymm5[15],zero,ymm5[16],zero,ymm5[17],zero,ymm5[18],zero,ymm5[19],zero,ymm5[20],zero,ymm5[21],zero,ymm5[22],zero,ymm5[23],zero,ymm5[24],zero,ymm5[25],zero,ymm5[26],zero,ymm5[27],zero,ymm5[28],zero,ymm5[29],zero,ymm5[30],zero,ymm5[31],zero
	vpaddw	%zmm21, %zmm21, %zmm4
	vpaddw	%zmm28, %zmm28, %zmm5
	vpaddw	%zmm5, %zmm4, %zmm4
	vpsllw	$2, %zmm1, %zmm5
	vpaddw	%zmm0, %zmm27, %zmm0
	vpaddw	%zmm4, %zmm0, %zmm0
	vpaddw	%zmm5, %zmm0, %zmm4
	vpcmpltuw	%zmm4, %zmm3, %k1
	vpsubw	%zmm4, %zmm3, %zmm0
	vpsubw	%zmm3, %zmm4, %zmm0 {%k1}
	vpmovzxbw	%ymm2, %zmm2            # zmm2 = ymm2[0],zero,ymm2[1],zero,ymm2[2],zero,ymm2[3],zero,ymm2[4],zero,ymm2[5],zero,ymm2[6],zero,ymm2[7],zero,ymm2[8],zero,ymm2[9],zero,ymm2[10],zero,ymm2[11],zero,ymm2[12],zero,ymm2[13],zero,ymm2[14],zero,ymm2[15],zero,ymm2[16],zero,ymm2[17],zero,ymm2[18],zero,ymm2[19],zero,ymm2[20],zero,ymm2[21],zero,ymm2[22],zero,ymm2[23],zero,ymm2[24],zero,ymm2[25],zero,ymm2[26],zero,ymm2[27],zero,ymm2[28],zero,ymm2[29],zero,ymm2[30],zero,ymm2[31],zero
	vpaddd	1408(%rsp), %zmm18, %zmm3       # 64-byte Folded Reload
	vpaddd	1152(%rsp), %zmm18, %zmm4       # 64-byte Folded Reload
	vextracti128	$1, %ymm4, %xmm5
	vpextrd	$1, %xmm5, %r10d
	vpextrd	$2, %xmm5, 1024(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm5, 1216(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm5, %r15d
	vextracti32x4	$2, %zmm4, %xmm5
	vpextrd	$1, %xmm5, %r14d
	vpextrd	$2, %xmm5, %r12d
	vmovd	%xmm5, %r13d
	vpextrd	$3, %xmm5, 1536(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %edi
	movslq	%esi, %rsi
	movslq	%edi, %rdi
	vpextrd	$2, %xmm3, %ebx
	vpextrd	$3, %xmm3, %ebp
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm3, %xmm5
	vmovd	%xmm5, %ecx
	vpextrd	$1, %xmm5, %edx
	vpextrd	$2, %xmm5, %r8d
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm5, %esi
	movslq	%ecx, %rcx
	vextracti32x4	$2, %zmm3, %xmm5
	vpinsrb	$1, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm5, %edi
	vpinsrb	$2, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$2, %xmm5, %ebx
	vpinsrb	$3, (%rax,%rbp), %xmm6, %xmm6
	vmovd	%xmm5, %ebp
	vpinsrb	$4, (%rax,%rcx), %xmm6, %xmm6
	vpextrd	$3, %xmm5, %ecx
	vpaddw	%zmm2, %zmm2, %zmm5
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm6, %xmm6
	vmovd	%xmm4, %edx
	movslq	%edx, %r11
	movslq	%r8d, %rdx
	vpinsrb	$6, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	movq	%rdx, 1472(%rsp)                # 8-byte Spill
	movslq	%esi, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rdx
	movq	%rdx, 1664(%rsp)                # 8-byte Spill
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vpextrd	$3, %xmm4, %ebp
	movslq	%ebp, %rdx
	movq	%rdx, 960(%rsp)                 # 8-byte Spill
	movslq	%r15d, %r9
	movslq	%r10d, %r15
	movslq	1024(%rsp), %r10                # 4-byte Folded Reload
	movslq	1216(%rsp), %r8                 # 4-byte Folded Reload
	movslq	%r13d, %r13
	movslq	%r14d, %r14
	movslq	%r12d, %r12
	vextracti32x4	$3, %zmm4, %xmm4
	movslq	%edi, %rdi
	vpinsrb	$9, (%rax,%rdi), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %edi
	movslq	1536(%rsp), %rdx                # 4-byte Folded Reload
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vextracti32x4	$3, %zmm3, %xmm3
	vpinsrb	$10, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$11, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$12, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	movslq	%ecx, %rcx
	vpextrd	$2, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm6
	movslq	%edi, %rdi
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm6, %xmm3
	vpextrd	$2, %xmm4, %ebx
	movzbl	(%rax,%r11), %esi
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm4, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rsi
	movq	1472(%rsp), %r11                # 8-byte Reload
	vpinsrb	$1, (%rax,%r11), %xmm6, %xmm4
	movq	1664(%rsp), %rbp                # 8-byte Reload
	vpinsrb	$2, (%rax,%rbp), %xmm4, %xmm4
	movq	960(%rsp), %rbp                 # 8-byte Reload
	vpinsrb	$3, (%rax,%rbp), %xmm4, %xmm4
	vpinsrb	$4, (%rax,%r9), %xmm4, %xmm4
	vpinsrb	$5, (%rax,%r15), %xmm4, %xmm4
	vpinsrb	$6, (%rax,%r10), %xmm4, %xmm4
	vpinsrb	$7, (%rax,%r8), %xmm4, %xmm4
	vpinsrb	$8, (%rax,%r13), %xmm4, %xmm4
	vpinsrb	$9, (%rax,%r14), %xmm4, %xmm4
	vpinsrb	$10, (%rax,%r12), %xmm4, %xmm4
	vpinsrb	$11, (%rax,%rdx), %xmm4, %xmm4
	vpinsrb	$12, (%rax,%rcx), %xmm4, %xmm4
	vpinsrb	$13, (%rax,%rdi), %xmm4, %xmm4
	vpinsrb	$14, (%rax,%rbx), %xmm4, %xmm4
	vpinsrb	$15, (%rax,%rsi), %xmm4, %xmm4
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpmovzxbw	%ymm3, %zmm4            # zmm4 = ymm3[0],zero,ymm3[1],zero,ymm3[2],zero,ymm3[3],zero,ymm3[4],zero,ymm3[5],zero,ymm3[6],zero,ymm3[7],zero,ymm3[8],zero,ymm3[9],zero,ymm3[10],zero,ymm3[11],zero,ymm3[12],zero,ymm3[13],zero,ymm3[14],zero,ymm3[15],zero,ymm3[16],zero,ymm3[17],zero,ymm3[18],zero,ymm3[19],zero,ymm3[20],zero,ymm3[21],zero,ymm3[22],zero,ymm3[23],zero,ymm3[24],zero,ymm3[25],zero,ymm3[26],zero,ymm3[27],zero,ymm3[28],zero,ymm3[29],zero,ymm3[30],zero,ymm3[31],zero
	vpmovzxbw	%ymm30, %zmm3           # zmm3 = ymm30[0],zero,ymm30[1],zero,ymm30[2],zero,ymm30[3],zero,ymm30[4],zero,ymm30[5],zero,ymm30[6],zero,ymm30[7],zero,ymm30[8],zero,ymm30[9],zero,ymm30[10],zero,ymm30[11],zero,ymm30[12],zero,ymm30[13],zero,ymm30[14],zero,ymm30[15],zero,ymm30[16],zero,ymm30[17],zero,ymm30[18],zero,ymm30[19],zero,ymm30[20],zero,ymm30[21],zero,ymm30[22],zero,ymm30[23],zero,ymm30[24],zero,ymm30[25],zero,ymm30[26],zero,ymm30[27],zero,ymm30[28],zero,ymm30[29],zero,ymm30[30],zero,ymm30[31],zero
	vpmovzxbw	%ymm16, %zmm20          # zmm20 = ymm16[0],zero,ymm16[1],zero,ymm16[2],zero,ymm16[3],zero,ymm16[4],zero,ymm16[5],zero,ymm16[6],zero,ymm16[7],zero,ymm16[8],zero,ymm16[9],zero,ymm16[10],zero,ymm16[11],zero,ymm16[12],zero,ymm16[13],zero,ymm16[14],zero,ymm16[15],zero,ymm16[16],zero,ymm16[17],zero,ymm16[18],zero,ymm16[19],zero,ymm16[20],zero,ymm16[21],zero,ymm16[22],zero,ymm16[23],zero,ymm16[24],zero,ymm16[25],zero,ymm16[26],zero,ymm16[27],zero,ymm16[28],zero,ymm16[29],zero,ymm16[30],zero,ymm16[31],zero
	vpaddw	%zmm3, %zmm19, %zmm6
	vpaddw	%zmm6, %zmm5, %zmm5
	vpaddw	%zmm4, %zmm20, %zmm4
	vpaddw	%zmm4, %zmm5, %zmm19
	vpaddd	1088(%rsp), %zmm18, %zmm4       # 64-byte Folded Reload
	vpaddd	1280(%rsp), %zmm18, %zmm16      # 64-byte Folded Reload
	vpextrd	$1, %xmm16, 1024(%rsp)          # 4-byte Folded Spill
	vpextrd	$2, %xmm16, 1216(%rsp)          # 4-byte Folded Spill
	vpextrd	$3, %xmm16, 1536(%rsp)          # 4-byte Folded Spill
	vextracti32x4	$1, %ymm16, %xmm5
	vmovd	%xmm5, %r14d
	vpextrd	$1, %xmm5, %r12d
	vpextrd	$2, %xmm5, %r15d
	vpextrd	$3, %xmm5, %r11d
	vextracti32x4	$2, %zmm16, %xmm5
	vpextrd	$1, %xmm5, %r13d
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rsi
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %r8
	vpextrd	$2, %xmm4, %edi
	movslq	%edi, %rdi
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm4, %xmm6
	vmovd	%xmm6, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm6, %edx
	vpextrd	$2, %xmm6, %ecx
	movzbl	(%rax,%rsi), %esi
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm6, %esi
	vextracti32x4	$2, %zmm4, %xmm6
	vpinsrb	$1, (%rax,%r8), %xmm7, %xmm7
	vpextrd	$1, %xmm6, %r8d
	vpinsrb	$2, (%rax,%rdi), %xmm7, %xmm7
	vpextrd	$2, %xmm6, %edi
	vpinsrb	$3, (%rax,%rbx), %xmm7, %xmm7
	vpextrd	$3, %xmm6, %ebx
	vpinsrb	$4, (%rax,%rbp), %xmm7, %xmm7
	vmovd	%xmm6, %ebp
	movslq	%edx, %rdx
	vpinsrb	$5, (%rax,%rdx), %xmm7, %xmm6
	vpextrd	$2, %xmm5, %r10d
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vpinsrb	$6, (%rax,%rcx), %xmm6, %xmm6
	vpinsrb	$7, (%rax,%rsi), %xmm6, %xmm6
	vmovd	%xmm16, %ecx
	vpextrd	$3, %xmm5, %r9d
	movslq	%ebp, %rbp
	vpinsrb	$8, (%rax,%rbp), %xmm6, %xmm6
	vextracti32x4	$3, %zmm16, %xmm7
	vpextrd	$1, %xmm7, %ebp
	movslq	%r8d, %rdx
	movl	416(%rsp), %r8d                 # 4-byte Reload
	vpinsrb	$9, (%rax,%rdx), %xmm6, %xmm6
	vpextrd	$2, %xmm7, %edx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$10, (%rax,%rdi), %xmm6, %xmm6
	vextracti32x4	$3, %zmm4, %xmm4
	vmovd	%xmm4, %edi
	vpinsrb	$11, (%rax,%rbx), %xmm6, %xmm6
	vpextrd	$1, %xmm4, %ebx
	movslq	%edi, %rdi
	movslq	%ebx, %rbx
	vpinsrb	$12, (%rax,%rdi), %xmm6, %xmm6
	vpinsrb	$13, (%rax,%rbx), %xmm6, %xmm6
	vmovd	%xmm5, %edi
	vpextrd	$2, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$14, (%rax,%rbx), %xmm6, %xmm5
	movslq	%ecx, %rcx
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vpinsrb	$15, (%rax,%rbx), %xmm5, %xmm4
	vmovd	%xmm7, %ebx
	movzbl	(%rax,%rcx), %ecx
	vmovd	%ecx, %xmm5
	vpextrd	$3, %xmm7, %ecx
	movslq	1024(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$1, (%rax,%rsi), %xmm5, %xmm5
	movslq	1216(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$2, (%rax,%rsi), %xmm5, %xmm5
	movslq	1536(%rsp), %rsi                # 4-byte Folded Reload
	vpinsrb	$3, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r14d, %rsi
	vpinsrb	$4, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r12d, %rsi
	vpinsrb	$5, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r15d, %rsi
	vpinsrb	$6, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r11d, %rsi
	vpinsrb	$7, (%rax,%rsi), %xmm5, %xmm5
	movslq	%edi, %rsi
	vpinsrb	$8, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r13d, %rsi
	vpinsrb	$9, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r10d, %rsi
	vpinsrb	$10, (%rax,%rsi), %xmm5, %xmm5
	movslq	%r9d, %rsi
	movl	$2, %r9d
	vpinsrb	$11, (%rax,%rsi), %xmm5, %xmm5
	movslq	%ebx, %rsi
	vpinsrb	$12, (%rax,%rsi), %xmm5, %xmm5
	movslq	%ebp, %rsi
	movq	136(%rsp), %rdi                 # 8-byte Reload
	vpinsrb	$13, (%rax,%rsi), %xmm5, %xmm5
	movslq	%edx, %rdx
	vpinsrb	$14, (%rax,%rdx), %xmm5, %xmm5
	movslq	%ecx, %rcx
	vpinsrb	$15, (%rax,%rcx), %xmm5, %xmm5
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpaddw	%zmm20, %zmm20, %zmm5
	vpmovzxbw	%ymm4, %zmm4            # zmm4 = ymm4[0],zero,ymm4[1],zero,ymm4[2],zero,ymm4[3],zero,ymm4[4],zero,ymm4[5],zero,ymm4[6],zero,ymm4[7],zero,ymm4[8],zero,ymm4[9],zero,ymm4[10],zero,ymm4[11],zero,ymm4[12],zero,ymm4[13],zero,ymm4[14],zero,ymm4[15],zero,ymm4[16],zero,ymm4[17],zero,ymm4[18],zero,ymm4[19],zero,ymm4[20],zero,ymm4[21],zero,ymm4[22],zero,ymm4[23],zero,ymm4[24],zero,ymm4[25],zero,ymm4[26],zero,ymm4[27],zero,ymm4[28],zero,ymm4[29],zero,ymm4[30],zero,ymm4[31],zero
	vpaddw	%zmm4, %zmm5, %zmm4
	vpaddw	%zmm1, %zmm2, %zmm1
	vpaddw	%zmm3, %zmm1, %zmm1
	vpaddw	%zmm4, %zmm1, %zmm1
	vpcmpltuw	%zmm1, %zmm19, %k1
	vpsubw	%zmm1, %zmm19, %zmm2
	vpsubw	%zmm19, %zmm1, %zmm2 {%k1}
	vpaddw	%zmm14, %zmm15, %zmm1
	vpaddw	%zmm0, %zmm2, %zmm0
	vpmovuswb	%zmm1, %ymm1
	vpmovuswb	%zmm0, %ymm0
	vpaddb	%ymm1, %ymm0, %ymm0
	movq	2176(%rsp), %rcx                # 8-byte Reload
	vmovdqu	%ymm0, (%rcx)
	addq	200(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	movq	2048(%rsp), %rcx                # 8-byte Reload
	incq	%rcx
	jne	.LBB214_65
# %bb.66:                               # %"end for output.s0.y.yi234.us"
                                        #   in Loop: Header=BB214_64 Depth=2
	movq	640(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	672(%rsp), %ecx                 # 4-byte Reload
	addl	$32, %ecx
	cmpq	1400(%rsp), %rdx                # 8-byte Folded Reload
	jne	.LBB214_64
# %bb.67:                               # %"end for output.s0.x.x228.loopexit.us"
                                        #   in Loop: Header=BB214_63 Depth=1
	movq	256(%rsp), %rbp                 # 8-byte Reload
	incq	%rbp
	movq	616(%rsp), %rdx                 # 8-byte Reload
	addl	$16, %edx
	movq	1984(%rsp), %rcx                # 8-byte Reload
	addq	$16, %rcx
	cmpq	480(%rsp), %rbp                 # 8-byte Folded Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB214_63
.LBB214_68:                             # %destructor_block
	xorl	%eax, %eax
	addq	$3848, %rsp                     # imm = 0xF08
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end214:
	.size	sobel5x5, .Lfunc_end214-sobel5x5
                                        # -- End function
	.section	.text.sobel5x5_argv,"ax",@progbits
	.globl	sobel5x5_argv                   # -- Begin function sobel5x5_argv
	.p2align	4, 0x90
	.type	sobel5x5_argv,@function
sobel5x5_argv:                          # @sobel5x5_argv
# %bb.0:                                # %entry
	pushq	%rax
	movq	(%rdi), %rax
	movq	8(%rdi), %rsi
	movq	%rax, %rdi
	callq	sobel5x5@PLT
	xorl	%eax, %eax
	popq	%rcx
	retq
.Lfunc_end215:
	.size	sobel5x5_argv, .Lfunc_end215-sobel5x5_argv
                                        # -- End function
	.section	.text.sobel5x5_metadata,"ax",@progbits
	.globl	sobel5x5_metadata               # -- Begin function sobel5x5_metadata
	.p2align	4, 0x90
	.type	sobel5x5_metadata,@function
sobel5x5_metadata:                      # @sobel5x5_metadata
# %bb.0:                                # %entry
	leaq	.Lsobel5x5_metadata_storage(%rip), %rax
	retq
.Lfunc_end216:
	.size	sobel5x5_metadata, .Lfunc_end216-sobel5x5_metadata
                                        # -- End function
	.type	_ZN6Halide7Runtime8Internal13custom_mallocE,@object # @_ZN6Halide7Runtime8Internal13custom_mallocE
	.data
	.weak	_ZN6Halide7Runtime8Internal13custom_mallocE
	.p2align	3
_ZN6Halide7Runtime8Internal13custom_mallocE:
	.quad	halide_default_malloc
	.size	_ZN6Halide7Runtime8Internal13custom_mallocE, 8

	.type	_ZN6Halide7Runtime8Internal11custom_freeE,@object # @_ZN6Halide7Runtime8Internal11custom_freeE
	.weak	_ZN6Halide7Runtime8Internal11custom_freeE
	.p2align	3
_ZN6Halide7Runtime8Internal11custom_freeE:
	.quad	halide_default_free
	.size	_ZN6Halide7Runtime8Internal11custom_freeE, 8

	.type	_ZN6Halide7Runtime8Internal13error_handlerE,@object # @_ZN6Halide7Runtime8Internal13error_handlerE
	.weak	_ZN6Halide7Runtime8Internal13error_handlerE
	.p2align	3
_ZN6Halide7Runtime8Internal13error_handlerE:
	.quad	halide_default_error
	.size	_ZN6Halide7Runtime8Internal13error_handlerE, 8

	.type	.L.str,@object                  # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"Error: "
	.size	.L.str, 8

	.type	_ZN6Halide7Runtime8Internal12custom_printE,@object # @_ZN6Halide7Runtime8Internal12custom_printE
	.data
	.weak	_ZN6Halide7Runtime8Internal12custom_printE
	.p2align	3
_ZN6Halide7Runtime8Internal12custom_printE:
	.quad	halide_default_print
	.size	_ZN6Halide7Runtime8Internal12custom_printE, 8

	.type	halide_reference_clock_inited,@object # @halide_reference_clock_inited
	.bss
	.weak	halide_reference_clock_inited
halide_reference_clock_inited:
	.byte	0                               # 0x0
	.size	halide_reference_clock_inited, 1

	.type	halide_reference_clock,@object  # @halide_reference_clock
	.weak	halide_reference_clock
	.p2align	3
halide_reference_clock:
	.zero	16
	.size	halide_reference_clock, 16

	.type	_ZN6Halide7Runtime8Internal15Synchronization5tableE,@object # @_ZN6Halide7Runtime8Internal15Synchronization5tableE
	.weak	_ZN6Halide7Runtime8Internal15Synchronization5tableE
	.p2align	3
_ZN6Halide7Runtime8Internal15Synchronization5tableE:
	.zero	24576
	.size	_ZN6Halide7Runtime8Internal15Synchronization5tableE, 24576

	.type	_ZN6Halide7Runtime8Internal10work_queueE,@object # @_ZN6Halide7Runtime8Internal10work_queueE
	.weak	_ZN6Halide7Runtime8Internal10work_queueE
	.p2align	3
_ZN6Halide7Runtime8Internal10work_queueE:
	.zero	8
	.long	0                               # 0x0
	.long	0                               # 0x0
	.quad	0
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	0                               # 0x0
	.zero	4
	.zero	8
	.zero	8
	.zero	8
	.long	0                               # 0x0
	.long	0                               # 0x0
	.zero	2048
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.zero	2
	.long	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal10work_queueE, 2128

	.type	_ZN6Halide7Runtime8Internal14custom_do_taskE,@object # @_ZN6Halide7Runtime8Internal14custom_do_taskE
	.data
	.weak	_ZN6Halide7Runtime8Internal14custom_do_taskE
	.p2align	3
_ZN6Halide7Runtime8Internal14custom_do_taskE:
	.quad	halide_default_do_task
	.size	_ZN6Halide7Runtime8Internal14custom_do_taskE, 8

	.type	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE,@object # @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE
	.weak	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE
	.p2align	3
_ZN6Halide7Runtime8Internal19custom_do_loop_taskE:
	.quad	halide_default_do_loop_task
	.size	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, 8

	.type	_ZN6Halide7Runtime8Internal17custom_do_par_forE,@object # @_ZN6Halide7Runtime8Internal17custom_do_par_forE
	.weak	_ZN6Halide7Runtime8Internal17custom_do_par_forE
	.p2align	3
_ZN6Halide7Runtime8Internal17custom_do_par_forE:
	.quad	halide_default_do_par_for
	.size	_ZN6Halide7Runtime8Internal17custom_do_par_forE, 8

	.type	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE,@object # @_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE
	.section	.data.rel.ro._ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE,"aGw",@progbits,_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE,comdat
	.weak	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE
	.p2align	3
_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE:
	.quad	0
	.quad	0
	.quad	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
	.quad	_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.size	_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE, 48

	.type	.L.str.5,@object                # @.str.5
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.5:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/synchronization_common.h:386 halide_abort_if_false() failed: next != nullptr\n"
	.size	.L.str.5, 160

	.type	_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE,@object # @_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE
	.section	.data.rel.ro._ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE,"aGw",@progbits,_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE,comdat
	.weak	_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE
	.p2align	3
_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE:
	.quad	0
	.quad	0
	.quad	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE
	.quad	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv
	.quad	_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.size	_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE, 48

	.type	.L.str.5.6,@object              # @.str.5.6
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.5.6:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/synchronization_common.h:994 halide_abort_if_false() failed: val & 0x1\n"
	.size	.L.str.5.6, 154

	.type	_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE,@object # @_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE
	.section	.data.rel.ro._ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE,"aGw",@progbits,_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE,comdat
	.weak	_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE
	.p2align	3
_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE:
	.quad	0
	.quad	0
	.quad	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib
	.quad	_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.size	_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE, 48

	.type	.L.str.6,@object                # @.str.6
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.6:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/thread_pool_common.h:155 halide_abort_if_false() failed: bytes == limit && \"Logic error in thread pool work queue initialization.\\n\"\n"
	.size	.L.str.6, 216

	.type	.L.str.3,@object                # @.str.3
.L.str.3:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/thread_pool_common.h:527 halide_abort_if_false() failed: (min_threads <= ((task_parent->task.min_threads * task_parent->active_workers) - task_parent->threads_reserved)) && \"Logic error: thread over commit.\\n\"\n"
	.size	.L.str.3, 293

	.type	.L.str.1,@object                # @.str.1
.L.str.1:
	.asciz	"HL_NUM_THREADS"
	.size	.L.str.1, 15

	.type	.L.str.2,@object                # @.str.2
.L.str.2:
	.asciz	"HL_NUMTHREADS"
	.size	.L.str.2, 14

	.type	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE,@object # @_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE
	.data
	.weak	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE
	.p2align	3
_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE:
	.quad	halide_default_do_parallel_tasks
	.size	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE, 8

	.type	_ZN6Halide7Runtime8Internal21custom_semaphore_initE,@object # @_ZN6Halide7Runtime8Internal21custom_semaphore_initE
	.weak	_ZN6Halide7Runtime8Internal21custom_semaphore_initE
	.p2align	3
_ZN6Halide7Runtime8Internal21custom_semaphore_initE:
	.quad	halide_default_semaphore_init
	.size	_ZN6Halide7Runtime8Internal21custom_semaphore_initE, 8

	.type	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE,@object # @_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE
	.weak	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE
	.p2align	3
_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE:
	.quad	halide_default_semaphore_try_acquire
	.size	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE, 8

	.type	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE,@object # @_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE
	.weak	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE
	.p2align	3
_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE:
	.quad	halide_default_semaphore_release
	.size	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE, 8

	.section	.fini_array,"aw",@fini_array
	.p2align	3
	.quad	halide_thread_pool_cleanup
	.quad	halide_trace_cleanup
	.quad	halide_cache_cleanup
	.quad	halide_profiler_shutdown
	.type	_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE,@object # @_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE
	.section	.data.rel.ro._ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE,"aGw",@progbits,_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE,comdat
	.weak	_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE
	.p2align	3
_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE:
	.quad	0
	.quad	0
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv
	.quad	_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib
	.quad	_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb
	.size	_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE, 48

	.type	.L.str.4,@object                # @.str.4
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.4:
	.asciz	"halide_set_num_threads: must be >= 0."
	.size	.L.str.4, 38

	.type	_ZN6Halide7Runtime8Internal17custom_get_symbolE,@object # @_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.data
	.weak	_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.p2align	3
_ZN6Halide7Runtime8Internal17custom_get_symbolE:
	.quad	halide_default_get_symbol
	.size	_ZN6Halide7Runtime8Internal17custom_get_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal19custom_load_libraryE,@object # @_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.weak	_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.p2align	3
_ZN6Halide7Runtime8Internal19custom_load_libraryE:
	.quad	halide_default_load_library
	.size	_ZN6Halide7Runtime8Internal19custom_load_libraryE, 8

	.type	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE,@object # @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.weak	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.p2align	3
_ZN6Halide7Runtime8Internal25custom_get_library_symbolE:
	.quad	halide_default_get_library_symbol
	.size	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE,@object # @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.p2align	2
_ZN6Halide7Runtime8Internal17halide_gpu_deviceE:
	.long	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE:
	.byte	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, 1

	.type	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE:
	.byte	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, 1

	.type	.L.str.8,@object                # @.str.8
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.8:
	.asciz	"HL_GPU_DEVICE"
	.size	.L.str.8, 14

	.type	_ZN6Halide7Runtime8Internal19halide_trace_bufferE,@object # @_ZN6Halide7Runtime8Internal19halide_trace_bufferE
	.bss
	.weak	_ZN6Halide7Runtime8Internal19halide_trace_bufferE
	.p2align	3
_ZN6Halide7Runtime8Internal19halide_trace_bufferE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal19halide_trace_bufferE, 8

	.type	_ZN6Halide7Runtime8Internal17halide_trace_fileE,@object # @_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.data
	.weak	_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.p2align	2
_ZN6Halide7Runtime8Internal17halide_trace_fileE:
	.long	4294967295                      # 0xffffffff
	.size	_ZN6Halide7Runtime8Internal17halide_trace_fileE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
_ZN6Halide7Runtime8Internal22halide_trace_file_lockE:
	.byte	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, 1

	.type	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE:
	.byte	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE,@object # @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
	.weak	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
	.p2align	3
_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, 8

	.type	_ZN6Halide7Runtime8Internal19halide_custom_traceE,@object # @_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.data
	.weak	_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.p2align	3
_ZN6Halide7Runtime8Internal19halide_custom_traceE:
	.quad	halide_default_trace
	.size	_ZN6Halide7Runtime8Internal19halide_custom_traceE, 8

	.type	_ZZ20halide_default_traceE3ids,@object # @_ZZ20halide_default_traceE3ids
	.p2align	2
_ZZ20halide_default_traceE3ids:
	.long	1                               # 0x1
	.size	_ZZ20halide_default_traceE3ids, 4

	.type	.L.str.32,@object               # @.str.32
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.32:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/tracing.cpp:115 halide_abort_if_false() failed: success && \"Could not write to trace file\"\n"
	.size	.L.str.32, 174

	.type	.L.str.31,@object               # @.str.31
.L.str.31:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/tracing.cpp:87 halide_abort_if_false() failed: size <= buffer_size\n"
	.size	.L.str.31, 150

	.type	.L.str.1.10,@object             # @.str.1.10
.L.str.1.10:
	.zero	1
	.size	.L.str.1.10, 1

	.type	.L.str.2.11,@object             # @.str.2.11
.L.str.2.11:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/tracing.cpp:218 halide_abort_if_false() failed: print_bits <= 64 && \"Tracing bad type\"\n"
	.size	.L.str.2.11, 170

	.type	.L__const.halide_default_trace.event_types,@object # @__const.halide_default_trace.event_types
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__const.halide_default_trace.event_types:
	.quad	.L.str.3.12
	.quad	.L.str.4.13
	.quad	.L.str.5.14
	.quad	.L.str.6.15
	.quad	.L.str.7
	.quad	.L.str.8.16
	.quad	.L.str.9.17
	.quad	.L.str.10
	.quad	.L.str.11
	.quad	.L.str.12
	.quad	.L.str.13
	.size	.L__const.halide_default_trace.event_types, 88

	.type	.L.str.17,@object               # @.str.17
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.17:
	.asciz	"<"
	.size	.L.str.17, 2

	.type	.L.str.20,@object               # @.str.20
.L.str.20:
	.asciz	">)"
	.size	.L.str.20, 3

	.type	.L.str.18,@object               # @.str.18
.L.str.18:
	.asciz	">, <"
	.size	.L.str.18, 5

	.type	.L.str.22,@object               # @.str.22
.L.str.22:
	.asciz	" = <"
	.size	.L.str.22, 5

	.type	.L.str.23,@object               # @.str.23
.L.str.23:
	.asciz	" = "
	.size	.L.str.23, 4

	.type	.L.str.24,@object               # @.str.24
.L.str.24:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/tracing.cpp:287 halide_abort_if_false() failed: print_bits >= 16 && \"Tracing a bad type\"\n"
	.size	.L.str.24, 172

	.type	.L.str.25,@object               # @.str.25
.L.str.25:
	.asciz	">"
	.size	.L.str.25, 2

	.type	.L.str.26,@object               # @.str.26
.L.str.26:
	.asciz	" tag = \""
	.size	.L.str.26, 9

	.type	.L.str.27,@object               # @.str.27
.L.str.27:
	.asciz	"\""
	.size	.L.str.27, 2

	.type	.L.str.3.12,@object             # @.str.3.12
.L.str.3.12:
	.asciz	"Load"
	.size	.L.str.3.12, 5

	.type	.L.str.4.13,@object             # @.str.4.13
.L.str.4.13:
	.asciz	"Store"
	.size	.L.str.4.13, 6

	.type	.L.str.5.14,@object             # @.str.5.14
.L.str.5.14:
	.asciz	"Begin realization"
	.size	.L.str.5.14, 18

	.type	.L.str.6.15,@object             # @.str.6.15
.L.str.6.15:
	.asciz	"End realization"
	.size	.L.str.6.15, 16

	.type	.L.str.7,@object                # @.str.7
.L.str.7:
	.asciz	"Produce"
	.size	.L.str.7, 8

	.type	.L.str.8.16,@object             # @.str.8.16
.L.str.8.16:
	.asciz	"End produce"
	.size	.L.str.8.16, 12

	.type	.L.str.9.17,@object             # @.str.9.17
.L.str.9.17:
	.asciz	"Consume"
	.size	.L.str.9.17, 8

	.type	.L.str.10,@object               # @.str.10
.L.str.10:
	.asciz	"End consume"
	.size	.L.str.10, 12

	.type	.L.str.11,@object               # @.str.11
.L.str.11:
	.asciz	"Begin pipeline"
	.size	.L.str.11, 15

	.type	.L.str.12,@object               # @.str.12
.L.str.12:
	.asciz	"End pipeline"
	.size	.L.str.12, 13

	.type	.L.str.13,@object               # @.str.13
.L.str.13:
	.asciz	"Tag"
	.size	.L.str.13, 4

	.type	.L.str.28,@object               # @.str.28
.L.str.28:
	.asciz	"HL_TRACE_FILE"
	.size	.L.str.28, 14

	.type	.L.str.29,@object               # @.str.29
.L.str.29:
	.asciz	"ab"
	.size	.L.str.29, 3

	.type	.L.str.30,@object               # @.str.30
.L.str.30:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/tracing.cpp:351 halide_abort_if_false() failed: file && \"Failed to open trace file\\n\"\n"
	.size	.L.str.30, 169

	.type	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE,@object # @_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.data
	.weak	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.p2align	1
_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE:
	.short	3                               # 0x3
	.short	3                               # 0x3
	.short	1                               # 0x1
	.short	2                               # 0x2
	.short	1                               # 0x1
	.short	2                               # 0x2
	.short	1                               # 0x1
	.short	2                               # 0x2
	.short	1                               # 0x1
	.short	2                               # 0x2
	.size	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE, 20

	.type	_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE,@object # @_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE
	.weak	_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE
_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE:
	.ascii	"\007\006\t\b\013\n\r\f\017\016"
	.size	_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE, 10

	.type	_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE,@object # @_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE
	.weak	_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE
_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE:
	.ascii	"\007\t\002\001\004\003\006\005\r\f"
	.size	_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE, 10

	.type	.L.str.34,@object               # @.str.34
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.34:
	.asciz	"Bounds query buffer passed to halide_debug_to_file"
	.size	.L.str.34, 51

	.type	.L.str.1.35,@object             # @.str.1.35
.L.str.1.35:
	.asciz	"Can't debug_to_file a Func with more than four dimensions\n"
	.size	.L.str.1.35, 59

	.type	.L.str.2.36,@object             # @.str.2.36
.L.str.2.36:
	.asciz	"wb"
	.size	.L.str.2.36, 3

	.type	.L.str.3.37,@object             # @.str.3.37
.L.str.3.37:
	.asciz	".tiff"
	.size	.L.str.3.37, 6

	.type	.L.str.4.38,@object             # @.str.4.38
.L.str.4.38:
	.asciz	".tif"
	.size	.L.str.4.38, 5

	.type	.L.str.5.39,@object             # @.str.5.39
.L.str.5.39:
	.asciz	".mat"
	.size	.L.str.5.39, 5

	.type	.L__const.halide_debug_to_file.header,@object # @__const.halide_debug_to_file.header
	.section	.rodata,"a",@progbits
.L__const.halide_debug_to_file.header:
	.asciz	"MATLAB 5.0 MAT-file, produced by Halide                                                                                     \000\001IM"
	.size	.L__const.halide_debug_to_file.header, 129

	.type	.L.str.6.40,@object             # @.str.6.40
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.6.40:
	.asciz	"Can't debug_to_file to a .mat file greater than 4GB\n"
	.size	.L.str.6.40, 53

	.type	_ZN6Halide7Runtime8Internal16memoization_lockE,@object # @_ZN6Halide7Runtime8Internal16memoization_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal16memoization_lockE
	.p2align	3
_ZN6Halide7Runtime8Internal16memoization_lockE:
	.zero	8
	.size	_ZN6Halide7Runtime8Internal16memoization_lockE, 8

	.type	_ZN6Halide7Runtime8Internal13cache_entriesE,@object # @_ZN6Halide7Runtime8Internal13cache_entriesE
	.weak	_ZN6Halide7Runtime8Internal13cache_entriesE
	.p2align	3
_ZN6Halide7Runtime8Internal13cache_entriesE:
	.zero	2048
	.size	_ZN6Halide7Runtime8Internal13cache_entriesE, 2048

	.type	_ZN6Halide7Runtime8Internal18most_recently_usedE,@object # @_ZN6Halide7Runtime8Internal18most_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal18most_recently_usedE
	.p2align	3
_ZN6Halide7Runtime8Internal18most_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal18most_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal19least_recently_usedE,@object # @_ZN6Halide7Runtime8Internal19least_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal19least_recently_usedE
	.p2align	3
_ZN6Halide7Runtime8Internal19least_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal19least_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal14max_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.data
	.weak	_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.p2align	3
_ZN6Halide7Runtime8Internal14max_cache_sizeE:
	.quad	1048576                         # 0x100000
	.size	_ZN6Halide7Runtime8Internal14max_cache_sizeE, 8

	.type	_ZN6Halide7Runtime8Internal18current_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.bss
	.weak	_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.p2align	3
_ZN6Halide7Runtime8Internal18current_cache_sizeE:
	.quad	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal18current_cache_sizeE, 8

	.type	.L.str.2.42,@object             # @.str.2.42
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.2.42:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:284 halide_abort_if_false() failed: prev_hash_entry != nullptr\n"
	.size	.L.str.2.42, 156

	.type	.L.str.3.43,@object             # @.str.3.43
.L.str.3.43:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:373 halide_abort_if_false() failed: entry->more_recent != nullptr\n"
	.size	.L.str.3.43, 159

	.type	.L.str.4.44,@object             # @.str.4.44
.L.str.4.44:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:377 halide_abort_if_false() failed: least_recently_used == entry\n"
	.size	.L.str.4.44, 158

	.type	.L.str.5.45,@object             # @.str.5.45
.L.str.5.45:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:380 halide_abort_if_false() failed: entry->more_recent != nullptr\n"
	.size	.L.str.5.45, 159

	.type	.L.str.9.46,@object             # @.str.9.46
.L.str.9.46:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:472 halide_abort_if_false() failed: no_host_pointers_equal\n"
	.size	.L.str.9.46, 152

	.type	.L.str.12.47,@object            # @.str.12.47
.L.str.12.47:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/cache.cpp:550 halide_abort_if_false() failed: entry->in_use_count > 0\n"
	.size	.L.str.12.47, 153

	.type	.L.str.50,@object               # @.str.50
.L.str.50:
	.asciz	"<nullptr>"
	.size	.L.str.50, 10

	.type	.L.str.1.57,@object             # @.str.1.57
.L.str.1.57:
	.asciz	"-nan"
	.size	.L.str.1.57, 5

	.type	.L.str.2.58,@object             # @.str.2.58
.L.str.2.58:
	.asciz	"nan"
	.size	.L.str.2.58, 4

	.type	.L.str.3.59,@object             # @.str.3.59
.L.str.3.59:
	.asciz	"-inf"
	.size	.L.str.3.59, 5

	.type	.L.str.4.60,@object             # @.str.4.60
.L.str.4.60:
	.asciz	"inf"
	.size	.L.str.4.60, 4

	.type	.L.str.5.61,@object             # @.str.5.61
.L.str.5.61:
	.asciz	"-0.000000e+00"
	.size	.L.str.5.61, 14

	.type	.L.str.6.62,@object             # @.str.6.62
.L.str.6.62:
	.asciz	"0.000000e+00"
	.size	.L.str.6.62, 13

	.type	.L.str.7.63,@object             # @.str.7.63
.L.str.7.63:
	.asciz	"-0.000000"
	.size	.L.str.7.63, 10

	.type	.L.str.8.64,@object             # @.str.8.64
.L.str.8.64:
	.asciz	"0.000000"
	.size	.L.str.8.64, 9

	.type	.L.str.9.65,@object             # @.str.9.65
.L.str.9.65:
	.asciz	"-"
	.size	.L.str.9.65, 2

	.type	.L.str.11.67,@object            # @.str.11.67
.L.str.11.67:
	.asciz	"e+"
	.size	.L.str.11.67, 3

	.type	.L.str.12.68,@object            # @.str.12.68
.L.str.12.68:
	.asciz	"e-"
	.size	.L.str.12.68, 3

	.type	.L.str.13.71,@object            # @.str.13.71
.L.str.13.71:
	.asciz	"0123456789abcdef"
	.size	.L.str.13.71, 17

	.type	.L.str.18.72,@object            # @.str.18.72
.L.str.18.72:
	.asciz	"bad_type_code"
	.size	.L.str.18.72, 14

	.type	.L.str.17.73,@object            # @.str.17.73
.L.str.17.73:
	.asciz	"handle"
	.size	.L.str.17.73, 7

	.type	.L.str.16.74,@object            # @.str.16.74
.L.str.16.74:
	.asciz	"float"
	.size	.L.str.16.74, 6

	.type	.L.str.15.75,@object            # @.str.15.75
.L.str.15.75:
	.asciz	"uint"
	.size	.L.str.15.75, 5

	.type	.L.str.14.76,@object            # @.str.14.76
.L.str.14.76:
	.asciz	"int"
	.size	.L.str.14.76, 4

	.type	.L.str.19.77,@object            # @.str.19.77
.L.str.19.77:
	.asciz	"x"
	.size	.L.str.19.77, 2

	.type	.L.str.20.78,@object            # @.str.20.78
.L.str.20.78:
	.asciz	"nullptr"
	.size	.L.str.20.78, 8

	.type	.L.str.21.79,@object            # @.str.21.79
.L.str.21.79:
	.asciz	"buffer("
	.size	.L.str.21.79, 8

	.type	.L.str.23.82,@object            # @.str.23.82
.L.str.23.82:
	.asciz	", {"
	.size	.L.str.23.82, 4

	.type	.L.str.24.83,@object            # @.str.24.83
.L.str.24.83:
	.asciz	"}"
	.size	.L.str.24.83, 2

	.type	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE,@object # @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE
	.data
	.weak	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE
_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE:
	.byte	1                               # 0x1
	.size	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, 1

	.type	_ZN6Halide7Runtime8Internal21allocation_pools_lockE,@object # @_ZN6Halide7Runtime8Internal21allocation_pools_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal21allocation_pools_lockE
	.p2align	3
_ZN6Halide7Runtime8Internal21allocation_pools_lockE:
	.zero	8
	.size	_ZN6Halide7Runtime8Internal21allocation_pools_lockE, 8

	.type	_ZN6Halide7Runtime8Internal23device_allocation_poolsE,@object # @_ZN6Halide7Runtime8Internal23device_allocation_poolsE
	.weak	_ZN6Halide7Runtime8Internal23device_allocation_poolsE
	.p2align	3
_ZN6Halide7Runtime8Internal23device_allocation_poolsE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal23device_allocation_poolsE, 8

	.type	_ZN6Halide7Runtime8Internal17device_copy_mutexE,@object # @_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.weak	_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.p2align	3
_ZN6Halide7Runtime8Internal17device_copy_mutexE:
	.zero	8
	.size	_ZN6Halide7Runtime8Internal17device_copy_mutexE, 8

	.type	.L.str.6.88,@object             # @.str.6.88
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.6.88:
	.asciz	"halide_copy_to_host"
	.size	.L.str.6.88, 20

	.type	.L.str.7.89,@object             # @.str.7.89
.L.str.7.89:
	.asciz	"halide_copy_to_device"
	.size	.L.str.7.89, 22

	.type	.L.str.9.90,@object             # @.str.9.90
.L.str.9.90:
	.asciz	"halide_copy_to_device does not support switching interfaces\n"
	.size	.L.str.9.90, 61

	.type	.L.str.17.91,@object            # @.str.17.91
.L.str.17.91:
	.asciz	"halide_device_malloc"
	.size	.L.str.17.91, 21

	.type	.L.str.20.92,@object            # @.str.20.92
.L.str.20.92:
	.asciz	"halide_device_malloc doesn't support switching interfaces\n"
	.size	.L.str.20.92, 59

	.type	.L.str.16.93,@object            # @.str.16.93
.L.str.16.93:
	.asciz	"halide_device_sync"
	.size	.L.str.16.93, 19

	.type	.L.str.21.96,@object            # @.str.21.96
.L.str.21.96:
	.asciz	"halide_device_free"
	.size	.L.str.21.96, 19

	.type	.L.str.22.97,@object            # @.str.22.97
.L.str.22.97:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:252 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.22.97, 157

	.type	.L.str.23.98,@object            # @.str.23.98
.L.str.23.98:
	.asciz	"halide_device_and_host_malloc"
	.size	.L.str.23.98, 30

	.type	.L.str.25.99,@object            # @.str.25.99
.L.str.25.99:
	.asciz	"halide_device_and_host_malloc doesn't support switching interfaces\n"
	.size	.L.str.25.99, 68

	.type	.L.str.26.100,@object           # @.str.26.100
.L.str.26.100:
	.asciz	"allocating host and device memory failed\n"
	.size	.L.str.26.100, 42

	.type	.L.str.27.101,@object           # @.str.27.101
.L.str.27.101:
	.asciz	"halide_device_and_host_free"
	.size	.L.str.27.101, 28

	.type	.L.str.28.102,@object           # @.str.28.102
.L.str.28.102:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:317 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.28.102, 157

	.type	.L.str.29.103,@object           # @.str.29.103
.L.str.29.103:
	.asciz	"halide_default_device_and_host_malloc"
	.size	.L.str.29.103, 38

	.type	.L.str.30.104,@object           # @.str.30.104
.L.str.30.104:
	.asciz	"halide_default_device_and_host_free"
	.size	.L.str.30.104, 36

	.type	.L.str.31.105,@object           # @.str.31.105
.L.str.31.105:
	.asciz	"halide_device_wrap_native"
	.size	.L.str.31.105, 26

	.type	.L.str.32.106,@object           # @.str.32.106
.L.str.32.106:
	.asciz	"halide_device_wrap_native doesn't support switching interfaces\n"
	.size	.L.str.32.106, 64

	.type	.L.str.33.107,@object           # @.str.33.107
.L.str.33.107:
	.asciz	"halide_device_detach_native"
	.size	.L.str.33.107, 28

	.type	.L.str.34.108,@object           # @.str.34.108
.L.str.34.108:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:403 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.34.108, 157

	.type	.L.str.35,@object               # @.str.35
.L.str.35:
	.asciz	"halide_default_device_detach_native"
	.size	.L.str.35, 36

	.type	.L.str.41,@object               # @.str.41
.L.str.41:
	.asciz	"halide_buffer_copy does not support switching device interfaces"
	.size	.L.str.41, 64

	.type	.L.str.58,@object               # @.str.58
.L.str.58:
	.asciz	"device_interface does not support cropping\n"
	.size	.L.str.58, 44

	.type	.L.str.59,@object               # @.str.59
.L.str.59:
	.asciz	"device_interface does not support slicing\n"
	.size	.L.str.59, 43

	.type	.L.str.60,@object               # @.str.60
.L.str.60:
	.asciz	"destination buffer already has a device allocation\n"
	.size	.L.str.60, 52

	.type	.L.str.61,@object               # @.str.61
.L.str.61:
	.asciz	"src and dst must have identical dimensionality\n"
	.size	.L.str.61, 48

	.type	.L.str.64,@object               # @.str.64
.L.str.64:
	.asciz	"dst must have exactly one fewer dimension than src\n"
	.size	.L.str.64, 52

	.type	.L.str.111,@object              # @.str.111
.L.str.111:
	.asciz	"Bounds inference call to external stage "
	.size	.L.str.111, 41

	.type	.L.str.1.112,@object            # @.str.1.112
.L.str.1.112:
	.asciz	" returned non-zero value: "
	.size	.L.str.1.112, 27

	.type	.L.str.2.113,@object            # @.str.2.113
.L.str.2.113:
	.asciz	"Call to external stage "
	.size	.L.str.2.113, 24

	.type	.L.str.3.114,@object            # @.str.3.114
.L.str.3.114:
	.asciz	"Bounds given for "
	.size	.L.str.3.114, 18

	.type	.L.str.4.115,@object            # @.str.4.115
.L.str.4.115:
	.asciz	" in "
	.size	.L.str.4.115, 5

	.type	.L.str.5.116,@object            # @.str.5.116
.L.str.5.116:
	.asciz	" (from "
	.size	.L.str.5.116, 8

	.type	.L.str.6.117,@object            # @.str.6.117
.L.str.6.117:
	.asciz	" to "
	.size	.L.str.6.117, 5

	.type	.L.str.7.118,@object            # @.str.7.118
.L.str.7.118:
	.asciz	") do not cover required region (from "
	.size	.L.str.7.118, 38

	.type	.L.str.8.119,@object            # @.str.8.119
.L.str.8.119:
	.asciz	")"
	.size	.L.str.8.119, 2

	.type	.L.str.9.120,@object            # @.str.9.120
.L.str.9.120:
	.asciz	" has type "
	.size	.L.str.9.120, 11

	.type	.L.str.10.121,@object           # @.str.10.121
.L.str.10.121:
	.asciz	" but type of the buffer passed in is "
	.size	.L.str.10.121, 38

	.type	.L.str.11.122,@object           # @.str.11.122
.L.str.11.122:
	.asciz	" requires a buffer of exactly "
	.size	.L.str.11.122, 31

	.type	.L.str.12.123,@object           # @.str.12.123
.L.str.12.123:
	.asciz	" dimensions, but the buffer passed in has "
	.size	.L.str.12.123, 43

	.type	.L.str.13.124,@object           # @.str.13.124
.L.str.13.124:
	.asciz	" dimensions"
	.size	.L.str.13.124, 12

	.type	.L.str.14.125,@object           # @.str.14.125
.L.str.14.125:
	.asciz	" is accessed at "
	.size	.L.str.14.125, 17

	.type	.L.str.15.126,@object           # @.str.15.126
.L.str.15.126:
	.asciz	", which is before the min ("
	.size	.L.str.15.126, 28

	.type	.L.str.16.127,@object           # @.str.16.127
.L.str.16.127:
	.asciz	") in dimension "
	.size	.L.str.16.127, 16

	.type	.L.str.17.128,@object           # @.str.17.128
.L.str.17.128:
	.asciz	", which is beyond the max ("
	.size	.L.str.17.128, 28

	.type	.L.str.18.129,@object           # @.str.18.129
.L.str.18.129:
	.asciz	"Total allocation for buffer "
	.size	.L.str.18.129, 29

	.type	.L.str.19.130,@object           # @.str.19.130
.L.str.19.130:
	.asciz	" is "
	.size	.L.str.19.130, 5

	.type	.L.str.20.131,@object           # @.str.20.131
.L.str.20.131:
	.asciz	", which exceeds the maximum size of "
	.size	.L.str.20.131, 37

	.type	.L.str.21.132,@object           # @.str.21.132
.L.str.21.132:
	.asciz	"The extents for buffer "
	.size	.L.str.21.132, 24

	.type	.L.str.22.133,@object           # @.str.22.133
.L.str.22.133:
	.asciz	" dimension "
	.size	.L.str.22.133, 12

	.type	.L.str.23.134,@object           # @.str.23.134
.L.str.23.134:
	.asciz	" is negative ("
	.size	.L.str.23.134, 15

	.type	.L.str.24.135,@object           # @.str.24.135
.L.str.24.135:
	.asciz	"Product of extents for buffer "
	.size	.L.str.24.135, 31

	.type	.L.str.25.136,@object           # @.str.25.136
.L.str.25.136:
	.asciz	"Applying the constraints on "
	.size	.L.str.25.136, 29

	.type	.L.str.26.137,@object           # @.str.26.137
.L.str.26.137:
	.asciz	" to the required region made it smaller in dimension "
	.size	.L.str.26.137, 54

	.type	.L.str.27.138,@object           # @.str.27.138
.L.str.27.138:
	.asciz	". "
	.size	.L.str.27.138, 3

	.type	.L.str.28.139,@object           # @.str.28.139
.L.str.28.139:
	.asciz	"Required size: "
	.size	.L.str.28.139, 16

	.type	.L.str.29.140,@object           # @.str.29.140
.L.str.29.140:
	.asciz	"Constrained size: "
	.size	.L.str.29.140, 19

	.type	.L.str.30.141,@object           # @.str.30.141
.L.str.30.141:
	.asciz	"."
	.size	.L.str.30.141, 2

	.type	.L.str.31.142,@object           # @.str.31.142
.L.str.31.142:
	.asciz	"Constraint violated: "
	.size	.L.str.31.142, 22

	.type	.L.str.32.143,@object           # @.str.32.143
.L.str.32.143:
	.asciz	" ("
	.size	.L.str.32.143, 3

	.type	.L.str.33.144,@object           # @.str.33.144
.L.str.33.144:
	.asciz	") == "
	.size	.L.str.33.144, 6

	.type	.L.str.34.145,@object           # @.str.34.145
.L.str.34.145:
	.asciz	"Parameter "
	.size	.L.str.34.145, 11

	.type	.L.str.35.146,@object           # @.str.35.146
.L.str.35.146:
	.asciz	" but must be at least "
	.size	.L.str.35.146, 23

	.type	.L.str.36,@object               # @.str.36
.L.str.36:
	.asciz	" but must be at most "
	.size	.L.str.36, 22

	.type	.L.str.37,@object               # @.str.37
.L.str.37:
	.asciz	"Out of memory (halide_malloc returned nullptr)"
	.size	.L.str.37, 47

	.type	.L.str.38,@object               # @.str.38
.L.str.38:
	.asciz	"Buffer argument "
	.size	.L.str.38, 17

	.type	.L.str.39,@object               # @.str.39
.L.str.39:
	.asciz	" is nullptr"
	.size	.L.str.39, 12

	.type	.L.str.40,@object               # @.str.40
.L.str.40:
	.asciz	"Failed to dump function "
	.size	.L.str.40, 25

	.type	.L.str.41.147,@object           # @.str.41.147
.L.str.41.147:
	.asciz	" to file "
	.size	.L.str.41.147, 10

	.type	.L.str.42,@object               # @.str.42
.L.str.42:
	.asciz	" with error "
	.size	.L.str.42, 13

	.type	.L.str.43,@object               # @.str.43
.L.str.43:
	.asciz	"The host pointer of "
	.size	.L.str.43, 21

	.type	.L.str.44,@object               # @.str.44
.L.str.44:
	.asciz	" is not aligned to a "
	.size	.L.str.44, 22

	.type	.L.str.45,@object               # @.str.45
.L.str.45:
	.asciz	" bytes boundary."
	.size	.L.str.45, 17

	.type	.L.str.46,@object               # @.str.46
.L.str.46:
	.asciz	"The buffer "
	.size	.L.str.46, 12

	.type	.L.str.47,@object               # @.str.47
.L.str.47:
	.asciz	" is dirty on device, but this pipeline was compiled "
	.size	.L.str.47, 53

	.type	.L.str.48,@object               # @.str.48
.L.str.48:
	.asciz	"with no support for device to host copies."
	.size	.L.str.48, 43

	.type	.L.str.49,@object               # @.str.49
.L.str.49:
	.asciz	" is null, but the pipeline will access it on the host."
	.size	.L.str.49, 55

	.type	.L.str.50.148,@object           # @.str.50.148
.L.str.50.148:
	.asciz	"The folded storage dimension "
	.size	.L.str.50.148, 30

	.type	.L.str.51,@object               # @.str.51
.L.str.51:
	.asciz	" of "
	.size	.L.str.51, 5

	.type	.L.str.52,@object               # @.str.52
.L.str.52:
	.asciz	" was accessed out of order by loop "
	.size	.L.str.52, 36

	.type	.L.str.53,@object               # @.str.53
.L.str.53:
	.asciz	"Cannot fold dimension "
	.size	.L.str.53, 23

	.type	.L.str.54,@object               # @.str.54
.L.str.54:
	.asciz	" because an extern stage accesses ["
	.size	.L.str.54, 36

	.type	.L.str.55,@object               # @.str.55
.L.str.55:
	.asciz	", "
	.size	.L.str.55, 3

	.type	.L.str.56,@object               # @.str.56
.L.str.56:
	.asciz	"],"
	.size	.L.str.56, 3

	.type	.L.str.57,@object               # @.str.57
.L.str.57:
	.asciz	" which is outside the range currently valid: ["
	.size	.L.str.57, 47

	.type	.L.str.58.149,@object           # @.str.58.149
.L.str.58.149:
	.asciz	"]."
	.size	.L.str.58.149, 3

	.type	.L.str.59.150,@object           # @.str.59.150
.L.str.59.150:
	.asciz	" which wraps around the boundary of the fold, "
	.size	.L.str.59.150, 47

	.type	.L.str.60.151,@object           # @.str.60.151
.L.str.60.151:
	.asciz	"which occurs at multiples of "
	.size	.L.str.60.151, 30

	.type	.L.str.61.152,@object           # @.str.61.152
.L.str.61.152:
	.asciz	"The fold factor ("
	.size	.L.str.61.152, 18

	.type	.L.str.62,@object               # @.str.62
.L.str.62:
	.asciz	") of dimension "
	.size	.L.str.62, 16

	.type	.L.str.63,@object               # @.str.63
.L.str.63:
	.asciz	" is too small to store the required region accessed by loop "
	.size	.L.str.63, 61

	.type	.L.str.64.153,@object           # @.str.64.153
.L.str.64.153:
	.asciz	")."
	.size	.L.str.64.153, 3

	.type	.L.str.65,@object               # @.str.65
.L.str.65:
	.asciz	"Requirement Failed: ("
	.size	.L.str.65, 22

	.type	.L.str.66,@object               # @.str.66
.L.str.66:
	.asciz	") "
	.size	.L.str.66, 3

	.type	.L.str.67,@object               # @.str.67
.L.str.67:
	.asciz	"A schedule specialized with specialize_fail() was chosen: "
	.size	.L.str.67, 59

	.type	.L.str.68,@object               # @.str.68
.L.str.68:
	.asciz	"Buffer has a non-zero device but no device interface.\n"
	.size	.L.str.68, 55

	.type	.L.str.69,@object               # @.str.69
.L.str.69:
	.asciz	"Buffer has a non-null device_interface but device is 0.\n"
	.size	.L.str.69, 57

	.type	.L.str.70,@object               # @.str.70
.L.str.70:
	.asciz	"Buffer has both host and device dirty bits set.\n"
	.size	.L.str.70, 49

	.type	.L.str.71,@object               # @.str.71
.L.str.71:
	.asciz	"Buffer pointer passed to "
	.size	.L.str.71, 26

	.type	.L.str.72,@object               # @.str.72
.L.str.72:
	.asciz	" is null.\n"
	.size	.L.str.72, 11

	.type	.L.str.73,@object               # @.str.73
.L.str.73:
	.asciz	"The explicit allocation bound ("
	.size	.L.str.73, 32

	.type	.L.str.74,@object               # @.str.74
.L.str.74:
	.asciz	" is too small to store the required region ("
	.size	.L.str.74, 45

	.type	.L.str.75,@object               # @.str.75
.L.str.75:
	.asciz	"Buffer could not be cropped (runtime error or unimplemented device option).\n"
	.size	.L.str.75, 77

	.type	.L.str.29.163,@object           # @.str.29.163
.L.str.29.163:
	.asciz	"Printer buffer allocation failed.\n"
	.size	.L.str.29.163, 35

	.type	.L.str.7.164,@object            # @.str.7.164
.L.str.7.164:
	.asciz	"\n"
	.size	.L.str.7.164, 2

	.type	.L.str.8.165,@object            # @.str.8.165
.L.str.8.165:
	.asciz	" total time: "
	.size	.L.str.8.165, 14

	.type	.L.str.9.166,@object            # @.str.9.166
.L.str.9.166:
	.asciz	" ms"
	.size	.L.str.9.166, 4

	.type	.L.str.10.167,@object           # @.str.10.167
.L.str.10.167:
	.asciz	"  samples: "
	.size	.L.str.10.167, 12

	.type	.L.str.11.168,@object           # @.str.11.168
.L.str.11.168:
	.asciz	"  runs: "
	.size	.L.str.11.168, 9

	.type	.L.str.12.169,@object           # @.str.12.169
.L.str.12.169:
	.asciz	"  time/run: "
	.size	.L.str.12.169, 13

	.type	.L.str.13.170,@object           # @.str.13.170
.L.str.13.170:
	.asciz	" ms\n"
	.size	.L.str.13.170, 5

	.type	.L.str.14.171,@object           # @.str.14.171
.L.str.14.171:
	.asciz	" average threads used: "
	.size	.L.str.14.171, 24

	.type	.L.str.15.172,@object           # @.str.15.172
.L.str.15.172:
	.asciz	" heap allocations: "
	.size	.L.str.15.172, 20

	.type	.L.str.16.173,@object           # @.str.16.173
.L.str.16.173:
	.asciz	"  peak heap usage: "
	.size	.L.str.16.173, 20

	.type	.L.str.17.174,@object           # @.str.17.174
.L.str.17.174:
	.asciz	" bytes\n"
	.size	.L.str.17.174, 8

	.type	.L.str.18.175,@object           # @.str.18.175
.L.str.18.175:
	.asciz	"  "
	.size	.L.str.18.175, 3

	.type	.L.str.19.176,@object           # @.str.19.176
.L.str.19.176:
	.asciz	": "
	.size	.L.str.19.176, 3

	.type	.L.str.20.177,@object           # @.str.20.177
.L.str.20.177:
	.asciz	" "
	.size	.L.str.20.177, 2

	.type	.L.str.21.178,@object           # @.str.21.178
.L.str.21.178:
	.asciz	"ms"
	.size	.L.str.21.178, 3

	.type	.L.str.22.179,@object           # @.str.22.179
.L.str.22.179:
	.asciz	"("
	.size	.L.str.22.179, 2

	.type	.L.str.23.180,@object           # @.str.23.180
.L.str.23.180:
	.asciz	"%)"
	.size	.L.str.23.180, 3

	.type	.L.str.24.181,@object           # @.str.24.181
.L.str.24.181:
	.asciz	"threads: "
	.size	.L.str.24.181, 10

	.type	.L.str.25.182,@object           # @.str.25.182
.L.str.25.182:
	.asciz	" peak: "
	.size	.L.str.25.182, 8

	.type	.L.str.26.183,@object           # @.str.26.183
.L.str.26.183:
	.asciz	" num: "
	.size	.L.str.26.183, 7

	.type	.L.str.27.184,@object           # @.str.27.184
.L.str.27.184:
	.asciz	" avg: "
	.size	.L.str.27.184, 7

	.type	.L.str.28.185,@object           # @.str.28.185
.L.str.28.185:
	.asciz	" stack: "
	.size	.L.str.28.185, 9

	.type	_ZZ25halide_profiler_get_stateE1s,@object # @_ZZ25halide_profiler_get_stateE1s
	.data
	.p2align	3
_ZZ25halide_profiler_get_stateE1s:
	.zero	8
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	0                               # 0x0
	.quad	0
	.quad	0
	.quad	0
	.size	_ZZ25halide_profiler_get_stateE1s, 48

	.type	.L.str.186,@object              # @.str.186
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.186:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:246 halide_abort_if_false() failed: p_stats != nullptr\n"
	.size	.L.str.186, 158

	.type	.L.str.1.187,@object            # @.str.1.187
.L.str.1.187:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:273 halide_abort_if_false() failed: p_stats != nullptr\n"
	.size	.L.str.1.187, 158

	.type	.L.str.2.188,@object            # @.str.2.188
.L.str.2.188:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:274 halide_abort_if_false() failed: func_id >= 0\n"
	.size	.L.str.2.188, 152

	.type	.L.str.3.189,@object            # @.str.3.189
.L.str.3.189:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:275 halide_abort_if_false() failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.3.189, 168

	.type	.L.str.4.190,@object            # @.str.4.190
.L.str.4.190:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:309 halide_abort_if_false() failed: p_stats != nullptr\n"
	.size	.L.str.4.190, 158

	.type	.L.str.5.191,@object            # @.str.5.191
.L.str.5.191:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:310 halide_abort_if_false() failed: func_id >= 0\n"
	.size	.L.str.5.191, 152

	.type	.L.str.6.192,@object            # @.str.6.192
.L.str.6.192:
	.asciz	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/profiler_common.cpp:311 halide_abort_if_false() failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.6.192, 168

	.type	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE,@object # @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.data
	.weak	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.p2align	3
_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE:
	.quad	halide_default_can_use_target_features
	.size	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, 8

	.type	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE,@object # @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE
	.p2align	3
_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE:
	.zero	8
	.size	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE, 8

	.type	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE,@object # @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE
	.weak	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE
_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE:
	.byte	0                               # 0x0
	.size	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE,@object # @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE
	.weak	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE
	.p2align	3
_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE:
	.zero	32
	.size	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, 32

	.type	.L.str.197,@object              # @.str.197
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.197:
	.asciz	"Internal error: wrong structure size passed to halide_can_use_target_features()\n"
	.size	.L.str.197, 81

	.type	.L__unnamed_1,@object           # @0
	.section	.rodata,"a",@progbits
	.p2align	4
.L__unnamed_1:
	.zero	32
	.size	.L__unnamed_1, 32

	.type	.Lstr,@object                   # @str
	.p2align	5
.Lstr:
	.asciz	"input"
	.size	.Lstr, 6

	.type	.L__unnamed_2,@object           # @1
	.p2align	4
.L__unnamed_2:
	.zero	32
	.size	.L__unnamed_2, 32

	.type	.Lstr.200,@object               # @str.200
	.p2align	5
.Lstr.200:
	.asciz	"output"
	.size	.Lstr.200, 7

	.type	.L__unnamed_3,@object           # @2
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_3:
	.quad	.Lstr
	.long	1                               # 0x1
	.long	2                               # 0x2
	.byte	1                               # 0x1
	.byte	8                               # 0x8
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_1
	.quad	.Lstr.200
	.long	2                               # 0x2
	.long	2                               # 0x2
	.byte	1                               # 0x1
	.byte	8                               # 0x8
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_2
	.size	.L__unnamed_3, 128

	.type	.Lstr.201,@object               # @str.201
	.section	.rodata,"a",@progbits
	.p2align	6
.Lstr.201:
	.asciz	"x86-64-linux-avx-avx2-avx512-avx512_skylake-f16c-fma-no_asserts-no_bounds_query-sse41"
	.size	.Lstr.201, 86

	.type	.Lstr.202,@object               # @str.202
	.p2align	5
.Lstr.202:
	.asciz	"sobel5x5"
	.size	.Lstr.202, 9

	.type	.Lsobel5x5_metadata_storage,@object # @sobel5x5_metadata_storage
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.Lsobel5x5_metadata_storage:
	.long	1                               # 0x1
	.long	2                               # 0x2
	.quad	.L__unnamed_3
	.quad	.Lstr.201
	.quad	.Lstr.202
	.size	.Lsobel5x5_metadata_storage, 32

	.type	.Lswitch.table.halide_type_to_string,@object # @switch.table.halide_type_to_string
	.p2align	3
.Lswitch.table.halide_type_to_string:
	.quad	.L.str.14.76
	.quad	.L.str.15.75
	.quad	.L.str.16.74
	.quad	.L.str.17.73
	.size	.Lswitch.table.halide_type_to_string, 32

	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.section	".note.GNU-stack","",@progbits
