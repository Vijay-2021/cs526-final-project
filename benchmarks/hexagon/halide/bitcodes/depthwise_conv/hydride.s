	.text
	.file	"qurt_allocator.cpp"
	.section	.text._ZN6Halide7Runtime8Internal14aligned_mallocEjj,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal14aligned_mallocEjj // -- Begin function _ZN6Halide7Runtime8Internal14aligned_mallocEjj
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal14aligned_mallocEjj,@function
_ZN6Halide7Runtime8Internal14aligned_mallocEjj: // @_ZN6Halide7Runtime8Internal14aligned_mallocEjj
// %bb.0:                               // %entry
	{
		r2 = add(r0,add(r1,#-1))
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		r17 = sub(#0,r16)
	}
	{
		r0 = and(r2,r17)
	}
	{
		call ##malloc
		r0 = add(r0,r16)
	}
	{
		r1 = add(r16,add(r0,#3))
		p0 = cmp.eq(r0,#0)
	}
	{
		r2 = and(r1,r17)
		r17:16 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r1 = mux(p0,#0,r2)
		if (!p0) memw(r2+##-4) = r0
	}
	{
		r0 = r1
		dealloc_return
	}
.Lfunc_end0:
	.size	_ZN6Halide7Runtime8Internal14aligned_mallocEjj, .Lfunc_end0-_ZN6Halide7Runtime8Internal14aligned_mallocEjj
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal12aligned_freeEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12aligned_freeEPv // -- Begin function _ZN6Halide7Runtime8Internal12aligned_freeEPv
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal12aligned_freeEPv,@function
_ZN6Halide7Runtime8Internal12aligned_freeEPv: // @_ZN6Halide7Runtime8Internal12aligned_freeEPv
// %bb.0:                               // %entry
	{
		p0 = cmp.eq(r0,#0)
		if (p0.new) jumpr:nt r31
	}
.LBB1_1:                                // %if.then
	{
		jump ##free
		r0 = memw(r0+#-4)
	}
.Lfunc_end1:
	.size	_ZN6Halide7Runtime8Internal12aligned_freeEPv, .Lfunc_end1-_ZN6Halide7Runtime8Internal12aligned_freeEPv
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv // -- Begin function _ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv,@function
_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv: // @_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv
// %bb.0:                               // %entry
	{
		r0 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		r16 = memw(r0+##_ZN6Halide7Runtime8Internal7mem_bufE@GOT)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#0)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#4)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#8)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#12)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#16)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#20)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#24)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#28)
	}
	{
		call ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r0 = memw(r16+#32)
	}
	{
		r0 = memw(r16+#36)
		r17:16 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		jump ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
		r31:30 = deallocframe(r30):raw
	}
.Lfunc_end2:
	.size	_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv, .Lfunc_end2-_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv
                                        // -- End function
	.section	.text.halide_default_malloc,"ax",@progbits
	.weak	halide_default_malloc           // -- Begin function halide_default_malloc
	.p2align	4
	.type	halide_default_malloc,@function
halide_default_malloc:                  // @halide_default_malloc
// %bb.0:                               // %entry
	{
		p0 = cmp.gtu(r1,##65536)
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB3_27
	}
// %bb.1:
	{
		r3 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		r4 = #1
	}
	.p2align	4
.LBB3_2:                                // %cmpxchg.start
                                        // =>This Inner Loop Header: Depth=1
	{
		r0 = add(r3,##_ZN6Halide7Runtime8Internal11buf_is_usedE@GOT)
	}
	{
		r2 = memw(r0+#0)
	}
	{
		r2 = memw_locked(r2)
	}
	{
		r2 = add(r3,##_ZN6Halide7Runtime8Internal7mem_bufE@GOT)
		p0 = cmp.eq(r2,#0); if (!p0.new) jump:nt .LBB3_3
	}
// %bb.22:                              // %cmpxchg.trystore
                                        //   in Loop: Header=BB3_2 Depth=1
	{
		r0 = memw(r0+#0)
	}
	{
		memw_locked(r0,p0) = r4
	}
	{
		if (!p0) jump:nt .LBB3_2
		r16 = memw(r2+#0)
	}
	{
		jump .LBB3_23
	}
.LBB3_3:
	{
		r3 = #1
	}
	.p2align	4
.LBB3_4:                                // %cmpxchg.start9
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#4)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_5
	}
// %bb.25:                              // %cmpxchg.trystore7
                                        //   in Loop: Header=BB3_4 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#4)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		if (!p0) jump:nt .LBB3_4
		r4 = memw(r2+#0)
	}
// %bb.26:
	{
		r16 = add(r4,#4)
		jump .LBB3_23
	}
.LBB3_5:
	{
		r3 = #1
	}
	.p2align	4
.LBB3_6:                                // %cmpxchg.start26
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#8)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_7
	}
// %bb.28:                              // %cmpxchg.trystore24
                                        //   in Loop: Header=BB3_6 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#8)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		if (!p0) jump:nt .LBB3_6
		r4 = memw(r2+#0)
	}
// %bb.29:
	{
		r16 = add(r4,#8)
		jump .LBB3_23
	}
.LBB3_7:
	{
		r3 = #1
	}
	.p2align	4
.LBB3_8:                                // %cmpxchg.start43
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#12)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_9
	}
// %bb.30:                              // %cmpxchg.trystore41
                                        //   in Loop: Header=BB3_8 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#12)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		if (!p0) jump:nt .LBB3_8
		r4 = memw(r2+#0)
	}
// %bb.31:
	{
		r16 = add(r4,#12)
		jump .LBB3_23
	}
.LBB3_9:
	{
		r3 = #1
	}
	.p2align	4
.LBB3_10:                               // %cmpxchg.start60
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#16)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_11
	}
// %bb.32:                              // %cmpxchg.trystore58
                                        //   in Loop: Header=BB3_10 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#16)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		if (!p0) jump:nt .LBB3_10
		r4 = memw(r2+#0)
	}
// %bb.33:
	{
		r16 = add(r4,#16)
		jump .LBB3_23
	}
.LBB3_11:
	{
		r3 = #1
	}
	.p2align	4
.LBB3_12:                               // %cmpxchg.start77
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#20)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_13
	}
// %bb.34:                              // %cmpxchg.trystore75
                                        //   in Loop: Header=BB3_12 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#20)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		r4 = memw(r2+#0)
	}
	{
		r16 = add(r4,#20)
		if (!p0) jump:nt .LBB3_12
	}
	{
		jump .LBB3_23
	}
.LBB3_13:
	{
		r3 = #1
	}
.LBB3_14:                               // %cmpxchg.start94
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#24)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_15
	}
// %bb.35:                              // %cmpxchg.trystore92
                                        //   in Loop: Header=BB3_14 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#24)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		r4 = memw(r2+#0)
	}
	{
		r16 = add(r4,#24)
		if (!p0) jump:nt .LBB3_14
	}
	{
		jump .LBB3_23
	}
.LBB3_15:
	{
		r3 = #1
	}
.LBB3_16:                               // %cmpxchg.start111
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#28)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_17
	}
// %bb.36:                              // %cmpxchg.trystore109
                                        //   in Loop: Header=BB3_16 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#28)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		r4 = memw(r2+#0)
	}
	{
		r16 = add(r4,#28)
		if (!p0) jump:nt .LBB3_16
	}
	{
		jump .LBB3_23
	}
.LBB3_17:
	{
		r3 = #1
	}
.LBB3_18:                               // %cmpxchg.start128
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#32)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_19
	}
// %bb.37:                              // %cmpxchg.trystore126
                                        //   in Loop: Header=BB3_18 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#32)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		r4 = memw(r2+#0)
	}
	{
		r16 = add(r4,#32)
		if (!p0) jump:nt .LBB3_18
	}
	{
		jump .LBB3_23
	}
.LBB3_19:
	{
		r3 = #1
	}
.LBB3_20:                               // %cmpxchg.start145
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#36)
	}
	{
		r4 = memw_locked(r4)
	}
	{
		p0 = cmp.eq(r4,#0); if (!p0.new) jump:nt .LBB3_27
	}
// %bb.21:                              // %cmpxchg.trystore143
                                        //   in Loop: Header=BB3_20 Depth=1
	{
		r4 = memw(r0+#0)
	}
	{
		r4 = add(r4,#36)
	}
	{
		memw_locked(r4,p0) = r3
	}
	{
		r4 = memw(r2+#0)
	}
	{
		r16 = add(r4,#36)
		if (!p0) jump:nt .LBB3_20
	}
.LBB3_23:                               // %if.then3
	{
		r0 = memw(r16+#0)
	}
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#0)
		if (!p0.new) r31:30 = dealloc_return(r30):t:raw
	}                                       // 8-byte Folded Reload
.LBB3_24:                               // %if.then5
	{
		r0 = #128
		r1 = ##65536
	}
	{
		call ##_ZN6Halide7Runtime8Internal14aligned_mallocEjj
	}
	{
		r17:16 = memd(r29+#0)
		memw(r16+#0) = r0
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB3_27:                               // %if.end9
	{
		r0 = #128
		r17:16 = memd(r29+#0)
		deallocframe
	}                                       // 8-byte Folded Reload
	{
		jump ##_ZN6Halide7Runtime8Internal14aligned_mallocEjj
	}
.Lfunc_end3:
	.size	halide_default_malloc, .Lfunc_end3-halide_default_malloc
                                        // -- End function
	.section	.text.halide_default_free,"ax",@progbits
	.weak	halide_default_free             // -- Begin function halide_default_free
	.p2align	4
	.type	halide_default_free,@function
halide_default_free:                    // @halide_default_free
// %bb.0:                               // %entry
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r0 = add(r2,##_ZN6Halide7Runtime8Internal7mem_bufE@GOT)
		r2 = add(r2,##_ZN6Halide7Runtime8Internal11buf_is_usedE@GOT)
	}
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#0)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_1:                                // %for.inc
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#4)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#4)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_2:                                // %for.inc.1
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#8)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#8)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_3:                                // %for.inc.2
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#12)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#12)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_4:                                // %for.inc.3
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#16)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#16)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_5:                                // %for.inc.4
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#20)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#20)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_6:                                // %for.inc.5
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#24)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#24)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_7:                                // %for.inc.6
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#28)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#28)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_8:                                // %for.inc.7
	{
		r3 = memw(r0+#0)
	}
	{
		r3 = memw(r3+#32)
	}
	{
		p0 = cmp.eq(r3,r1)
		r3 = memw(r2+#0)
	}
	{
		if (p0) r3 = add(r3,#32)
	}
	{
		if (p0) jumpr:nt r31
		if (p0) memw(r3+#0) = #0
	}
.LBB4_9:                                // %for.inc.8
	{
		r0 = memw(r0+#0)
	}
	{
		r0 = memw(r0+#36)
	}
	{
		p0 = cmp.eq(r0,r1); if (p0.new) jump:nt .LBB4_10
		r0 = memw(r2+#0)
	}
// %bb.11:                              // %for.inc.9
	{
		r0 = r1 ; jump ##_ZN6Halide7Runtime8Internal12aligned_freeEPv
	}
.LBB4_10:
	{
		r3 = add(r0,#36)
	}
	{
		jumpr r31
		memw(r3+#0) = #0
	}
.Lfunc_end4:
	.size	halide_default_free, .Lfunc_end4-halide_default_free
                                        // -- End function
	.section	.text.halide_set_custom_malloc,"ax",@progbits
	.weak	halide_set_custom_malloc        // -- Begin function halide_set_custom_malloc
	.p2align	4
	.type	halide_set_custom_malloc,@function
halide_set_custom_malloc:               // @halide_set_custom_malloc
// %bb.0:                               // %entry
	{
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		r1 = add(pc,##.L.str@PCREL)
		r0 = #0
	}
	{
		call ##halide_print
	}
	{
		r0 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r1 = memw(r0+##_ZN6Halide7Runtime8Internal13custom_mallocE@GOT)
	}
	{
		r0 = memw(r1+#0)
		memw(r1+#0) = r16

	} :mem_noshuf
	{
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end5:
	.size	halide_set_custom_malloc, .Lfunc_end5-halide_set_custom_malloc
                                        // -- End function
	.section	.text.halide_set_custom_free,"ax",@progbits
	.weak	halide_set_custom_free          // -- Begin function halide_set_custom_free
	.p2align	4
	.type	halide_set_custom_free,@function
halide_set_custom_free:                 // @halide_set_custom_free
// %bb.0:                               // %entry
	{
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		r1 = add(pc,##.L.str@PCREL)
		r0 = #0
	}
	{
		call ##halide_print
	}
	{
		r0 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r1 = memw(r0+##_ZN6Halide7Runtime8Internal11custom_freeE@GOT)
	}
	{
		r0 = memw(r1+#0)
		memw(r1+#0) = r16

	} :mem_noshuf
	{
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end6:
	.size	halide_set_custom_free, .Lfunc_end6-halide_set_custom_free
                                        // -- End function
	.section	.text.halide_malloc,"ax",@progbits
	.weak	halide_malloc                   // -- Begin function halide_malloc
	.p2align	4
	.type	halide_malloc,@function
halide_malloc:                          // @halide_malloc
// %bb.0:                               // %entry
	{
		jump ##halide_default_malloc
	}
.Lfunc_end7:
	.size	halide_malloc, .Lfunc_end7-halide_malloc
                                        // -- End function
	.section	.text.halide_free,"ax",@progbits
	.weak	halide_free                     // -- Begin function halide_free
	.p2align	4
	.type	halide_free,@function
halide_free:                            // @halide_free
// %bb.0:                               // %entry
	{
		jump ##halide_default_free
	}
.Lfunc_end8:
	.size	halide_free, .Lfunc_end8-halide_free
                                        // -- End function
	.section	.text.halide_default_do_task,"ax",@progbits
	.weak	halide_default_do_task          // -- Begin function halide_default_do_task
	.p2align	4
	.type	halide_default_do_task,@function
halide_default_do_task:                 // @halide_default_do_task
// %bb.0:                               // %entry
	{
		r1 = r2
		r2 = r3
		r4 = r1
		allocframe(#0)
	}
	{
		callr r4
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end9:
	.size	halide_default_do_task, .Lfunc_end9-halide_default_do_task
                                        // -- End function
	.section	.text.halide_default_do_loop_task,"ax",@progbits
	.weak	halide_default_do_loop_task     // -- Begin function halide_default_do_loop_task
	.p2align	4
	.type	halide_default_do_loop_task,@function
halide_default_do_loop_task:            // @halide_default_do_loop_task
// %bb.0:                               // %entry
	{
		r1 = r2
		r2 = r3
		r6 = r1
		r3 = r4
	}
	{
		callr r6
		r4 = r5
		allocframe(#0)
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end10:
	.size	halide_default_do_loop_task, .Lfunc_end10-halide_default_do_loop_task
                                        // -- End function
	.section	.text.halide_default_do_par_for,"ax",@progbits
	.weak	halide_default_do_par_for       // -- Begin function halide_default_do_par_for
	.p2align	4
	.type	halide_default_do_par_for,@function
halide_default_do_par_for:              // @halide_default_do_par_for
// %bb.0:                               // %entry
	{
		p0 = cmp.gt(r3,#0)
		memd(r29+#-16) = r17:16
		allocframe(#24)
	}                                       // 8-byte Folded Spill
	{
		if (!p0) jump:nt .LBB11_1
		memd(r29+#8) = r19:18
		memd(r29+#0) = r21:20
	}                                       // 8-byte Folded Spill
// %bb.4:
	{
		r17:16 = combine(r2,r4)
		r19:18 = combine(r0,r1)
		r20 = add(r3,r2)
	}
	.p2align	4
.LBB11_5:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	{
		call ##halide_do_task
		r1:0 = combine(r18,r19)
		r3:2 = combine(r16,r17)
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:nt .LBB11_6
	}
// %bb.2:                               // %for.cond
                                        //   in Loop: Header=BB11_5 Depth=1
	{
		r17 = add(r17,#1)
		if (cmp.gt(r20,r17.new)) jump:t .LBB11_5
	}
// %bb.3:
	{
		r0 = #0
	}
.LBB11_6:                               // %cleanup1
	{
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB11_1:
	{
		r0 = #0
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end11:
	.size	halide_default_do_par_for, .Lfunc_end11-halide_default_do_par_for
                                        // -- End function
	.section	.text.halide_do_task,"ax",@progbits
	.weak	halide_do_task                  // -- Begin function halide_do_task
	.p2align	4
	.type	halide_do_task,@function
halide_do_task:                         // @halide_do_task
// %bb.0:                               // %entry
	{
		r4 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r4 = memw(r4+##_ZN6Halide7Runtime8Internal14custom_do_taskE@GOT)
	}
	{
		r4 = memw(r4+#0)
	}
	{
		callr r4
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end12:
	.size	halide_do_task, .Lfunc_end12-halide_do_task
                                        // -- End function
	.section	.text.halide_default_do_parallel_tasks,"ax",@progbits
	.weak	halide_default_do_parallel_tasks // -- Begin function halide_default_do_parallel_tasks
	.p2align	4
	.type	halide_default_do_parallel_tasks,@function
halide_default_do_parallel_tasks:       // @halide_default_do_parallel_tasks
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.1@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #-1
		dealloc_return
	}
.Lfunc_end13:
	.size	halide_default_do_parallel_tasks, .Lfunc_end13-halide_default_do_parallel_tasks
                                        // -- End function
	.section	.text.halide_default_semaphore_init,"ax",@progbits
	.weak	halide_default_semaphore_init   // -- Begin function halide_default_semaphore_init
	.p2align	4
	.type	halide_default_semaphore_init,@function
halide_default_semaphore_init:          // @halide_default_semaphore_init
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.1.2@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #0
		dealloc_return
	}
.Lfunc_end14:
	.size	halide_default_semaphore_init, .Lfunc_end14-halide_default_semaphore_init
                                        // -- End function
	.section	.text.halide_default_semaphore_try_acquire,"ax",@progbits
	.weak	halide_default_semaphore_try_acquire // -- Begin function halide_default_semaphore_try_acquire
	.p2align	4
	.type	halide_default_semaphore_try_acquire,@function
halide_default_semaphore_try_acquire:   // @halide_default_semaphore_try_acquire
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.3@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #0
		dealloc_return
	}
.Lfunc_end15:
	.size	halide_default_semaphore_try_acquire, .Lfunc_end15-halide_default_semaphore_try_acquire
                                        // -- End function
	.section	.text.halide_default_semaphore_release,"ax",@progbits
	.weak	halide_default_semaphore_release // -- Begin function halide_default_semaphore_release
	.p2align	4
	.type	halide_default_semaphore_release,@function
halide_default_semaphore_release:       // @halide_default_semaphore_release
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.2@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #0
		dealloc_return
	}
.Lfunc_end16:
	.size	halide_default_semaphore_release, .Lfunc_end16-halide_default_semaphore_release
                                        // -- End function
	.section	.text.halide_spawn_thread,"ax",@progbits
	.weak	halide_spawn_thread             // -- Begin function halide_spawn_thread
	.p2align	4
	.type	halide_spawn_thread,@function
halide_spawn_thread:                    // @halide_spawn_thread
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.4@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #0
		dealloc_return
	}
.Lfunc_end17:
	.size	halide_spawn_thread, .Lfunc_end17-halide_spawn_thread
                                        // -- End function
	.section	.text.halide_join_thread,"ax",@progbits
	.weak	halide_join_thread              // -- Begin function halide_join_thread
	.p2align	4
	.type	halide_join_thread,@function
halide_join_thread:                     // @halide_join_thread
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.5@PCREL)
		r0 = #0
	}
	{
		jump ##halide_error
	}
.Lfunc_end18:
	.size	halide_join_thread, .Lfunc_end18-halide_join_thread
                                        // -- End function
	.section	.text.halide_mutex_lock,"ax",@progbits
	.weak	halide_mutex_lock               // -- Begin function halide_mutex_lock
	.p2align	4
	.type	halide_mutex_lock,@function
halide_mutex_lock:                      // @halide_mutex_lock
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end19:
	.size	halide_mutex_lock, .Lfunc_end19-halide_mutex_lock
                                        // -- End function
	.section	.text.halide_mutex_unlock,"ax",@progbits
	.weak	halide_mutex_unlock             // -- Begin function halide_mutex_unlock
	.p2align	4
	.type	halide_mutex_unlock,@function
halide_mutex_unlock:                    // @halide_mutex_unlock
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end20:
	.size	halide_mutex_unlock, .Lfunc_end20-halide_mutex_unlock
                                        // -- End function
	.section	.text.halide_mutex_array_create,"ax",@progbits
	.weak	halide_mutex_array_create       // -- Begin function halide_mutex_array_create
	.p2align	4
	.type	halide_mutex_array_create,@function
halide_mutex_array_create:              // @halide_mutex_array_create
// %bb.0:                               // %entry
	{
		r0 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		jumpr r31
		r0 = memw(r0+##_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE@GOT)
	}
.Lfunc_end21:
	.size	halide_mutex_array_create, .Lfunc_end21-halide_mutex_array_create
                                        // -- End function
	.section	.text.halide_mutex_array_destroy,"ax",@progbits
	.weak	halide_mutex_array_destroy      // -- Begin function halide_mutex_array_destroy
	.p2align	4
	.type	halide_mutex_array_destroy,@function
halide_mutex_array_destroy:             // @halide_mutex_array_destroy
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end22:
	.size	halide_mutex_array_destroy, .Lfunc_end22-halide_mutex_array_destroy
                                        // -- End function
	.section	.text.halide_mutex_array_lock,"ax",@progbits
	.weak	halide_mutex_array_lock         // -- Begin function halide_mutex_array_lock
	.p2align	4
	.type	halide_mutex_array_lock,@function
halide_mutex_array_lock:                // @halide_mutex_array_lock
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end23:
	.size	halide_mutex_array_lock, .Lfunc_end23-halide_mutex_array_lock
                                        // -- End function
	.section	.text.halide_mutex_array_unlock,"ax",@progbits
	.weak	halide_mutex_array_unlock       // -- Begin function halide_mutex_array_unlock
	.p2align	4
	.type	halide_mutex_array_unlock,@function
halide_mutex_array_unlock:              // @halide_mutex_array_unlock
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end24:
	.size	halide_mutex_array_unlock, .Lfunc_end24-halide_mutex_array_unlock
                                        // -- End function
	.section	.text.halide_shutdown_thread_pool,"ax",@progbits
	.weak	halide_shutdown_thread_pool     // -- Begin function halide_shutdown_thread_pool
	.p2align	4
	.type	halide_shutdown_thread_pool,@function
halide_shutdown_thread_pool:            // @halide_shutdown_thread_pool
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end25:
	.size	halide_shutdown_thread_pool, .Lfunc_end25-halide_shutdown_thread_pool
                                        // -- End function
	.section	.text.halide_set_num_threads,"ax",@progbits
	.weak	halide_set_num_threads          // -- Begin function halide_set_num_threads
	.p2align	4
	.type	halide_set_num_threads,@function
halide_set_num_threads:                 // @halide_set_num_threads
// %bb.0:                               // %entry
	{
		if (p0.new) r0 = #1
		p0 = cmp.eq(r0,#1)
		if (p0.new) jumpr:nt r31
	}
.LBB26_1:                               // %if.then
	{
		r1 = add(pc,##.L.str.6@PCREL)
		r0 = #0
		allocframe(#0)
	}
	{
		call ##halide_error
	}
	{
		r0 = #1
		deallocframe
	}
	{
		jumpr r31
	}
.Lfunc_end26:
	.size	halide_set_num_threads, .Lfunc_end26-halide_set_num_threads
                                        // -- End function
	.section	.text.halide_set_custom_do_task,"ax",@progbits
	.weak	halide_set_custom_do_task       // -- Begin function halide_set_custom_do_task
	.p2align	4
	.type	halide_set_custom_do_task,@function
halide_set_custom_do_task:              // @halide_set_custom_do_task
// %bb.0:                               // %entry
	{
		r1 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r2 = memw(r1+##_ZN6Halide7Runtime8Internal14custom_do_taskE@GOT)
	}
	{
		r1 = memw(r2+#0)
		memw(r2+#0) = r0

	} :mem_noshuf
	{
		r0 = r1
		jumpr r31
	}
.Lfunc_end27:
	.size	halide_set_custom_do_task, .Lfunc_end27-halide_set_custom_do_task
                                        // -- End function
	.section	.text.halide_set_custom_do_par_for,"ax",@progbits
	.weak	halide_set_custom_do_par_for    // -- Begin function halide_set_custom_do_par_for
	.p2align	4
	.type	halide_set_custom_do_par_for,@function
halide_set_custom_do_par_for:           // @halide_set_custom_do_par_for
// %bb.0:                               // %entry
	{
		r1 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r2 = memw(r1+##_ZN6Halide7Runtime8Internal17custom_do_par_forE@GOT)
	}
	{
		r1 = memw(r2+#0)
		memw(r2+#0) = r0

	} :mem_noshuf
	{
		r0 = r1
		jumpr r31
	}
.Lfunc_end28:
	.size	halide_set_custom_do_par_for, .Lfunc_end28-halide_set_custom_do_par_for
                                        // -- End function
	.section	.text.halide_do_par_for,"ax",@progbits
	.weak	halide_do_par_for               // -- Begin function halide_do_par_for
	.p2align	4
	.type	halide_do_par_for,@function
halide_do_par_for:                      // @halide_do_par_for
// %bb.0:                               // %entry
	{
		r5 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r5 = memw(r5+##_ZN6Halide7Runtime8Internal17custom_do_par_forE@GOT)
	}
	{
		r5 = memw(r5+#0)
	}
	{
		callr r5
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end29:
	.size	halide_do_par_for, .Lfunc_end29-halide_do_par_for
                                        // -- End function
	.section	.text.halide_do_loop_task,"ax",@progbits
	.weak	halide_do_loop_task             // -- Begin function halide_do_loop_task
	.p2align	4
	.type	halide_do_loop_task,@function
halide_do_loop_task:                    // @halide_do_loop_task
// %bb.0:                               // %entry
	{
		r6 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r6 = memw(r6+##_ZN6Halide7Runtime8Internal19custom_do_loop_taskE@GOT)
	}
	{
		r6 = memw(r6+#0)
	}
	{
		callr r6
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end30:
	.size	halide_do_loop_task, .Lfunc_end30-halide_do_loop_task
                                        // -- End function
	.section	.text.halide_do_parallel_tasks,"ax",@progbits
	.weak	halide_do_parallel_tasks        // -- Begin function halide_do_parallel_tasks
	.p2align	4
	.type	halide_do_parallel_tasks,@function
halide_do_parallel_tasks:               // @halide_do_parallel_tasks
// %bb.0:                               // %entry
	{
		r4 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r4 = memw(r4+##_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE@GOT)
	}
	{
		r4 = memw(r4+#0)
	}
	{
		callr r4
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end31:
	.size	halide_do_parallel_tasks, .Lfunc_end31-halide_do_parallel_tasks
                                        // -- End function
	.section	.text.halide_semaphore_init,"ax",@progbits
	.weak	halide_semaphore_init           // -- Begin function halide_semaphore_init
	.p2align	4
	.type	halide_semaphore_init,@function
halide_semaphore_init:                  // @halide_semaphore_init
// %bb.0:                               // %entry
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r2 = memw(r2+##_ZN6Halide7Runtime8Internal21custom_semaphore_initE@GOT)
	}
	{
		r2 = memw(r2+#0)
	}
	{
		callr r2
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end32:
	.size	halide_semaphore_init, .Lfunc_end32-halide_semaphore_init
                                        // -- End function
	.section	.text.halide_semaphore_release,"ax",@progbits
	.weak	halide_semaphore_release        // -- Begin function halide_semaphore_release
	.p2align	4
	.type	halide_semaphore_release,@function
halide_semaphore_release:               // @halide_semaphore_release
// %bb.0:                               // %entry
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r2 = memw(r2+##_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE@GOT)
	}
	{
		r2 = memw(r2+#0)
	}
	{
		callr r2
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end33:
	.size	halide_semaphore_release, .Lfunc_end33-halide_semaphore_release
                                        // -- End function
	.section	.text.halide_semaphore_try_acquire,"ax",@progbits
	.weak	halide_semaphore_try_acquire    // -- Begin function halide_semaphore_try_acquire
	.p2align	4
	.type	halide_semaphore_try_acquire,@function
halide_semaphore_try_acquire:           // @halide_semaphore_try_acquire
// %bb.0:                               // %entry
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r2 = memw(r2+##_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE@GOT)
	}
	{
		r2 = memw(r2+#0)
	}
	{
		callr r2
	}
	{
		p0 = r0
	}
	{
		r0 = mux(p0,#1,#0)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end34:
	.size	halide_semaphore_try_acquire, .Lfunc_end34-halide_semaphore_try_acquire
                                        // -- End function
	.section	.text.halide_set_gpu_device,"ax",@progbits
	.weak	halide_set_gpu_device           // -- Begin function halide_set_gpu_device
	.p2align	4
	.type	halide_set_gpu_device,@function
halide_set_gpu_device:                  // @halide_set_gpu_device
// %bb.0:                               // %entry
	{
		r1 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r2 = memw(r1+##_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOT)
	}
	{
		memw(r2+#0) = r0
		r0 = memw(r1+##_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOT)

	} :mem_noshuf
	{
		jumpr r31
		memb(r0+#0) = #1
	}
.Lfunc_end35:
	.size	halide_set_gpu_device, .Lfunc_end35-halide_set_gpu_device
                                        // -- End function
	.section	.text.halide_get_gpu_device,"ax",@progbits
	.weak	halide_get_gpu_device           // -- Begin function halide_get_gpu_device
	.p2align	4
	.type	halide_get_gpu_device,@function
halide_get_gpu_device:                  // @halide_get_gpu_device
// %bb.0:                               // %entry
	{
		r17 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		r0 = #255
	}
	.p2align	4
.LBB36_1:                               // %atomicrmw.start
                                        // =>This Inner Loop Header: Depth=1
	{
		r16 = add(r17,##_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE@GOT)
	}
	{
		r1 = memw(r16+#0)
	}
	{
		r2 = and(r1,#3)
		r3 = and(r1,#-4)
	}
	{
		r1 = asl(r2,#3)
	}
	{
		r2 = memw_locked(r3)
	}
	{
		r4 = asl(r0,r1)
		r5 = lsl(#1,r1)
	}
	{
		r5 |= and(r2,~r4)
	}
	{
		memw_locked(r3,p0) = r5
	}
	{
		if (!p0) jump:nt .LBB36_1
	}
// %bb.2:                               // %atomicrmw.end
                                        //   in Loop: Header=BB36_1 Depth=1
	{
		r3 = memw(r16+#0)
	}
	{
		r1 = insert(r3,#2,#3)
	}
	{
		r1 = lsr(r2,r1)
	}
	{
		p0 = !bitsclr(r1,r0)
		if (p0.new) jump:t .LBB36_1
	}
// %bb.3:                               // %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
	{
		r0 = memw(r17+##_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOT)
	}
	{
		r0 = memb(r0+#0)
		if (cmp.eq(r0.new,#0)) jump:nt .LBB36_5
	}
// %bb.4:                               // %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge
	{
		r0 = memw(r17+##_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOT)
	}
	{
		r0 = memw(r0+#0)
		r1 = memw(r16+#0)
	}
	{
		memb(r1+#0) = #0
		r17:16 = memd(r29+#0)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB36_5:                               // %if.then
	{
		r0 = add(pc,##.L.str.7@PCREL)
		call ##getenv
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB36_6
	}
// %bb.7:                               // %if.then2
	{
		call ##atoi
	}
	{
		jump .LBB36_8
	}
.LBB36_6:
	{
		r0 = #-1
	}
.LBB36_8:                               // %if.end
	{
		r1 = memw(r17+##_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOT)
	}
	{
		memw(r1+#0) = r0
		r1 = memw(r17+##_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOT)

	} :mem_noshuf
	{
		memb(r1+#0) = #1
		r1 = memw(r16+#0)

	} :mem_noshuf
	{
		memb(r1+#0) = #0
		r17:16 = memd(r29+#0)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end36:
	.size	halide_get_gpu_device, .Lfunc_end36-halide_get_gpu_device
                                        // -- End function
	.section	.text.halide_string_to_string,"ax",@progbits
	.weak	halide_string_to_string         // -- Begin function halide_string_to_string
	.p2align	4
	.type	halide_string_to_string,@function
halide_string_to_string:                // @halide_string_to_string
// %bb.0:                               // %entry
	{
		p0 = cmp.gtu(r1,r0); if (!p0.new) jump:t .LBB37_5
	}
// %bb.1:                               // %if.end
	{
		r3 = add(pc,##.L.str.8@PCREL)
		p0 = cmp.eq(r2,#0)
		r4 = sub(r1,r0)
	}
	{
		loop0(.LBB37_2,r4)
		if (!p0) r3 = add(r2,#0)
	}
	.p2align	4
.Ltmp0:                                 // Block address taken
.LBB37_2:                               // %if.end5
                                        // =>This Inner Loop Header: Depth=1
	{
		r4 = memb(r3+#0)
		memb(r0+#0) = r4.new
	}
	{
		p0 = cmp.eq(r4,#0)
		if (p0.new) jumpr:nt r31
	}
.LBB37_3:                               // %if.end8
                                        //   in Loop: Header=BB37_2 Depth=1
	{
		r0 = add(r0,#1)
		r2 = r0
		r3 = add(r3,#1)
	} :endloop0
// %bb.4:                               // %if.then4
	{
		r0 = r1
		memb(r2+#0) = #0
	}
.LBB37_5:
	{
		jumpr r31
	}
.Lfunc_end37:
	.size	halide_string_to_string, .Lfunc_end37-halide_string_to_string
                                        // -- End function
	.section	.text.halide_uint64_to_string,"ax",@progbits
	.weak	halide_uint64_to_string         // -- Begin function halide_uint64_to_string
	.p2align	4
	.type	halide_uint64_to_string,@function
halide_uint64_to_string:                // @halide_uint64_to_string
// %bb.0:                               // %entry
	{
		r7:6 = combine(#0,#0)
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		p0 = cmp.eq(r3:2,r7:6)
		r5 = add(r29,#0)
		memd(r29+#32) = r19:18
		memb(r29+#31) = #0
	}                                       // 8-byte Folded Spill
	{
		if (!p0) jump:nt .LBB38_4
	}
// %bb.1:                               // %entry
	{
		p0 = cmp.gt(r4,#0); if (p0.new) jump:nt .LBB38_4
	}
// %bb.2:
	{
		r5 = add(r5,#30)
	}
.LBB38_3:                               // %for.cond.cleanup
	{
		call ##halide_string_to_string
		r2 = add(r5,#1)
	}
	{
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB38_4:                               // %entry.for.body_crit_edge
	{
		r5 = add(r5,#29)
		r7 = ##-858993459
		r6 = #1
	}
	{
		r9 = #0
		r12 = ##-858993460
		r13 = #-10
	}
	{
		r15:14 = combine(#0,#9)
	}
.LBB38_5:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	{
		r3:2 = mpyu(r2,r7)
		r11:10 = combine(r3,r2)
	}
	{
		r17:16 = mpyu(r10,r12)
		p0 = cmp.gtu(r11:10,r15:14)
		r8 = r3
	}
	{
		r3:2 = combine(r9,r8)
		r19:18 = combine(r9,r17)
		r17 = r9
	}
	{
		r3:2 += mpyu(r11,r7)
	}
	{
		r3:2 = add(r3:2,r17:16)
	}
	{
		r2 = r3
		r3 = r9
	}
	{
		r3:2 += mpyu(r11,r12)
	}
	{
		r17:16 = add(r3:2,r19:18)
	}
	{
		r2 = lsr(r16,#3)
		r3 = lsr(r17,#3)
	}
	{
		r2 = insert(r17,#3,#29)
	}
	{
		r17:16 = mpyu(r2,r13)
	}
	{
		r17 -= mpyi(r2,#1)
	}
	{
		r17 -= mpyi(r3,#10)
	}
	{
		r11:10 = add(r17:16,r11:10)
	}
	{
		r8 = add(r10,#48)
		if (p0) jump:nt .LBB38_7
		memb(r5+#1) = r8.new
	}
// %bb.6:                               // %for.body
                                        //   in Loop: Header=BB38_5 Depth=1
	{
		p0 = cmp.gt(r4,r6); if (!p0.new) jump:t .LBB38_3
	}
	.p2align	4
.LBB38_7:                               // %for.body.for.body_crit_edge
                                        //   in Loop: Header=BB38_5 Depth=1
	{
		jump .LBB38_5
		r6 = add(r6,#1)
		r5 = add(r5,#-1)
	}
.Lfunc_end38:
	.size	halide_uint64_to_string, .Lfunc_end38-halide_uint64_to_string
                                        // -- End function
	.section	.text.halide_int64_to_string,"ax",@progbits
	.weak	halide_int64_to_string          // -- Begin function halide_int64_to_string
	.p2align	4
	.type	halide_int64_to_string,@function
halide_int64_to_string:                 // @halide_int64_to_string
// %bb.0:                               // %entry
	{
		p0 = cmp.gtu(r1,r0); if (p0.new) jump:nt .LBB39_1
	}
.LBB39_3:                               // %if.end
	{
		jump ##halide_uint64_to_string
	}
.LBB39_1:                               // %entry
	{
		r7:6 = combine(#-1,#-1)
	}
	{
		p0 = cmp.gt(r3:2,r7:6)
		if (p0.new) jump:t .LBB39_3
	}
// %bb.2:                               // %if.then
	{
		r3:2 = neg(r3:2)
		r5 = #45
		memb(r0++#1) = r5.new
	}
	{
		jump ##halide_uint64_to_string
	}
.Lfunc_end39:
	.size	halide_int64_to_string, .Lfunc_end39-halide_int64_to_string
                                        // -- End function
	.section	.text.halide_double_to_string,"ax",@progbits
	.weak	halide_double_to_string         // -- Begin function halide_double_to_string
	.p2align	4
	.type	halide_double_to_string,@function
halide_double_to_string:                // @halide_double_to_string
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		memd(r29+#-16) = r17:16
		allocframe(r29,#576):raw
	}                                       // 8-byte Folded Spill
	{
		r25:24 = combine(#0,#0)
		r0 = add(r29,#512)
		memd(r29+#536) = r25:24
		memd(r29+#520) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1 = add(r29,#520)
		r2 = #8
		memd(r29+#560) = r19:18
		memd(r29+#552) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r18 = r4
		memd(r29+#544) = r23:22
		memd(r29+#512) = r25:24
	}                                       // 8-byte Folded Spill
	{
		call ##memcpy
	}
	{
		r21:20 = memd(r29+#512)
	}
	{
		r19 = extractu(r21,#11,#20)
		r23 = extractu(r21,#20,#0)
		r22 = r20
	}
	{
		p0 = cmp.eq(r19,##2047)
		if (!p0.new) jump:t .LBB40_9
	}
// %bb.1:                               // %if.then
	{
		p0 = cmp.eq(r23:22,r25:24)
		r1:0 = combine(#-1,#-1)
		if (p0.new) jump:nt .LBB40_6
	}
// %bb.2:                               // %if.then4
	{
		p0 = cmp.gt(r21:20,r1:0)
		if (!p0.new) jump:nt .LBB40_3
	}
// %bb.5:                               // %if.else
	{
		r2 = add(pc,##.L.str.2.10@PCREL)
		jump .LBB40_4
	}
.LBB40_9:                               // %if.else15
	{
		p0 = cmp.eq(r23:22,r25:24)
		if (!p0.new) jump:nt .LBB40_18
	}
// %bb.10:                              // %if.else15
	{
		p0 = cmp.eq(r19,#0); if (!p0.new) jump:nt .LBB40_18
	}
// %bb.11:                              // %if.then18
	{
		r1:0 = combine(#-1,#-1)
		p0 = cmp.eq(r18,#0); if (p0.new) jump:nt .LBB40_15
	}
// %bb.12:                              // %if.then20
	{
		p0 = cmp.gt(r21:20,r1:0)
		if (!p0.new) jump:nt .LBB40_13
	}
// %bb.14:                              // %if.else24
	{
		r2 = add(pc,##.L.str.6.14@PCREL)
		jump .LBB40_4
	}
.LBB40_18:                              // %if.end32
	{
		r1:0 = combine(#-1,#-1)
	}
	{
		p0 = cmp.gt(r21:20,r1:0)
		if (p0.new) jump:nt .LBB40_20
	}
// %bb.19:                              // %if.then34
	{
		r2 = add(pc,##.L.str.9@PCREL)
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_string_to_string
	}
	{
		r17 = r0
		r3:2 = memd(r29+#520)
	}
	{
		r3 = togglebit(r3,#31)
	}
	{
		memd(r29+#520) = r3:2
	}
.LBB40_20:                              // %if.end36
	{
		p0 = cmp.eq(r18,#0); if (p0.new) jump:nt .LBB40_35
	}
// %bb.21:                              // %while.condthread-pre-split
	{
		r22 = #0
		r23 = ##1072693248
		r1:0 = memd(r29+#520)
	}
	{
		p0 = dfcmp.ge(r1:0,r23:22)
		p1 = dfcmp.uo(r1:0,r23:22)
		r18 = #0
	}
	{
		p0 = or(p1,p0)
		if (p0.new) jump:t .LBB40_26
	}
// %bb.22:
	{
		r20 = #0
		r21 = ##1076101120
		r18 = #-1
	}
	.p2align	4
.LBB40_23:                              // %while.body
                                        // =>This Inner Loop Header: Depth=1
	{
		call ##__hexagon_muldf3
		r3:2 = combine(r21,r20)
	}
	{
		p0 = dfcmp.ge(r1:0,r23:22)
		p1 = dfcmp.uo(r1:0,r23:22)
	}
	{
		p0 = or(p1,p0)
		if (p0.new) jump:nt .LBB40_25
	}
// %bb.24:                              // %while.body.while.body_crit_edge
                                        //   in Loop: Header=BB40_23 Depth=1
	{
		r18 = add(r18,#-1)
		jump .LBB40_23
	}
.LBB40_6:                               // %if.else9
	{
		p0 = cmp.gt(r21:20,r1:0)
		if (!p0.new) jump:nt .LBB40_7
	}
// %bb.8:                               // %if.else13
	{
		r2 = add(pc,##.L.str.4.12@PCREL)
		jump .LBB40_4
	}
.LBB40_3:                               // %if.then6
	{
		r2 = add(pc,##.L.str.1.9@PCREL)
		jump .LBB40_4
	}
.LBB40_15:                              // %if.else26
	{
		p0 = cmp.gt(r21:20,r1:0)
		if (!p0.new) jump:nt .LBB40_16
	}
// %bb.17:                              // %if.else30
	{
		r2 = add(pc,##.L.str.8.16@PCREL)
		jump .LBB40_4
	}
.LBB40_35:                              // %if.else61
	{
		p0 = cmp.eq(r19,#0); if (p0.new) jump:nt .LBB40_36
	}
// %bb.37:                              // %if.end65
	{
		r21 = setbit(r23,#20)
		p0 = cmp.gtu(r19,##1074)
		r22 = add(r19,#-1075)
	}
	{
		if (p0) jump:nt .LBB40_38
	}
// %bb.39:                              // %if.then71
	{
		p0 = cmp.gtu(r19,##1022)
		r24 = #0
		r23 = #0
	}
	{
		if (p0) jump:nt .LBB40_41
	}
// %bb.40:
	{
		r25 = #0
		jump .LBB40_42
	}
.LBB40_25:                              // %while.cond.while.cond40thread-pre-split_crit_edge
	{
		memd(r29+#520) = r1:0
	}
.LBB40_26:                              // %while.cond40thread-pre-split
	{
		r21 = ##1076101120
		r20 = #0
	}
	{
		p0 = dfcmp.gt(r21:20,r1:0)
		p1 = dfcmp.uo(r21:20,r1:0)
	}
	{
		p0 = or(p1,p0)
		if (p0.new) jump:t .LBB40_29
	}
	.p2align	4
.LBB40_27:                              // %while.body42
                                        // =>This Inner Loop Header: Depth=1
	{
		call ##__hexagon_divdf3
		r3:2 = combine(r21,r20)
	}
	{
		p0 = dfcmp.ge(r1:0,r21:20)
		r18 = add(r18,#1)
		if (p0.new) jump:t .LBB40_27
	}
// %bb.28:                              // %while.cond40.while.end43_crit_edge
	{
		memd(r29+#520) = r1:0
	}
.LBB40_29:                              // %while.end43
	{
		r3 = ##1093567616
		r2 = #0
	}
	{
		call ##__hexagon_muldf3
	}
	{
		r3 = ##1071644672
		r2 = #0
	}
	{
		r1:0 = dfadd(r1:0,r3:2)
	}
	{
		r7:6 = convert_df2ud(r1:0):chop
		r0 = ##-675924773
	}
	{
		r5:4 = mpyu(r6,r0)
		r1 = ##1125899906
	}
	{
		r9:8 = mpyu(r6,r1)
		r3:2 = combine(#0,r5)
	}
	{
		r5:4 = combine(r3,r2)
		r2 = r8
	}
	{
		r5:4 += mpyu(r7,r0)
	}
	{
		r5:4 = add(r5:4,r3:2)
	}
	{
		r2 = r5
	}
	{
		r5:4 = combine(r3,r2)
		r2 = r9
	}
	{
		r5:4 += mpyu(r7,r1)
	}
	{
		r1:0 = add(r5:4,r3:2)
		r4 = #1
	}
	{
		r2 = lsr(r0,#18)
		r3 = lsr(r1,#18)
		r0 = ##-1000000
	}
	{
		r2 = insert(r1,#18,#14)
	}
	{
		r25:24 = mpyu(r2,r0)
	}
	{
		r25 -= mpyi(r2,#1)
	}
	{
		r25 += mpyi(r0,r3)
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_int64_to_string
		r21:20 = add(r25:24,r7:6)
	}
	{
		r2 = add(pc,##.L.str.30.66@PCREL)
		r1 = r16
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #6
		r1 = r16
	}
	{
		call ##halide_int64_to_string
	}
	{
		p0 = cmp.gt(r18,#-1); if (!p0.new) jump:nt .LBB40_31
	}
// %bb.30:                              // %if.then53
	{
		r2 = add(pc,##.L.str.11@PCREL)
		r1 = r16
	}
	{
		call ##halide_string_to_string
	}
	{
		jump .LBB40_32
	}
.LBB40_7:                               // %if.then11
	{
		r2 = add(pc,##.L.str.3.11@PCREL)
		jump .LBB40_4
	}
.LBB40_13:                              // %if.then22
	{
		r2 = add(pc,##.L.str.5.13@PCREL)
		jump .LBB40_4
	}
.LBB40_31:                              // %if.else55
	{
		r2 = add(pc,##.L.str.12@PCREL)
		r1 = r16
	}
	{
		call ##halide_string_to_string
		r18 = sub(#0,r18)
	}
.LBB40_32:                              // %if.end58
	{
		r1 = r16
		r19 = #0
		r4 = #2
	}
	{
		r3:2 = combine(r19,r18)
		jump .LBB40_33
	}
.LBB40_16:                              // %if.then28
	{
		r2 = add(pc,##.L.str.7.15@PCREL)
	}
.LBB40_4:                               // %cleanup147
	{
		call ##halide_string_to_string
		r1:0 = combine(r16,r17)
	}
	{
		jump .LBB40_34
	}
.LBB40_36:                              // %if.then63
	{
		r1:0 = combine(r16,r17)
		r4 = #0
		r3:2 = combine(#0,#0)
	}
	{
		call ##halide_double_to_string
	}
	{
		jump .LBB40_34
	}
.LBB40_38:
	{
		r23 = r22 ; jump .LBB40_43
		r19:18 = combine(#0,#0)
	}
.LBB40_41:                              // %if.else75
	{
		r0 = sub(##1075,r19)
	}
	{
		r25:24 = lsr(r21:20,r0)
	}
	{
		r21:20 -= asl(r25:24,r0)
	}
.LBB40_42:                              // %if.end83
	{
		r3:2 = convert_ud2df(r21:20)
		r1 = ##1093567616
		r0 = #0
	}
	{
		r1:0 += asl(r23:22,#52)
		call ##__hexagon_muldf3
	}
	{
		r3 = ##1071644672
		r2 = #0
	}
	{
		r1:0 = dfadd(r1:0,r3:2)
		r4 = ##1000000
		r5 = #0
	}
	{
		r3:2 = convert_df2ud(r1:0):chop
	}
	{
		r7:6 = convert_ud2df(r3:2)
		p0 = tstbit(r2,#0)
	}
	{
		p1 = dfcmp.eq(r1:0,r7:6)
		r7:6 = combine(#0,#1)
	}
	{
		p0 = and(p1,p0)
	}
	{
		r0 = mux(p0,#-1,#0)
	}
	{
		r1:0 = add(r1:0,r3:2):raw:lo
		r3:2 = add(r25:24,r7:6)
	}
	{
		p0 = cmp.eq(r1:0,r5:4)
	}
	{
		r20 = mux(p0,r2,r24)
		r18 = mux(p0,r23,r0)
		r21 = mux(p0,r3,r25)
		r19 = mux(p0,r23,r1)
	}
.LBB40_43:                              // %if.end104
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r0 = add(r29,#0)
	}
	{
		r20 = add(r0,#480)
		r1 = add(r0,#512)
	}
	{
		call ##halide_int64_to_string
		r0 = r20
	}
	{
		p0 = cmp.gt(r23,#0); if (!p0.new) jump:nt .LBB40_44
	}
// %bb.45:                              // %for.cond111.preheader.preheader
	{
		r1 = add(r23,#-1)
	}
	{
		r1 = and(r23,#3)
		p0 = cmp.gtu(r1,#2); if (!p0.new) jump:t .LBB40_64
	}
// %bb.46:                              // %for.cond111.preheader.preheader.new
	{
		r2 = and(r23,#-4)
	}
	{
		r3 = lsr(r2,#2)
		r2 = #49
	}
	{
		loop1(.LBB40_47,r3)
		jump .LBB40_47
	}
.LBB40_72:                              // %if.then135.3
                                        //   in Loop: Header=BB40_47 Depth=1
	{
		r20 = add(r3,#-1)
		memb(r3+#-1) = r2
	}
.LBB40_73:                              // %if.end137.3
                                        //   in Loop: Header=BB40_47 Depth=1
	{
		nop
		nop
		nop
	} :endloop1
	{
		jump .LBB40_64
	}
.Ltmp1:                                 // Block address taken
.LBB40_47:                              // %for.cond111.preheader
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB40_49 Depth 2
                                        //     Child Loop BB40_53 Depth 2
                                        //     Child Loop BB40_57 Depth 2
                                        //     Child Loop BB40_61 Depth 2
	{
		p0 = cmp.eq(r0,r20); if (p0.new) jump:nt .LBB40_51
		r5 = #0
		r3 = r0
	}
// %bb.48:                              //   in Loop: Header=BB40_47 Depth=1
	{
		r6 = #-96
		r3 = sub(r0,r20)
		r7 = r0
		r4 = memub(r0+#-1)
	}
	{
		r6 += asl(r4,#1)
		r8 = add(r3,#-1)
		p0 = cmp.gtu(r3,#1)
		r4 = add(r0,#-1)
	}
	{
		loop0(.LBB40_49,r8)
		r3 = or(r6,r5)
		r5 = r0
	}
	{
		r8 = sxtb(r3)
		r6 = add(r3,#-10)
		if (!p0) jump:nt .LBB40_50
	}
	.p2align	4
.LBB40_49:                              // %for.body115
                                        //   Parent Loop BB40_47 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		p0 = cmp.gt(r8,#9)
		r5 = r4
		r8 = #-96
		r4 = memub(r4+#-1)
	}
	{
		r8 += asl(r4,#1)
		if (!p0) r6 = add(r3,#0)
		r4 = add(r5,#-1)
		r9 = mux(p0,#1,#0)
	}
	{
		r6 = add(r6,#48)
		r7 = r5
		r3 = or(r8,r9)
		memb(r7+#-1) = r6.new
	}
	{
		r8 = sxtb(r3)
		r6 = add(r3,#-10)
	} :endloop0
.LBB40_50:                              //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.gt(r8,#9)
		if (!p0.new) r6 = add(r3,#0)
	}
	{
		r3 = add(r6,#48)
		memb(r5+#-1) = r3.new
	}
	{
		if (p0) r3 = add(r20,#-1)
		if (!p0) r3 = add(r20,#0)
		if (p0) memb(r20+##-1) = r2
	}
.LBB40_51:                              // %if.end137
                                        //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.eq(r0,r3); if (p0.new) jump:nt .LBB40_55
		r6 = #0
		r4 = r0
	}
// %bb.52:                              //   in Loop: Header=BB40_47 Depth=1
	{
		r7 = #-96
		r4 = sub(r0,r3)
		r8 = r0
		r5 = memub(r0+#-1)
	}
	{
		r7 += asl(r5,#1)
		r9 = add(r4,#-1)
		p0 = cmp.gtu(r4,#1)
		r5 = add(r0,#-1)
	}
	{
		loop0(.LBB40_53,r9)
		r4 = or(r7,r6)
		r6 = r0
	}
	{
		r9 = sxtb(r4)
		r7 = add(r4,#-10)
		if (!p0) jump:nt .LBB40_54
	}
	.p2align	4
.LBB40_53:                              // %for.body115.1
                                        //   Parent Loop BB40_47 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		p0 = cmp.gt(r9,#9)
		r6 = r5
		r9 = #-96
		r5 = memub(r5+#-1)
	}
	{
		r9 += asl(r5,#1)
		if (!p0) r7 = add(r4,#0)
		r5 = add(r6,#-1)
		r12 = mux(p0,#1,#0)
	}
	{
		r7 = add(r7,#48)
		r8 = r6
		r4 = or(r9,r12)
		memb(r8+#-1) = r7.new
	}
	{
		r9 = sxtb(r4)
		r7 = add(r4,#-10)
	} :endloop0
.LBB40_54:                              //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.gt(r9,#9)
		if (!p0.new) r7 = add(r4,#0)
	}
	{
		r4 = add(r7,#48)
		memb(r6+#-1) = r4.new
	}
	{
		if (p0) r4 = add(r3,#-1)
		if (!p0) r4 = add(r3,#0)
		if (p0) memb(r3+##-1) = r2
	}
.LBB40_55:                              // %if.end137.1
                                        //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.eq(r0,r4); if (p0.new) jump:nt .LBB40_59
		r6 = #0
		r3 = r0
	}
// %bb.56:                              //   in Loop: Header=BB40_47 Depth=1
	{
		r7 = #-96
		r3 = sub(r0,r4)
		r8 = r0
		r5 = memub(r0+#-1)
	}
	{
		r7 += asl(r5,#1)
		r9 = add(r3,#-1)
		p0 = cmp.gtu(r3,#1)
		r5 = add(r0,#-1)
	}
	{
		loop0(.LBB40_57,r9)
		r3 = or(r7,r6)
		r6 = r0
	}
	{
		r9 = sxtb(r3)
		r7 = add(r3,#-10)
		if (!p0) jump:nt .LBB40_58
	}
	.p2align	4
.LBB40_57:                              // %for.body115.2
                                        //   Parent Loop BB40_47 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		p0 = cmp.gt(r9,#9)
		r6 = r5
		r9 = #-96
		r5 = memub(r5+#-1)
	}
	{
		r9 += asl(r5,#1)
		if (!p0) r7 = add(r3,#0)
		r5 = add(r6,#-1)
		r12 = mux(p0,#1,#0)
	}
	{
		r7 = add(r7,#48)
		r8 = r6
		r3 = or(r9,r12)
		memb(r8+#-1) = r7.new
	}
	{
		r9 = sxtb(r3)
		r7 = add(r3,#-10)
	} :endloop0
.LBB40_58:                              //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.gt(r9,#9)
		if (!p0.new) r7 = add(r3,#0)
	}
	{
		r3 = add(r7,#48)
		memb(r6+#-1) = r3.new
	}
	{
		if (p0) r3 = add(r4,#-1)
		if (!p0) r3 = add(r4,#0)
		if (p0) memb(r4+##-1) = r2
	}
.LBB40_59:                              // %if.end137.2
                                        //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.eq(r0,r3); if (p0.new) jump:nt .LBB40_73
		r4 = #0
		r20 = r0
	}
// %bb.60:                              //   in Loop: Header=BB40_47 Depth=1
	{
		r6 = #-96
		r5 = sub(r0,r3)
		r8 = r0
		r7 = memub(r0+#-1)
	}
	{
		r6 += asl(r7,#1)
		r9 = add(r5,#-1)
		p0 = cmp.gtu(r5,#1)
		r5 = add(r0,#-1)
	}
	{
		loop0(.LBB40_61,r9)
		r4 = or(r6,r4)
		r6 = r0
	}
	{
		r9 = sxtb(r4)
		r7 = add(r4,#-10)
		if (!p0) jump:nt .LBB40_62
	}
	.p2align	4
.LBB40_61:                              // %for.body115.3
                                        //   Parent Loop BB40_47 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		p0 = cmp.gt(r9,#9)
		r6 = r5
		r9 = #-96
		r5 = memub(r5+#-1)
	}
	{
		r9 += asl(r5,#1)
		if (!p0) r7 = add(r4,#0)
		r5 = add(r6,#-1)
		r12 = mux(p0,#1,#0)
	}
	{
		r7 = add(r7,#48)
		r8 = r6
		r4 = or(r9,r12)
		memb(r8+#-1) = r7.new
	}
	{
		r9 = sxtb(r4)
		r7 = add(r4,#-10)
	} :endloop0
.LBB40_62:                              //   in Loop: Header=BB40_47 Depth=1
	{
		p0 = cmp.gt(r9,#9)
		if (!p0.new) r7 = add(r4,#0)
	}
	{
		r4 = add(r7,#48)
		if (p0) jump:nt .LBB40_72
		memb(r6+#-1) = r4.new
	}
// %bb.63:                              //   in Loop: Header=BB40_47 Depth=1
	{
		r20 = r3
		nop
		nop
	} :endloop1
.LBB40_64:                              // %for.cond.cleanup.loopexit.unr-lcssa
	{
		r2 = r20
		p0 = cmp.eq(r1,#0); if (p0.new) jump:nt .LBB40_71
	}
// %bb.65:
	{
		loop1(.LBB40_66,r1)
		r3 = #49 ; jump .LBB40_66
	}
	.p2align	4
.LBB40_69:                              //   in Loop: Header=BB40_66 Depth=1
	{
		p0 = cmp.gt(r7,#9)
		if (p0.new) r2 = add(r20,#-1)
		if (!p0.new) r5 = add(r1,#0)
		if (!p0.new) r2 = add(r20,#0)
	}
	{
		r1 = add(r5,#48)
		memb(r4+#-1) = r1.new
	}
	{
		if (p0) memb(r20+##-1) = r3
	}
.LBB40_70:                              // %if.end137.epil
                                        //   in Loop: Header=BB40_66 Depth=1
	{
		r20 = r2
		nop
		nop
	} :endloop1
	{
		jump .LBB40_71
	}
.Ltmp2:                                 // Block address taken
.LBB40_66:                              // %for.cond111.preheader.epil
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB40_68 Depth 2
	{
		p0 = cmp.eq(r0,r20); if (p0.new) jump:nt .LBB40_70
		r1 = #0
		r2 = r0
	}
// %bb.67:                              //   in Loop: Header=BB40_66 Depth=1
	{
		r4 = #-96
		r2 = sub(r0,r20)
		r6 = r0
		r5 = memub(r0+#-1)
	}
	{
		r4 += asl(r5,#1)
		p0 = cmp.gtu(r2,#1)
		r7 = add(r2,#-1)
		r2 = add(r0,#-1)
	}
	{
		loop0(.LBB40_68,r7)
		r1 = or(r4,r1)
		r4 = r0
	}
	{
		r7 = sxtb(r1)
		r5 = add(r1,#-10)
		if (!p0) jump:nt .LBB40_69
	}
	.p2align	4
.LBB40_68:                              // %for.body115.epil
                                        //   Parent Loop BB40_66 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		p0 = cmp.gt(r7,#9)
		r4 = r2
		r7 = #-96
		r2 = memub(r2+#-1)
	}
	{
		r7 += asl(r2,#1)
		if (!p0) r5 = add(r1,#0)
		r2 = add(r4,#-1)
		r8 = mux(p0,#1,#0)
	}
	{
		r5 = add(r5,#48)
		r6 = r4
		r1 = or(r7,r8)
		memb(r6+#-1) = r5.new
	}
	{
		r7 = sxtb(r1)
		r5 = add(r1,#-10)
	} :endloop0
	{
		jump .LBB40_69
	}
.LBB40_44:
	{
		r2 = r20
	}
.LBB40_71:                              // %for.cond.cleanup
	{
		call ##halide_string_to_string
		r1:0 = combine(r16,r17)
	}
	{
		r2 = add(pc,##.L.str.30.66@PCREL)
		r1 = r16
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #6
		r1 = r16
	}
.LBB40_33:                              // %cleanup147
	{
		call ##halide_int64_to_string
	}
.LBB40_34:                              // %cleanup147
	{
		r17:16 = memd(r29+#568)
		r19:18 = memd(r29+#560)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#552)
		r23:22 = memd(r29+#544)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#536)
		r31:30 = dealloc_return(r30):raw
	}                                       // 8-byte Folded Reload
.Ltmp3:                                 // Address of block that was removed by CodeGen
.Ltmp4:                                 // Address of block that was removed by CodeGen
.Ltmp5:                                 // Address of block that was removed by CodeGen
.Ltmp6:                                 // Address of block that was removed by CodeGen
.Ltmp7:                                 // Address of block that was removed by CodeGen
.Lfunc_end40:
	.size	halide_double_to_string, .Lfunc_end40-halide_double_to_string
                                        // -- End function
	.section	.text.halide_pointer_to_string,"ax",@progbits
	.weak	halide_pointer_to_string        // -- Begin function halide_pointer_to_string
	.p2align	4
	.type	halide_pointer_to_string,@function
halide_pointer_to_string:               // @halide_pointer_to_string
// %bb.0:                               // %entry
	{
		r4 = add(pc,##.L.str.13@PCREL)
		r9:8 = bitsplit(r2,#4)
		allocframe(r29,#24):raw
	}
	{
		r7:6 = combine(#0,#0)
		r5 = add(r29,#0)
		p0 = cmp.eq(r9,#0)
		memw(r29+#16) = #0
	}
	{
		r3 = add(r5,#17)
		memd(r29+#8) = r7:6
		memd(r29+#0) = r7:6
	}
	{
		if (p0) jump:nt .LBB41_8
		r6 = memub(r4+r8<<#0)
		memb(r29+#18) = r6.new
	}
// %bb.1:                               // %for.cond
	{
		r2 = lsr(r2,#4)
		r6 = add(r5,#16)
	}
	{
		r5 = extractu(r2,#24,#4)
		r7 = and(r2,#15)
	}
	{
		p0 = cmp.eq(r5,#0); if (p0.new) jump:nt .LBB41_9
		r5 = memub(r4+r7<<#0)
		memb(r29+#17) = r5.new
	}
// %bb.2:                               // %for.cond.1
	{
		r3 = extractu(r2,#4,#4)
		r7 = extractu(r2,#20,#8)
		r5 = add(r29,#0)
	}
	{
		p0 = cmp.eq(r7,#0)
	}
	{
		r3 = memub(r4+r3<<#0)
		memb(r29+#16) = r3.new
	}
	{
		r3 = add(r5,#15)
		if (p0) jump:nt .LBB41_12
	}
// %bb.3:                               // %for.cond.2
	{
		r6 = extractu(r2,#16,#12)
		r7 = extractu(r2,#4,#8)
	}
	{
		p0 = cmp.eq(r6,#0)
		r6 = add(r5,#14)
	}
	{
		if (p0) jump:nt .LBB41_9
		r5 = memub(r4+r7<<#0)
		memb(r29+#15) = r5.new
	}
// %bb.4:                               // %for.cond.3
	{
		r3 = extractu(r2,#4,#12)
		r7 = extractu(r2,#12,#16)
		r5 = add(r29,#0)
	}
	{
		p0 = cmp.eq(r7,#0)
	}
	{
		r3 = memub(r4+r3<<#0)
		memb(r29+#14) = r3.new
	}
	{
		r3 = add(r5,#13)
		if (p0) jump:nt .LBB41_12
	}
// %bb.5:                               // %for.cond.4
	{
		r6 = extractu(r2,#8,#20)
		r7 = extractu(r2,#4,#16)
	}
	{
		p0 = cmp.eq(r6,#0)
		r6 = add(r5,#12)
	}
	{
		if (p0) jump:nt .LBB41_9
		r5 = memub(r4+r7<<#0)
		memb(r29+#13) = r5.new
	}
// %bb.6:                               // %for.cond.5
	{
		r3 = extractu(r2,#4,#20)
		r2 = extractu(r2,#4,#24)
		r7 = add(r29,#0)
	}
	{
		p0 = cmp.eq(r2,#0)
	}
	{
		r3 = memub(r4+r3<<#0)
		memb(r29+#12) = r3.new
	}
	{
		r3 = add(r7,#11)
		if (p0) jump:nt .LBB41_12
	}
// %bb.7:                               // %for.cond.6
	{
		r5 = r3
		r3 = add(r7,#10)
		r2 = memub(r4+r2<<#0)
		memb(r29+#11) = r2.new
	}
	{
		jump .LBB41_13
	}
.LBB41_8:
	{
		r5 = add(r5,#18)
		jump .LBB41_13
	}
.LBB41_9:
	{
		r5 = r3 ; jump .LBB41_13
		r3 = r6
	}
.LBB41_12:
	{
		r5 = r6
	}
.LBB41_13:                              // %cleanup
	{
		r2 = add(r5,#-2)
		r4 = #48
		memb(r3+#0) = #120
	}
	{
		call ##halide_string_to_string
		memb(r5+#-2) = r4
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end41:
	.size	halide_pointer_to_string, .Lfunc_end41-halide_pointer_to_string
                                        // -- End function
	.section	.text.halide_type_to_string,"ax",@progbits
	.weak	halide_type_to_string           // -- Begin function halide_type_to_string
	.p2align	4
	.type	halide_type_to_string,@function
halide_type_to_string:                  // @halide_type_to_string
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r2)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r2 = memb(r2+#0)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		p0 = cmp.gtu(r2,#3); if (p0.new) jump:t .LBB42_1
	}
// %bb.2:                               // %switch.lookup
	{
		r1 = add(pc,##.Lswitch.table.halide_type_to_string@PCREL)
	}
	{
		jump .LBB42_3
		r2 = memw(r1+r2<<#2)
	}
.LBB42_1:
	{
		r2 = add(pc,##.L.str.18@PCREL)
	}
.LBB42_3:                               // %sw.epilog
	{
		call ##halide_string_to_string
		r1 = r17
	}
	{
		r1 = r17
		r4 = #1
		r19 = #0
		r18 = memub(r16+#1)
	}
	{
		call ##halide_uint64_to_string
		r3:2 = combine(r19,r18)
	}
	{
		r1 = memh(r16+#2)
	}
	{
		p0 = cmp.eq(r1,#1)
		if (p0.new) r17:16 = memd(r29+#8)
		if (p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (p0) r31:30 = dealloc_return(r30):raw
	}
.LBB42_4:                               // %if.then
	{
		r2 = add(pc,##.L.str.19@PCREL)
		r1 = r17
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = r17
		r4 = #1
		r18 = memuh(r16+#2)
		r17:16 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = combine(r19,r18)
		r19:18 = memd(r29+#0)
		deallocframe
	}                                       // 8-byte Folded Reload
	{
		jump ##halide_uint64_to_string
	}
.Lfunc_end42:
	.size	halide_type_to_string, .Lfunc_end42-halide_type_to_string
                                        // -- End function
	.section	.text.halide_buffer_to_string,"ax",@progbits
	.weak	halide_buffer_to_string         // -- Begin function halide_buffer_to_string
	.p2align	4
	.type	halide_buffer_to_string,@function
halide_buffer_to_string:                // @halide_buffer_to_string
// %bb.0:                               // %entry
	{
		p0 = cmp.eq(r2,#0)
		r16 = r1
		memd(r29+#-16) = r17:16
		allocframe(#32)
	}                                       // 8-byte Folded Spill
	{
		memd(r29+#16) = r19:18
		memd(r29+#8) = r21:20
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB43_1
		memd(r29+#0) = r23:22
	}                                       // 8-byte Folded Spill
// %bb.3:                               // %if.end
	{
		r2 = add(pc,##.L.str.21@PCREL)
		r17 = r2
		r1 = r16
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = r16
		r4 = #1
		r3:2 = memd(r17+#0)
	}
	{
		r18 = add(pc,##.L.str.55@PCREL)
		call ##halide_uint64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r18
	}
	{
		call ##halide_pointer_to_string
		r1 = r16
		r2 = memw(r17+#8)
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r18
	}
	{
		call ##halide_pointer_to_string
		r1 = r16
		r2 = memw(r17+#12)
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r18
	}
	{
		r1 = r16
		r4 = #1
		r3:2 = memd(r17+#16)
	}
	{
		call ##halide_uint64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r18
	}
	{
		call ##halide_type_to_string
		r2 = add(r17,#24)
		r1 = r16
	}
	{
		r1 = memw(r17+#28)
		if (!cmp.gt(r1.new,#0)) jump:t .LBB43_6
	}
// %bb.4:                               // %for.body.lr.ph
	{
		r18 = add(pc,##.L.str.23@PCREL)
		r21 = #0
		r22 = #0
	}
	{
		r19 = add(pc,##.L.str.55@PCREL)
	}
	{
		r20 = add(pc,##.L.str.24@PCREL)
	}
	.p2align	4
.LBB43_5:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r18
	}
	{
		r4 = #1
		r1 = memw(r17+#32)
	}
	{
		r1 = r16
		r2 = memw(r1+r21<<#0)
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r19
	}
	{
		r4 = #1
		r1 = memw(r17+#32)
	}
	{
		r1 = add(r1,r21)
	}
	{
		r1 = r16
		r2 = memw(r1+#4)
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r19
	}
	{
		r4 = #1
		r1 = memw(r17+#32)
	}
	{
		r1 = add(r1,r21)
	}
	{
		r1 = r16
		r2 = memw(r1+#8)
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		call ##halide_string_to_string
		r1 = r16
		r2 = r20
	}
	{
		r21 = add(r21,#16)
		r22 = add(r22,#1)
	}
	{
		r1 = memw(r17+#28)
		if (cmp.gt(r1.new,r22)) jump:t .LBB43_5
	}
.LBB43_6:                               // %for.cond.cleanup
	{
		r2 = add(pc,##.L.str.8.44@PCREL)
		jump .LBB43_2
	}
.LBB43_1:                               // %if.then
	{
		r2 = add(pc,##.L.str.20@PCREL)
	}
.LBB43_2:                               // %if.then
	{
		r1 = r16
		r17:16 = memd(r29+#24)
		r19:18 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#8)
		r23:22 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		jump ##halide_string_to_string
		r31:30 = deallocframe(r30):raw
	}
.Lfunc_end43:
	.size	halide_buffer_to_string, .Lfunc_end43-halide_buffer_to_string
                                        // -- End function
	.section	.text.halide_malloc_alignment,"ax",@progbits
	.weak	halide_malloc_alignment         // -- Begin function halide_malloc_alignment
	.p2align	4
	.type	halide_malloc_alignment,@function
halide_malloc_alignment:                // @halide_malloc_alignment
// %bb.0:                               // %entry
	{
		r0 = #128
		jumpr r31
	}
.Lfunc_end44:
	.size	halide_malloc_alignment, .Lfunc_end44-halide_malloc_alignment
                                        // -- End function
	.section	.text.halide_reuse_device_allocations,"ax",@progbits
	.weak	halide_reuse_device_allocations // -- Begin function halide_reuse_device_allocations
	.p2align	4
	.type	halide_reuse_device_allocations,@function
halide_reuse_device_allocations:        // @halide_reuse_device_allocations
// %bb.0:                               // %entry
	{
		p0 = tstbit(r1,#0)
		r17:16 = combine(#0,r0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r18 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r2 = memw(r18+##_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE@GOT)
	}
	{
		if (p0) jump:nt .LBB45_3
		memb(r2+#0) = r1
	}
// %bb.1:                               // %if.then
	{
		call ##halide_mutex_lock
		r0 = memw(r18+##_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOT)
	}
	{
		r17 = #0
		r0 = memw(r18+##_ZN6Halide7Runtime8Internal23device_allocation_poolsE@GOT)
	}
	{
		r19 = memw(r0+#0)
		if (cmp.eq(r19.new,#0)) jump:t .LBB45_2
	}
	.p2align	4
.LBB45_4:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	{
		r0 = r16
		r1 = memw(r19+#0)
	}
	{
		callr r1
	}
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17 = add(r0,#0)
		r19 = memw(r19+#4)
		if (!cmp.eq(r19.new,#0)) jump:t .LBB45_4
	}
.LBB45_2:                               // %for.cond.cleanup
	{
		call ##halide_mutex_unlock
		r0 = memw(r18+##_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOT)
	}
.LBB45_3:                               // %if.end5
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end45:
	.size	halide_reuse_device_allocations, .Lfunc_end45-halide_reuse_device_allocations
                                        // -- End function
	.section	.text.halide_can_reuse_device_allocations,"ax",@progbits
	.weak	halide_can_reuse_device_allocations // -- Begin function halide_can_reuse_device_allocations
	.p2align	4
	.type	halide_can_reuse_device_allocations,@function
halide_can_reuse_device_allocations:    // @halide_can_reuse_device_allocations
// %bb.0:                               // %entry
	{
		r0 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r0 = memw(r0+##_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE@GOT)
	}
	{
		r0 = memub(r0+#0)
		jumpr r31
	}
.Lfunc_end46:
	.size	halide_can_reuse_device_allocations, .Lfunc_end46-halide_can_reuse_device_allocations
                                        // -- End function
	.section	.text.halide_register_device_allocation_pool,"ax",@progbits
	.weak	halide_register_device_allocation_pool // -- Begin function halide_register_device_allocation_pool
	.p2align	4
	.type	halide_register_device_allocation_pool,@function
halide_register_device_allocation_pool: // @halide_register_device_allocation_pool
// %bb.0:                               // %entry
	{
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r18 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r17 = memw(r18+##_ZN6Halide7Runtime8Internal21allocation_pools_lockE@GOT)
	}
	{
		call ##halide_mutex_lock
		r0 = r17
	}
	{
		r0 = r17
		r1 = memw(r18+##_ZN6Halide7Runtime8Internal23device_allocation_poolsE@GOT)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r2 = memw(r1+#0)
		memw(r16+#4) = r2.new
	}
	{
		r17:16 = memd(r29+#8)
		memw(r1+#0) = r16
	}                                       // 8-byte Folded Reload
	{
		jump ##halide_mutex_unlock
		r31:30 = deallocframe(r30):raw
	}
.Lfunc_end47:
	.size	halide_register_device_allocation_pool, .Lfunc_end47-halide_register_device_allocation_pool
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx // -- Begin function _ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx,@function
_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx: // @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
// %bb.0:                               // %entry
	{
		p0 = cmp.gt(r1,#-1)
		r17:16 = combine(r5,r4)
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r21:20 = combine(r3,r2)
		r18 = r0
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		memd(r29+#16) = r23:22
		memd(r29+#8) = r25:24
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB48_1
		memd(r29+#0) = r27:26
	}                                       // 8-byte Folded Spill
.LBB48_5:                               // %while.end
	{
		p0 = cmp.eq(r1,#-1); if (p0.new) jump:nt .LBB48_4
	}
// %bb.6:                               // %for.cond.preheader
	{
		r0 = addasl(r18,r1,#3)
		r3:2 = combine(#0,#0)
	}
	{
		r5:4 = memd(r0+#24)
	}
	{
		p0 = cmp.eq(r5:4,r3:2)
		if (p0.new) jump:nt .LBB48_10
	}
// %bb.7:                               // %for.body.lr.ph
	{
		r22 = add(r0,#24)
		r19 = add(r1,#-1)
		r23 = add(r0,#152)
		r24 = add(r0,#280)
	}
	{
		r27:26 = combine(#0,#1)
	}
	.p2align	4
.LBB48_8:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	{
		r1:0 = combine(r19,r18)
		r3:2 = combine(r21,r20)
		r5:4 = combine(r17,r16)
	}
	{
		call ##_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
	}
	{
		r1:0 = memd(r22+#0)
	}
	{
		p0 = cmp.gtu(r1:0,r27:26)
		if (!p0.new) jump:nt .LBB48_10
	}
// %bb.9:                               // %for.body.for.body_crit_edge
                                        //   in Loop: Header=BB48_8 Depth=1
	{
		r5:4 = combine(#0,#1)
		r1:0 = memd(r23+#0)
		r3:2 = memd(r24+#0)
	}
	{
		r21:20 = add(r1:0,r21:20)
		r27:26 = add(r27:26,r5:4)
	}
	{
		r17:16 = add(r3:2,r17:16)
		jump .LBB48_8
	}
.LBB48_10:                              // %if.end
	{
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#8)
		r27:26 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB48_1:                               // %land.rhs.preheader
	{
		r0 = asl(r1,#3)
		r4 = add(r1,#1)
		r3:2 = combine(#0,#1)
	}
	{
		loop0(.LBB48_2,r4)
		r0 = add(r0,add(r18,#24))
	}
.Ltmp8:                                 // Block address taken
.LBB48_2:                               // %land.rhs
                                        // =>This Inner Loop Header: Depth=1
	{
		r5:4 = memd(r0+#0)
	}
	{
		p0 = cmp.eq(r5:4,r3:2)
		if (!p0.new) jump:t .LBB48_5
	}
// %bb.3:                               // %while.body
                                        //   in Loop: Header=BB48_2 Depth=1
	{
		nop
		r0 = add(r0,#-8)
		r1 = add(r1,#-1)
	} :endloop0
.LBB48_4:                               // %if.then
	{
		r1:0 = memd(r18+#0)
		r5:4 = memd(r18+#8)
	}
	{
		r7:6 = add(r1:0,r21:20)
		r1:0 = add(r5:4,r17:16)
		r2 = memw(r18+#408)
		r17:16 = memd(r29+#40)
	}                                       // 8-byte Folded Reload
	{
		r1 = r6
		r19:18 = memd(r29+#32)
		r21:20 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = memd(r29+#16)
		r25:24 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		jump ##memcpy
		r27:26 = memd(r29+#0)
		r31:30 = deallocframe(r30):raw
	}                                       // 8-byte Folded Reload
.Lfunc_end48:
	.size	_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx, .Lfunc_end48-_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv // -- Begin function _ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv,@function
_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv: // @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
// %bb.0:                               // %entry
	{
		r3:2 = memd(r0+#0)
		r5:4 = memd(r0+#8)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (p0.new) jumpr:nt r31
	}
.LBB49_1:                               // %if.then
	{
		r1 = #15
		r5:4 = combine(#0,#0)
		r3:2 = memd(r0+#16)
	}
	{
		jump ##_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx
	}
.Lfunc_end49:
	.size	_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv, .Lfunc_end49-_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b // -- Begin function _ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b,@function
_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b: // @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
// %bb.0:                               // %entry
	{
		p1 = tstbit(r2,#0)
		p0 = tstbit(r4,#0)
		r7:6 = combine(#0,#1)
		allocframe(r29,#456):raw
	}
	{
		r13:12 = combine(#0,#0)
		if (!p1) r5:4 = memd(r1+#0)
		r2 = memub(r1+#25)
	}
	{
		if (p1) r5 = #0
		r2 = add(r2,#7)
		if (p1) r4 = memw(r1+#12)
		memd(r29+#24) = r7:6
	}
	{
		if (!p0) r5:4 = memd(r3+#0)
		memd(r29+#0) = r5:4
	}
	{
		if (p0) r5 = #0
		if (p0) r4 = memw(r3+#12)
		memd(r29+#32) = r7:6
	}
	{
		r4 = lsr(r2,#3)
		r5 = #0
		r2 = memw(r1+#28)
		memd(r29+#8) = r5:4
	}
	{
		p0 = cmp.gt(r2,#0)
		memd(r29+#40) = r7:6
		memd(r29+#48) = r7:6
	}
	{
		memd(r29+#56) = r7:6
		memd(r29+#64) = r7:6
	}
	{
		memd(r29+#72) = r7:6
		memd(r29+#80) = r7:6
	}
	{
		memd(r29+#88) = r7:6
		memd(r29+#96) = r7:6
	}
	{
		memd(r29+#104) = r7:6
		memd(r29+#112) = r7:6
	}
	{
		memd(r29+#120) = r7:6
		memd(r29+#128) = r7:6
	}
	{
		r7:6 = combine(r5,r5)
		memd(r29+#136) = r7:6
		memd(r29+#144) = r7:6
	}
	{
		memd(r29+#448) = r17:16
		memd(r29+#440) = r19:18
	}                                       // 8-byte Folded Spill
	{
		memd(r29+#432) = r21:20
		memd(r29+#424) = r23:22
	}                                       // 8-byte Folded Spill
	{
		memd(r29+#152) = r13:12
		memd(r29+#280) = r13:12
	}
	{
		memd(r29+#160) = r13:12
		memd(r29+#288) = r13:12
	}
	{
		memd(r29+#168) = r13:12
		memd(r29+#296) = r13:12
	}
	{
		memd(r29+#176) = r13:12
		memd(r29+#304) = r13:12
	}
	{
		memd(r29+#184) = r13:12
		memd(r29+#312) = r13:12
	}
	{
		memd(r29+#192) = r13:12
		memd(r29+#320) = r13:12
	}
	{
		memd(r29+#200) = r13:12
		memd(r29+#328) = r13:12
	}
	{
		memd(r29+#208) = r13:12
		memd(r29+#336) = r13:12
	}
	{
		memd(r29+#408) = r5:4
		memd(r29+#216) = r13:12
	}
	{
		memd(r29+#344) = r13:12
		memd(r29+#224) = r13:12
	}
	{
		memd(r29+#352) = r13:12
		memd(r29+#232) = r13:12
	}
	{
		memd(r29+#360) = r13:12
		memd(r29+#240) = r13:12
	}
	{
		memd(r29+#368) = r13:12
		memd(r29+#248) = r13:12
	}
	{
		memd(r29+#376) = r13:12
		memd(r29+#256) = r13:12
	}
	{
		memd(r29+#384) = r13:12
		memd(r29+#264) = r13:12
	}
	{
		memd(r29+#392) = r13:12
		memd(r29+#272) = r13:12
	}
	{
		if (!p0) jump:nt .LBB50_7
		memd(r29+#400) = r13:12
	}
// %bb.1:                               // %for.body17.lr.ph
	{
		r5 = add(r2,#-1)
		r8 = memw(r1+#32)
		r9 = memw(r3+#32)
	}
	{
		r5 = and(r2,#7)
		p1 = cmp.gtu(r5,#6); if (p1.new) jump:t .LBB50_11
	}
// %bb.2:
	{
		r14 = #0
	}
	{
		p1 = cmp.eq(r5,#0); if (!p1.new) jump:t .LBB50_4
	}
	{
		jump .LBB50_7
	}
.LBB50_11:                              // %for.body17.lr.ph.new
	{
		r13 = and(r2,#-8)
		r10 = add(r8,#64)
		r12 = add(r9,#64)
		r7:6 = combine(#0,#0)
	}
	{
		r13 = lsr(r13,#3)
		r14 = #0
		r15 = memw(r10+#-56)
		r28 = memw(r12+#-64)
	}
	{
		p1 = cmp.gtu(r13,#1)
		r13 = add(r13,#-1)
		r14 = add(r14,#8)
		r11 = memw(r10+#-64)
	}
	{
		loop0(.LBB50_12,r13)
		r28 = sub(r28,r11)
		r16 = memw(r10+#-40)
		r13 = memw(r12+#-48)
	}
	{
		r7:6 += mpy(r28,r15)
		r11 = memw(r10+#-48)
		r17 = memw(r10+#-24)
	}
	{
		r13 = sub(r13,r11)
		r15 = memw(r12+#-32)
		r28 = memw(r10+#-32)
	}
	{
		r7:6 += mpy(r13,r16)
		r15 = sub(r15,r28)
		r13 = memw(r10+#-8)
		r11 = memw(r12+#-16)
	}
	{
		r7:6 += mpy(r15,r17)
		r28 = memw(r10+#-16)
		r16 = memw(r10+#8)
	}
	{
		r15 = sub(r11,r28)
		r28 = memw(r12+#0)
		r11 = memw(r10+#0)
	}
	{
		r7:6 += mpy(r15,r13)
		r15 = sub(r28,r11)
		r28 = memw(r10+#16)
		r13 = memw(r12+#16)
	}
	{
		r7:6 += mpy(r15,r16)
		r19 = sub(r13,r28)
		r18 = memw(r10+#24)
		r11 = memw(r12+#32)
	}
	{
		r16 = add(r12,#128)
		r13 = memw(r10+#32)
		r15 = memw(r10+#40)
	}
	{
		r11 = sub(r11,r13)
		r12 = add(r10,#128)
		r13 = memw(r12+#48)
		r28 = memw(r10+#56)
	}
	{
		r7:6 += mpy(r19,r18)
		if (!p1) jump:nt .LBB50_13
		r17 = memw(r10+#48)
	}
	.p2align	4
.LBB50_12:                              // %for.body17
                                        // =>This Inner Loop Header: Depth=1
	{
		r7:6 += mpy(r11,r15)
		r13 = sub(r13,r17)
		r14 = add(r14,#8)
		r15 = memw(r16+#-64)
	}
	{
		r7:6 += mpy(r13,r28)
		r10 = memw(r12+#-64)
		r11 = memw(r12+#-56)
	}
	{
		r13 = sub(r15,r10)
		r15 = memw(r16+#-48)
		r28 = memw(r12+#-48)
	}
	{
		r7:6 += mpy(r13,r11)
		r13 = sub(r15,r28)
		r10 = memw(r12+#-40)
		r15 = memw(r12+#-24)
	}
	{
		r17 = memw(r16+#-32)
		r18 = memw(r12+#-32)
	}
	{
		r7:6 += mpy(r13,r10)
		r13 = sub(r17,r18)
		r11 = memw(r12+#-8)
		r28 = memw(r16+#-16)
	}
	{
		r7:6 += mpy(r13,r15)
		r10 = memw(r12+#-16)
		r15 = memw(r12+#0)
	}
	{
		r13 = sub(r28,r10)
		r17 = memw(r16+#0)
		r22 = memw(r12+#8)
	}
	{
		r7:6 += mpy(r13,r11)
		r20 = sub(r17,r15)
		r28 = memw(r16+#16)
		r10 = memw(r12+#16)
	}
	{
		r7:6 += mpy(r20,r22)
		r10 = sub(r28,r10)
		r19 = memw(r12+#24)
		r11 = memw(r16+#32)
	}
	{
		r16 = add(r16,#128)
		r21 = memw(r12+#32)
		r13 = memw(r16+#48)
	}
	{
		r7:6 += mpy(r10,r19)
		r11 = sub(r11,r21)
		r15 = memw(r12+#40)
		r28 = memw(r12+#56)
	}
	{
		r12 = add(r12,#128)
		r17 = memw(r12+#48)
	} :endloop0
.LBB50_13:
	{
		r7:6 += mpy(r11,r15)
		r12 = sub(r13,r17)
	}
	{
		r7:6 += mpy(r12,r28)
	}
	{
		r13:12 = combine(r7,r6)
		p1 = cmp.eq(r5,#0); if (p1.new) jump:nt .LBB50_7
	}
.LBB50_4:                               // %for.body17.epil.preheader
	{
		r6 = addasl(r9,r14,#4)
		r7 = asl(r14,#4)
		r23 = add(r5,#-1)
		p1 = cmp.gtu(r5,#1)
	}
	{
		loop0(.LBB50_5,r23)
		r9 = add(r8,add(r7,#8))
	}
	{
		r7 = memw(r6++#16)
	}
	{
		r9 = add(r9,#16)
		if (!p1) jump:nt .LBB50_6
		r5 = memw(r9+#0)
		r8 = memw(r9+#-8)
	}
	.p2align	4
.LBB50_5:                               // %for.body17.epil
                                        // =>This Inner Loop Header: Depth=1
	{
		r14 = sub(r7,r8)
		r7 = memw(r6++#16)
		r8 = memw(r9+#-8)
	}
	{
		r13:12 += mpy(r14,r5)
		r9 = add(r9,#16)
		r5 = memw(r9+#0)
	} :endloop0
.LBB50_6:
	{
		r6 = sub(r7,r8)
	}
	{
		r13:12 += mpy(r6,r5)
	}
	{
		r7:6 = combine(r13,r12)
	}
.LBB50_7:                               // %for.cond.cleanup16
	{
		r9:8 = mpyu(r6,r4)
		r5 = memw(r3+#28)
	}
	{
		r9 += mpyi(r4,r7)
		p1 = cmp.eq(r2,r5)
	}
	{
		if (!p1) jump:nt .LBB50_10
		memd(r29+#16) = r9:8
	}
// %bb.8:                               // %lor.lhs.false
	{
		p1 = cmp.gt(r2,#16); if (p1.new) jump:nt .LBB50_10
	}
// %bb.9:                               // %lor.lhs.false
	{
		r5 = memub(r3+#25)
	}
	{
		r5 = add(r5,#7)
	}
	{
		r5 = lsr(r5,#3)
		if (!cmp.eq(r5.new,r4)) jump:nt .LBB50_10
	}
// %bb.14:                              // %if.end
	{
		p1 = cmp.eq(r4,#0); if (p1.new) jump:nt .LBB50_10
	}
// %bb.15:                              // %for.cond49.preheader
	{
		if (!p0) jump:nt .LBB50_26
	}
// %bb.16:                              // %for.body53.lr.ph
	{
		loop1(.LBB50_17,r2)
		r13 = add(r29,#0)
		r3 = memw(r3+#32)
		r1 = memw(r1+#32)
	}
	{
		r2 = add(r13,#24)
		r8 = add(r13,#152)
		r5 = #0
		r7:6 = combine(#0,#0)
	}
	{
		r9 = add(r13,#280)
		r12 = add(r13,#120)
		r13 = add(r13,#144)
		jump .LBB50_17
	}
	.p2align	4
.LBB50_38:                              // %for.cond.cleanup86
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		r19:18 = mpyu(r11,r4)
		r28 = asr(r11,#31)
		r5 = add(r5,#1)
		r16 = memw(r28+#4)
	}
	{
		r19 += mpyi(r4,r28)
		r17 = asr(r16,#31)
		r13 = add(r13,#8)
		memd(r9+r10<<#3) = r15:14
	}
	{
		nop
		memd(r2+r10<<#3) = r17:16
		memd(r8+r10<<#3) = r19:18
	} :endloop1
	{
		jump .LBB50_22
	}
.Ltmp9:                                 // Block address taken
.LBB50_17:                              // %for.body53
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB50_20 Depth 2
                                        //     Child Loop BB50_33 Depth 2
                                        //     Child Loop BB50_37 Depth 2
	{
		r28 = addasl(r3,r5,#4)
		p0 = cmp.eq(r5,#0)
		r10 = #0
	}
	{
		r14 = memw(r28+#8)
	}
	{
		r11 = asr(r14,#31)
	}
	{
		r15:14 = mpyu(r14,r4)
	}
	{
		r15 += mpyi(r4,r11)
		if (p0) jump:nt .LBB50_30
	}
// %bb.18:                              // %for.body74.lr.ph
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		p0 = cmp.eq(r15:14,r7:6)
		r11:10 = combine(#0,r5)
		if (p0.new) jump:nt .LBB50_30
	}
// %bb.19:                              //   in Loop: Header=BB50_17 Depth=1
	{
		loop0(.LBB50_20,r5)
		r10 = r9
	}
	.p2align	4
.Ltmp10:                                // Block address taken
.LBB50_20:                              // %for.body74.us
                                        //   Parent Loop BB50_17 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r17:16 = memd(r10+#0)
	}
	{
		p0 = cmp.gtu(r17:16,r15:14)
		if (p0.new) jump:nt .LBB50_21
	}
// %bb.28:                              // %for.inc81.us
                                        //   in Loop: Header=BB50_20 Depth=2
	{
		r11 = add(r11,#1)
		r10 = add(r10,#8)
	} :endloop0
// %bb.29:                              //   in Loop: Header=BB50_17 Depth=1
	{
		r10 = r5
	}
	.p2align	4
.LBB50_30:                              // %for.end83
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		r11 = addasl(r1,r5,#4)
		p0 = cmp.gtu(r5,r10)
	}
	{
		if (!p0) jump:nt .LBB50_38
		r11 = memw(r11+#8)
	}
.LBB50_31:                              // %for.body87.preheader
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		r17 = sub(r5,r10)
		r16 = r5
	}
	{
		p0 = bitsclr(r17,#7)
		if (p0.new) jump:nt .LBB50_35
	}
// %bb.32:                              //   in Loop: Header=BB50_17 Depth=1
	{
		r16 = and(r17,#7)
		r17 = add(r13,#-8)
		r19:18 = memd(r13+#-128)
		r21:20 = memd(r13+#0)
	}
	{
		r22 = add(r16,#-1)
		p0 = cmp.gtu(r16,#1)
		memd(r13+#-120) = r19:18
		memd(r13+#8) = r21:20
	}
	{
		loop0(.LBB50_33,r22)
		r16 = add(r5,#-1)
		r21:20 = combine(r13,r13)
		r19:18 = memd(r13+#128)
	}
	{
		if (!p0) jump:nt .LBB50_34
	}
	.p2align	4
.LBB50_33:                              // %for.body87.prol
                                        //   Parent Loop BB50_17 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r20 = r17
		r17 = add(r17,#-8)
		r16 = add(r16,#-1)
		memd(r21+#136) = r19:18
	}
	{
		r21 = r20
		r19:18 = memd(r20+#0)
		r23:22 = memd(r20+#-128)
	}
	{
		r19:18 = memd(r20+#128)
		memd(r20+#8) = r19:18
	}
	{
		nop
		memd(r20+#-120) = r23:22
	} :endloop0
.LBB50_34:                              //   in Loop: Header=BB50_17 Depth=1
	{
		memd(r20+#136) = r19:18
	}
.LBB50_35:                              // %for.body87.prol.loopexit
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		r17 = sub(#-1,r10)
	}
	{
		r17 = add(r5,r17)
		if (!cmp.gtu(r17.new,#6)) jump:t .LBB50_38
	}
// %bb.36:                              // %for.body87.preheader1
                                        //   in Loop: Header=BB50_17 Depth=1
	{
		r16 = addasl(r12,r16,#3)
		r17 = sub(r16,r10)
	}
	{
		r17 = add(r17,#7)
	}
	{
		r17 = lsr(r17,#3)
	}
	{
		loop0(.LBB50_37,r17)
	}
	.p2align	4
.Ltmp11:                                // Block address taken
.LBB50_37:                              // %for.body87
                                        //   Parent Loop BB50_17 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r19:18 = memd(r16+#-104)
		r23:22 = memd(r16+#-112)
	}
	{
		r21:20 = memd(r16+#152)
		memd(r16+#-96) = r19:18
	}
	{
		r19:18 = memd(r16+#24)
		memd(r16+#160) = r21:20
	}
	{
		r21:20 = memd(r16+#144)
		memd(r16+#-104) = r23:22
	}
	{
		r23:22 = memd(r16+#-120)
		memd(r16+#32) = r19:18
	}
	{
		r19:18 = memd(r16+#16)
		memd(r16+#152) = r21:20
	}
	{
		r21:20 = memd(r16+#136)
		memd(r16+#-112) = r23:22
	}
	{
		r23:22 = memd(r16+#-128)
		memd(r16+#24) = r19:18
	}
	{
		r19:18 = memd(r16+#8)
		memd(r16+#144) = r21:20
	}
	{
		r21:20 = memd(r16+#128)
		memd(r16+#16) = r19:18
	}
	{
		r19:18 = memd(r16+#0)
		memd(r16+#136) = r21:20
	}
	{
		r19:18 = memd(r16+#-136)
		memd(r16+#8) = r19:18
	}
	{
		r21:20 = memd(r16+#120)
		memd(r16+#-128) = r19:18
	}
	{
		r19:18 = memd(r16+#-144)
		memd(r16+#-120) = r23:22
	}
	{
		r23:22 = memd(r16+#-8)
		memd(r16+#128) = r21:20
	}
	{
		r21:20 = memd(r16+#112)
		memd(r16+#-136) = r19:18
	}
	{
		r19:18 = memd(r16+#-152)
		memd(r16+#0) = r23:22
	}
	{
		r23:22 = memd(r16+#-16)
		memd(r16+#120) = r21:20
	}
	{
		r21:20 = memd(r16+#104)
		memd(r16+#-144) = r19:18
	}
	{
		r19:18 = memd(r16+#-160)
		memd(r16+#-8) = r23:22
	}
	{
		r23:22 = memd(r16+#-24)
		memd(r16+#112) = r21:20
	}
	{
		r21:20 = memd(r16+#96)
		memd(r16+#-16) = r23:22
	}
	{
		r23:22 = memd(r16+#-32)
		memd(r16+#-152) = r19:18
	}
	{
		r16 = add(r16,#-64)
		memd(r16+#104) = r21:20
		memd(r16+#-24) = r23:22
	} :endloop0
	{
		jump .LBB50_38
	}
.LBB50_21:                              //   in Loop: Header=BB50_17 Depth=1
	{
		r11 = addasl(r1,r5,#4)
		r10 = r11
	}
	{
		p0 = cmp.gtu(r5,r10)
	}
	{
		if (!p0) jump:nt .LBB50_38
		r11 = memw(r11+#8)
	}
	{
		jump .LBB50_31
	}
.LBB50_10:                              // %if.then
	{
		call ##memset
		r1 = #0
		r2 = #416
	}
.LBB50_27:                              // %cleanup
	{
		r17:16 = memd(r29+#448)
		r19:18 = memd(r29+#440)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#432)
		r23:22 = memd(r29+#424)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB50_22:                              // %while.cond.preheader
	{
		r3:2 = memd(r29+#408)
		r5:4 = memd(r29+#152)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:t .LBB50_26
	}
// %bb.23:                              // %land.rhs.lr.ph
	{
		r7:6 = combine(#0,#1)
		r9:8 = combine(#0,#0)
		r5:4 = memd(r29+#280)
	}
	.p2align	4
.LBB50_24:                              // %land.rhs
                                        // =>This Inner Loop Header: Depth=1
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB50_26
	}
// %bb.25:                              // %while.body
                                        //   in Loop: Header=BB50_24 Depth=1
	{
		r17:16 = memd(r29+#40)
		r3:2 = memd(r29+#32)
	}
	{
		r21:20 = memd(r29+#48)
		memd(r29+#32) = r17:16
	}
	{
		r15:14 = memd(r29+#24)
		memd(r29+#40) = r21:20
	}
	{
		r19:18 = memd(r29+#168)
		memd(r29+#24) = r3:2
	}
	{
		r3:2 = mpyu(r14,r4)
		r13:12 = memd(r29+#160)
		memd(r29+#160) = r19:18

	} :mem_noshuf
	{
		r3 += mpyi(r14,r5)
		r23:22 = memd(r29+#176)
		r19:18 = memd(r29+#56)
	}
	{
		r3 += mpyi(r4,r15)
		r23:22 = memd(r29+#184)
		memd(r29+#168) = r23:22
	}
	{
		p0 = cmp.eq(r3:2,r13:12)
		r23:22 = memd(r29+#64)
		memd(r29+#176) = r23:22
	}
	{
		r11:10 = memd(r29+#296)
		memd(r29+#56) = r23:22
	}
	{
		r21:20 = memd(r29+#304)
		memd(r29+#48) = r19:18
	}
	{
		r23:22 = memd(r29+#72)
		memd(r29+#296) = r21:20
	}
	{
		r19:18 = memd(r29+#312)
		memd(r29+#64) = r23:22
	}
	{
		r21:20 = memd(r29+#192)
		memd(r29+#304) = r19:18
	}
	{
		r23:22 = memd(r29+#80)
		memd(r29+#184) = r21:20
	}
	{
		r19:18 = memd(r29+#320)
		memd(r29+#72) = r23:22
	}
	{
		r21:20 = memd(r29+#200)
		memd(r29+#312) = r19:18
	}
	{
		r23:22 = memd(r29+#88)
		memd(r29+#192) = r21:20
	}
	{
		r19:18 = memd(r29+#328)
		memd(r29+#80) = r23:22
	}
	{
		r21:20 = memd(r29+#208)
		memd(r29+#320) = r19:18
	}
	{
		r23:22 = memd(r29+#96)
		memd(r29+#200) = r21:20
	}
	{
		r19:18 = memd(r29+#336)
		memd(r29+#88) = r23:22
	}
	{
		r21:20 = memd(r29+#216)
		memd(r29+#328) = r19:18
	}
	{
		r23:22 = memd(r29+#224)
		memd(r29+#208) = r21:20
	}
	{
		r5:4 = memd(r29+#288)
		memd(r29+#216) = r23:22
	}
	{
		r21:20 = memd(r29+#104)
		memd(r29+#288) = r11:10
	}
	{
		r19:18 = memd(r29+#344)
		memd(r29+#96) = r21:20
	}
	{
		r19:18 = memd(r29+#352)
		memd(r29+#336) = r19:18
	}
	{
		r23:22 = memd(r29+#232)
		memd(r29+#344) = r19:18
	}
	{
		r21:20 = memd(r29+#112)
		memd(r29+#224) = r23:22
	}
	{
		r19:18 = memd(r29+#360)
		memd(r29+#104) = r21:20
	}
	{
		r21:20 = memd(r29+#240)
		memd(r29+#352) = r19:18
	}
	{
		r23:22 = memd(r29+#368)
		memd(r29+#232) = r21:20
	}
	{
		r19:18 = memd(r29+#120)
		memd(r29+#360) = r23:22
	}
	{
		r19:18 = memd(r29+#248)
		memd(r29+#112) = r19:18
	}
	{
		r23:22 = memd(r29+#128)
		memd(r29+#240) = r19:18
	}
	{
		r21:20 = memd(r29+#376)
		memd(r29+#120) = r23:22
	}
	{
		r23:22 = memd(r29+#256)
		memd(r29+#368) = r21:20
	}
	{
		r19:18 = memd(r29+#384)
		memd(r29+#248) = r23:22
	}
	{
		r21:20 = memd(r29+#136)
		memd(r29+#376) = r19:18
	}
	{
		r19:18 = memd(r29+#144)
		memd(r29+#128) = r21:20
	}
	{
		r21:20 = memd(r29+#264)
		memd(r29+#136) = r19:18
	}
	{
		r23:22 = memd(r29+#392)
		memd(r29+#256) = r21:20
	}
	{
		r21:20 = memd(r29+#272)
		memd(r29+#384) = r23:22
	}
	{
		r23:22 = memd(r29+#400)
		memd(r29+#408) = r3:2
	}
	{
		memd(r29+#152) = r13:12
		memd(r29+#264) = r21:20
	}
	{
		memd(r29+#280) = r5:4
		memd(r29+#392) = r23:22
	}
	{
		memd(r29+#144) = r7:6
		memd(r29+#272) = r9:8
	}
	{
		if (p0) jump:nt .LBB50_24
		memd(r29+#400) = r9:8
	}
.LBB50_26:                              // %while.end
	{
		call ##__hexagon_memcpy_likely_aligned_min32bytes_mult8bytes
		r1 = add(r29,#0)
		r2 = #416
	}
	{
		jump .LBB50_27
	}
.Ltmp12:                                // Address of block that was removed by CodeGen
.Ltmp13:                                // Address of block that was removed by CodeGen
.Ltmp14:                                // Address of block that was removed by CodeGen
.Lfunc_end50:
	.size	_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b, .Lfunc_end50-_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t // -- Begin function _ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t,@function
_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t: // @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
// %bb.0:                               // %entry
	{
		r17 = #0
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r1+#16)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		p0 = !tstbit(r2,#1)
		if (p0.new) jump:t .LBB51_6
	}
// %bb.1:                               // %if.end
	{
		r17 = #-14
		p0 = tstbit(r2,#0); if (p0.new) jump:t .LBB51_6
	}
// %bb.2:                               // %if.end9
	{
		r16 = r1
		r18 = r0
		r0 = memw(r1+#8)
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB51_3
	}
// %bb.4:                               // %if.end15
	{
		r1 = memw(r0+#60)
	}
	{
		r1:0 = combine(r16,r18)
		r2 = memw(r1+#24)
	}
	{
		callr r2
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB51_5
	}
.LBB51_6:                               // %return
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB51_3:
	{
		r17 = #-19
	}
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB51_5:                               // %if.end23
	{
		r1:0 = combine(r16,r18)
		r3:2 = memd(r16+#16)
	}
	{
		r2 = clrbit(r2,#1)
	}
	{
		call ##halide_msan_annotate_buffer_is_initialized
		r17 = #0
		memd(r16+#16) = r3:2
	}
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end51:
	.size	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t, .Lfunc_end51-_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
                                        // -- End function
	.section	.text.halide_device_release,"ax",@progbits
	.weak	halide_device_release           // -- Begin function halide_device_release
	.p2align	4
	.type	halide_device_release,@function
halide_device_release:                  // @halide_device_release
// %bb.0:                               // %entry
	{
		allocframe(r29,#0):raw
	}
	{
		r1 = memw(r1+#60)
	}
	{
		r1 = memw(r1+#20)
	}
	{
		callr r1
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end52:
	.size	halide_device_release, .Lfunc_end52-halide_device_release
                                        // -- End function
	.section	.text.halide_copy_to_host,"ax",@progbits
	.weak	halide_copy_to_host             // -- Begin function halide_copy_to_host
	.p2align	4
	.type	halide_copy_to_host,@function
halide_copy_to_host:                    // @halide_copy_to_host
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r19 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		call ##halide_mutex_lock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		p0 = cmp.eq(r17,#0); if (p0.new) jump:nt .LBB53_1
	}
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r4 = memw(r17+#8)
		r1:0 = memd(r17+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r4,#0)
		if (p1.new) jump:nt .LBB53_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB53_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r16
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB53_12
	}
	{
		jump .LBB53_11
	}
.LBB53_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.6.17@PCREL)
		r0 = r16
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB53_12
	}
	{
		jump .LBB53_11
	}
.LBB53_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB53_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB53_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r16
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB53_12
	}
	{
		jump .LBB53_11
	}
.LBB53_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r17+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB53_11
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r16
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB53_12
	}
.LBB53_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split
	{
		call ##_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
		r1:0 = combine(r17,r16)
	}
	{
		r18 = r0
	}
.LBB53_12:                              // %cleanup
	{
		call ##halide_mutex_unlock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r0 = r18
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end53:
	.size	halide_copy_to_host, .Lfunc_end53-halide_copy_to_host
                                        // -- End function
	.section	.text.copy_to_device_already_locked,"ax",@progbits
	.weak	copy_to_device_already_locked   // -- Begin function copy_to_device_already_locked
	.p2align	4
	.type	copy_to_device_already_locked,@function
copy_to_device_already_locked:          // @copy_to_device_already_locked
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB54_1
		r18 = r2
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r4 = memw(r16+#8)
		r1:0 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r4,#0)
		if (p1.new) jump:nt .LBB54_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB54_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
	{
		jump .LBB54_11
	}
.LBB54_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.18@PCREL)
		r0 = r17
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
	{
		jump .LBB54_11
	}
.LBB54_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB54_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB54_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB54_11
	}
.LBB54_21:                              // %cleanup
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB54_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r16+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB54_11
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
.LBB54_11:                              // %if.end
	{
		p0 = cmp.eq(r18,#0); if (!p0.new) jump:t .LBB54_13
	}
// %bb.12:                              // %if.then2
	{
		r18 = memw(r16+#8)
		if (cmp.eq(r18.new,#0)) jump:nt .LBB54_22
	}
.LBB54_13:                              // %if.end11
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r16+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB54_16
	}
// %bb.14:                              // %land.lhs.true
	{
		r0 = memw(r16+#8)
		if (cmp.eq(r0.new,r18)) jump:nt .LBB54_17
	}
// %bb.15:                              // %if.then14
	{
		r1 = add(pc,##.L.str.9.19@PCREL)
		r0 = r17
	}
	{
		call ##halide_error
		r19 = #-42
	}
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB54_16:                              // %if.then18
	{
		call ##halide_device_malloc
		r1:0 = combine(r16,r17)
		r2 = r18
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
.LBB54_17:                              // %if.end27
	{
		r19 = #0
		r1:0 = memd(r16+#16)
	}
	{
		p0 = tstbit(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
// %bb.18:                              // %if.then29
	{
		p0 = tstbit(r0,#1)
		r19 = #-15
		if (p0.new) jump:t .LBB54_21
	}
// %bb.19:                              // %if.else
	{
		r1 = memw(r18+#60)
	}
	{
		r1:0 = combine(r16,r17)
		r2 = memw(r1+#28)
	}
	{
		callr r2
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB54_21
	}
// %bb.20:                              // %if.then46
	{
		r19 = #0
		r1:0 = memd(r16+#16)
	}
	{
		r0 = clrbit(r0,#0)
	}
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		memd(r16+#16) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB54_22:                              // %if.then7
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		jump ##halide_error_no_device_interface
		r31:30 = deallocframe(r30):raw
	}
.Lfunc_end54:
	.size	copy_to_device_already_locked, .Lfunc_end54-copy_to_device_already_locked
                                        // -- End function
	.section	.text.halide_device_malloc,"ax",@progbits
	.weak	halide_device_malloc            // -- Begin function halide_device_malloc
	.p2align	4
	.type	halide_device_malloc,@function
halide_device_malloc:                   // @halide_device_malloc
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r2)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB55_1
		r18 = r0
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r5:4 = combine(#0,#0)
		r0 = memw(r17+#8)
		r3:2 = memd(r17+#0)
	}
	{
		p1 = cmp.eq(r3:2,r5:4)
		p0 = cmp.eq(r0,#0)
		if (p1.new) jump:nt .LBB55_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB55_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r18
	}
	{
		jump .LBB55_10
	}
.LBB55_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.17.20@PCREL)
		r0 = r18
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB55_10
	}
.LBB55_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB55_8
	}
// %bb.6:                               // %if.end10.i
	{
		r5:4 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB55_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r18
	}
	{
		jump .LBB55_10
	}
.LBB55_8:                               // %if.end16.i
	{
		r5:4 = combine(#0,#3)
		r3:2 = memd(r17+#16)
	}
	{
		r2 = and(r2,#3)
		r3 = #0
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:t .LBB55_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r18
	}
.LBB55_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB55_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r0 = memw(r17+#8)
	}
.LBB55_12:                              // %if.end
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB55_15
	}
// %bb.13:                              // %if.end
	{
		p0 = cmp.eq(r0,r16); if (p0.new) jump:nt .LBB55_15
	}
// %bb.14:                              // %if.then6
	{
		r1 = add(pc,##.L.str.20.21@PCREL)
		r0 = r18
	}
	{
		call ##halide_error
	}
	{
		r0 = #-42
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB55_15:                              // %if.end7
	{
		r0 = memw(r16+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = memw(r16+#60)
	}
	{
		r1:0 = combine(r17,r18)
		r2 = memw(r0+#8)
	}
	{
		callr r2
	}
	{
		r16 = r0
		r1 = memw(r16+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		p0 = cmp.eq(r16,#0)
	}
	{
		r0 = mux(p0,#0,#-16)
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end55:
	.size	halide_device_malloc, .Lfunc_end55-halide_device_malloc
                                        // -- End function
	.section	.text.halide_copy_to_device,"ax",@progbits
	.weak	halide_copy_to_device           // -- Begin function halide_copy_to_device
	.p2align	4
	.type	halide_copy_to_device,@function
halide_copy_to_device:                  // @halide_copy_to_device
// %bb.0:                               // %entry
	{
		r3 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r2)
		r16 = r0
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r17 = memw(r3+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		call ##halide_mutex_lock
		r0 = r17
	}
	{
		call ##copy_to_device_already_locked
		r1:0 = combine(r19,r16)
		r2 = r18
	}
	{
		call ##halide_mutex_unlock
		r16 = r0
		r0 = r17
	}
	{
		r0 = r16
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end56:
	.size	halide_copy_to_device, .Lfunc_end56-halide_copy_to_device
                                        // -- End function
	.section	.text.halide_device_sync,"ax",@progbits
	.weak	halide_device_sync              // -- Begin function halide_device_sync
	.p2align	4
	.type	halide_device_sync,@function
halide_device_sync:                     // @halide_device_sync
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB57_1
	}
// %bb.2:                               // %if.end.i
	{
		r5:4 = combine(#0,#0)
		r0 = memw(r16+#8)
		r3:2 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r3:2,r5:4)
		p0 = cmp.eq(r0,#0)
		if (p1.new) jump:nt .LBB57_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB57_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r17
	}
	{
		jump .LBB57_10
	}
.LBB57_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.16.22@PCREL)
		r0 = r17
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB57_10
	}
.LBB57_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB57_8
	}
// %bb.6:                               // %if.end10.i
	{
		r5:4 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB57_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r17
	}
	{
		jump .LBB57_10
	}
.LBB57_8:                               // %if.end16.i
	{
		r5:4 = combine(#0,#3)
		r3:2 = memd(r16+#16)
	}
	{
		r2 = and(r2,#3)
		r3 = #0
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:t .LBB57_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r17
	}
.LBB57_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#0)
		if (!p0.new) r31:30 = dealloc_return(r30):t:raw
	}                                       // 8-byte Folded Reload
.LBB57_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r0 = memw(r16+#8)
	}
.LBB57_12:                              // %if.end
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB57_14
	}
// %bb.13:                              // %if.end5
	{
		r1 = memw(r0+#60)
	}
	{
		r1:0 = combine(r16,r17)
		r2 = memw(r1+#16)
	}
	{
		callr r2
	}
	{
		p0 = cmp.eq(r0,#0)
		r17:16 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r0 = mux(p0,#0,#-17)
		r31:30 = dealloc_return(r30):raw
	}
.LBB57_14:                              // %if.then3
	{
		r0 = r17
		r17:16 = memd(r29+#0)
		deallocframe
	}                                       // 8-byte Folded Reload
	{
		jump ##halide_error_no_device_interface
	}
.Lfunc_end57:
	.size	halide_device_sync, .Lfunc_end57-halide_device_sync
                                        // -- End function
	.section	.text.halide_device_free,"ax",@progbits
	.weak	halide_device_free              // -- Begin function halide_device_free
	.p2align	4
	.type	halide_device_free,@function
halide_device_free:                     // @halide_device_free
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r0)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB58_1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r18 = memw(r17+#8)
		r1:0 = memd(r17+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r18,#0)
		if (p1.new) jump:nt .LBB58_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB58_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r16
	}
	{
		jump .LBB58_10
	}
.LBB58_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.21.23@PCREL)
		r0 = r16
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB58_10
	}
.LBB58_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB58_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB58_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r16
	}
	{
		jump .LBB58_10
	}
.LBB58_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r17+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB58_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r16
	}
.LBB58_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB58_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r18 = memw(r17+#8)
	}
.LBB58_12:                              // %if.end
	{
		p0 = cmp.eq(r18,#0); if (p0.new) jump:nt .LBB58_16
	}
// %bb.13:                              // %if.then3
	{
		r0 = memw(r18+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = memw(r18+#60)
	}
	{
		r1:0 = combine(r17,r16)
		r2 = memw(r0+#12)
	}
	{
		callr r2
	}
	{
		r18 = r0
		r1 = memw(r18+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r17+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB58_15
	}
// %bb.14:                              // %if.then8
	{
		r1 = add(pc,##.L.str.22.24@PCREL)
		r0 = r16
	}
	{
		call ##halide_print
	}
	{
		call ##abort
	}
.LBB58_15:                              // %do.end
	{
		p0 = cmp.eq(r18,#0)
	}
	{
		r0 = mux(p0,#0,#-18)
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB58_16:                              // %if.end11
	{
		r0 = #0
		r3:2 = memd(r17+#16)
	}
	{
		r2 = clrbit(r2,#1)
	}
	{
		r17:16 = memd(r29+#8)
		memd(r17+#16) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end58:
	.size	halide_device_free, .Lfunc_end58-halide_device_free
                                        // -- End function
	.section	.text.halide_device_free_as_destructor,"ax",@progbits
	.weak	halide_device_free_as_destructor // -- Begin function halide_device_free_as_destructor
	.p2align	4
	.type	halide_device_free_as_destructor,@function
halide_device_free_as_destructor:       // @halide_device_free_as_destructor
// %bb.0:                               // %entry
	{
		jump ##halide_device_free
	}
.Lfunc_end59:
	.size	halide_device_free_as_destructor, .Lfunc_end59-halide_device_free_as_destructor
                                        // -- End function
	.section	.text.halide_device_and_host_malloc,"ax",@progbits
	.weak	halide_device_and_host_malloc   // -- Begin function halide_device_and_host_malloc
	.p2align	4
	.type	halide_device_and_host_malloc,@function
halide_device_and_host_malloc:          // @halide_device_and_host_malloc
// %bb.0:                               // %entry
	{
		p0 = cmp.eq(r1,#0)
		r17:16 = combine(r2,r0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB60_1
		r18 = r1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r5:4 = combine(#0,#0)
		r0 = memw(r18+#8)
		r3:2 = memd(r18+#0)
	}
	{
		p1 = cmp.eq(r3:2,r5:4)
		p0 = cmp.eq(r0,#0)
		if (p1.new) jump:nt .LBB60_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB60_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r16
	}
	{
		jump .LBB60_10
	}
.LBB60_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.23.25@PCREL)
		r0 = r16
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB60_10
	}
.LBB60_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB60_8
	}
// %bb.6:                               // %if.end10.i
	{
		r5:4 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB60_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r16
	}
	{
		jump .LBB60_10
	}
.LBB60_8:                               // %if.end16.i
	{
		r5:4 = combine(#0,#3)
		r3:2 = memd(r18+#16)
	}
	{
		r2 = and(r2,#3)
		r3 = #0
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:t .LBB60_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r16
	}
.LBB60_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB60_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r0 = memw(r18+#8)
	}
.LBB60_12:                              // %if.end
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB60_15
	}
// %bb.13:                              // %if.end
	{
		p0 = cmp.eq(r0,r17); if (p0.new) jump:nt .LBB60_15
	}
// %bb.14:                              // %if.then6
	{
		r1 = add(pc,##.L.str.25.26@PCREL)
		r0 = r16
	}
	{
		call ##halide_error
	}
	{
		r0 = #-42
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB60_15:                              // %if.end7
	{
		r0 = memw(r17+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = memw(r17+#60)
	}
	{
		r1:0 = combine(r18,r16)
		r2 = memw(r0+#32)
	}
	{
		callr r2
	}
	{
		r17 = r0
		r1 = memw(r17+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		p0 = cmp.eq(r17,#0); if (p0.new) jump:nt .LBB60_16
	}
// %bb.17:                              // %if.then12
	{
		r1 = add(pc,##.L.str.26@PCREL)
		r0 = r16
	}
	{
		call ##halide_error
	}
	{
		r0 = #-16
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB60_16:
	{
		r0 = #0
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end60:
	.size	halide_device_and_host_malloc, .Lfunc_end60-halide_device_and_host_malloc
                                        // -- End function
	.section	.text.halide_device_and_host_free,"ax",@progbits
	.weak	halide_device_and_host_free     // -- Begin function halide_device_and_host_free
	.p2align	4
	.type	halide_device_and_host_free,@function
halide_device_and_host_free:            // @halide_device_and_host_free
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB61_1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r18 = memw(r16+#8)
		r1:0 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r18,#0)
		if (p1.new) jump:nt .LBB61_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB61_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r17
	}
	{
		jump .LBB61_10
	}
.LBB61_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.27@PCREL)
		r0 = r17
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB61_10
	}
.LBB61_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB61_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB61_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r17
	}
	{
		jump .LBB61_10
	}
.LBB61_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r16+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB61_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r17
	}
.LBB61_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB61_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r18 = memw(r16+#8)
	}
.LBB61_12:                              // %if.end
	{
		p0 = cmp.eq(r18,#0); if (p0.new) jump:nt .LBB61_16
	}
// %bb.13:                              // %if.then3
	{
		r0 = memw(r18+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = memw(r18+#60)
	}
	{
		r1:0 = combine(r16,r17)
		r2 = memw(r0+#36)
	}
	{
		callr r2
	}
	{
		r18 = r0
		r1 = memw(r18+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r16+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB61_15
	}
// %bb.14:                              // %if.then8
	{
		r1 = add(pc,##.L.str.28@PCREL)
		r0 = r17
	}
	{
		call ##halide_print
	}
	{
		call ##abort
	}
.LBB61_15:                              // %do.end
	{
		p0 = cmp.eq(r18,#0)
	}
	{
		r0 = mux(p0,#0,#-18)
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB61_16:                              // %if.else11
	{
		r1 = memw(r16+#12)
		if (cmp.eq(r1.new,#0)) jump:nt .LBB61_18
	}
// %bb.17:                              // %if.then13
	{
		call ##halide_free
		r0 = r17
	}
	{
		memw(r16+#12) = #0
	}
.LBB61_18:                              // %if.end17
	{
		r0 = #0
		r3:2 = memd(r16+#16)
	}
	{
		r2 = clrbit(r2,#1)
	}
	{
		r17:16 = memd(r29+#8)
		memd(r16+#16) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end61:
	.size	halide_device_and_host_free, .Lfunc_end61-halide_device_and_host_free
                                        // -- End function
	.section	.text.halide_default_device_and_host_malloc,"ax",@progbits
	.weak	halide_default_device_and_host_malloc // -- Begin function halide_default_device_and_host_malloc
	.p2align	4
	.type	halide_default_device_and_host_malloc,@function
halide_default_device_and_host_malloc:  // @halide_default_device_and_host_malloc
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB62_1
		r18 = r2
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r4 = memw(r16+#8)
		r1:0 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r4,#0)
		if (p1.new) jump:nt .LBB62_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB62_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB62_69
	}
	{
		jump .LBB62_11
	}
.LBB62_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.29@PCREL)
		r0 = r17
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB62_69
	}
	{
		jump .LBB62_11
	}
.LBB62_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB62_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB62_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB62_11
	}
.LBB62_69:                              // %cleanup13
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB62_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r16+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB62_11
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r17
	}
	{
		r19 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB62_69
	}
.LBB62_11:                              // %if.end
	{
		r4 = memw(r16+#28)
		if (!cmp.gt(r4.new,#0)) jump:nt .LBB62_12
	}
// %bb.13:                              // %for.body.lr.ph.i.i
	{
		r2 = and(r4,#7)
		r0 = add(r4,#-1)
		r1 = memw(r16+#32)
	}
	{
		r3 = add(r1,#68)
		p0 = cmp.gtu(r0,#6); if (p0.new) jump:t .LBB62_30
	}
// %bb.14:
	{
		r0 = #0
		r5 = #0
	}
.LBB62_15:                              // %for.body.i11.i.preheader.unr-lcssa
	{
		p1 = cmp.eq(r2,#0); if (p1.new) jump:nt .LBB62_20
	}
// %bb.16:                              // %for.body.i.i.epil.preheader
	{
		loop0(.LBB62_17,r2)
		r5 = asl(r5,#4)
	}
	{
		r5 = add(r1,add(r5,#8))
		jump .LBB62_17
	}
	.p2align	4
.LBB62_19:                              // %if.end.i.i.epil
                                        //   in Loop: Header=BB62_17 Depth=1
	{
		r5 = add(r5,#16)
		nop
	} :endloop0
	{
		jump .LBB62_20
	}
.Ltmp15:                                // Block address taken
.LBB62_17:                              // %for.body.i.i.epil
                                        // =>This Inner Loop Header: Depth=1
	{
		r6 = memw(r5+#0)
		if (!cmp.gt(r6.new,#0)) jump:t .LBB62_19
	}
// %bb.18:                              // %if.then.i.i.epil
                                        //   in Loop: Header=BB62_17 Depth=1
	{
		r7 = memw(r5+#-4)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r0 += mpyi(r7,r6)
		jump .LBB62_19
	}
.LBB62_20:                              // %for.body.i11.i.preheader
	{
		if (p0) jump:nt .LBB62_48
	}
// %bb.21:
	{
		r5:4 = combine(#0,#0)
	}
.LBB62_22:                              // %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit.unr-lcssa
	{
		if (p1) jump:nt .LBB62_27
	}
// %bb.23:                              // %for.body.i11.i.epil.preheader
	{
		loop0(.LBB62_24,r2)
		r3 = asl(r5,#4)
	}
	{
		r1 = add(r1,add(r3,#8))
		jump .LBB62_24
	}
	.p2align	4
.LBB62_26:                              // %if.end.i20.i.epil
                                        //   in Loop: Header=BB62_24 Depth=1
	{
		r1 = add(r1,#16)
		nop
	} :endloop0
	{
		jump .LBB62_27
	}
.Ltmp16:                                // Block address taken
.LBB62_24:                              // %for.body.i11.i.epil
                                        // =>This Inner Loop Header: Depth=1
	{
		r2 = memw(r1+#0)
		if (cmp.gt(r2.new,#-1)) jump:nt .LBB62_26
	}
// %bb.25:                              // %if.then.i16.i.epil
                                        //   in Loop: Header=BB62_24 Depth=1
	{
		r3 = memw(r1+#-4)
	}
	{
		r3 = add(r3,#-1)
	}
	{
		r4 += mpyi(r3,r2)
		jump .LBB62_26
	}
.LBB62_27:                              // %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
	{
		r1 = add(r0,sub(#1,r4))
		jump .LBB62_28
	}
.LBB62_12:
	{
		r1 = #1
	}
.LBB62_28:                              // %_ZNK15halide_buffer_t13size_in_bytesEv.exit
	{
		r0 = r17
		r2 = memub(r16+#25)
	}
	{
		r2 = add(r2,#7)
	}
	{
		r2 = lsr(r2,#3)
	}
	{
		r1 = mpyi(r2,r1)
		call ##halide_malloc
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB62_29
		memw(r16+#12) = r0
	}
// %bb.66:                              // %if.end6
	{
		call ##halide_device_malloc
		r1:0 = combine(r16,r17)
		r2 = r18
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB62_67
	}
// %bb.68:                              // %if.then9
	{
		r0 = r17
		r19 = r0
		r1 = memw(r16+#12)
	}
	{
		call ##halide_free
	}
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		memw(r16+#12) = #0
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB62_29:
	{
		r19 = #-1
	}
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB62_30:                              // %for.body.lr.ph.i.i.new
	{
		r0 = and(r4,#-8)
		r6 = r3
	}
	{
		r5 = lsr(r0,#3)
		r0 = #0
	}
	{
		loop0(.LBB62_31,r5)
		r5 = #0 ; jump .LBB62_31
	}
	.p2align	4
.LBB62_47:                              // %if.end.i.i.7
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r5 = add(r5,#8)
		r6 = add(r6,#128)
	} :endloop0
	{
		jump .LBB62_15
	}
.Ltmp17:                                // Block address taken
.LBB62_31:                              // %for.body.i.i
                                        // =>This Inner Loop Header: Depth=1
	{
		r7 = memw(r6+#-60)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_32
	}
// %bb.33:                              // %if.end.i.i
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#-44)
		if (!cmp.gt(r7.new,#0)) jump:nt .LBB62_35
	}
.LBB62_34:                              // %if.then.i.i.1
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#-48)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
	}
.LBB62_35:                              // %if.end.i.i.1
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#-28)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_36
	}
// %bb.37:                              // %if.end.i.i.2
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#-12)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_38
	}
.LBB62_39:                              // %if.end.i.i.3
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#4)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_40
	}
.LBB62_41:                              // %if.end.i.i.4
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#20)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_42
	}
.LBB62_43:                              // %if.end.i.i.5
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#36)
		if (cmp.gt(r7.new,#0)) jump:nt .LBB62_44
	}
.LBB62_45:                              // %if.end.i.i.6
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r7 = memw(r6+#52)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_47
	}
	{
		jump .LBB62_46
	}
	.p2align	4
.LBB62_32:                              // %if.then.i.i
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#-64)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#-44)
		if (cmp.gt(r7.new,#0)) jump:t .LBB62_34
	}
	{
		jump .LBB62_35
	}
	.p2align	4
.LBB62_36:                              // %if.then.i.i.2
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#-32)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#-12)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_39
	}
.LBB62_38:                              // %if.then.i.i.3
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#-16)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#4)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_41
	}
.LBB62_40:                              // %if.then.i.i.4
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#0)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#20)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_43
	}
.LBB62_42:                              // %if.then.i.i.5
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#16)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#36)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_45
	}
.LBB62_44:                              // %if.then.i.i.6
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#32)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		r7 = memw(r6+#52)
		if (!cmp.gt(r7.new,#0)) jump:t .LBB62_47
	}
.LBB62_46:                              // %if.then.i.i.7
                                        //   in Loop: Header=BB62_31 Depth=1
	{
		r8 = memw(r6+#48)
	}
	{
		r8 = add(r8,#-1)
	}
	{
		r0 += mpyi(r8,r7)
		jump .LBB62_47
	}
.LBB62_48:                              // %for.body.i11.i.preheader.new
	{
		r5 = and(r4,#-8)
	}
	{
		r5 = lsr(r5,#3)
	}
	{
		loop0(.LBB62_49,r5)
		r5:4 = combine(#0,#0)
		jump .LBB62_49
	}
	.p2align	4
.LBB62_65:                              // %if.end.i20.i.7
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r5 = add(r5,#8)
		r3 = add(r3,#128)
	} :endloop0
	{
		jump .LBB62_22
	}
.Ltmp18:                                // Block address taken
.LBB62_49:                              // %for.body.i11.i
                                        // =>This Inner Loop Header: Depth=1
	{
		r6 = memw(r3+#-60)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_51
	}
// %bb.50:                              // %if.then.i16.i
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#-64)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_51:                              // %if.end.i20.i
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#-44)
		if (!cmp.gt(r6.new,#-1)) jump:nt .LBB62_52
	}
// %bb.53:                              // %if.end.i20.i.1
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#-28)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_55
	}
.LBB62_54:                              // %if.then.i16.i.2
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#-32)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_55:                              // %if.end.i20.i.2
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#-12)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_57
	}
// %bb.56:                              // %if.then.i16.i.3
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#-16)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_57:                              // %if.end.i20.i.3
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#4)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_59
	}
// %bb.58:                              // %if.then.i16.i.4
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#0)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_59:                              // %if.end.i20.i.4
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#20)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_61
	}
// %bb.60:                              // %if.then.i16.i.5
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#16)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_61:                              // %if.end.i20.i.5
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#36)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_63
	}
// %bb.62:                              // %if.then.i16.i.6
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#32)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
	}
.LBB62_63:                              // %if.end.i20.i.6
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r6 = memw(r3+#52)
		if (cmp.gt(r6.new,#-1)) jump:nt .LBB62_65
	}
// %bb.64:                              // %if.then.i16.i.7
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#48)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
		jump .LBB62_65
	}
	.p2align	4
.LBB62_52:                              // %if.then.i16.i.1
                                        //   in Loop: Header=BB62_49 Depth=1
	{
		r7 = memw(r3+#-48)
	}
	{
		r7 = add(r7,#-1)
	}
	{
		r4 += mpyi(r7,r6)
		r6 = memw(r3+#-28)
		if (!cmp.gt(r6.new,#-1)) jump:t .LBB62_54
	}
	{
		jump .LBB62_55
	}
.LBB62_67:
	{
		r19 = #0
	}
	{
		r0 = r19
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end62:
	.size	halide_default_device_and_host_malloc, .Lfunc_end62-halide_default_device_and_host_malloc
                                        // -- End function
	.section	.text.halide_default_device_and_host_free,"ax",@progbits
	.weak	halide_default_device_and_host_free // -- Begin function halide_default_device_and_host_free
	.p2align	4
	.type	halide_default_device_and_host_free,@function
halide_default_device_and_host_free:    // @halide_default_device_and_host_free
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB63_1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r4 = memw(r16+#8)
		r1:0 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r4,#0)
		if (p1.new) jump:nt .LBB63_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB63_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r17
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB63_14
	}
	{
		jump .LBB63_11
	}
.LBB63_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.30@PCREL)
		r0 = r17
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB63_14
	}
	{
		jump .LBB63_11
	}
.LBB63_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB63_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB63_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r17
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB63_11
	}
.LBB63_14:                              // %cleanup
	{
		r0 = r18
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB63_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r16+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB63_11
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r17
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB63_14
	}
.LBB63_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split
	{
		call ##halide_device_free
		r1:0 = combine(r16,r17)
	}
	{
		r18 = r0
		r1 = memw(r16+#12)
		if (cmp.eq(r1.new,#0)) jump:nt .LBB63_13
	}
// %bb.12:                              // %if.then2
	{
		call ##halide_free
		r0 = r17
	}
	{
		memw(r16+#12) = #0
	}
.LBB63_13:                              // %if.end5
	{
		r2 = #-4
		r1:0 = memd(r16+#16)
	}
	{
		r0 = and(r0,r2)
	}
	{
		r0 = r18
		r17:16 = memd(r29+#8)
		memd(r16+#16) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end63:
	.size	halide_default_device_and_host_free, .Lfunc_end63-halide_default_device_and_host_free
                                        // -- End function
	.section	.text.halide_device_wrap_native,"ax",@progbits
	.weak	halide_device_wrap_native       // -- Begin function halide_device_wrap_native
	.p2align	4
	.type	halide_device_wrap_native,@function
halide_device_wrap_native:              // @halide_device_wrap_native
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r4)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#24)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r3,r2)
		r20 = r0
		memd(r29+#8) = r19:18
		memd(r29+#0) = r21:20
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB64_1
	}
// %bb.2:                               // %if.end.i
	{
		r5:4 = combine(#0,#0)
		r0 = memw(r17+#8)
		r3:2 = memd(r17+#0)
	}
	{
		p1 = cmp.eq(r3:2,r5:4)
		p0 = cmp.eq(r0,#0)
		if (p1.new) jump:nt .LBB64_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB64_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r20
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB64_16
	}
	{
		jump .LBB64_11
	}
.LBB64_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.31@PCREL)
		r0 = r20
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB64_16
	}
	{
		jump .LBB64_11
	}
.LBB64_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB64_8
	}
// %bb.6:                               // %if.end10.i
	{
		r5:4 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB64_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r20
	}
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB64_11
	}
.LBB64_16:                              // %cleanup12
	{
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB64_8:                               // %if.end16.i
	{
		r5:4 = combine(#0,#3)
		r3:2 = memd(r17+#16)
	}
	{
		r2 = and(r2,#3)
		r3 = #0
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:t .LBB64_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r20
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB64_16
	}
.LBB64_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r0 = memw(r17+#8)
	}
.LBB64_12:                              // %if.end
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB64_15
	}
// %bb.13:                              // %if.end
	{
		p0 = cmp.eq(r0,r16); if (p0.new) jump:nt .LBB64_15
	}
// %bb.14:                              // %if.then4
	{
		r1 = add(pc,##.L.str.32@PCREL)
		r0 = r20
	}
	{
		call ##halide_error
	}
	{
		r0 = #-42
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB64_15:                              // %if.end5
	{
		r0 = memw(r16+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r3:2 = combine(r19,r18)
		r0 = memw(r16+#60)
		memw(r17+#8) = r16
	}
	{
		r1:0 = combine(r17,r20)
		r4 = memw(r0+#56)
	}
	{
		callr r4
	}
	{
		r16 = r0
		r1 = memw(r16+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		p0 = cmp.eq(r16,#0)
	}
	{
		r0 = mux(p0,#0,#-16)
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end64:
	.size	halide_device_wrap_native, .Lfunc_end64-halide_device_wrap_native
                                        // -- End function
	.section	.text.halide_device_detach_native,"ax",@progbits
	.weak	halide_device_detach_native     // -- Begin function halide_device_detach_native
	.p2align	4
	.type	halide_device_detach_native,@function
halide_device_detach_native:            // @halide_device_detach_native
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r0)
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB65_1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r3:2 = combine(#0,#0)
		r18 = memw(r17+#8)
		r1:0 = memd(r17+#0)
	}
	{
		p1 = cmp.eq(r1:0,r3:2)
		p0 = cmp.eq(r18,#0)
		if (p1.new) jump:nt .LBB65_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB65_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
		r0 = r16
	}
	{
		jump .LBB65_10
	}
.LBB65_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.33@PCREL)
		r0 = r16
	}
	{
		call ##halide_error_buffer_is_null
	}
	{
		jump .LBB65_10
	}
.LBB65_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB65_8
	}
// %bb.6:                               // %if.end10.i
	{
		r3:2 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:nt .LBB65_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
		r0 = r16
	}
	{
		jump .LBB65_10
	}
.LBB65_8:                               // %if.end16.i
	{
		r3:2 = combine(#0,#3)
		r1:0 = memd(r17+#16)
	}
	{
		r0 = and(r0,#3)
		r1 = #0
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (!p0.new) jump:t .LBB65_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
		r0 = r16
	}
.LBB65_10:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
	{
		p0 = cmp.eq(r0,#0)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB65_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r18 = memw(r17+#8)
	}
.LBB65_12:                              // %if.end
	{
		p0 = cmp.eq(r18,#0); if (p0.new) jump:nt .LBB65_13
	}
// %bb.14:                              // %if.then3
	{
		r0 = memw(r18+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = memw(r18+#60)
	}
	{
		r1:0 = combine(r17,r16)
		r2 = memw(r0+#60)
	}
	{
		callr r2
	}
	{
		r18 = r0
		r1 = memw(r18+#60)
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r17+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB65_16
	}
// %bb.15:                              // %if.then8
	{
		r1 = add(pc,##.L.str.34@PCREL)
		r0 = r16
	}
	{
		call ##halide_print
	}
	{
		call ##abort
	}
.LBB65_16:                              // %do.end
	{
		p0 = cmp.eq(r18,#0)
	}
	{
		r0 = mux(p0,#0,#-33)
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB65_13:
	{
		r0 = #0
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end65:
	.size	halide_device_detach_native, .Lfunc_end65-halide_device_detach_native
                                        // -- End function
	.section	.text.halide_default_device_wrap_native,"ax",@progbits
	.weak	halide_default_device_wrap_native // -- Begin function halide_default_device_wrap_native
	.p2align	4
	.type	halide_default_device_wrap_native,@function
halide_default_device_wrap_native:      // @halide_default_device_wrap_native
// %bb.0:                               // %entry
	{
		r5:4 = combine(#0,#0)
		r16 = r1
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r0 = #-32
		r7:6 = memd(r1+#0)
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		p0 = cmp.eq(r7:6,r5:4)
		if (!p0.new) r17:16 = memd(r29+#8)
		if (!p0.new) r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB66_1:                               // %if.end
	{
		r19:18 = combine(r3,r2)
		r0 = memw(r16+#8)
	}
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r0 = #0
		r17:16 = memd(r29+#8)
		memd(r16+#0) = r19:18
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end66:
	.size	halide_default_device_wrap_native, .Lfunc_end66-halide_default_device_wrap_native
                                        // -- End function
	.section	.text.halide_default_device_detach_native,"ax",@progbits
	.weak	halide_default_device_detach_native // -- Begin function halide_default_device_detach_native
	.p2align	4
	.type	halide_default_device_detach_native,@function
halide_default_device_detach_native:    // @halide_default_device_detach_native
// %bb.0:                               // %entry
	{
		r16 = r1
		p0 = cmp.eq(r1,#0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB67_1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.2:                               // %if.end.i
	{
		r5:4 = combine(#0,#0)
		r1 = memw(r16+#8)
		r3:2 = memd(r16+#0)
	}
	{
		p1 = cmp.eq(r3:2,r5:4)
		p0 = cmp.eq(r1,#0)
		if (p1.new) jump:nt .LBB67_5
	}
// %bb.3:                               // %if.end.i
	{
		if (!p0) jump:nt .LBB67_5
	}
// %bb.4:                               // %if.then8.i
	{
		call ##halide_error_no_device_interface
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB67_14
	}
	{
		jump .LBB67_11
	}
.LBB67_1:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.35@PCREL)
		call ##halide_error_buffer_is_null
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB67_14
	}
	{
		jump .LBB67_11
	}
.LBB67_5:                               // %if.end10.i
	{
		if (p0) jump:nt .LBB67_8
	}
// %bb.6:                               // %if.end10.i
	{
		r5:4 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (!p0.new) jump:nt .LBB67_8
	}
// %bb.7:                               // %if.then14.i
	{
		call ##halide_error_device_interface_no_device
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB67_11
	}
.LBB67_14:                              // %cleanup
	{
		r0 = r17
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB67_8:                               // %if.end16.i
	{
		r7:6 = combine(#0,#3)
		r5:4 = memd(r16+#16)
	}
	{
		r4 = and(r4,#3)
		r5 = #0
	}
	{
		p0 = cmp.eq(r5:4,r7:6)
		if (!p0.new) jump:t .LBB67_12
	}
// %bb.9:                               // %if.then24.i
	{
		call ##halide_error_host_and_device_dirty
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB67_14
	}
.LBB67_11:                              // %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge
	{
		r3:2 = memd(r16+#0)
	}
.LBB67_12:                              // %if.end
	{
		r17 = #0
		r19:18 = combine(#0,#0)
	}
	{
		p0 = cmp.eq(r3:2,r19:18)
		if (p0.new) jump:t .LBB67_14
	}
// %bb.13:                              // %if.end3
	{
		r0 = memw(r16+#8)
	}
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#4)
	}
	{
		callr r0
	}
	{
		r0 = r17
		memd(r16+#0) = r19:18
		memw(r16+#8) = #0
	}
	{
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end67:
	.size	halide_default_device_detach_native, .Lfunc_end67-halide_default_device_detach_native
                                        // -- End function
	.section	.text.halide_device_and_host_free_as_destructor,"ax",@progbits
	.weak	halide_device_and_host_free_as_destructor // -- Begin function halide_device_and_host_free_as_destructor
	.p2align	4
	.type	halide_device_and_host_free_as_destructor,@function
halide_device_and_host_free_as_destructor: // @halide_device_and_host_free_as_destructor
// %bb.0:                               // %entry
	{
		jump ##halide_device_and_host_free
	}
.Lfunc_end68:
	.size	halide_device_and_host_free_as_destructor, .Lfunc_end68-halide_device_and_host_free_as_destructor
                                        // -- End function
	.section	.text.halide_device_host_nop_free,"ax",@progbits
	.weak	halide_device_host_nop_free     // -- Begin function halide_device_host_nop_free
	.p2align	4
	.type	halide_device_host_nop_free,@function
halide_device_host_nop_free:            // @halide_device_host_nop_free
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end69:
	.size	halide_device_host_nop_free, .Lfunc_end69-halide_device_host_nop_free
                                        // -- End function
	.section	.text.halide_default_buffer_copy,"ax",@progbits
	.weak	halide_default_buffer_copy      // -- Begin function halide_default_buffer_copy
	.p2align	4
	.type	halide_default_buffer_copy,@function
halide_default_buffer_copy:             // @halide_default_buffer_copy
// %bb.0:                               // %entry
	{
		r0 = #-39
		jumpr r31
	}
.Lfunc_end70:
	.size	halide_default_buffer_copy, .Lfunc_end70-halide_default_buffer_copy
                                        // -- End function
	.section	.text.halide_buffer_copy_already_locked,"ax",@progbits
	.weak	halide_buffer_copy_already_locked // -- Begin function halide_buffer_copy_already_locked
	.p2align	4
	.type	halide_buffer_copy_already_locked,@function
halide_buffer_copy_already_locked:      // @halide_buffer_copy_already_locked
// %bb.0:                               // %entry
	{
		p2 = cmp.eq(r2,#0)
		r17:16 = combine(r1,r3)
		memd(r29+#-16) = r17:16
		allocframe(r29,#472):raw
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r2,r0)
		if (p2) jump:nt .LBB71_6
		memd(r29+#456) = r19:18
		memd(r29+#448) = r21:20
	}                                       // 8-byte Folded Spill
// %bb.1:                               // %land.lhs.true
	{
		r0 = memw(r16+#8)
		if (cmp.eq(r0.new,#0)) jump:nt .LBB71_4
	}
// %bb.2:                               // %land.lhs.true
	{
		p0 = cmp.eq(r0,r19); if (p0.new) jump:nt .LBB71_4
	}
// %bb.3:                               // %if.then
	{
		r1 = add(pc,##.L.str.41@PCREL)
		r0 = r18
	}
	{
		call ##halide_error
	}
	{
		r0 = #-42
	}
.LBB71_41:                              // %cleanup143
	{
		r17:16 = memd(r29+#464)
		r19:18 = memd(r29+#456)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#448)
		r31:30 = dealloc_return(r30):raw
	}                                       // 8-byte Folded Reload
.LBB71_4:                               // %land.lhs.true5
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r16+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB71_5
	}
.LBB71_6:                               // %if.end13
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r17+#0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB71_10
		r0 = memw(r17+#12)
	}
// %bb.7:                               // %land.rhs
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB71_8
	}
// %bb.9:                               // %land.end.thread264
	{
		r1:0 = memd(r17+#16)
	}
	{
		p0 = tstbit(r0,#0)
		jump .LBB71_14
	}
.LBB71_10:                              // %land.end
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB71_11
	}
// %bb.13:                              // %land.end.land.rhs26_crit_edge
	{
		p0 = or(p0,!p0)
		r1:0 = memd(r17+#16)
	}
.LBB71_14:                              // %land.rhs26
	{
		r1 = p0
		p0 = !tstbit(r0,#1)
		memw(r29+#20) = r1.new
	}                                       // 4-byte Folded Spill
	{
		p1 = and(p1,!p1)
		if (!p0) jump:nt .LBB71_16
	}
// %bb.15:
	{
		p0 = or(p1,p1)
		r0 = memw(r16+#12)
	}
	{
		p3 = cmp.eq(r0,#0)
		if (p2) jump:nt .LBB71_18
	}
	{
		jump .LBB71_19
	}
.LBB71_16:                              // %lor.rhs28
	{
		r0 = memw(r17+#8)
	}
	{
		p0 = cmp.eq(r0,#0)
	}
	{
		p0 = not(p0)
		r0 = memw(r16+#12)
	}
	{
		p3 = cmp.eq(r0,#0)
		if (p2) jump:nt .LBB71_18
	}
	{
		jump .LBB71_19
	}
.LBB71_8:
	{
		p0 = and(p0,!p0)
		p1 = or(p1,!p1)
	}
	{
		r0 = p0
		jump .LBB71_12
	}
.LBB71_5:                               // %if.then7
	{
		r3 = p2
		r1:0 = combine(r16,r18)
		r2 = r19
	}
	{
		call ##halide_device_malloc
		r20 = r3
		memw(r29+#16) = r20.new
	}                                       // 4-byte Folded Spill
	{
		r1 = r20
		p0 = cmp.eq(r0,#0)
	}
	{
		p2 = r1
		if (!p0) jump:nt .LBB71_41
	}
	{
		jump .LBB71_6
	}
.LBB71_11:
	{
		p1 = or(p1,!p1)
	}
	{
		r0 = p1
	}
.LBB71_12:                              // %land.end32
	{
		p0 = or(p1,p1)
		r0 = memw(r16+#12)
		memw(r29+#20) = r0
	}                                       // 4-byte Folded Spill
	{
		p3 = cmp.eq(r0,#0)
		if (!p2) jump:nt .LBB71_19
	}
.LBB71_18:                              // %land.end32
	{
		r0 = #-34
		if (p3) jump:nt .LBB71_41
	}
.LBB71_19:                              // %if.end41
	{
		r3 = p0
		r0 = p3
		r2 = memw(r29+#20)
		memw(r29+#12) = r0.new
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		memw(r29+#8) = r3
	}                                       // 4-byte Folded Spill
	{
		p0 = or(p2,p0)
		if (p0.new) jump:t .LBB71_21
	}
// %bb.20:                              // %if.end49
	{
		r5 = p2
		r6 = p1
		r1:0 = combine(r17,r18)
		r3 = memw(r19+#60)
	}
	{
		r20 = r5
		r21 = r6
		memw(r29+#16) = r20.new
	}                                       // 4-byte Folded Spill
	{
		r3:2 = combine(r16,r19)
		r4 = memw(r3+#40)
	}
	{
		callr r4
	}
	{
		r1 = r21
		p0 = cmp.eq(r0,#-42)
	}
	{
		p1 = r1
		r1 = r20
	}
	{
		p2 = r1
		if (!p0) jump:nt .LBB71_35
	}
.LBB71_21:                              // %if.then51
	{
		r0 = memw(r29+#12)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		r0 = #-42
	}
	{
		p0 = and(p1,p0)
		if (p0.new) jump:t .LBB71_41
	}
// %bb.22:                              // %if.end58
	{
		p0 = not(p2)
		r1 = memw(r29+#8)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
		r1 = p2
		memw(r29+#16) = r1.new
	}                                       // 4-byte Folded Spill
	{
		p1 = or(p1,p0)
		if (!p1.new) jump:t .LBB71_23
	}
// %bb.24:                              // %if.else
	{
		r1 = memw(r29+#20)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
	}
	{
		p0 = or(p1,p0)
		if (!p0.new) jump:t .LBB71_25
	}
// %bb.28:                              // %if.else81
	{
		r1 = memw(r29+#12)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		r1 = memw(r29+#20)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
	}
	{
		p0 = or(p1,p0)
		if (!p0.new) jump:t .LBB71_29
	}
// %bb.31:                              // %if.else98
	{
		if (p2) jump:nt .LBB71_41
	}
// %bb.32:                              // %if.then100
	{
		call ##_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
		r1:0 = combine(r17,r18)
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB71_41
	}
// %bb.33:                              // %if.then105
	{
		r1:0 = combine(r17,r18)
		r3 = memw(r19+#60)
	}
	{
		r3:2 = combine(r16,r19)
		r4 = memw(r3+#40)
	}
	{
		callr r4
	}
	{
		jump .LBB71_34
	}
.LBB71_23:                              // %if.end117.thread258
	{
		r1 = r17
		r3:2 = combine(r16,#1)
		r4 = #1
		r0 = add(r29,#24)
	}
	{
		call ##_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b
	}
	{
		call ##_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv
		r0 = add(r29,#24)
		r1 = r18
	}
	{
		r0 = memw(r29+#16)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		jump .LBB71_36
	}
.LBB71_25:                              // %if.then66
	{
		r1:0 = combine(r17,r18)
		r3 = memw(r17+#8)
	}
	{
		r3 = memw(r3+#60)
	}
	{
		r3:2 = combine(r16,#0)
		r4 = memw(r3+#40)
	}
	{
		callr r4
	}
	{
		p0 = cmp.eq(r0,#-42)
		r1 = memw(r29+#16)
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		if (!p0) jump:nt .LBB71_35
	}
// %bb.26:                              // %if.then74
	{
		call ##_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t
		r1:0 = combine(r17,r18)
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB71_41
	}
// %bb.27:                              // %if.then77
	{
		call ##halide_buffer_copy_already_locked
		r1:0 = combine(r17,r18)
		r3:2 = combine(r16,#0)
	}
	{
		jump .LBB71_34
	}
.LBB71_29:                              // %if.then85
	{
		r1:0 = combine(r17,r18)
		r3 = memw(r17+#8)
	}
	{
		r3 = memw(r3+#60)
	}
	{
		r3:2 = combine(r16,#0)
		r4 = memw(r3+#40)
	}
	{
		callr r4
	}
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB71_41
	}
// %bb.30:                              // %if.then95
	{
		r1:0 = combine(r16,r18)
		r2 = r19
		r5:4 = memd(r16+#16)
	}
	{
		r4 = setbit(r4,#0)
	}
	{
		call ##copy_to_device_already_locked
		memd(r16+#16) = r5:4
	}
.LBB71_34:                              // %if.end117
	{
		r1 = memw(r29+#16)
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
	}
.LBB71_35:                              // %if.end117
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB71_41
	}
.LBB71_36:                              // %land.lhs.true126
	{
		r0 = #0
		p0 = cmp.eq(r16,r17); if (p0.new) jump:t .LBB71_41
	}
// %bb.37:                              // %if.then128
	{
		r1 = #-4
		r3:2 = memd(r16+#16)
	}
	{
		r1 = and(r2,r1)
		if (p2) jump:nt .LBB71_39
	}
// %bb.38:                              // %if.then130
	{
		r2 = setbit(r1,#1)
		jump .LBB71_40
	}
.LBB71_39:                              // %if.else133
	{
		r2 = setbit(r1,#0)
	}
.LBB71_40:                              // %cleanup143
	{
		r17:16 = memd(r29+#464)
		memd(r16+#16) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r19:18 = memd(r29+#456)
		r21:20 = memd(r29+#448)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end71:
	.size	halide_buffer_copy_already_locked, .Lfunc_end71-halide_buffer_copy_already_locked
                                        // -- End function
	.section	.text.halide_buffer_copy,"ax",@progbits
	.weak	halide_buffer_copy              // -- Begin function halide_buffer_copy
	.p2align	4
	.type	halide_buffer_copy,@function
halide_buffer_copy:                     // @halide_buffer_copy
// %bb.0:                               // %entry
	{
		r17:16 = combine(r2,r1)
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r20 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#16) = r21:20
		memd(r29+#24) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r3,r0)
	}
	{
		call ##halide_mutex_lock
		r0 = memw(r20+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		p0 = cmp.eq(r17,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB72_2
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.1:                               // %if.then
	{
		r0 = memw(r17+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
.LBB72_2:                               // %if.end
	{
		r0 = memw(r16+#8)
		if (cmp.eq(r0.new,#0)) jump:nt .LBB72_4
	}
// %bb.3:                               // %if.then12
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
.LBB72_4:                               // %if.end16
	{
		call ##halide_buffer_copy_already_locked
		r1:0 = combine(r16,r18)
		r3:2 = combine(r19,r17)
	}
	{
		r18 = r0
		r2 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		if (p0.new) jump:nt .LBB72_6
	}
// %bb.5:                               // %if.then18
	{
		r0 = memw(r17+#60)
	}
	{
		r0 = memw(r0+#4)
	}
	{
		callr r0
	}
.LBB72_6:                               // %if.end20
	{
		r0 = memw(r16+#8)
		if (cmp.eq(r0.new,#0)) jump:nt .LBB72_8
	}
// %bb.7:                               // %if.then23
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#4)
	}
	{
		callr r0
	}
.LBB72_8:                               // %if.end27
	{
		call ##halide_mutex_unlock
		r0 = memw(r20+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r0 = r18
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end72:
	.size	halide_buffer_copy, .Lfunc_end72-halide_buffer_copy
                                        // -- End function
	.section	.text.halide_default_device_crop,"ax",@progbits
	.weak	halide_default_device_crop      // -- Begin function halide_default_device_crop
	.p2align	4
	.type	halide_default_device_crop,@function
halide_default_device_crop:             // @halide_default_device_crop
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.58@PCREL)
		allocframe(r29,#0):raw
	}
	{
		call ##halide_error
	}
	{
		r0 = #-40
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end73:
	.size	halide_default_device_crop, .Lfunc_end73-halide_default_device_crop
                                        // -- End function
	.section	.text.halide_default_device_slice,"ax",@progbits
	.weak	halide_default_device_slice     // -- Begin function halide_default_device_slice
	.p2align	4
	.type	halide_default_device_slice,@function
halide_default_device_slice:            // @halide_default_device_slice
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.59@PCREL)
		allocframe(r29,#0):raw
	}
	{
		call ##halide_error
	}
	{
		r0 = #-40
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end74:
	.size	halide_default_device_slice, .Lfunc_end74-halide_default_device_slice
                                        // -- End function
	.section	.text.halide_device_crop,"ax",@progbits
	.weak	halide_device_crop              // -- Begin function halide_device_crop
	.p2align	4
	.type	halide_device_crop,@function
halide_device_crop:                     // @halide_device_crop
// %bb.0:                               // %entry
	{
		r17:16 = combine(r2,r0)
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		r19 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		r18 = r1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		call ##halide_mutex_lock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r1:0 = combine(#0,#0)
		r3:2 = memd(r18+#0)
	}
	{
		p0 = cmp.eq(r3:2,r1:0)
		if (p0.new) jump:nt .LBB75_1
	}
// %bb.2:                               // %if.end
	{
		r3:2 = memd(r17+#0)
	}
	{
		p0 = cmp.eq(r3:2,r1:0)
		if (p0.new) jump:nt .LBB75_5
	}
// %bb.3:                               // %if.then3
	{
		r1 = add(pc,##.L.str.60@PCREL)
		jump .LBB75_4
	}
.LBB75_1:
	{
		r16 = #0 ; jump .LBB75_8
	}
.LBB75_5:                               // %if.end4
	{
		r0 = memw(r18+#28)
	}
	{
		r1 = memw(r17+#28)
		if (!cmp.eq(r1.new,r0)) jump:t .LBB75_6
	}
// %bb.7:                               // %if.end7
	{
		r0 = memw(r18+#8)
	}
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r2 = r17
		r0 = memw(r18+#8)
	}
	{
		r0 = memw(r0+#60)
	}
	{
		r1:0 = combine(r18,r16)
		r3 = memw(r0+#44)
	}
	{
		callr r3
	}
	{
		r16 = r0 ; jump .LBB75_8
	}
.LBB75_6:                               // %if.then6
	{
		r1 = add(pc,##.L.str.61@PCREL)
	}
.LBB75_4:                               // %cleanup
	{
		call ##halide_error
		r0 = r16
		r16 = #-41
	}
.LBB75_8:                               // %cleanup
	{
		call ##halide_mutex_unlock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r0 = r16
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end75:
	.size	halide_device_crop, .Lfunc_end75-halide_device_crop
                                        // -- End function
	.section	.text.halide_device_slice,"ax",@progbits
	.weak	halide_device_slice             // -- Begin function halide_device_slice
	.p2align	4
	.type	halide_device_slice,@function
halide_device_slice:                    // @halide_device_slice
// %bb.0:                               // %entry
	{
		r17:16 = combine(r4,r0)
		memd(r29+#-16) = r17:16
		allocframe(#24)
	}                                       // 8-byte Folded Spill
	{
		r21 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#0) = r21:20
		memd(r29+#8) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r2,r3)
		r20 = r1
	}
	{
		call ##halide_mutex_lock
		r0 = memw(r21+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r1:0 = combine(#0,#0)
		r3:2 = memd(r20+#0)
	}
	{
		p0 = cmp.eq(r3:2,r1:0)
		if (p0.new) jump:nt .LBB76_1
	}
// %bb.2:                               // %if.end
	{
		r3:2 = memd(r17+#0)
	}
	{
		p0 = cmp.eq(r3:2,r1:0)
		if (p0.new) jump:nt .LBB76_5
	}
// %bb.3:                               // %if.then3
	{
		r1 = add(pc,##.L.str.60@PCREL)
		jump .LBB76_4
	}
.LBB76_1:
	{
		r16 = #0 ; jump .LBB76_8
	}
.LBB76_5:                               // %if.end4
	{
		r0 = memw(r17+#28)
		r1 = memw(r20+#28)
	}
	{
		r0 = add(r0,#1)
		if (!cmp.eq(r0.new,r1)) jump:t .LBB76_6
	}
// %bb.7:                               // %if.end7
	{
		r0 = memw(r20+#8)
	}
	{
		r0 = memw(r0+#60)
	}
	{
		r0 = memw(r0+#0)
	}
	{
		callr r0
	}
	{
		r3:2 = combine(r18,r19)
		r4 = r17
		r1 = memw(r20+#8)
	}
	{
		r1 = memw(r1+#60)
	}
	{
		r1:0 = combine(r20,r16)
		r5 = memw(r1+#48)
	}
	{
		callr r5
	}
	{
		r16 = r0 ; jump .LBB76_8
	}
.LBB76_6:                               // %if.then6
	{
		r1 = add(pc,##.L.str.64@PCREL)
	}
.LBB76_4:                               // %cleanup
	{
		call ##halide_error
		r0 = r16
		r16 = #-41
	}
.LBB76_8:                               // %cleanup
	{
		call ##halide_mutex_unlock
		r0 = memw(r21+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		r0 = r16
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end76:
	.size	halide_device_slice, .Lfunc_end76-halide_device_slice
                                        // -- End function
	.section	.text.halide_default_device_release_crop,"ax",@progbits
	.weak	halide_default_device_release_crop // -- Begin function halide_default_device_release_crop
	.p2align	4
	.type	halide_default_device_release_crop,@function
halide_default_device_release_crop:     // @halide_default_device_release_crop
// %bb.0:                               // %entry
	{
		r5:4 = combine(#0,#0)
		r3:2 = memd(r1+#0)
	}
	{
		p0 = cmp.eq(r3:2,r5:4)
		if (p0.new) r0 = #0
		if (p0.new) jumpr:nt r31
	}
.LBB77_1:                               // %if.end
	{
		r1 = add(pc,##.L.str.58@PCREL)
		allocframe(r29,#0):raw
	}
	{
		call ##halide_error
	}
	{
		r0 = #-40
		r31:30 = deallocframe(r30):raw
	}
	{
		jumpr r31
	}
.Lfunc_end77:
	.size	halide_default_device_release_crop, .Lfunc_end77-halide_default_device_release_crop
                                        // -- End function
	.section	.text.halide_device_release_crop,"ax",@progbits
	.weak	halide_device_release_crop      // -- Begin function halide_device_release_crop
	.p2align	4
	.type	halide_device_release_crop,@function
halide_device_release_crop:             // @halide_device_release_crop
// %bb.0:                               // %entry
	{
		memd(r29+#-16) = r17:16
		allocframe(#24)
	}                                       // 8-byte Folded Spill
	{
		r21:20 = combine(#0,#0)
		r3:2 = memd(r1+#0)
		memd(r29+#0) = r21:20
	}                                       // 8-byte Folded Spill
	{
		p0 = cmp.eq(r3:2,r21:20)
		if (p0.new) jump:nt .LBB78_2
		memd(r29+#8) = r19:18
	}                                       // 8-byte Folded Spill
// %bb.1:                               // %if.then
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		r17:16 = combine(r0,r1)
	}
	{
		r18 = memw(r2+##_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOT)
	}
	{
		call ##halide_mutex_lock
		r0 = r18
	}
	{
		r19 = memw(r16+#8)
	}
	{
		r0 = memw(r19+#60)
	}
	{
		r1:0 = combine(r16,r17)
		r2 = memw(r0+#52)
	}
	{
		callr r2
	}
	{
		r17 = r0
		r1 = memw(r19+#60)
		memd(r16+#0) = r21:20
	}
	{
		r1 = memw(r1+#4)
	}
	{
		callr r1
	}
	{
		call ##halide_mutex_unlock
		r0 = r18
		memw(r16+#8) = #0
	}
	{
		r0 = r17
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.LBB78_2:                               // %return
	{
		r0 = #0
		r17:16 = memd(r29+#16)
		r19:18 = memd(r29+#8)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end78:
	.size	halide_device_release_crop, .Lfunc_end78-halide_device_release_crop
                                        // -- End function
	.section	.text.halide_float16_bits_to_float,"ax",@progbits
	.weak	halide_float16_bits_to_float    // -- Begin function halide_float16_bits_to_float
	.p2align	4
	.type	halide_float16_bits_to_float,@function
halide_float16_bits_to_float:           // @halide_float16_bits_to_float
// %bb.0:                               // %entry
	{
		r3 = extractu(r0,#5,#10)
		r1 = #1023
	}
	{
		r1 = extractu(r0,#10,#0)
		p1 = bitsclr(r0,r1)
		p0 = cmp.eq(r3,#0)
		if (p1.new) jump:nt .LBB79_3
	}
// %bb.1:                               // %entry
	{
		if (!p0) jump:nt .LBB79_3
	}
// %bb.2:                               // %if.then
	{
		r2 = cl0(r1)
		r3 = #31
		r4 = #-2
		r5 = #-2
	}
	{
		r2 = sub(##1124073472,asl(r2,#23))
		r3 = xor(r2,r3)
	}
	{
		r5:4 = asl(r5:4,r3)
		r3 = sub(#23,r3)
	}
	{
		r1 = and(r1,r5)
	}
	{
		r1 = asl(r1,r3)
	}
.LBB79_7:                               // %if.end28
	{
		r0 = and(##-2147483648,asl(r0,#16))
	}
	{
		r1 |= or(r2,r0)
	}
	{
		r0 = r1
		jumpr r31
	}
.LBB79_3:                               // %if.else
	{
		r2 = asl(r1,#13)
		if (p0) jump:nt .LBB79_4
	}
// %bb.5:                               // %if.else18
	{
		r1 = ##2139095040
		p0 = cmp.eq(r3,#31); if (p0.new) jump:t .LBB79_7
	}
// %bb.6:                               // %if.else21
	{
		r3 = add(##939524096,asl(r3,#23))
	}
	{
		r0 = and(##-2147483648,asl(r0,#16))
		r1 = r3
	}
	{
		r1 |= or(r2,r0)
	}
	{
		r0 = r1
		jumpr r31
	}
.LBB79_4:
	{
		r0 = and(##-2147483648,asl(r0,#16))
		r1 = #0
	}
	{
		r1 |= or(r2,r0)
	}
	{
		r0 = r1
		jumpr r31
	}
.Lfunc_end79:
	.size	halide_float16_bits_to_float, .Lfunc_end79-halide_float16_bits_to_float
                                        // -- End function
	.section	.text.halide_float16_bits_to_double,"ax",@progbits
	.weak	halide_float16_bits_to_double   // -- Begin function halide_float16_bits_to_double
	.p2align	4
	.type	halide_float16_bits_to_double,@function
halide_float16_bits_to_double:          // @halide_float16_bits_to_double
// %bb.0:                               // %entry
	{
		call ##halide_float16_bits_to_float
		allocframe(r29,#0):raw
	}
	{
		r1:0 = convert_sf2df(r0)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end80:
	.size	halide_float16_bits_to_double, .Lfunc_end80-halide_float16_bits_to_double
                                        // -- End function
	.section	.text.halide_error_bounds_inference_call_failed,"ax",@progbits
	.weak	halide_error_bounds_inference_call_failed // -- Begin function halide_error_bounds_inference_call_failed
	.p2align	4
	.type	halide_error_bounds_inference_call_failed,@function
halide_error_bounds_inference_call_failed: // @halide_error_bounds_inference_call_failed
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r2)
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r19 = r0
		r0 = #1024
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB81_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.36@PCREL)
		r20 = add(r18,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r18)
		jump .LBB81_3
		memb(r18+#1023) = r3
	}
.LBB81_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.36@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB81_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r17
	}
	{
		r2 = add(pc,##.L.str.1.37@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r17 = asr(r16,#31)
	}
	{
		r3:2 = combine(r17,r16)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB81_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r18))
		r1:0 = combine(r18,r19)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r18,r19)
		jump .LBB81_6
	}
.LBB81_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r19
	}
.LBB81_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r18
	}
	{
		r0 = r16
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end81:
	.size	halide_error_bounds_inference_call_failed, .Lfunc_end81-halide_error_bounds_inference_call_failed
                                        // -- End function
	.section	.text.halide_error_extern_stage_failed,"ax",@progbits
	.weak	halide_error_extern_stage_failed // -- Begin function halide_error_extern_stage_failed
	.p2align	4
	.type	halide_error_extern_stage_failed,@function
halide_error_extern_stage_failed:       // @halide_error_extern_stage_failed
// %bb.0:                               // %entry
	{
		r17:16 = combine(r1,r2)
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r19 = r0
		r0 = #1024
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r18 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB82_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.2.38@PCREL)
		r20 = add(r18,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r18)
		jump .LBB82_3
		memb(r18+#1023) = r3
	}
.LBB82_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.2.38@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB82_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r17
	}
	{
		r2 = add(pc,##.L.str.1.37@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r17 = asr(r16,#31)
	}
	{
		r3:2 = combine(r17,r16)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB82_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r18))
		r1:0 = combine(r18,r19)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r18,r19)
		jump .LBB82_6
	}
.LBB82_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r19
	}
.LBB82_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r18
	}
	{
		r0 = r16
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end82:
	.size	halide_error_extern_stage_failed, .Lfunc_end82-halide_error_extern_stage_failed
                                        // -- End function
	.section	.text.halide_error_explicit_bounds_too_small,"ax",@progbits
	.weak	halide_error_explicit_bounds_too_small // -- Begin function halide_error_explicit_bounds_too_small
	.p2align	4
	.type	halide_error_explicit_bounds_too_small,@function
halide_error_explicit_bounds_too_small: // @halide_error_explicit_bounds_too_small
// %bb.0:                               // %entry
	{
		r16 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#64)
	}                                       // 8-byte Folded Spill
	{
		r21 = r2
		r19 = r1
		memd(r29+#48) = r19:18
		memd(r29+#40) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r22 = r5
		r24 = r4
		memd(r29+#32) = r23:22
		memd(r29+#24) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r26 = r3
		r18 = memw(r29+#72)
		memd(r29+#16) = r27:26
	}
	{
		call ##malloc
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB83_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.3.39@PCREL)
		r20 = add(r17,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r17)
		jump .LBB83_3
		memb(r17+#1023) = r3
	}
.LBB83_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.3.39@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB83_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r2 = add(pc,##.L.str.4.40@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.5.41@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r27 = asr(r26,#31)
	}
	{
		r3:2 = combine(r27,r26)
		r4 = #1
		r1 = r20
	}
	{
		r19 = add(pc,##.L.str.6.42@PCREL)
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r25 = asr(r24,#31)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
		r3:2 = combine(r25,r24)
	}
	{
		r2 = add(pc,##.L.str.7.43@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r19 = asr(r18,#31)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
		r3:2 = combine(r19,r18)
	}
	{
		r2 = add(pc,##.L.str.8.44@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB83_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r17,r16)
		jump .LBB83_6
	}
.LBB83_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
.LBB83_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-2
		r17:16 = memd(r29+#56)
		r19:18 = memd(r29+#48)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#40)
		r23:22 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#24)
		r27:26 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end83:
	.size	halide_error_explicit_bounds_too_small, .Lfunc_end83-halide_error_explicit_bounds_too_small
                                        // -- End function
	.section	.text.halide_error_bad_type,"ax",@progbits
	.weak	halide_error_bad_type           // -- Begin function halide_error_bad_type
	.p2align	4
	.type	halide_error_bad_type,@function
halide_error_bad_type:                  // @halide_error_bad_type
// %bb.0:                               // %entry
	{
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19 = r1
		r0 = add(r29,#12)
		memd(r29+#32) = r19:18
		memw(r29+#20) = r2
	}                                       // 8-byte Folded Spill
	{
		r1 = add(r29,#16)
		r2 = #4
		memw(r29+#16) = r3
		memh(r29+#12) = #0
	}
	{
		memh(r29+#14) = #0
		memh(r29+#8) = #0
	}
	{
		call ##memcpy
		memh(r29+#10) = #0
	}
	{
		r1 = add(r29,#20)
		r2 = #4
		r0 = add(r29,#8)
	}
	{
		call ##memcpy
	}
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB84_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r18 = add(r17,#1023)
		r3:2 = combine(#0,r19)
	}
	{
		r1:0 = combine(r18,r17)
		jump .LBB84_3
		memb(r17+#1023) = r3
	}
.LBB84_1:                               // %entry.split
	{
		r2 = r19
		r18 = #0
		r1:0 = combine(#0,#0)
	}
.LBB84_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.9.45@PCREL)
		r1 = r18
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_type_to_string
		r2 = add(r29,#12)
		r1 = r18
	}
	{
		r2 = add(pc,##.L.str.10.46@PCREL)
		r1 = r18
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_type_to_string
		r2 = add(r29,#8)
		r1 = r18
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB84_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r17,r16)
		jump .LBB84_6
	}
.LBB84_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
.LBB84_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-3
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end84:
	.size	halide_error_bad_type, .Lfunc_end84-halide_error_bad_type
                                        // -- End function
	.section	.text.halide_error_bad_dimensions,"ax",@progbits
	.weak	halide_error_bad_dimensions     // -- Begin function halide_error_bad_dimensions
	.p2align	4
	.type	halide_error_bad_dimensions,@function
halide_error_bad_dimensions:            // @halide_error_bad_dimensions
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r22 = r3
		r19:18 = combine(r1,r2)
		memd(r29+#32) = r19:18
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB85_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r20 = add(r16,#1023)
		r3:2 = combine(#0,r19)
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB85_3
		memb(r16+#1023) = r3
	}
.LBB85_1:                               // %entry.split
	{
		r2 = r19
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB85_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.11.47@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.12.48@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.13.49@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB85_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB85_6
	}
.LBB85_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB85_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-43
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end85:
	.size	halide_error_bad_dimensions, .Lfunc_end85-halide_error_bad_dimensions
                                        // -- End function
	.section	.text.halide_error_access_out_of_bounds,"ax",@progbits
	.weak	halide_error_access_out_of_bounds // -- Begin function halide_error_access_out_of_bounds
	.p2align	4
	.type	halide_error_access_out_of_bounds,@function
halide_error_access_out_of_bounds:      // @halide_error_access_out_of_bounds
// %bb.0:                               // %entry
	{
		p0 = cmp.gt(r5,r3)
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#64)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r2)
		memd(r29+#48) = r19:18
		memd(r29+#40) = r21:20
	}                                       // 8-byte Folded Spill
	{
		memd(r29+#32) = r23:22
		memd(r29+#24) = r25:24
	}                                       // 8-byte Folded Spill
	{
		if (!p0) jump:nt .LBB86_7
		memd(r29+#16) = r27:26
	}                                       // 8-byte Folded Spill
// %bb.1:                               // %if.then
	{
		r0 = #1024
		r20 = r5
		r26 = r3
	}
	{
		call ##malloc
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB86_2
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.3:                               // %if.then6.i
	{
		r22 = add(r17,#1023)
		r3:2 = combine(#0,r19)
	}
	{
		r1:0 = combine(r22,r17)
		jump .LBB86_4
		memb(r17+#1023) = r3
	}
.LBB86_7:                               // %if.else
	{
		r24 = r4
		r20 = memw(r29+#72)
		if (!cmp.gt(r4,r20.new)) jump:t .LBB86_14
	}
// %bb.8:                               // %if.then8
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB86_9
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.10:                              // %if.then6.i59
	{
		r22 = add(r17,#1023)
		r3:2 = combine(#0,r19)
	}
	{
		r1:0 = combine(r22,r17)
		jump .LBB86_11
		memb(r17+#1023) = r3
	}
.LBB86_2:                               // %if.then.split
	{
		r2 = r19
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB86_4:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.14.50@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
		r27 = asr(r26,#31)
	}
	{
		r3:2 = combine(r27,r26)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.15.51@PCREL)
		jump .LBB86_5
	}
.LBB86_9:                               // %if.then8.split
	{
		r2 = r19
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB86_11:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.14.50@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
		r25 = asr(r24,#31)
	}
	{
		r3:2 = combine(r25,r24)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.17.53@PCREL)
	}
.LBB86_5:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
		r21 = asr(r20,#31)
		r1 = r22
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.16.52@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB86_6
	}
// %bb.12:                              // %if.else.i101
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		call ##halide_error
		r1:0 = combine(r17,r16)
	}
	{
		jump .LBB86_13
	}
.LBB86_6:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
	{
		call ##halide_error
		r17 = #0
	}
.LBB86_13:                              // %if.end17.sink.split
	{
		call ##free
		r0 = r17
	}
.LBB86_14:                              // %if.end17
	{
		r0 = #-4
		r17:16 = memd(r29+#56)
		r19:18 = memd(r29+#48)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#40)
		r23:22 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#24)
		r27:26 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end86:
	.size	halide_error_access_out_of_bounds, .Lfunc_end86-halide_error_access_out_of_bounds
                                        // -- End function
	.section	.text.halide_error_buffer_allocation_too_large,"ax",@progbits
	.weak	halide_error_buffer_allocation_too_large // -- Begin function halide_error_buffer_allocation_too_large
	.p2align	4
	.type	halide_error_buffer_allocation_too_large,@function
halide_error_buffer_allocation_too_large: // @halide_error_buffer_allocation_too_large
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB87_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.18.54@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB87_3
		memb(r16+#1023) = r3
	}
.LBB87_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.18.54@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB87_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r2 = add(pc,##.L.str.20.56@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB87_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB87_6
	}
.LBB87_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB87_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-5
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end87:
	.size	halide_error_buffer_allocation_too_large, .Lfunc_end87-halide_error_buffer_allocation_too_large
                                        // -- End function
	.section	.text.halide_error_buffer_extents_negative,"ax",@progbits
	.weak	halide_error_buffer_extents_negative // -- Begin function halide_error_buffer_extents_negative
	.p2align	4
	.type	halide_error_buffer_extents_negative,@function
halide_error_buffer_extents_negative:   // @halide_error_buffer_extents_negative
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r3)
		r22 = r2
		memd(r29+#32) = r19:18
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB88_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.21.57@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB88_3
		memb(r16+#1023) = r3
	}
.LBB88_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.21.57@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB88_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.22.58@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.23.59@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.8.44@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB88_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB88_6
	}
.LBB88_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB88_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-28
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end88:
	.size	halide_error_buffer_extents_negative, .Lfunc_end88-halide_error_buffer_extents_negative
                                        // -- End function
	.section	.text.halide_error_buffer_extents_too_large,"ax",@progbits
	.weak	halide_error_buffer_extents_too_large // -- Begin function halide_error_buffer_extents_too_large
	.p2align	4
	.type	halide_error_buffer_extents_too_large,@function
halide_error_buffer_extents_too_large:  // @halide_error_buffer_extents_too_large
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB89_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.24.60@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB89_3
		memb(r16+#1023) = r3
	}
.LBB89_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.24.60@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB89_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.20.56@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB89_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB89_6
	}
.LBB89_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB89_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-6
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end89:
	.size	halide_error_buffer_extents_too_large, .Lfunc_end89-halide_error_buffer_extents_too_large
                                        // -- End function
	.section	.text.halide_error_constraints_make_required_region_smaller,"ax",@progbits
	.weak	halide_error_constraints_make_required_region_smaller // -- Begin function halide_error_constraints_make_required_region_smaller
	.p2align	4
	.type	halide_error_constraints_make_required_region_smaller,@function
halide_error_constraints_make_required_region_smaller: // @halide_error_constraints_make_required_region_smaller
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#72)
	}                                       // 8-byte Folded Spill
	{
		r2 = add(r3,add(r4,#-1))
		r19:18 = combine(r1,r2)
		r6 = memw(r29+#80)
		memd(r29+#56) = r19:18
	}
	{
		r24 = add(r5,add(r6,#-1))
		r22 = r3
		memd(r29+#40) = r23:22
		memd(r29+#32) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r26 = r5
		memd(r29+#24) = r27:26
		memd(r29+#48) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		memd(r29+#0) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB90_1
		memw(r29+#12) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.25.61@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB90_3
		memb(r16+#1023) = r3
	}
.LBB90_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.25.61@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB90_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.26.62@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		r18 = add(pc,##.L.str.27.63@PCREL)
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.28.64@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r27 = asr(r26,#31)
	}
	{
		r3:2 = combine(r27,r26)
		r4 = #1
		r1 = r20
	}
	{
		r19 = add(pc,##.L.str.6.42@PCREL)
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r25 = asr(r24,#31)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
		r3:2 = combine(r25,r24)
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.29.65@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r4 = #1
		r1 = r20
		r3:2 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		r2 = add(pc,##.L.str.30.66@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#12)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB90_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB90_6
	}
.LBB90_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB90_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-7
		r17:16 = memd(r29+#64)
		r19:18 = memd(r29+#56)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#48)
		r23:22 = memd(r29+#40)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#32)
		r27:26 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end90:
	.size	halide_error_constraints_make_required_region_smaller, .Lfunc_end90-halide_error_constraints_make_required_region_smaller
                                        // -- End function
	.section	.text.halide_error_constraint_violated,"ax",@progbits
	.weak	halide_error_constraint_violated // -- Begin function halide_error_constraint_violated
	.p2align	4
	.type	halide_error_constraint_violated,@function
halide_error_constraint_violated:       // @halide_error_constraint_violated
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r3,r4)
		r21 = r1
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r22 = r2
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB91_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.31.67@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB91_3
		memb(r16+#1023) = r3
	}
.LBB91_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.31.67@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB91_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r21 = add(pc,##.L.str.32.68@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
		r2 = r21
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.33.69@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r19 = asr(r18,#31)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
		r3:2 = combine(r19,r18)
	}
	{
		r2 = add(pc,##.L.str.8.44@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB91_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB91_6
	}
.LBB91_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB91_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-8
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end91:
	.size	halide_error_constraint_violated, .Lfunc_end91-halide_error_constraint_violated
                                        // -- End function
	.section	.text.halide_error_param_too_small_i64,"ax",@progbits
	.weak	halide_error_param_too_small_i64 // -- Begin function halide_error_param_too_small_i64
	.p2align	4
	.type	halide_error_param_too_small_i64,@function
halide_error_param_too_small_i64:       // @halide_error_param_too_small_i64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB92_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB92_3
		memb(r16+#1023) = r3
	}
.LBB92_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB92_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.35.71@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB92_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB92_6
	}
.LBB92_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB92_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-9
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end92:
	.size	halide_error_param_too_small_i64, .Lfunc_end92-halide_error_param_too_small_i64
                                        // -- End function
	.section	.text.halide_error_param_too_small_u64,"ax",@progbits
	.weak	halide_error_param_too_small_u64 // -- Begin function halide_error_param_too_small_u64
	.p2align	4
	.type	halide_error_param_too_small_u64,@function
halide_error_param_too_small_u64:       // @halide_error_param_too_small_u64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB93_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB93_3
		memb(r16+#1023) = r3
	}
.LBB93_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB93_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r2 = add(pc,##.L.str.35.71@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB93_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB93_6
	}
.LBB93_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB93_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-9
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end93:
	.size	halide_error_param_too_small_u64, .Lfunc_end93-halide_error_param_too_small_u64
                                        // -- End function
	.section	.text.halide_error_param_too_small_f64,"ax",@progbits
	.weak	halide_error_param_too_small_f64 // -- Begin function halide_error_param_too_small_f64
	.p2align	4
	.type	halide_error_param_too_small_f64,@function
halide_error_param_too_small_f64:       // @halide_error_param_too_small_f64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB94_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB94_3
		memb(r16+#1023) = r3
	}
.LBB94_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB94_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_double_to_string
	}
	{
		r2 = add(pc,##.L.str.35.71@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_double_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB94_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB94_6
	}
.LBB94_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB94_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-9
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end94:
	.size	halide_error_param_too_small_f64, .Lfunc_end94-halide_error_param_too_small_f64
                                        // -- End function
	.section	.text.halide_error_param_too_large_i64,"ax",@progbits
	.weak	halide_error_param_too_large_i64 // -- Begin function halide_error_param_too_large_i64
	.p2align	4
	.type	halide_error_param_too_large_i64,@function
halide_error_param_too_large_i64:       // @halide_error_param_too_large_i64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB95_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB95_3
		memb(r16+#1023) = r3
	}
.LBB95_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB95_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.36.72@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB95_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB95_6
	}
.LBB95_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB95_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-10
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end95:
	.size	halide_error_param_too_large_i64, .Lfunc_end95-halide_error_param_too_large_i64
                                        // -- End function
	.section	.text.halide_error_param_too_large_u64,"ax",@progbits
	.weak	halide_error_param_too_large_u64 // -- Begin function halide_error_param_too_large_u64
	.p2align	4
	.type	halide_error_param_too_large_u64,@function
halide_error_param_too_large_u64:       // @halide_error_param_too_large_u64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB96_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB96_3
		memb(r16+#1023) = r3
	}
.LBB96_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB96_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r2 = add(pc,##.L.str.36.72@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_uint64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB96_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB96_6
	}
.LBB96_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB96_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-10
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end96:
	.size	halide_error_param_too_large_u64, .Lfunc_end96-halide_error_param_too_large_u64
                                        // -- End function
	.section	.text.halide_error_param_too_large_f64,"ax",@progbits
	.weak	halide_error_param_too_large_f64 // -- Begin function halide_error_param_too_large_f64
	.p2align	4
	.type	halide_error_param_too_large_f64,@function
halide_error_param_too_large_f64:       // @halide_error_param_too_large_f64
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r5,r4)
		r21:20 = combine(r3,r2)
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r23 = r1
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB97_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r16)
		jump .LBB97_3
		memb(r16+#1023) = r3
	}
.LBB97_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.34.70@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB97_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r23
	}
	{
		r2 = add(pc,##.L.str.19.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_double_to_string
	}
	{
		r2 = add(pc,##.L.str.36.72@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_double_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB97_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB97_6
	}
.LBB97_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB97_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-10
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end97:
	.size	halide_error_param_too_large_f64, .Lfunc_end97-halide_error_param_too_large_f64
                                        // -- End function
	.section	.text.halide_error_out_of_memory,"ax",@progbits
	.weak	halide_error_out_of_memory      // -- Begin function halide_error_out_of_memory
	.p2align	4
	.type	halide_error_out_of_memory,@function
halide_error_out_of_memory:             // @halide_error_out_of_memory
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.L.str.37@PCREL)
		allocframe(r29,#0):raw
	}
	{
		call ##halide_error
	}
	{
		r0 = #-11
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end98:
	.size	halide_error_out_of_memory, .Lfunc_end98-halide_error_out_of_memory
                                        // -- End function
	.section	.text.halide_error_buffer_argument_is_null,"ax",@progbits
	.weak	halide_error_buffer_argument_is_null // -- Begin function halide_error_buffer_argument_is_null
	.p2align	4
	.type	halide_error_buffer_argument_is_null,@function
halide_error_buffer_argument_is_null:   // @halide_error_buffer_argument_is_null
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#32)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r18 = r1
		memd(r29+#16) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB99_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.38@PCREL)
		r19 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r19,r16)
		jump .LBB99_3
		memb(r16+#1023) = r3
	}
.LBB99_1:                               // %entry.split
	{
		r2 = add(pc,##.L.str.38@PCREL)
		r19 = #0
		r1:0 = combine(#0,#0)
	}
.LBB99_3:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.39@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB99_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB99_6
	}
.LBB99_4:                               // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB99_6:                               // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-12
		r17:16 = memd(r29+#24)
		r19:18 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end99:
	.size	halide_error_buffer_argument_is_null, .Lfunc_end99-halide_error_buffer_argument_is_null
                                        // -- End function
	.section	.text.halide_error_debug_to_file_failed,"ax",@progbits
	.weak	halide_error_debug_to_file_failed // -- Begin function halide_error_debug_to_file_failed
	.p2align	4
	.type	halide_error_debug_to_file_failed,@function
halide_error_debug_to_file_failed:      // @halide_error_debug_to_file_failed
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r2,r3)
		r21 = r1
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB100_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.40@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB100_3
		memb(r16+#1023) = r3
	}
.LBB100_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.40@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB100_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r2 = add(pc,##.L.str.41.73@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.42@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB100_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB100_6
	}
.LBB100_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB100_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-13
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end100:
	.size	halide_error_debug_to_file_failed, .Lfunc_end100-halide_error_debug_to_file_failed
                                        // -- End function
	.section	.text.halide_error_unaligned_host_ptr,"ax",@progbits
	.weak	halide_error_unaligned_host_ptr // -- Begin function halide_error_unaligned_host_ptr
	.p2align	4
	.type	halide_error_unaligned_host_ptr,@function
halide_error_unaligned_host_ptr:        // @halide_error_unaligned_host_ptr
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r2)
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB101_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.43@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB101_3
		memb(r16+#1023) = r3
	}
.LBB101_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.43@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB101_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.44@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.45@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB101_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB101_6
	}
.LBB101_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB101_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-24
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end101:
	.size	halide_error_unaligned_host_ptr, .Lfunc_end101-halide_error_unaligned_host_ptr
                                        // -- End function
	.section	.text.halide_error_device_dirty_with_no_device_support,"ax",@progbits
	.weak	halide_error_device_dirty_with_no_device_support // -- Begin function halide_error_device_dirty_with_no_device_support
	.p2align	4
	.type	halide_error_device_dirty_with_no_device_support,@function
halide_error_device_dirty_with_no_device_support: // @halide_error_device_dirty_with_no_device_support
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#32)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r19 = r1
		memd(r29+#16) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB102_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.46@PCREL)
		r18 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r18,r16)
		jump .LBB102_3
		memb(r16+#1023) = r3
	}
.LBB102_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.46@PCREL)
		r18 = #0
		r1:0 = combine(#0,#0)
	}
.LBB102_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r18
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.47@PCREL)
		r1 = r18
	}
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.48@PCREL)
		r1 = r18
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB102_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB102_6
	}
.LBB102_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB102_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-44
		r17:16 = memd(r29+#24)
		r19:18 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end102:
	.size	halide_error_device_dirty_with_no_device_support, .Lfunc_end102-halide_error_device_dirty_with_no_device_support
                                        // -- End function
	.section	.text.halide_error_host_is_null,"ax",@progbits
	.weak	halide_error_host_is_null       // -- Begin function halide_error_host_is_null
	.p2align	4
	.type	halide_error_host_is_null,@function
halide_error_host_is_null:              // @halide_error_host_is_null
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#32)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r18 = r1
		memd(r29+#16) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB103_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.43@PCREL)
		r19 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r19,r16)
		jump .LBB103_3
		memb(r16+#1023) = r3
	}
.LBB103_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.43@PCREL)
		r19 = #0
		r1:0 = combine(#0,#0)
	}
.LBB103_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.49@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB103_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB103_6
	}
.LBB103_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB103_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-34
		r17:16 = memd(r29+#24)
		r19:18 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end103:
	.size	halide_error_host_is_null, .Lfunc_end103-halide_error_host_is_null
                                        // -- End function
	.section	.text.halide_error_bad_fold,"ax",@progbits
	.weak	halide_error_bad_fold           // -- Begin function halide_error_bad_fold
	.p2align	4
	.type	halide_error_bad_fold,@function
halide_error_bad_fold:                  // @halide_error_bad_fold
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r18 = r3
		r21:20 = combine(r2,r1)
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB104_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.50@PCREL)
		r19 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r19,r16)
		jump .LBB104_3
		memb(r16+#1023) = r3
	}
.LBB104_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.50@PCREL)
		r19 = #0
		r1:0 = combine(#0,#0)
	}
.LBB104_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r21
	}
	{
		r2 = add(pc,##.L.str.51@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r20
	}
	{
		r2 = add(pc,##.L.str.52@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.30.66@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB104_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB104_6
	}
.LBB104_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB104_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-25
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end104:
	.size	halide_error_bad_fold, .Lfunc_end104-halide_error_bad_fold
                                        // -- End function
	.section	.text.halide_error_bad_extern_fold,"ax",@progbits
	.weak	halide_error_bad_extern_fold    // -- Begin function halide_error_bad_extern_fold
	.p2align	4
	.type	halide_error_bad_extern_fold,@function
halide_error_bad_extern_fold:           // @halide_error_bad_extern_fold
// %bb.0:                               // %entry
	{
		p0 = cmp.gt(r5,r3)
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#64)
	}                                       // 8-byte Folded Spill
	{
		r20 = r3
		r19 = r1
		memd(r29+#48) = r19:18
		memd(r29+#40) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r23:22 = combine(r4,r5)
		r26 = r2
		memd(r29+#32) = r23:22
		memd(r29+#16) = r27:26
	}                                       // 8-byte Folded Spill
	{
		if (p0) jump:nt .LBB105_2
		r18 = memw(r29+#72)
		memd(r29+#24) = r25:24
	}
// %bb.1:                               // %lor.lhs.false
	{
		r24 = add(r23,r20)
	}
	{
		r0 = add(r18,r22)
		if (!cmp.gt(r24,r0.new)) jump:t .LBB105_8
	}
.LBB105_2:                              // %if.then
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB105_3
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.4:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.53@PCREL)
		r24 = add(r17,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r24,r17)
		jump .LBB105_5
		memb(r17+#1023) = r3
	}
.LBB105_3:                              // %if.then.split
	{
		r2 = add(pc,##.L.str.53@PCREL)
		r1:0 = combine(#0,#0)
		r24 = #0
	}
.LBB105_5:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
		r27 = asr(r26,#31)
	}
	{
		r1 = r24
		r4 = #1
		r3:2 = combine(r27,r26)
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.51@PCREL)
		r1 = r24
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r24
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.54@PCREL)
		r1 = r24
	}
	{
		call ##halide_string_to_string
		r21 = asr(r20,#31)
	}
	{
		r1 = r24
		r4 = #1
		r3:2 = combine(r21,r20)
	}
	{
		r19 = add(pc,##.L.str.55@PCREL)
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r24
		r2 = r19
	}
	{
		r2 = add(r23,add(r20,#-1))
		r1 = r24
		r4 = #1
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		r2 = add(pc,##.L.str.56@PCREL)
		r1 = r24
	}
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.57@PCREL)
		r1 = r24
	}
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r1 = r24
		r4 = #1
		r3:2 = combine(r23,r22)
	}
	{
		call ##halide_int64_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r24
		r2 = r19
	}
	{
		r2 = add(r18,add(r22,#-1))
		r1 = r24
		r4 = #1
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		r2 = add(pc,##.L.str.58.74@PCREL)
		r1 = r24
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB105_7
	}
.LBB105_12:                             // %if.else.i168
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		call ##halide_error
		r1:0 = combine(r17,r16)
	}
	{
		jump .LBB105_13
	}
.LBB105_8:                              // %if.else
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB105_9
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.10:                              // %if.then6.i107
	{
		r2 = add(pc,##.L.str.53@PCREL)
		r22 = add(r17,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r22,r17)
		jump .LBB105_11
		memb(r17+#1023) = r3
	}
.LBB105_9:                              // %if.else.split
	{
		r2 = add(pc,##.L.str.53@PCREL)
		r22 = #0
		r1:0 = combine(#0,#0)
	}
.LBB105_11:                             // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110
	{
		call ##halide_string_to_string
		r27 = asr(r26,#31)
	}
	{
		r3:2 = combine(r27,r26)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.51@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r22
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.54@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
		r21 = asr(r20,#31)
	}
	{
		r3:2 = combine(r21,r20)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.55@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(r24,#-1)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
		r3 = asr(r2,#31)
	}
	{
		r2 = add(pc,##.L.str.56@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.59.75@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r2 = add(pc,##.L.str.60.76@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r22
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.30.66@PCREL)
		r1 = r22
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (!p0.new) jump:t .LBB105_12
	}
.LBB105_7:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
	{
		call ##halide_error
		r17 = #0
	}
.LBB105_13:                             // %if.end
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-35
		r17:16 = memd(r29+#56)
		r19:18 = memd(r29+#48)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#40)
		r23:22 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#24)
		r27:26 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end105:
	.size	halide_error_bad_extern_fold, .Lfunc_end105-halide_error_bad_extern_fold
                                        // -- End function
	.section	.text.halide_error_fold_factor_too_small,"ax",@progbits
	.weak	halide_error_fold_factor_too_small // -- Begin function halide_error_fold_factor_too_small
	.p2align	4
	.type	halide_error_fold_factor_too_small,@function
halide_error_fold_factor_too_small:     // @halide_error_fold_factor_too_small
// %bb.0:                               // %entry
	{
		r16 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#56)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r4,r5)
		r21 = r1
		memd(r29+#40) = r19:18
		memd(r29+#32) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r24 = r3
		r22 = r2
		memd(r29+#24) = r23:22
		memd(r29+#16) = r25:24
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB106_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.61.77@PCREL)
		r20 = add(r17,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r17)
		jump .LBB106_3
		memb(r17+#1023) = r3
	}
.LBB106_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.61.77@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB106_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
		r25 = asr(r24,#31)
	}
	{
		r3:2 = combine(r25,r24)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.62@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r22
	}
	{
		r2 = add(pc,##.L.str.51@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r2 = add(pc,##.L.str.63@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.32.68@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.64.78@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB106_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r17,r16)
		jump .LBB106_6
	}
.LBB106_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
.LBB106_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-26
		r17:16 = memd(r29+#48)
		r19:18 = memd(r29+#40)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#32)
		r23:22 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r29+#16)
		r31:30 = dealloc_return(r30):raw
	}                                       // 8-byte Folded Reload
.Lfunc_end106:
	.size	halide_error_fold_factor_too_small, .Lfunc_end106-halide_error_fold_factor_too_small
                                        // -- End function
	.section	.text.halide_error_requirement_failed,"ax",@progbits
	.weak	halide_error_requirement_failed // -- Begin function halide_error_requirement_failed
	.p2align	4
	.type	halide_error_requirement_failed,@function
halide_error_requirement_failed:        // @halide_error_requirement_failed
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#40)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r2)
		memd(r29+#24) = r19:18
		memd(r29+#16) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB107_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.65@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB107_3
		memb(r16+#1023) = r3
	}
.LBB107_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.65@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB107_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.66@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r18
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB107_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB107_6
	}
.LBB107_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB107_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-27
		r17:16 = memd(r29+#32)
		r19:18 = memd(r29+#24)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#16)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end107:
	.size	halide_error_requirement_failed, .Lfunc_end107-halide_error_requirement_failed
                                        // -- End function
	.section	.text.halide_error_specialize_fail,"ax",@progbits
	.weak	halide_error_specialize_fail    // -- Begin function halide_error_specialize_fail
	.p2align	4
	.type	halide_error_specialize_fail,@function
halide_error_specialize_fail:           // @halide_error_specialize_fail
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#16)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r18 = r1
		memd(r29+#0) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB108_1
	}
// %bb.2:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.67@PCREL)
		r19 = add(r16,#1023)
		r3 = #0
	}
	{
		call ##halide_string_to_string
		r1:0 = combine(r19,r16)
		memb(r16+#1023) = r3
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r18
	}
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB108_3
	}
.LBB108_1:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.67@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = #0
		r2 = r18
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB108_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-31
		r17:16 = memd(r29+#8)
		r19:18 = memd(r29+#0)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end108:
	.size	halide_error_specialize_fail, .Lfunc_end108-halide_error_specialize_fail
                                        // -- End function
	.section	.text.halide_error_no_device_interface,"ax",@progbits
	.weak	halide_error_no_device_interface // -- Begin function halide_error_no_device_interface
	.p2align	4
	.type	halide_error_no_device_interface,@function
halide_error_no_device_interface:       // @halide_error_no_device_interface
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB109_1
	}
// %bb.2:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.68@PCREL)
		r1 = add(r16,#1023)
		r0 = r16
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r16+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB109_3
	}
.LBB109_1:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.68@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB109_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-19
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end109:
	.size	halide_error_no_device_interface, .Lfunc_end109-halide_error_no_device_interface
                                        // -- End function
	.section	.text.halide_error_device_interface_no_device,"ax",@progbits
	.weak	halide_error_device_interface_no_device // -- Begin function halide_error_device_interface_no_device
	.p2align	4
	.type	halide_error_device_interface_no_device,@function
halide_error_device_interface_no_device: // @halide_error_device_interface_no_device
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB110_1
	}
// %bb.2:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.69@PCREL)
		r1 = add(r16,#1023)
		r0 = r16
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r16+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB110_3
	}
.LBB110_1:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.69@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB110_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-36
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end110:
	.size	halide_error_device_interface_no_device, .Lfunc_end110-halide_error_device_interface_no_device
                                        // -- End function
	.section	.text.halide_error_host_and_device_dirty,"ax",@progbits
	.weak	halide_error_host_and_device_dirty // -- Begin function halide_error_host_and_device_dirty
	.p2align	4
	.type	halide_error_host_and_device_dirty,@function
halide_error_host_and_device_dirty:     // @halide_error_host_and_device_dirty
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB111_1
	}
// %bb.2:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.70@PCREL)
		r1 = add(r16,#1023)
		r0 = r16
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r16+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB111_3
	}
.LBB111_1:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.70@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB111_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-37
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end111:
	.size	halide_error_host_and_device_dirty, .Lfunc_end111-halide_error_host_and_device_dirty
                                        // -- End function
	.section	.text.halide_error_buffer_is_null,"ax",@progbits
	.weak	halide_error_buffer_is_null     // -- Begin function halide_error_buffer_is_null
	.p2align	4
	.type	halide_error_buffer_is_null,@function
halide_error_buffer_is_null:            // @halide_error_buffer_is_null
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#32)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r18 = r1
		memd(r29+#16) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB112_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.71@PCREL)
		r19 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r19,r16)
		jump .LBB112_3
		memb(r16+#1023) = r3
	}
.LBB112_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.71@PCREL)
		r19 = #0
		r1:0 = combine(#0,#0)
	}
.LBB112_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r19
		r2 = r18
	}
	{
		r2 = add(pc,##.L.str.72@PCREL)
		r1 = r19
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB112_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB112_6
	}
.LBB112_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB112_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-38
		r17:16 = memd(r29+#24)
		r19:18 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end112:
	.size	halide_error_buffer_is_null, .Lfunc_end112-halide_error_buffer_is_null
                                        // -- End function
	.section	.text.halide_error_storage_bound_too_small,"ax",@progbits
	.weak	halide_error_storage_bound_too_small // -- Begin function halide_error_storage_bound_too_small
	.p2align	4
	.type	halide_error_storage_bound_too_small,@function
halide_error_storage_bound_too_small:   // @halide_error_storage_bound_too_small
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19:18 = combine(r1,r4)
		r21 = r2
		memd(r29+#32) = r19:18
		memd(r29+#24) = r21:20
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
		r22 = r3
		memd(r29+#16) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r16 = r0
		p0 = cmp.eq(r0,#0)
	}
	{
		r0 = p0
		if (p0) jump:nt .LBB113_1
		memw(r29+#4) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.2:                               // %if.then6.i
	{
		r2 = add(pc,##.L.str.73@PCREL)
		r20 = add(r16,#1023)
		r3 = #0
	}
	{
		r1:0 = combine(r20,r16)
		jump .LBB113_3
		memb(r16+#1023) = r3
	}
.LBB113_1:                              // %entry.split
	{
		r2 = add(pc,##.L.str.73@PCREL)
		r20 = #0
		r1:0 = combine(#0,#0)
	}
.LBB113_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
	{
		call ##halide_string_to_string
		r23 = asr(r22,#31)
	}
	{
		r3:2 = combine(r23,r22)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.62@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r21
	}
	{
		r2 = add(pc,##.L.str.51@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		call ##halide_string_to_string
		r1 = r20
		r2 = r19
	}
	{
		r2 = add(pc,##.L.str.74@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
		r19 = asr(r18,#31)
	}
	{
		r3:2 = combine(r19,r18)
		r4 = #1
		r1 = r20
	}
	{
		call ##halide_int64_to_string
	}
	{
		r2 = add(pc,##.L.str.64.78@PCREL)
		r1 = r20
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = memw(r29+#4)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (p0.new) jump:nt .LBB113_4
	}
// %bb.5:                               // %if.else.i
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB113_6
	}
.LBB113_4:                              // %if.then.i
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB113_6:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-45
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r29+#24)
		r23:22 = memd(r29+#16)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end113:
	.size	halide_error_storage_bound_too_small, .Lfunc_end113-halide_error_storage_bound_too_small
                                        // -- End function
	.section	.text.halide_error_device_crop_failed,"ax",@progbits
	.weak	halide_error_device_crop_failed // -- Begin function halide_error_device_crop_failed
	.p2align	4
	.type	halide_error_device_crop_failed,@function
halide_error_device_crop_failed:        // @halide_error_device_crop_failed
// %bb.0:                               // %entry
	{
		r17 = r0
		r0 = #1024
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##malloc
	}
	{
		r16 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB114_1
	}
// %bb.2:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.75@PCREL)
		r1 = add(r16,#1023)
		r0 = r16
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r16+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r16))
		r1:0 = combine(r16,r17)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r16,r17)
		jump .LBB114_3
	}
.LBB114_1:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.75@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r17
	}
.LBB114_3:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r16
	}
	{
		r0 = #-41
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end114:
	.size	halide_error_device_crop_failed, .Lfunc_end114-halide_error_device_crop_failed
                                        // -- End function
	.section	.text.halide_msan_annotate_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_memory_is_initialized // -- Begin function halide_msan_annotate_memory_is_initialized
	.p2align	4
	.type	halide_msan_annotate_memory_is_initialized,@function
halide_msan_annotate_memory_is_initialized: // @halide_msan_annotate_memory_is_initialized
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end115:
	.size	halide_msan_annotate_memory_is_initialized, .Lfunc_end115-halide_msan_annotate_memory_is_initialized
                                        // -- End function
	.section	.text.halide_msan_check_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_check_memory_is_initialized // -- Begin function halide_msan_check_memory_is_initialized
	.p2align	4
	.type	halide_msan_check_memory_is_initialized,@function
halide_msan_check_memory_is_initialized: // @halide_msan_check_memory_is_initialized
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end116:
	.size	halide_msan_check_memory_is_initialized, .Lfunc_end116-halide_msan_check_memory_is_initialized
                                        // -- End function
	.section	.text.halide_msan_check_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_check_buffer_is_initialized // -- Begin function halide_msan_check_buffer_is_initialized
	.p2align	4
	.type	halide_msan_check_buffer_is_initialized,@function
halide_msan_check_buffer_is_initialized: // @halide_msan_check_buffer_is_initialized
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end117:
	.size	halide_msan_check_buffer_is_initialized, .Lfunc_end117-halide_msan_check_buffer_is_initialized
                                        // -- End function
	.section	.text.halide_msan_annotate_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized // -- Begin function halide_msan_annotate_buffer_is_initialized
	.p2align	4
	.type	halide_msan_annotate_buffer_is_initialized,@function
halide_msan_annotate_buffer_is_initialized: // @halide_msan_annotate_buffer_is_initialized
// %bb.0:                               // %entry
	{
		r0 = #0
		jumpr r31
	}
.Lfunc_end118:
	.size	halide_msan_annotate_buffer_is_initialized, .Lfunc_end118-halide_msan_annotate_buffer_is_initialized
                                        // -- End function
	.section	.text.halide_msan_annotate_buffer_is_initialized_as_destructor,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized_as_destructor // -- Begin function halide_msan_annotate_buffer_is_initialized_as_destructor
	.p2align	4
	.type	halide_msan_annotate_buffer_is_initialized_as_destructor,@function
halide_msan_annotate_buffer_is_initialized_as_destructor: // @halide_msan_annotate_buffer_is_initialized_as_destructor
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end119:
	.size	halide_msan_annotate_buffer_is_initialized_as_destructor, .Lfunc_end119-halide_msan_annotate_buffer_is_initialized_as_destructor
                                        // -- End function
	.section	.text.halide_qurt_hvx_lock,"ax",@progbits
	.weak	halide_qurt_hvx_lock            // -- Begin function halide_qurt_hvx_lock
	.p2align	4
	.type	halide_qurt_hvx_lock,@function
halide_qurt_hvx_lock:                   // @halide_qurt_hvx_lock
// %bb.0:                               // %entry
	{
		r16 = r0
		r0 = #1
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##qurt_hvx_lock
	}
	{
		p0 = cmp.eq(r0,#0)
		if (p0.new) r0 = #0
		if (p0.new) r17:16 = memd(r29+#0)
		if (p0.new) r31:30 = dealloc_return(r30):t:raw
	}                                       // 8-byte Folded Reload
.LBB120_1:                              // %if.then
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB120_2
	}
// %bb.3:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.4.91@PCREL)
		r1 = add(r17,#1023)
		r0 = r17
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r17+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r17,r16)
		jump .LBB120_4
	}
.LBB120_2:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.4.91@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
.LBB120_4:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-1
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end120:
	.size	halide_qurt_hvx_lock, .Lfunc_end120-halide_qurt_hvx_lock
                                        // -- End function
	.section	.text.halide_qurt_hvx_unlock,"ax",@progbits
	.weak	halide_qurt_hvx_unlock          // -- Begin function halide_qurt_hvx_unlock
	.p2align	4
	.type	halide_qurt_hvx_unlock,@function
halide_qurt_hvx_unlock:                 // @halide_qurt_hvx_unlock
// %bb.0:                               // %entry
	{
		r16 = r0
		memd(r29+#-16) = r17:16
		allocframe(#8)
	}                                       // 8-byte Folded Spill
	{
		call ##qurt_hvx_unlock
	}
	{
		p0 = cmp.eq(r0,#0)
		if (p0.new) r0 = #0
		if (p0.new) r17:16 = memd(r29+#0)
		if (p0.new) r31:30 = dealloc_return(r30):t:raw
	}                                       // 8-byte Folded Reload
.LBB121_1:                              // %if.then
	{
		call ##malloc
		r0 = #1024
	}
	{
		r17 = r0
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB121_2
	}
// %bb.3:                               // %if.else.i
	{
		r2 = add(pc,##.L.str.6.93@PCREL)
		r1 = add(r17,#1023)
		r0 = r17
	}
	{
		call ##halide_string_to_string
		r3 = #0
		memb(r17+#1023) = r3.new
	}
	{
		r2 = add(r0,sub(#1,r17))
		r1:0 = combine(r17,r16)
	}
	{
		call ##halide_msan_annotate_memory_is_initialized
		r3 = asr(r2,#31)
	}
	{
		r1:0 = combine(r17,r16)
		jump .LBB121_4
	}
.LBB121_2:                              // %if.then.i
	{
		r2 = add(pc,##.L.str.6.93@PCREL)
		r1:0 = combine(#0,#0)
	}
	{
		call ##halide_string_to_string
	}
	{
		r1 = add(pc,##.L.str.7.92@PCREL)
		r0 = r16
	}
.LBB121_4:                              // %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit
	{
		call ##halide_error
	}
	{
		call ##free
		r0 = r17
	}
	{
		r0 = #-1
		r17:16 = memd(r29+#0)
		dealloc_return
	}                                       // 8-byte Folded Reload
.Lfunc_end121:
	.size	halide_qurt_hvx_unlock, .Lfunc_end121-halide_qurt_hvx_unlock
                                        // -- End function
	.section	.text.halide_qurt_hvx_unlock_as_destructor,"ax",@progbits
	.weak	halide_qurt_hvx_unlock_as_destructor // -- Begin function halide_qurt_hvx_unlock_as_destructor
	.p2align	4
	.type	halide_qurt_hvx_unlock_as_destructor,@function
halide_qurt_hvx_unlock_as_destructor:   // @halide_qurt_hvx_unlock_as_destructor
// %bb.0:                               // %entry
	{
		jump ##halide_qurt_hvx_unlock
	}
.Lfunc_end122:
	.size	halide_qurt_hvx_unlock_as_destructor, .Lfunc_end122-halide_qurt_hvx_unlock_as_destructor
                                        // -- End function
	.section	.text.halide_vtcm_malloc,"ax",@progbits
	.weak	halide_vtcm_malloc              // -- Begin function halide_vtcm_malloc
	.p2align	4
	.type	halide_vtcm_malloc,@function
halide_vtcm_malloc:                     // @halide_vtcm_malloc
// %bb.0:                               // %entry
	{
		r0 = r1 ; jump ##HAP_request_VTCM
		r1 = #1
	}
.Lfunc_end123:
	.size	halide_vtcm_malloc, .Lfunc_end123-halide_vtcm_malloc
                                        // -- End function
	.section	.text.halide_vtcm_free,"ax",@progbits
	.weak	halide_vtcm_free                // -- Begin function halide_vtcm_free
	.p2align	4
	.type	halide_vtcm_free,@function
halide_vtcm_free:                       // @halide_vtcm_free
// %bb.0:                               // %entry
	{
		r0 = r1 ; jump ##HAP_release_VTCM
	}
.Lfunc_end124:
	.size	halide_vtcm_free, .Lfunc_end124-halide_vtcm_free
                                        // -- End function
	.section	.text.halide_default_can_use_target_features,"ax",@progbits
	.weak	halide_default_can_use_target_features // -- Begin function halide_default_can_use_target_features
	.p2align	4
	.type	halide_default_can_use_target_features,@function
halide_default_can_use_target_features: // @halide_default_can_use_target_features
// %bb.0:                               // %entry
	{
		r17:16 = combine(r0,r1)
		memd(r29+#-16) = r17:16
		allocframe(#48)
	}                                       // 8-byte Folded Spill
	{
		r19 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		memd(r29+#32) = r19:18
	}                                       // 8-byte Folded Spill
	{
		call ##halide_mutex_lock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOT)
	}
	{
		r18 = add(r19,##_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE@GOT)
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE@GOT)
	}
	{
		r0 = memb(r0+#0)
		if (cmp.eq(r0.new,#0)) jump:nt .LBB125_1
	}
// %bb.2:                               // %if.end
	{
		call ##halide_mutex_unlock
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOT)
	}
	{
		p0 = cmp.eq(r17,#2); if (p0.new) jump:t .LBB125_4
	}
.LBB125_3:                              // %if.then1
	{
		r1 = add(pc,##.L.str.94@PCREL)
		r0 = #0
	}
	{
		call ##halide_error
	}
.LBB125_4:                              // %if.end2
	{
		r3:2 = combine(#0,#0)
		r1:0 = memd(r16+#0)
		r4 = memw(r18+#0)
	}
	{
		r5:4 = memd(r4+#0)
	}
	{
		r1:0 = and(r5:4,r1:0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB125_6
	}
// %bb.5:                               // %if.then6
	{
		r4 = memw(r18+#0)
	}
	{
		r5:4 = memd(r4+#16)
	}
	{
		r5:4 = and(r5:4,r1:0)
	}
	{
		p0 = cmp.eq(r5:4,r1:0)
		r0 = #0
		if (!p0.new) r17:16 = memd(r29+#40)
		if (!p0.new) r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		if (!p0) r31:30 = dealloc_return(r30):raw
	}
.LBB125_6:                              // %for.inc.critedge
	{
		r1:0 = memd(r16+#8)
		r4 = memw(r18+#0)
	}
	{
		r5:4 = memd(r4+#8)
	}
	{
		r1:0 = and(r5:4,r1:0)
	}
	{
		p0 = cmp.eq(r1:0,r3:2)
		if (p0.new) jump:nt .LBB125_8
	}
// %bb.7:                               // %if.then6.1
	{
		r2 = memw(r18+#0)
	}
	{
		r3:2 = memd(r2+#24)
	}
	{
		r3:2 = and(r3:2,r1:0)
	}
	{
		p0 = cmp.eq(r3:2,r1:0)
		r0 = #0
	}
	{
		if (p0) r0 = #1
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB125_1:                              // %if.then
	{
		call ##_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
		r0 = add(r29,#0)
	}
	{
		r2 = #32
		r1 = add(r29,#0)
		r0 = memw(r18+#0)
	}
	{
		call ##memcpy
	}
	{
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE@GOT)
	}
	{
		memb(r0+#0) = #1
		r0 = memw(r19+##_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE@GOT)

	} :mem_noshuf
	{
		call ##halide_mutex_unlock
	}
	{
		p0 = cmp.eq(r17,#2); if (!p0.new) jump:t .LBB125_3
	}
	{
		jump .LBB125_4
	}
.LBB125_8:                              // %for.inc.critedge.1
	{
		r0 = #1
		r17:16 = memd(r29+#40)
		r19:18 = memd(r29+#32)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end125:
	.size	halide_default_can_use_target_features, .Lfunc_end125-halide_default_can_use_target_features
                                        // -- End function
	.section	.text.halide_set_custom_can_use_target_features,"ax",@progbits
	.weak	halide_set_custom_can_use_target_features // -- Begin function halide_set_custom_can_use_target_features
	.p2align	4
	.type	halide_set_custom_can_use_target_features,@function
halide_set_custom_can_use_target_features: // @halide_set_custom_can_use_target_features
// %bb.0:                               // %entry
	{
		r1 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
	}
	{
		r2 = memw(r1+##_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOT)
	}
	{
		r1 = memw(r2+#0)
		memw(r2+#0) = r0

	} :mem_noshuf
	{
		r0 = r1
		jumpr r31
	}
.Lfunc_end126:
	.size	halide_set_custom_can_use_target_features, .Lfunc_end126-halide_set_custom_can_use_target_features
                                        // -- End function
	.section	.text.halide_can_use_target_features,"ax",@progbits
	.weak	halide_can_use_target_features  // -- Begin function halide_can_use_target_features
	.p2align	4
	.type	halide_can_use_target_features,@function
halide_can_use_target_features:         // @halide_can_use_target_features
// %bb.0:                               // %entry
	{
		r2 = add(pc,##_GLOBAL_OFFSET_TABLE_@PCREL)
		allocframe(r29,#0):raw
	}
	{
		r2 = memw(r2+##_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOT)
	}
	{
		r2 = memw(r2+#0)
	}
	{
		callr r2
	}
	{
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end127:
	.size	halide_can_use_target_features, .Lfunc_end127-halide_can_use_target_features
                                        // -- End function
	.section	.text._ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv // -- Begin function _ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
	.p2align	4
	.type	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,@function
_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv: // @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
// %bb.0:                               // %entry
	{
		r3:2 = combine(#0,#0)
	}
	{
		memd(r0+#0) = r3:2
		memd(r0+#16) = r3:2
	}
	{
		jumpr r31
		memd(r0+#8) = r3:2
		memd(r0+#24) = r3:2
	}
.Lfunc_end128:
	.size	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv, .Lfunc_end128-_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
                                        // -- End function
	.section	.text.halide_use_jit_module,"ax",@progbits
	.weak	halide_use_jit_module           // -- Begin function halide_use_jit_module
	.p2align	4
	.type	halide_use_jit_module,@function
halide_use_jit_module:                  // @halide_use_jit_module
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end129:
	.size	halide_use_jit_module, .Lfunc_end129-halide_use_jit_module
                                        // -- End function
	.section	.text.halide_release_jit_module,"ax",@progbits
	.weak	halide_release_jit_module       // -- Begin function halide_release_jit_module
	.p2align	4
	.type	halide_release_jit_module,@function
halide_release_jit_module:              // @halide_release_jit_module
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end130:
	.size	halide_release_jit_module, .Lfunc_end130-halide_release_jit_module
                                        // -- End function
	.section	.rodata,"a",@progbits
	.p2align	7                               // -- Begin function depthwise_conv_hvx128
.LCPI131_0:
	.word	0                               // 0x0
	.word	1                               // 0x1
	.word	2                               // 0x2
	.word	3                               // 0x3
	.word	4                               // 0x4
	.word	5                               // 0x5
	.word	6                               // 0x6
	.word	7                               // 0x7
	.word	8                               // 0x8
	.word	9                               // 0x9
	.word	10                              // 0xa
	.word	11                              // 0xb
	.word	12                              // 0xc
	.word	13                              // 0xd
	.word	14                              // 0xe
	.word	15                              // 0xf
	.word	16                              // 0x10
	.word	17                              // 0x11
	.word	18                              // 0x12
	.word	19                              // 0x13
	.word	20                              // 0x14
	.word	21                              // 0x15
	.word	22                              // 0x16
	.word	23                              // 0x17
	.word	24                              // 0x18
	.word	25                              // 0x19
	.word	26                              // 0x1a
	.word	27                              // 0x1b
	.word	28                              // 0x1c
	.word	29                              // 0x1d
	.word	30                              // 0x1e
	.word	31                              // 0x1f
.LCPI131_1:
	.word	32                              // 0x20
	.word	33                              // 0x21
	.word	34                              // 0x22
	.word	35                              // 0x23
	.word	36                              // 0x24
	.word	37                              // 0x25
	.word	38                              // 0x26
	.word	39                              // 0x27
	.word	40                              // 0x28
	.word	41                              // 0x29
	.word	42                              // 0x2a
	.word	43                              // 0x2b
	.word	44                              // 0x2c
	.word	45                              // 0x2d
	.word	46                              // 0x2e
	.word	47                              // 0x2f
	.word	48                              // 0x30
	.word	49                              // 0x31
	.word	50                              // 0x32
	.word	51                              // 0x33
	.word	52                              // 0x34
	.word	53                              // 0x35
	.word	54                              // 0x36
	.word	55                              // 0x37
	.word	56                              // 0x38
	.word	57                              // 0x39
	.word	58                              // 0x3a
	.word	59                              // 0x3b
	.word	60                              // 0x3c
	.word	61                              // 0x3d
	.word	62                              // 0x3e
	.word	63                              // 0x3f
.LCPI131_2:
	.word	64                              // 0x40
	.word	65                              // 0x41
	.word	66                              // 0x42
	.word	67                              // 0x43
	.word	68                              // 0x44
	.word	69                              // 0x45
	.word	70                              // 0x46
	.word	71                              // 0x47
	.word	72                              // 0x48
	.word	73                              // 0x49
	.word	74                              // 0x4a
	.word	75                              // 0x4b
	.word	76                              // 0x4c
	.word	77                              // 0x4d
	.word	78                              // 0x4e
	.word	79                              // 0x4f
	.word	80                              // 0x50
	.word	81                              // 0x51
	.word	82                              // 0x52
	.word	83                              // 0x53
	.word	84                              // 0x54
	.word	85                              // 0x55
	.word	86                              // 0x56
	.word	87                              // 0x57
	.word	88                              // 0x58
	.word	89                              // 0x59
	.word	90                              // 0x5a
	.word	91                              // 0x5b
	.word	92                              // 0x5c
	.word	93                              // 0x5d
	.word	94                              // 0x5e
	.word	95                              // 0x5f
.LCPI131_3:
	.word	96                              // 0x60
	.word	97                              // 0x61
	.word	98                              // 0x62
	.word	99                              // 0x63
	.word	100                             // 0x64
	.word	101                             // 0x65
	.word	102                             // 0x66
	.word	103                             // 0x67
	.word	104                             // 0x68
	.word	105                             // 0x69
	.word	106                             // 0x6a
	.word	107                             // 0x6b
	.word	108                             // 0x6c
	.word	109                             // 0x6d
	.word	110                             // 0x6e
	.word	111                             // 0x6f
	.word	112                             // 0x70
	.word	113                             // 0x71
	.word	114                             // 0x72
	.word	115                             // 0x73
	.word	116                             // 0x74
	.word	117                             // 0x75
	.word	118                             // 0x76
	.word	119                             // 0x77
	.word	120                             // 0x78
	.word	121                             // 0x79
	.word	122                             // 0x7a
	.word	123                             // 0x7b
	.word	124                             // 0x7c
	.word	125                             // 0x7d
	.word	126                             // 0x7e
	.word	127                             // 0x7f
.LCPI131_4:
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
.LCPI131_5:
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
.LCPI131_6:
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
	.byte	1                               // 0x1
	.byte	2                               // 0x2
	.byte	4                               // 0x4
	.byte	8                               // 0x8
	.byte	16                              // 0x10
	.byte	32                              // 0x20
	.byte	64                              // 0x40
	.byte	128                             // 0x80
.LCPI131_7:
	.word	63                              // 0x3f
	.word	62                              // 0x3e
	.word	61                              // 0x3d
	.word	60                              // 0x3c
	.word	59                              // 0x3b
	.word	58                              // 0x3a
	.word	57                              // 0x39
	.word	56                              // 0x38
	.word	55                              // 0x37
	.word	54                              // 0x36
	.word	53                              // 0x35
	.word	52                              // 0x34
	.word	51                              // 0x33
	.word	50                              // 0x32
	.word	49                              // 0x31
	.word	48                              // 0x30
	.word	47                              // 0x2f
	.word	46                              // 0x2e
	.word	45                              // 0x2d
	.word	44                              // 0x2c
	.word	43                              // 0x2b
	.word	42                              // 0x2a
	.word	41                              // 0x29
	.word	40                              // 0x28
	.word	39                              // 0x27
	.word	38                              // 0x26
	.word	37                              // 0x25
	.word	36                              // 0x24
	.word	35                              // 0x23
	.word	34                              // 0x22
	.word	33                              // 0x21
	.word	32                              // 0x20
.LCPI131_8:
	.word	31                              // 0x1f
	.word	30                              // 0x1e
	.word	29                              // 0x1d
	.word	28                              // 0x1c
	.word	27                              // 0x1b
	.word	26                              // 0x1a
	.word	25                              // 0x19
	.word	24                              // 0x18
	.word	23                              // 0x17
	.word	22                              // 0x16
	.word	21                              // 0x15
	.word	20                              // 0x14
	.word	19                              // 0x13
	.word	18                              // 0x12
	.word	17                              // 0x11
	.word	16                              // 0x10
	.word	15                              // 0xf
	.word	14                              // 0xe
	.word	13                              // 0xd
	.word	12                              // 0xc
	.word	11                              // 0xb
	.word	10                              // 0xa
	.word	9                               // 0x9
	.word	8                               // 0x8
	.word	7                               // 0x7
	.word	6                               // 0x6
	.word	5                               // 0x5
	.word	4                               // 0x4
	.word	3                               // 0x3
	.word	2                               // 0x2
	.word	1                               // 0x1
	.word	0                               // 0x0
.LCPI131_9:
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
.LCPI131_10:
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	64                              // 0x40
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
.LCPI131_11:
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
.LCPI131_12:
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	32                              // 0x20
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
.LCPI131_13:
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	255                             // 0xff
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.byte	0                               // 0x0
	.section	.text.depthwise_conv_hvx128,"ax",@progbits
	.globl	depthwise_conv_hvx128
	.p2align	4
	.type	depthwise_conv_hvx128,@function
depthwise_conv_hvx128:                  // @depthwise_conv_hvx128
// %bb.0:                               // %entry
	{
		allocframe(r29,#0):raw
	}
	{
		r29 = add(r29,##-47104)
		r6 = memw(r30+#40)
		memd(r30+#-40) = r25:24
	}
	{
		r29 = and(r29,#-256)
		r23 = r5
		r24 = memw(r30+#36)
		memd(r30+#-32) = r23:22
	}
	{
		r6 = memw(r30+#32)
		memw(r30+#-3376) = r6
	}
	{
		r22 = memw(r30+#28)
		memw(r30+#-1840) = r6
	}
	{
		r6 = memw(r30+#24)
		memw(r30+##-6456) = r6.new
	}
	{
		r6 = memw(r30+#20)
		memw(r30+##-4400) = r6.new
	}
	{
		r6 = memw(r30+#16)
		memw(r30+#-3888) = r6.new
	}
	{
		r18 = and(r30,#-256)
		r19 = r3
		r6 = memw(r30+#12)
		memd(r30+#-16) = r19:18
	}
	{
		r21:20 = combine(r2,r0)
		r0 = #0
		r7 = memw(r30+#8)
		memd(r30+#-24) = r21:20
	}
	{
		r6 = add(r18,#-27404)
		memw(r30+##-23352) = r6
	}                                       // 4-byte Folded Spill
	{
		r7 = add(r18,#-28044)
		memw(r30+##-10304) = r7
		memd(r30+#-8) = r17:16
	}                                       // 4-byte Folded Spill
	{
		r17 = r4
		memd(r30+#-48) = r27:26
		memw(r6+#0) = #0
	}                                       // 8-byte Folded Spill
	{
		memw(r6+#4) = #0
		memw(r6+#8) = #0
	}
	{
		memw(r7+#0) = #0
		memw(r7+#4) = #0
	}
	{
		memw(r30+#-3632) = r1
		memw(r7+#8) = #0
	}                                       // 4-byte Folded Spill
	{
		call ##halide_qurt_hvx_lock
		r16 = memw(r30+#44)
	}
	{
		v0 = vsplat(r23)
		r0 = add(r18,#-28032)
		r1 = memw(r21+#32)
		r7 = memw(r17+#12)
	}
	{
		r7 = memw(r21+#12)
		memw(r30+##-24752) = r7
	}                                       // 4-byte Folded Spill
	{
		r21 = memw(r1+#4)
		memw(r30+##-24728) = r7
	}                                       // 4-byte Folded Spill
	{
		p0 = cmp.gt(r21,#127)
		r3 = memw(r1+#20)
		memw(r30+##-5696) = r3.new
	}                                       // 4-byte Folded Spill
	{
		p1 = cmp.gt(r3,#0)
		r3 = add(r30,#-24624)
		r2 = memw(r20+#32)
		r7 = memw(r1+#24)
	}
	{
		r4 = memw(r1+#36)
		memw(r30+#-3896) = r18
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r1+#40)
		memw(r30+##-23672) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r20+#12)
		memw(r30+##-23536) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r20 = memw(r2+#16)
		memw(r30+##-5688) = r4
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r2+#24)
		memw(r30+##-23336) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r2+#40)
		memw(r30+##-23344) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r5 = memw(r2+#48)
		memw(r30+##-23624) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r5 = memw(r2+#52)
		memw(r30+##-23648) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r11 = memw(r2+#32)
		memw(r30+##-24736) = r7
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r2+#56)
		memw(r30+##-23632) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r16+#32)
		memw(r30+##-19776) = r21
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r16+#12)
		memw(r30+##-7224) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r1+#16)
		memw(r30+##-10296) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r1+#24)
		memw(r30+##-21264) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r1+#32)
		memw(r30+##-23360) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r1+#40)
		memw(r30+##-23368) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r0,#128)
		r6 = memw(r1+#20)
		r8 = memw(r1+#36)
	}
	{
		r1 = memw(r1+#56)
		memw(r30+##-23640) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r0,#256)
		r1 = add(r18,#-27392)
		memw(r30+##-23560) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r1,#128)
		memw(r30+##-23544) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r0,#384)
		memw(r30+##-7232) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r1,#256)
		memw(r30+##-23552) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-10312) = r2
		memw(r30+##-21144) = r8
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r1,#384)
		memw(r30+##-10320) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = p1
		p1 = cmp.gt(r4,#0)
		memw(r30+##-19768) = r6
	}                                       // 4-byte Folded Spill
	{
		r2 = p1
		memw(r30+##-23584) = r2
		memw(r30+#-2608) = r22
	}                                       // 4-byte Folded Spill
	{
		v0 = vsplat(r22)
		p1 = cmp.gt(r5,#0)
		vmemu(r3+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r3 = p1
		r2 = add(r30,#-21424)
		memw(r30+##-23568) = r2
	}                                       // 4-byte Folded Spill
	{
		r3 = add(pc,##.LCPI131_7@PCREL)
		memw(r30+##-24744) = r3
	}                                       // 4-byte Folded Spill
	{
		p1 = cmp.eq(r23,#1)
		memw(r30+##-24632) = r3
	}                                       // 4-byte Folded Spill
	{
		r2 = add(pc,##.LCPI131_8@PCREL)
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r3 = add(pc,##.LCPI131_4@PCREL)
		memw(r30+##-24640) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = add(pc,##.LCPI131_5@PCREL)
		memw(r30+#-3912) = r3
	}                                       // 4-byte Folded Spill
	{
		r3 = add(pc,##.LCPI131_9@PCREL)
		memw(r30+#-3904) = r2
		memw(r30+#-3120) = r23
	}                                       // 4-byte Folded Spill
	{
		r2 = add(pc,##.LCPI131_10@PCREL)
		memw(r30+##-21432) = r3
	}                                       // 4-byte Folded Spill
	{
		r2 = p1
		memw(r30+##-21440) = r2
	}                                       // 4-byte Folded Spill
	{
		r3 = add(pc,##.LCPI131_11@PCREL)
		p1 = cmp.gt(r8,#0)
	}
	{
		r3 = p1
		p1 = cmp.gt(r6,#0)
		memw(r30+##-21448) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23656) = r2
		memw(r30+##-23664) = r3
	}                                       // 4-byte Folded Spill
	{
		r2 = p1
		memw(r30+##-23376) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r3 = add(pc,##.LCPI131_12@PCREL)
		memw(r30+##-21456) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(pc,##.LCPI131_13@PCREL)
		memw(r30+##-21464) = r2.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt ##.LBB131_86
	}
// %bb.1:                               // %entry
	{
		r0 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		p2 = tstbit(r0,#0)
		p1 = cmp.gt(r0,#7)
	}
	{
		p1 = or(p1,!p2)
		if (!p1.new) jump:nt ##.LBB131_86
	}
// %bb.2:                               // %entry
	{
		r0 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		p2 = tstbit(r0,#0)
		p1 = cmp.gt(r0,#7)
	}
	{
		p1 = or(p1,!p2)
		if (!p1.new) jump:nt ##.LBB131_86
	}
// %bb.3:                               // %then_bb
	{
		r4 = add(r21,#127)
		r25 = memw(r30+##-19768)
		r23 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		r5 = #2
		r18 = r24
		r28 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		r3 = min(r25,r5)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r0,#-1)
		r24 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r0 = min(r24,r5)
		r26 = r0
		r22 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r28,#-1)
		r4 = memw(r30+##-23336)
		memw(r30+#-304) = r4
	}                                       // 4-byte Folded Reload
	{
		r15 = add(r0,#-2)
		r0 = and(r19,#255)
		r5 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r16 = mpyi(r4,r20)
		r21 = memw(r30+##-10304)
		r4 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r11 = mpyi(r5,r11)
		v0.h = vsplat(r0)
		r0 = memw(r30+##-23624)
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r30,#-23856)
		r4 = and(r4,#255)
		r20 = memw(r30+##-23352)
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r4)
		r4 = add(r30,#-25008)
		vmemu(r5+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r17 = #-1
		p3 = cmp.gt(r20,#-1)
		r14 = add(r3,#-2)
		r5 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r10 = mpyi(r1,r22)
		r5 = and(r5,#255)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r13 = asr(r22,#31)
		v1 = vsplat(r17)
		r4 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		r7 = mpyi(r2,r23)
		r8 = asr(r23,#31)
		r12 = add(r24,#-1)
		p2 = cmp.gt(r23,#-1)
	}
	{
		r4 = mpyi(r4,r0)
		p0 = cmp.gt(r21,#-1)
		p1 = cmp.gt(r22,#-1)
		r0 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		v0.b = vsplat(r0)
		r0 = add(r30,#-22832)
		r9 = add(r25,#-1)
	}
	{
		r2 = and(r8,r7)
		r6 = and(r13,r10)
		memw(r30+##-24760) = r2
	}                                       // 4-byte Folded Spill
	{
		if (!p1) r10 = #0
		if (!p2) r7 = #0
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v0.h = vsplat(r5)
		r5 = add(r30,#-22960)
		r0 = #0
		p1 = cmp.eq(r26,#3)
	}
	{
		p2 = cmp.eq(r28,#3)
		r17 = mux(p3,r15,r12)
	}
	{
		v0.b = vsplat(r18)
		r5 = add(r30,#-23088)
		vmemu(r5+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r3 = asl(r22,#1)
		r19 = mux(p0,r9,r14)
		r27 = memw(r30+##-23360)
	}                                       // 4-byte Folded Reload
	{
		r5 = max(r28,r0)
		r1 = asl(r23,#1)
		vmemu(r5+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r5 = r26
		r28 = mux(p0,r14,r9)
		r9 = add(r17,r27)
		memw(r30+#-560) = r5
	}                                       // 4-byte Folded Spill
	{
		p1 = and(p1,p2)
		r18 = mux(p3,r12,r15)
		r26 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r17 = mpyi(r9,r20)
		if (p1) r12 = and(r13,r3)
		if (p1) r9 = and(r8,r1)
		r8 = r4
	}
	{
		r14 = add(r28,r26)
		r15 = add(r18,r27)
		if (!p1) r12 = add(r6,#0)
	}
	{
		r18 = mpyi(r14,r21)
		r8 += add(r11,r16)
		r14 = add(r19,r26)
		r16 = #0
	}
	{
		r19 = max(r23,r0)
		if (!p1) r9 = add(r2,#0)
		memw(r30+##-24712) = r16
	}                                       // 4-byte Folded Spill
	{
		r16 = max(r22,r0)
		r11 = asl(r19,#1)
		p2 = cmp.gt(r12,r6)
	}
	{
		r20 = mpyi(r15,r20)
		r16 = asl(r16,#1)
		memw(r30+#-816) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = p2
		if (!p1) r11 = add(r7,#0)
		p0 = cmp.gt(r9,r2)
		memw(r30+#-1328) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r28 = mpyi(r14,r21)
		r15 = add(r18,r2)
		p2 = cmp.gt(r11,r7)
	}
	{
		if (p0) r19 = add(r15,#0)
		if (!p1) r16 = add(r10,#0)
		r0 = add(r28,r7)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		p0 = r1
		if (!p0) r19 = add(r18,r9)
		if (p2) r21 = add(r28,r11)
		p3 = cmp.gt(r16,r10)
	}
	{
		r14 = add(r17,r6)
		r13 = add(r20,r10)
		if (p3) r16 = add(r20,r16)
		if (!p2) r21 = add(r0,#0)
	}
	{
		if (p0) r20 = add(r14,#0)
		r10 = sub(r21,r19)
		if (!p3) r16 = add(r13,#0)
		r18 = #-1
	}
	{
		r17 = max(r10,r18)
		if (!p0) r20 = add(r17,r12)
		r1 = #131
		v3:2.w = vsub(v3:2.w,v3:2.w)
	}
	{
		r17 = add(#128,asl(r17,#7))
		r7 = max(r7,r11)
		r16 = sub(r16,r20)
		v0 = v1
	}
	{
		r16 = max(r16,r18)
		r12 = min(r6,r12)
		r21 = r5
		r7 = add(r7,r28)
	}
	{
		r9 = min(r2,r9)
		r16 = add(r16,#1)
		r6 = sub(r6,r12)
		memw(r30+#-1072) = r3
	}                                       // 4-byte Folded Spill
	{
		r17 = add(r1,mpyi(r17,r16))
		r16 = asl(r5,#8)
		r5 = add(r30,#-24368)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r28 = asr(r1,#31)
		p0 = cmp.eq(r1,#0)
		memw(r30+##-23880) = r17
	}                                       // 4-byte Folded Spill
	{
		p3 = or(p1,p1)
		r11 = sub(#-1,r28)
		if (!p0) v1:0 = vcombine(v3,v2)
	}
	{
		memw(r30+##-23400) = r19
		memw(r30+##-5936) = r16
	}                                       // 4-byte Folded Spill
	{
		r28 = sub(r11,r28)
		r11 = sub(r0,r15)
		memw(r30+##-24672) = r28.new
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-24680) = r11
	}                                       // 4-byte Folded Spill
	{
		r28 = add(r7,sub(#1,r19))
		r5 = add(r30,#-24240)
		vmemu(r5+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		vmemu(r5+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r2 += mpyi(r28,r6)
		r5 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r7 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p0 = r7
		r2 = sub(r2,r9)
		r0 = memw(r30+##-23368)
	}                                       // 4-byte Folded Reload
	{
		r12 = mpyi(r5,r15)
		r6 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r15 = sub(r13,r14)
		r7 = mux(p0,r16,#0)
		r5 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r0,r27)
		r17 = add(r6,#-128)
		memw(r30+##-23392) = r15
	}                                       // 4-byte Folded Spill
	{
		r13 = mpyi(r5,r14)
		r14 = r12
		r5 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		r14 += add(r4,r13)
		r13 += add(r4,r12)
		memw(r30+##-23416) = r28
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23408) = r20
		memw(r30+##-25032) = r17
	}                                       // 4-byte Folded Spill
	{
		r5 = mpyi(r7,r5)
		r4 = sub(r14,r8)
		r7 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-21264)
		memw(r30+##-25040) = r4
	}                                       // 4-byte Folded Reload
	{
		r1 = asr(r7,#7)
		r6 = or(r5,#134)
		r5 = memw(r30+##-23624)
	}                                       // 4-byte Folded Reload
	{
		r7 = sub(r13,r8)
		r4 = add(r25,#1)
		memw(r30+##-25048) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r0 += mpyi(r3,r26)
		memw(r30+##-25024) = r1
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r30+##-23640)
		memw(r30+##-23864) = r6
	}                                       // 4-byte Folded Reload
	{
		r0 += mpyi(r3,r5)
		r3 = add(r24,#1)
		r5 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r7 = asr(r3,#1)
		memw(r30+##-25016) = r0
	}                                       // 4-byte Folded Spill
	{
		r5 = lsl(#1,r5)
		r7 = add(r10,#1)
		memw(r30+##-23424) = r7
	}                                       // 4-byte Folded Spill
	{
		r3 = asl(r2,#7)
		r2 = r6
		memw(r30+##-23432) = r7
	}                                       // 4-byte Folded Spill
	{
		r7 = add(r24,#-2)
		memw(r30+##-23440) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r7 = asr(r4,#1)
		r4 = asr(r5,#1)
		memw(r30+##-24688) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23216) = r7
		memw(r30+##-23448) = r4
	}                                       // 4-byte Folded Spill
	{
		r4 = mpyi(r22,r28)
		r5 = sub(r22,r20)
		r7 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r0 = sub(r7,r20)
		r7 = add(r25,#-2)
		memw(r30+##-23464) = r5
	}                                       // 4-byte Folded Spill
	{
		r4 = asl(r4,#7)
		r5 = memw(r30+#-816)
		memw(r30+##-23456) = r0
	}                                       // 4-byte Folded Reload
	{
		r7 = asl(r23,#7)
		r0 = sub(r5,r19)
		memw(r30+##-21808) = r7
	}                                       // 4-byte Folded Spill
	{
		r5 = sub(r23,r19)
		memw(r30+##-6192) = r4
	}                                       // 4-byte Folded Spill
	{
		r4 = ##16744702
		memw(r30+##-23472) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r4,#-32767)
		memw(r30+##-4784) = r7
	}                                       // 4-byte Folded Spill
	{
		r5 = extractu(r16,#2,#8)
		memw(r30+##-23480) = r5
	}                                       // 4-byte Folded Spill
	{
		r7 = memw(r30+##-6456)
		memw(r30+##-23488) = r0
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r7,#31)
		r4 = and(r21,#-4)
		memw(r30+##-23616) = r5
	}                                       // 4-byte Folded Spill
	{
		r7 = or(r15,r11)
		r5 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r5,#2)
		r5 = asl(r28,#7)
		memw(r30+##-6448) = r0
	}                                       // 4-byte Folded Spill
	{
		r5 = add(r27,#1)
		memw(r30+##-23504) = r5
	}                                       // 4-byte Folded Spill
	{
		r4 = add(r11,#1)
		memw(r30+##-24376) = r4
	}                                       // 4-byte Folded Spill
	{
		r5 = memw(r30+##-23568)
		memw(r30+##-23528) = r5
	}                                       // 4-byte Folded Reload
	{
		r4 = sub(#0,r19)
		r19 = #0
		memw(r30+##-23496) = r4
	}                                       // 4-byte Folded Spill
	{
		p2 = r5
		r7 = setbit(r3,#6)
		memw(r30+##-24696) = r7
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-24704) = r7
		memw(r30+##-23512) = r4
	}                                       // 4-byte Folded Spill
	{
		r7 = sub(#0,r20)
		r4 = add(r26,#1)
		memw(r30+##-23520) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p1
		r7 = #0
		memw(r30+##-23312) = r4
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23576) = r0
		memw(r30+##-24720) = r7
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-24664) = r4
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_4:                              // %"for output.s0.c.co"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_11 Depth 2
                                        //       Child Loop BB131_13 Depth 3
                                        //       Child Loop BB131_17 Depth 3
                                        //     Child Loop BB131_19 Depth 2
                                        //       Child Loop BB131_24 Depth 3
                                        //       Child Loop BB131_28 Depth 3
                                        //     Child Loop BB131_42 Depth 2
                                        //       Child Loop BB131_61 Depth 3
                                        //         Child Loop BB131_62 Depth 4
                                        //       Child Loop BB131_48 Depth 3
                                        //         Child Loop BB131_49 Depth 4
                                        //       Child Loop BB131_67 Depth 3
                                        //         Child Loop BB131_71 Depth 4
                                        //           Child Loop BB131_75 Depth 5
                                        //             Child Loop BB131_76 Depth 6
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-28044)
	}
	{
		r1 = memw(r0+#4)
	}
	{
		p0 = cmp.gtu(r2,r1); if (p0.new) jump:nt .LBB131_31
		r1 = memw(r0+#0)
	}
// %bb.5:                               //   in Loop: Header=BB131_4 Depth=1
	{
		r0 = r1
		r3 = memw(r30+##-23616)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB131_37
		memw(r30+##-23296) = r0
	}                                       // 4-byte Folded Spill
.LBB131_6:                              // %"produce filter_zeroed"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r0 = memw(r30+##-24712)
	}                                       // 4-byte Folded Reload
	{
		r0 = min(r0,r17)
		if (!p2) jump:nt .LBB131_38
		memw(r30+##-23600) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_7:                              // %"for filter_zeroed.s0.y.preheader"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		p1 = cmp.eq(r3,#0)
		r0 = memw(r30+##-23296)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-24760)
		r2 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p2 = r2
		r14 = add(r0,#512)
		p0 = cmp.gtu(r4,#2)
		if (!p2.new) jump:nt .LBB131_18
	}
// %bb.8:                               // %"for filter_zeroed.s0.y.us.preheader"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r12 = #0
		r0 = memw(r30+##-23296)
	}                                       // 4-byte Folded Reload
	{
		r15 = #0
		r7 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r13 = add(r0,#128)
		r6 = memw(r30+##-23600)
	}                                       // 4-byte Folded Reload
	{
		r28 = r14
		r3 = add(r7,r6)
		r4 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r6 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		loop1(.LBB131_11,r6)
		jump .LBB131_11
	}
	.p2align	4
.LBB131_9:                              //   in Loop: Header=BB131_11 Depth=2
	{
		v0 = valign(v2,v0,r6)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v31:30.uh = vunpack(v0.ub)
		v1.h = vsub(v28.h,v8.h)
		vmem(r0+#0) = v1.new
	}
	{
		v0.h = vsub(v30.h,v8.h)
		vmem(r0+#-1) = v0.new
	}
.LBB131_10:                             // %"end for filter_zeroed.s0.x.loopexit.us"
                                        //   in Loop: Header=BB131_11 Depth=2
	{
		r0 = memw(r30+##-23672)
		r7 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r3,r0)
		r28 = add(r28,r7)
		r6 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r15 = add(r15,r6)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_18
	}
.Ltmp19:                                // Block address taken
.LBB131_11:                             // %"for filter_zeroed.s0.y.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_13 Depth 3
                                        //       Child Loop BB131_17 Depth 3
	{
		r10 = #0
		if (!p0) jump:nt .LBB131_15
		memw(r30+#-304) = r3
	}                                       // 4-byte Folded Spill
// %bb.12:                              //   in Loop: Header=BB131_11 Depth=2
	{
		r1 = add(r3,#64)
		r0 = memw(r30+##-24376)
		v0 = vmem(r3+#0)
	}                                       // 4-byte Folded Reload
	{
		r8 = lsr(r0,#2)
		r11 = r4
		r0 = add(r3,r4)
		v1 = vmem(r1+#0)
	}
	{
		r10 = add(r12,#4)
		r6 = add(r0,r4)
		r2 = add(r0,#64)
		v3 = vmem(r0+#0)
	}
	{
		p2 = cmp.gtu(r8,#1)
		r7 = add(r6,r4)
		v8 = valign(v6,v1,r1)
		v6.cur = vmem(r1+#1)
	}
	{
		r5 = add(r7,#64)
		r4 = memw(r30+##-23576)
		v1 = vmem(r0+#1)
	}                                       // 4-byte Folded Reload
	{
		r9 = r28
		v7 = valign(v1,v3,r0)
		v4 = vmem(r6+#0)
	}
	{
		v5 = vmem(r2+#0)
	}
	{
		v1 = vmem(r2+#1)
	}
	{
		r2 = add(r3,r4)
		r4 = add(r30,#-23856)
		v6 = valign(v1,v5,r2)
		v9 = vmem(r5+#0)
	}
	{
		v1 = vmem(r6+#1)
	}
	{
		r6 = add(r6,#64)
		v1 = valign(v1,v4,r6)
		v2 = vmem(r3+#1)
	}
	{
		v0 = valign(v2,v0,r3)
		v2 = vmem(r5+#1)
	}
	{
		r5 = add(r8,#-1)
		r8 = add(r28,#1024)
		v5 = valign(v2,v9,r5)
		v4 = vmem(r7+#0)
	}
	{
		loop0(.LBB131_13,r5)
		v3 = vmem(r6+#0)
	}
	{
		v3 = valign(v2,v3,r6)
		v2.cur = vmem(r6+#1)
	}
	{
		r6 = r28
		v2 = vmem(r7+#1)
	}
	{
		v2 = valign(v2,v4,r7)
	}
	{
		v5:4.uh = vunpack(v5.ub)
	}
	{
		if (!p2) jump:nt .LBB131_14
		v22 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	.p2align	4
.LBB131_13:                             // %"for filter_zeroed.s0.x.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_11 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		r1 = add(r2,r11)
		v15:14.uh = vunpack(v1.ub)
		v4.h = vsub(v4.h,v22.h)
		vmem(r9+#3) = v4.new
	}
	{
		r3 = add(r1,r11)
		r0 = add(r1,#64)
		v13:12.uh = vunpack(v3.ub)
		v5 = vmem(r2+#0)
	}
	{
		r4 = add(r3,r11)
		r7 = add(r3,#64)
		v17:16.uh = vunpack(v6.ub)
		v9 = vmem(r1+#0)
	}
	{
		r5 = add(r4,#64)
		r6 = r8
		v11:10.uh = vunpack(v2.ub)
		v1 = vmem(r4+#0)
	}
	{
		v19:18.uh = vunpack(v7.ub)
		v7.h = vsub(v14.h,v22.h)
		v3 = vmem(r7+#0)
	}
	{
		v21:20.uh = vunpack(v8.ub)
		v6 = vmem(r5+#0)
		vmem(r9+#0) = v7
	}
	{
		v8.h = vsub(v18.h,v22.h)
		v2 = vmem(r4+#1)
		vmem(r9+#-2) = v8.new
	}
	{
		r8 = add(r8,#1024)
		r10 = add(r10,#4)
		v2 = valign(v2,v1,r4)
		v1 = vmem(r5+#1)
	}
	{
		v4 = valign(v1,v6,r5)
		v6.h = vsub(v10.h,v22.h)
		v1 = vmem(r7+#1)
		vmem(r9+#2) = v6.new
	}
	{
		r7 = add(r2,#64)
		v3 = valign(v1,v3,r7)
	}
	{
		v11:10.uh = vunpack(v0.ub)
		v0.h = vsub(v12.h,v22.h)
		v1 = vmem(r3+#0)
		vmem(r9+#1) = v0.new
	}
	{
		v1 = valign(v0,v1,r3)
		v0.cur = vmem(r3+#1)
	}
	{
		r3 = memw(r30+##-23576)
		v0 = vmem(r2+#1)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r2,r3)
		v0 = valign(v0,v5,r2)
		v5 = vmem(r7+#0)
	}
	{
		v6 = vmem(r0+#0)
	}
	{
		v8 = vmem(r7+#1)
	}
	{
		v8 = valign(v8,v5,r7)
		v5.h = vsub(v20.h,v22.h)
		v7 = vmem(r0+#1)
		vmem(r9+#-3) = v5.new
	}
	{
		v6 = valign(v7,v6,r0)
		v5.h = vsub(v10.h,v22.h)
		v7.h = vsub(v16.h,v22.h)
		vmem(r9+#-4) = v5.new
	}
	{
		r9 = r6
		v5:4.uh = vunpack(v4.ub)
		v7 = vmem(r1+#1)
		vmem(r9+#-1) = v7
	}
	{
		nop
		v7 = valign(v7,v9,r1)
	} :endloop0
.LBB131_14:                             //   in Loop: Header=BB131_11 Depth=2
	{
		r4 = r11
		v9:8.uh = vunpack(v8.ub)
		r3 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v11:10.uh = vunpack(v0.ub)
	}
	{
		v11:10.uh = vunpack(v7.ub)
		v0.h = vsub(v10.h,v22.h)
		vmem(r6+#-4) = v0.new
	}
	{
		v7:6.uh = vunpack(v6.ub)
		v0.h = vsub(v8.h,v22.h)
		vmem(r6+#-3) = v0.new
	}
	{
		v29:28.uh = vunpack(v1.ub)
		v0.h = vsub(v10.h,v22.h)
		v1.h = vsub(v6.h,v22.h)
		vmem(r6+#-2) = v0.new
	}
	{
		v7:6.uh = vunpack(v3.ub)
		v0.h = vsub(v28.h,v22.h)
		vmem(r6+#-1) = v1
	}
	{
		v31:30.uh = vunpack(v2.ub)
		v1.h = vsub(v6.h,v22.h)
		vmem(r6+#0) = v0
	}
	{
		v0.h = vsub(v30.h,v22.h)
		v31.h = vsub(v4.h,v22.h)
		vmem(r6+#1) = v1
	}
	{
		vmem(r6+#2) = v0
	}
	{
		vmem(r6+#3) = v31
	}
.LBB131_15:                             // %"end for filter_zeroed.s0.x.loopexit.us.unr-lcssa"
                                        //   in Loop: Header=BB131_11 Depth=2
	{
		if (p1) jump:nt .LBB131_10
	}
// %bb.16:                              // %"for filter_zeroed.s0.x.us.epil.preheader"
                                        //   in Loop: Header=BB131_11 Depth=2
	{
		r0 = mpyi(r4,r10)
		r1 = r13
		r2 = add(r10,r15)
	}
	{
		r1 += asl(r2,#8)
		r6 = add(r3,r0)
		r2 = memw(r30+##-23616)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r6,#64)
		r3 = add(r2,#-1)
		p2 = cmp.gtu(r2,#1)
		r2 = add(r0,r4)
	}
	{
		loop0(.LBB131_17,r3)
		r5 = add(r1,#256)
		r3 = memw(r30+#-304)
		v1 = vmem(r7+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = r1
		r7 = add(r30,#-23856)
		v1 = valign(v2,v1,r7)
		v2.cur = vmem(r7+#1)
	}
	{
		v0 = vmem(r6+#0)
	}
	{
		v2 = vmem(r6+#1)
	}
	{
		if (!p2) jump:nt .LBB131_9
		v8 = vmemu(r7+#0)
	}                                       // 128-byte Folded Reload
	.p2align	4
.LBB131_17:                             // %"for filter_zeroed.s0.x.us.epil"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_11 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		r6 = add(r3,r2)
		r0 = r5
		r5 = add(r5,#256)
		v2 = valign(v2,v0,r6)
	}
	{
		r7 = add(r6,#64)
		r2 = add(r2,r4)
		v5:4.uh = vunpack(v1.ub)
		v0 = vmem(r6+#0)
	}
	{
		v7:6.uh = vunpack(v2.ub)
		v2.h = vsub(v4.h,v8.h)
		v1 = vmem(r7+#0)
	}
	{
		v3.h = vsub(v6.h,v8.h)
		vmem(r1+#0) = v2
	}
	{
		r1 = r0
		v2 = vmem(r6+#1)
		vmem(r1+#-1) = v3
	}
	{
		v1 = valign(v3,v1,r7)
		v3.cur = vmem(r7+#1)
	} :endloop0
	{
		jump .LBB131_9
	}
	.p2align	4
.LBB131_18:                             // %"for sum_filter.s1.r19$y.preheader"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r3:2 = combine(#0,#0)
		v3:2.w = vsub(v3:2.w,v3:2.w)
		r0 = memw(r30+##-23296)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r0,#128)
		v1:0 = vcombine(v3,v2)
	}
	.p2align	4
.LBB131_19:                             // %"for sum_filter.s1.r19$y"
                                        //   Parent Loop BB131_4 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_24 Depth 3
                                        //       Child Loop BB131_28 Depth 3
	{
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		if (p2.new) jump:nt .LBB131_21
	}
.LBB131_20:                             // %"end for sum_filter.s1.r19$x"
                                        //   in Loop: Header=BB131_19 Depth=2
	{
		r3 = add(r3,#1)
		r0 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		p2 = cmp.eq(r3,r0)
		r0 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		r14 = add(r14,r0)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r2,r0)
		if (!p2) jump:nt .LBB131_19
	}
	{
		jump .LBB131_39
	}
.LBB131_21:                             // %"for sum_filter.s1.r19$x.preheader"
                                        //   in Loop: Header=BB131_19 Depth=2
	{
		r4 = #0
		if (p0) jump:nt .LBB131_23
	}
// %bb.22:                              //   in Loop: Header=BB131_19 Depth=2
	{
		jump .LBB131_25
		r6 = memw(r30+##-23616)
	}                                       // 4-byte Folded Reload
.LBB131_23:                             //   in Loop: Header=BB131_19 Depth=2
	{
		r5 = r14
		r6 = memw(r30+##-23616)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-24376)
	}                                       // 4-byte Folded Reload
	{
		r0 = lsr(r0,#2)
	}
	{
		loop0(.LBB131_24,r0)
	}
.Ltmp20:                                // Block address taken
.LBB131_24:                             // %"for sum_filter.s1.r19$x"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_19 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		v4 = vmem(r5+#-4)
	}
	{
		r4 = add(r4,#4)
		v5:4.w = vunpack(v4.h)
		v6 = vmem(r5+#-3)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#-2)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#-1)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#0)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#1)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#2)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#3)
	}
	{
		r5 = add(r5,#1024)
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		nop
		v1:0.w = vadd(v7:6.w,v1:0.w)
	} :endloop0
.LBB131_25:                             // %"end for sum_filter.s1.r19$x.loopexit.unr-lcssa"
                                        //   in Loop: Header=BB131_19 Depth=2
	{
		if (p1) jump:nt .LBB131_20
	}
// %bb.26:                              // %"for sum_filter.s1.r19$x.epil.preheader"
                                        //   in Loop: Header=BB131_19 Depth=2
	{
		p2 = cmp.gtu(r6,#1)
		r0 = r1
		r4 = add(r4,r2)
	}
	{
		r0 += asl(r4,#8)
	}
	{
		r4 = add(r0,#256)
	}
	{
		v7:6.w = vunpack(v4.h)
		v4.cur = vmem(r0+#0)
	}
	{
		if (!p2) jump:nt .LBB131_30
		v1:0.w = vadd(v7:6.w,v1:0.w)
		v4 = vmem(r0+#-1)
	}
// %bb.27:                              // %"for sum_filter.s1.r19$x.epil"
                                        //   in Loop: Header=BB131_19 Depth=2
	{
		r0 = add(r6,#-2)
		p2 = cmp.gtu(r6,#2)
		v9:8.w = vunpack(v5.h)
		v5.cur = vmem(r4+#0)
	}
	{
		loop0(.LBB131_28,r0)
		r0 = add(r4,#256)
		v7:6.w = vunpack(v4.h)
		v1:0.w = vadd(v9:8.w,v1:0.w)
	}
	{
		if (!p2) jump:nt .LBB131_29
		v4 = vmem(r4+#-1)
	}
.LBB131_28:                             // %"for sum_filter.s1.r19$x.epil"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_19 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		v9:8.w = vunpack(v5.h)
		v5.cur = vmem(r0+#0)
	}
	{
		v7:6.w = vunpack(v4.h)
		v3:2.w = vadd(v7:6.w,v3:2.w)
	}
	{
		r0 = add(r0,#256)
		v1:0.w = vadd(v9:8.w,v1:0.w)
		v4 = vmem(r0+#-1)
	} :endloop0
.LBB131_29:                             //   in Loop: Header=BB131_19 Depth=2
	{
		v3:2.w = vadd(v7:6.w,v3:2.w)
	}
.LBB131_30:                             //   in Loop: Header=BB131_19 Depth=2
	{
		v5:4.w = vunpack(v4.h)
	}
	{
		jump .LBB131_20
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
.LBB131_31:                             // %if.then.i1938
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		p0 = cmp.eq(r1,#0); if (p0.new) jump:nt .LBB131_34
		r16 = memw(r0+#8)
	}
// %bb.32:                              // %if.then.i1938
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r0 = #16384
		if (!cmp.gtu(r16,r0.new)) jump:nt .LBB131_34
	}
// %bb.33:                              // %if.then3.i
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		call ##halide_free
		r0 = #0
	}
	{
		r2 = memw(r30+##-23864)
		r0 = memw(r30+##-24664)
	}                                       // 4-byte Folded Reload
	{
		p3 = r0
	}
.LBB131_34:                             // %if.end.i
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r0 = add(r16,r2)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r1,#-28044)
		r1 = #16384
	}
	{
		p0 = cmp.gtu(r0,r1); if (!p0.new) jump:t .LBB131_36
		r0 = #0
		memw(r16+#8) = r0
	}
// %bb.35:                              // %if.then8.i
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		call ##halide_malloc
		r1:0 = combine(r2,#0)
	}
	{
		r2 = memw(r30+##-23864)
		r1 = memw(r30+##-24664)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
	}
.LBB131_36:                             // %if.end11.i
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r1 = memw(r30+##-23568)
		memw(r16+#0) = r0
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		r3 = memw(r30+##-23616)
		memw(r16+#4) = r2
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:t .LBB131_6
		memw(r30+##-23296) = r0
	}                                       // 4-byte Folded Spill
.LBB131_37:                             // %then_bb2
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r2 = add(#7,asl(r2,#2))
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r2,#-8)
		r1 = add(r1,#-28044)
	}
	{
		r0 = sub(r29,r0)
	}
	{
		r0 = and(r0,#-128)
		memw(r30+##-23296) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r29 = r0
		r0 = memw(r30+##-24712)
		memw(r1+#0) = r0
	}                                       // 4-byte Folded Reload
	{
		r0 = min(r0,r17)
		if (p2) jump:nt .LBB131_7
		memw(r30+##-23600) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_38:                             // %"produce sum_filter.thread"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		v3:2.w = vsub(v3:2.w,v3:2.w)
	}
	{
		v1:0 = vcombine(v3,v2)
	}
	.p2align	4
.LBB131_39:                             // %"consume sum_filter"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r1 = add(r30,#-25008)
		r0 = memw(r30+##-24720)
		r7 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r0,#7)
		r1 = add(r7,#-28032)
		v7 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r3 = min(r0,r17)
		r7 = memw(r30+##-24752)
	}                                       // 4-byte Folded Reload
	{
		r6 = addasl(r7,r3,#2)
		v5.w = vmpyieo(v7.h,v2.h)
		r5 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vmpyieo(v7.h,v3.h)
		r7 = add(r6,#256)
		r4 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v5.w += vmpyie(v7.w,v2.h)
		r2 = memw(r30+##-23864)
		v2 = vmem(r6+#0)
	}                                       // 4-byte Folded Reload
	{
		v4.w += vmpyie(v7.w,v3.h)
		v3 = vmem(r6+#1)
	}
	{
		v2 = valign(v3,v2,r6)
		v6 = vmem(r6+#2)
	}
	{
		r6 = and(r7,#-128)
		v3 = valign(v6,v3,r6)
	}
	{
		v3:2.w = vsub(v3:2.w,v5:4.w)
		vmem(r5+#0) = v3.new
	}
	{
		v3.w = vmpyieo(v7.h,v0.h)
		r5 = memw(r30+##-23568)
		vmem(r1+#0) = v2
	}                                       // 4-byte Folded Reload
	{
		p2 = r5
		v2.w = vmpyieo(v7.h,v1.h)
		v4 = vmem(r6+#1)
	}
	{
		v3.w += vmpyie(v7.w,v0.h)
		v0 = vmem(r6+#0)
	}
	{
		v2.w += vmpyie(v7.w,v1.h)
		v0 = valign(v4,v0,r7)
		v1 = vmem(r6+#2)
	}
	{
		v1 = valign(v1,v4,r7)
		r7 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v1:0.w = vsub(v1:0.w,v3:2.w)
		r6 = memw(r30+##-23584)
		vmem(r4+#0) = v0.new
	}                                       // 4-byte Folded Reload
	{
		p1 = r6
		r4 = memw(r30+##-24744)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		if (!p0.new) jump:nt ##.LBB131_81
		vmem(r7+#0) = v1
	}
// %bb.40:                              // %"for output.s0.b.rebased.preheader"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r4 = add(r3,#64)
		r1 = memw(r30+##-23296)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,#128)
		r2 = memw(r30+##-25016)
	}                                       // 4-byte Folded Reload
	{
		r6 = memw(r30+##-23536)
		memw(r30+##-23304) = r0
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r1,#2048)
		r0 = sub(r3,r2)
		r5 = memw(r30+##-25048)
	}                                       // 4-byte Folded Reload
	{
		r4 = #0
		memw(r30+##-24656) = r4
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r30+##-23600)
		memw(r30+##-23872) = r0
	}                                       // 4-byte Folded Reload
	{
		r2 += add(r6,r5)
		memw(r30+##-23600) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r30+##-25040)
		memw(r30+##-22704) = r7
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-23592) = r4
		memw(r30+##-23608) = r2
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_42
		memw(r30+##-24648) = r3
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_41:                             // %"end for output.s0.y.yo"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r1 = memw(r30+##-23592)
		r6 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r0 = memw(r30+##-23648)
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-23608)
		memw(r30+##-23592) = r1
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		r1 = add(r5,r6)
		r4 = memw(r30+##-23600)
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-23864)
		memw(r30+##-23608) = r1
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r4,r6)
		memw(r30+##-23600) = r1.new
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt ##.LBB131_80
	}
.LBB131_42:                             // %"for output.s0.b.rebased"
                                        //   Parent Loop BB131_4 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_61 Depth 3
                                        //         Child Loop BB131_62 Depth 4
                                        //       Child Loop BB131_48 Depth 3
                                        //         Child Loop BB131_49 Depth 4
                                        //       Child Loop BB131_67 Depth 3
                                        //         Child Loop BB131_71 Depth 4
                                        //           Child Loop BB131_75 Depth 5
                                        //             Child Loop BB131_76 Depth 6
	{
		r0 = memw(r30+#-3896)
		r17 = memw(r30+##-23880)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27404)
	}
	{
		r1 = memw(r0+#4)
	}
	{
		p0 = cmp.gtu(r17,r1); if (p0.new) jump:nt .LBB131_50
		r2 = memw(r0+#0)
	}
// %bb.43:                              //   in Loop: Header=BB131_42 Depth=2
	{
		r20 = #68
		p0 = cmp.eq(r2,#0); if (p0.new) jump:nt .LBB131_57
	}
.LBB131_44:                             // %"produce resampled_input"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-23656)
		memw(r30+##-22456) = r2
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_58
	}
.LBB131_45:                             // %then_bb7
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-24696)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_64
	}                                       // 4-byte Folded Reload
// %bb.46:                              // %"for resampled_input.s0.y.rebased.us.preheader"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-24688)
		r1 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		r1 = #0
		r7 = memw(r30+##-23600)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_48
	}
	.p2align	4
.LBB131_47:                             //   in Loop: Header=BB131_48 Depth=3
	{
		v0 = valign(v1,v0,r4)
		r5 = memw(r30+##-23504)
		vmem(r2++#1) = v0.new
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r5)
		r2 = memw(r30+##-23392)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r2)
		r1 = add(r1,#1)
		r4 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r4)
		if (p0) jump:nt .LBB131_64
	}
.LBB131_48:                             // %"for resampled_input.s0.x.rebased.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_49 Depth 4
	{
		r2 = memw(r30+##-23496)
		v0 = vmem(r7+#0)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r2,#-1)
		p0 = cmp.gtu(r2,#1)
		r4 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		loop0(.LBB131_49,r3)
		r6 = add(r7,r4)
		r3:2 = combine(r4,r0)
		v1 = vmem(r7+#1)
	}
	{
		r5:4 = combine(r7,r7)
		if (!p0) jump:nt .LBB131_47
	}
	.p2align	4
.LBB131_49:                             // %"for resampled_input.s0.x.rebased.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        //       Parent Loop BB131_48 Depth=3
                                        // =>      This Inner Loop Header: Depth=4
	{
		r4 = r6
		v0 = valign(v1,v0,r5)
		v1 = vmem(r6+#0)
		vmem(r2++#1) = v0.new
	}
	{
		r6 = add(r6,r3)
		r5 = r4
		v0 = v1
		v1 = vmem(r4+#1)
	} :endloop0
	{
		jump .LBB131_47
	}
.LBB131_50:                             // %if.then.i1943
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		p0 = cmp.eq(r2,#0); if (p0.new) jump:nt .LBB131_53
		r0 = memw(r0+#8)
	}
// %bb.51:                              // %if.then.i1943
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r20 = #68
		r1 = #16384
		if (!cmp.gtu(r0,r1.new)) jump:nt .LBB131_54
	}
// %bb.52:                              // %if.then3.i1947
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		call ##halide_free
		r1:0 = combine(r2,#0)
	}
	{
		r0 = memw(r30+##-24664)
	}                                       // 4-byte Folded Reload
	{
		p3 = r0
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27404)
	}
	{
		jump .LBB131_54
		r0 = memw(r0+#8)
	}
.LBB131_53:                             //   in Loop: Header=BB131_42 Depth=2
	{
		r20 = #68
	}
.LBB131_54:                             // %if.end.i1951
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r4 = #16384
		r17 = memw(r30+##-23880)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r1,#-27404)
		r2 = #0
		r0 = add(r0,r17)
	}
	{
		p0 = cmp.gtu(r0,r4); if (!p0.new) jump:t .LBB131_56
		memw(r16+#8) = r0
	}
// %bb.55:                              // %if.then8.i1953
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		call ##halide_malloc
		r1:0 = combine(r17,#0)
	}
	{
		r2 = r0
		r1 = memw(r30+##-24664)
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-23568)
		r4 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
		p2 = r5
	}
	{
		p1 = r4
	}
.LBB131_56:                             // %if.end11.i1955
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		p0 = cmp.eq(r2,#0); if (!p0.new) jump:t .LBB131_44
		memw(r16+#0) = r2
		memw(r16+#4) = r17
	}
.LBB131_57:                             // %then_bb5
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r17 = add(#7,asl(r17,#2))
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r17,#-8)
		r1 = add(r1,#-27404)
	}
	{
		r2 = sub(r29,r0)
		r29 = sub(r29,r0)
	}
	{
		r2 = and(r2,#-128)
		r29 = and(r29,#-128)
		memw(r1+#0) = r2.new
	}
	{
		r0 = memw(r30+##-23656)
		memw(r30+##-22456) = r2
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (p0.new) jump:t .LBB131_45
	}
	.p2align	4
.LBB131_58:                             // %next_bb8
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-23392)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_64
	}                                       // 4-byte Folded Reload
// %bb.59:                              // %next_bb8
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-24680)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_64
	}                                       // 4-byte Folded Reload
// %bb.60:                              // %"for resampled_input.s0.y.rebased9.preheader.split.us"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		v7:6.w = vsub(v7:6.w,v7:6.w)
		r0 = memw(r30+##-24632)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-24648)
		v0 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v2 = vsplat(r0)
		r0 = add(r30,#-24624)
	}
	{
		v3 = v2
		v5 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v4 = v5
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r17 = add(r0,##-35840)
		r19 = add(r0,##-35584)
	}
	{
		r1 = setbit(r17,#7)
		r0 = memw(r30+##-24640)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-5424)
		v1 = vmem(r0+#0)
	}
	{
		v1:0.w = vsub(v7:6.w,v1:0.w)
	}
	{
		r0 = add(r30,#-5296)
		vmemu(r0+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-24368)
		v1:0.w = vsub(v3:2.w,v1:0.w)
		vmemu(r0+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-24240)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = setbit(r19,#7)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3:2.w = vsub(v5:4.w,v3:2.w)
		vmem(r0+#0) = v3.new
	}
	{
		vmem(r1+#0) = v1
	}
	{
		vmem(r19+#0) = v2
	}
	{
		r0 = memw(r17+#252)
		vmem(r17+#0) = v0
	}
	{
		memw(r30+#-1584) = r0
		r0 = memw(r19+#252)

	} :mem_noshuf
	{
		memw(r30+#-560) = r0
		r0 = memw(r19+#248)

	} :mem_noshuf
	{
		memw(r30+#-816) = r0
		r0 = memw(r17+#248)

	} :mem_noshuf
	{
		memw(r30+#-2096) = r0
		r0 = memw(r19+#244)

	} :mem_noshuf
	{
		memw(r30+#-1072) = r0
		r0 = memw(r17+#244)

	} :mem_noshuf
	{
		memw(r30+#-2352) = r0
		r0 = memw(r19+#240)

	} :mem_noshuf
	{
		memw(r30+#-1328) = r0
		r0 = memw(r17+#240)

	} :mem_noshuf
	{
		memw(r30+#-3120) = r0
		r0 = memw(r19+#236)

	} :mem_noshuf
	{
		memw(r30+#-1840) = r0
		r0 = memw(r17+#236)

	} :mem_noshuf
	{
		memw(r30+#-3632) = r0
		r0 = memw(r19+#232)

	} :mem_noshuf
	{
		memw(r30+#-2608) = r0
		r0 = memw(r17+#232)

	} :mem_noshuf
	{
		memw(r30+#-3888) = r0
		r0 = memw(r19+#228)

	} :mem_noshuf
	{
		memw(r30+#-2864) = r0
		r0 = memw(r17+#228)

	} :mem_noshuf
	{
		memw(r30+##-4400) = r0
		r0 = memw(r19+#224)

	} :mem_noshuf
	{
		memw(r30+#-3376) = r0
		r0 = memw(r17+#224)

	} :mem_noshuf
	{
		memw(r30+##-4656) = r0
		r21 = memw(r19+#220)

	} :mem_noshuf
	{
		memw(r30+##-10288) = r21
		r0 = memw(r17+#220)

	} :mem_noshuf
	{
		memw(r30+##-5168) = r0
		r22 = memw(r19+#216)

	} :mem_noshuf
	{
		memw(r30+##-10800) = r22
		r0 = memw(r17+#216)

	} :mem_noshuf
	{
		memw(r30+##-12592) = r0
		r25 = memw(r19+#212)

	} :mem_noshuf
	{
		memw(r30+##-10928) = r25
		r0 = memw(r17+#212)

	} :mem_noshuf
	{
		memw(r30+##-12848) = r0
		r26 = memw(r19+#208)

	} :mem_noshuf
	{
		memw(r30+##-11056) = r26
		r0 = memw(r17+#208)

	} :mem_noshuf
	{
		memw(r30+##-12976) = r0
		r16 = memw(r19+#204)

	} :mem_noshuf
	{
		memw(r30+##-11184) = r16
		r0 = memw(r17+#204)

	} :mem_noshuf
	{
		memw(r30+##-13104) = r0
		r0 = memw(r17+#132)

	} :mem_noshuf
	{
		r24 = memw(r19+#200)
		memw(r30+##-11568) = r24.new
	}
	{
		r1 = memw(r17+#200)
		memw(r30+##-13232) = r1.new
	}
	{
		r27 = memw(r19+#192)
		memw(r30+##-12336) = r27.new
	}
	{
		r1 = memw(r19+#132)
		memw(r30+##-13616) = r1.new
	}
	{
		r2 = memw(r17+#192)
		memw(r30+##-13360) = r2.new
	}
	{
		r23 = memw(r19+#196)
		memw(r30+##-11824) = r23.new
	}
	{
		r2 = memw(r17+#196)
		memw(r30+##-13744) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		memw(r30+##-6960) = r0
		r1 = memw(r19+#128)

	} :mem_noshuf
	{
		memw(r30+##-5680) = r1
		r0 = memw(r17+#128)

	} :mem_noshuf
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-7216)
		v0 = vxor(v0,v0)
	}
	{
		v0.w = vinsert(r0)
		vmemu(r1+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r30+##-6960)
		r1 = memw(r19+#136)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		memw(r30+##-6960) = r1
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#136)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7216)
		r2 = add(r30,#-7472)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#140)
		memw(r30+##-7216) = r1.new
	}
	{
		r0 = memw(r17+#140)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7472)
		r2 = add(r30,#-7728)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#144)
		memw(r30+##-7472) = r1.new
	}
	{
		r0 = memw(r17+#144)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7728)
		r2 = add(r30,#-7984)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#148)
		memw(r30+##-7728) = r1.new
	}
	{
		r0 = memw(r17+#148)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7984)
		r2 = add(r30,#-8240)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#152)
		memw(r30+##-7984) = r1.new
	}
	{
		r0 = memw(r17+#152)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8240)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#156)
		memw(r30+##-8240) = r1.new
	}
	{
		r0 = memw(r17+#156)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8752)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#160)
		memw(r30+##-8496) = r1.new
	}
	{
		r0 = memw(r17+#160)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8752)
		r2 = add(r30,#-9008)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#164)
		memw(r30+##-8752) = r1.new
	}
	{
		r0 = memw(r17+#164)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9008)
		r2 = add(r30,#-9264)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#168)
		memw(r30+##-9008) = r1.new
	}
	{
		r0 = memw(r17+#168)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9264)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#172)
		memw(r30+##-9264) = r1.new
	}
	{
		r0 = memw(r17+#172)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9776)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#176)
		memw(r30+##-9520) = r1.new
	}
	{
		r0 = memw(r17+#176)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9776)
		r2 = add(r30,#-10032)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#180)
		memw(r30+##-9776) = r1.new
	}
	{
		r0 = memw(r17+#180)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10032)
		r2 = add(r30,#-12080)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#184)
		memw(r30+##-10032) = r1.new
	}
	{
		r0 = memw(r17+#184)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-12080)
		r2 = add(r30,#-14128)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#188)
		memw(r30+##-12080) = r1.new
	}
	{
		r0 = memw(r17+#188)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14128)
		r2 = add(r30,#-13744)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r20)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r23 = r0
		r1 = r27
		r0 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-13232)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r23)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13232)
		r2 = add(r30,#-13104)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13104)
		r2 = add(r30,#-12976)
	}
	{
		r1 = r26
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-12976)
		r2 = add(r30,#-12848)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-12848)
		r2 = add(r30,#-12592)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-12592)
		r2 = add(r30,#-5168)
	}
	{
		r1 = r21
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5168)
		r2 = add(r30,#-4656)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-4656)
		r1 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-4656)
		r2 = add(r30,#-4400)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-4400)
		r1 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-4400)
		r2 = add(r30,#-3888)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-3888)
		r1 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3888)
		r2 = add(r30,#-3632)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-3632)
		r1 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3632)
		r2 = add(r30,#-3120)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-3120)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3120)
		r2 = add(r30,#-2352)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-2352)
		r1 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2352)
		r2 = add(r30,#-2096)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-2096)
		r1 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2096)
		r2 = add(r30,#-1584)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-1584)
		r1 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1584)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r19+#124)
		memw(r30+#-1584) = r0.new
	}
	{
		r0 = memw(r17+#124)
		memw(r30+##-15024) = r0.new
	}
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r19+#120)
		memw(r30+#-2096) = r0.new
	}
	{
		r0 = memw(r17+#120)
		memw(r30+##-15280) = r0.new
	}
	{
		r0 = memw(r19+#116)
		memw(r30+#-2352) = r0.new
	}
	{
		r0 = memw(r17+#116)
		memw(r30+##-15792) = r0.new
	}
	{
		r0 = memw(r19+#112)
		memw(r30+#-3120) = r0.new
	}
	{
		r0 = memw(r17+#112)
		memw(r30+##-15920) = r0.new
	}
	{
		r0 = memw(r19+#108)
		memw(r30+#-3632) = r0.new
	}
	{
		r0 = memw(r17+#108)
		memw(r30+##-16048) = r0.new
	}
	{
		r0 = memw(r19+#104)
		memw(r30+#-3888) = r0.new
	}
	{
		r0 = memw(r17+#104)
		memw(r30+##-16176) = r0.new
	}
	{
		r0 = memw(r19+#100)
		memw(r30+##-4400) = r0.new
	}
	{
		r0 = memw(r17+#100)
		memw(r30+##-16304) = r0.new
	}
	{
		r0 = memw(r19+#96)
		memw(r30+##-4656) = r0.new
	}
	{
		r0 = memw(r17+#96)
		memw(r30+##-16432) = r0.new
	}
	{
		r0 = memw(r19+#92)
		memw(r30+##-5168) = r0.new
	}
	{
		r0 = memw(r17+#92)
		memw(r30+##-16560) = r0.new
	}
	{
		r16 = memw(r19+#88)
		r0 = memw(r17+#88)
	}
	{
		memw(r30+##-16688) = r0
		r21 = memw(r19+#84)

	} :mem_noshuf
	{
		r0 = memw(r17+#84)
		memw(r30+##-16816) = r0.new
	}
	{
		r0 = memw(r19+#80)
		memw(r30+##-16944) = r0.new
	}
	{
		r0 = memw(r17+#80)
		memw(r30+##-17072) = r0.new
	}
	{
		r22 = memw(r19+#76)
		r0 = memw(r17+#76)
	}
	{
		memw(r30+##-17200) = r0
		r0 = memw(r17+#72)

	} :mem_noshuf
	{
		r0 = add(r30,#-13744)
		r25 = memw(r19+#72)
		memw(r30+##-17328) = r0

	} :mem_noshuf
	{
		r0 = add(r30,#-15664)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vor(v1,v0)
	}
	{
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r17+#4)
		r26 = memw(r19+#64)
	}
	{
		r1 = memw(r17+#64)
		memw(r30+##-17456) = r1.new
	}
	{
		r23 = memw(r19+#68)
		r1 = memw(r17+#68)
	}
	{
		memw(r30+##-17584) = r1
		r1 = memw(r19+#4)

	} :mem_noshuf
	{
		r27 = memw(r19+#60)
		memw(r30+##-15152) = r1

	} :mem_noshuf
	{
		r2 = memw(r17+#60)
		memw(r30+##-17712) = r2.new
	}
	{
		r24 = memw(r19+#56)
		r2 = memw(r17+#56)
	}
	{
		call ##__hexagon_divsi3
		memw(r30+##-17840) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-12848) = r0
		r1 = memw(r19+#0)

	} :mem_noshuf
	{
		memw(r30+##-12592) = r1
		r0 = memw(r17+#0)

	} :mem_noshuf
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-12976)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-12848)
		r1 = memw(r19+#8)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-12848) = r1
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#8)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-12976)
		r2 = add(r30,#-13104)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#12)
		memw(r30+##-12976) = r1.new
	}
	{
		r0 = memw(r17+#12)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13104)
		r2 = add(r30,#-13232)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#16)
		memw(r30+##-13104) = r1.new
	}
	{
		r0 = memw(r17+#16)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13232)
		r2 = add(r30,#-13360)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#20)
		memw(r30+##-13232) = r1.new
	}
	{
		r0 = memw(r17+#20)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13360)
		r2 = add(r30,#-13744)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#24)
		memw(r30+##-13360) = r1.new
	}
	{
		r0 = memw(r17+#24)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13744)
		r2 = add(r30,#-14128)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#28)
		memw(r30+##-13744) = r1.new
	}
	{
		r0 = memw(r17+#28)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14128)
		r2 = add(r30,#-14256)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#32)
		memw(r30+##-14128) = r1.new
	}
	{
		r0 = memw(r17+#32)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14256)
		r2 = add(r30,#-14384)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#36)
		memw(r30+##-14256) = r1.new
	}
	{
		r0 = memw(r17+#36)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14384)
		r2 = add(r30,#-14512)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#40)
		memw(r30+##-14384) = r1.new
	}
	{
		r0 = memw(r17+#40)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14512)
		r2 = add(r30,#-14640)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#44)
		memw(r30+##-14512) = r1.new
	}
	{
		r0 = memw(r17+#44)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14640)
		r2 = add(r30,#-14896)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#48)
		memw(r30+##-14640) = r1.new
	}
	{
		r0 = memw(r17+#48)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14896)
		r2 = add(r30,#-17968)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r19+#52)
		memw(r30+##-14896) = r1.new
	}
	{
		r0 = memw(r17+#52)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17968)
		r2 = add(r30,#-17840)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17840)
		r2 = add(r30,#-17712)
	}
	{
		r1 = r27
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17712)
		r2 = add(r30,#-17584)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r20)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r17 = r0
		r1 = r26
		r0 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-17328)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r17)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17328)
		r2 = add(r30,#-17200)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17200)
		r2 = add(r30,#-17072)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r19 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r1 = r19
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17072)
		r2 = add(r30,#-16816)
	}
	{
		r1 = r21
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16816)
		r2 = add(r30,#-16688)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16688)
		r2 = add(r30,#-16560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16560)
		r2 = add(r30,#-16432)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16432)
		r2 = add(r30,#-16304)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16304)
		r2 = add(r30,#-16176)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16176)
		r1 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16176)
		r2 = add(r30,#-16048)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16048)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16048)
		r2 = add(r30,#-15920)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15920)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15920)
		r2 = add(r30,#-15792)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15792)
		r1 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15792)
		r2 = add(r30,#-15280)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15280)
		r1 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15280)
		r2 = add(r30,#-15024)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15024)
		r1 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15024)
		r2 = add(r30,#-17584)
		v3:2.w = vsub(v3:2.w,v3:2.w)
	}
	{
		v1 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.w = vinsert(r0)
		r0 = memw(r30+##-24672)
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r0)
		r0 = add(r30,#-24368)
		r1 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-24240)
		v9 = vand(v2,v0)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-15280)
		v6 = vnot(v4)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v8 = vand(v3,v0)
		v7 = vnot(v5)
		v0 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = valign(v1,v1,#4)
		r2 = memw(r30+##-24656)
	}                                       // 4-byte Folded Reload
	{
		v2 = vsplat(r2)
		r0 = add(r30,#-15024)
		vmemu(r0+#0) = v6
	}                                       // 128-byte Folded Spill
	{
		r2 = add(r30,#-15664)
		v0 = vor(v0,v1)
		vmemu(r0+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		r2 = add(r30,#-5424)
		v3 = v2
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v1:0.w = vadd(v9:8.w,v1:0.w)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r17 = add(r0,##-35328)
		r0 = add(r30,#-13616)
	}
	{
		r2 = add(r30,#-5296)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-13488)
		vmemu(r0+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = setbit(r17,#7)
		v3:2.w = vsub(v3:2.w,v5:4.w)
		vmemu(r0+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v4 = vand(v1,v6)
		v5 = vand(v0,v7)
		vmem(r0+#0) = v3
	}
	{
		r0 = memw(r17+#252)
		vmem(r17+#0) = v2
	}
	{
		memw(r30+##-15664) = r0
		r0 = memw(r17+#248)

	} :mem_noshuf
	{
		memw(r30+##-15792) = r0
		r0 = memw(r17+#244)

	} :mem_noshuf
	{
		memw(r30+##-15920) = r0
		r0 = memw(r17+#240)

	} :mem_noshuf
	{
		memw(r30+##-16048) = r0
		r0 = memw(r17+#236)

	} :mem_noshuf
	{
		memw(r30+##-16176) = r0
		r0 = memw(r17+#232)

	} :mem_noshuf
	{
		memw(r30+##-16304) = r0
		r0 = memw(r17+#228)

	} :mem_noshuf
	{
		memw(r30+##-16432) = r0
		r0 = memw(r17+#224)

	} :mem_noshuf
	{
		memw(r30+##-16560) = r0
		r0 = memw(r17+#220)

	} :mem_noshuf
	{
		memw(r30+##-16688) = r0
		r0 = memw(r17+#216)

	} :mem_noshuf
	{
		memw(r30+##-16816) = r0
		r0 = memw(r17+#212)

	} :mem_noshuf
	{
		memw(r30+##-17072) = r0
		r0 = memw(r17+#208)

	} :mem_noshuf
	{
		memw(r30+##-17200) = r0
		r0 = memw(r17+#204)

	} :mem_noshuf
	{
		memw(r30+##-17328) = r0
		r2 = memw(r17+#200)

	} :mem_noshuf
	{
		r2 = add(r30,#-5424)
		r0 = memw(r17+#132)
		memw(r30+##-17456) = r2

	} :mem_noshuf
	{
		r2 = add(r30,#-5296)
		vmemu(r2+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		r2 = memw(r17+#192)
		memw(r30+##-17584) = r2.new
	}
	{
		r2 = memw(r17+#196)
		memw(r30+##-17712) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = memw(r30+##-5680)
		memw(r30+##-17840) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r0 = memw(r17+#128)
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#136)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#140)
		r1 = memw(r30+##-7216)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#144)
		r1 = memw(r30+##-7472)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#148)
		r1 = memw(r30+##-7728)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#152)
		r1 = memw(r30+##-7984)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#156)
		r1 = memw(r30+##-8240)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#160)
		r1 = memw(r30+##-8496)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#164)
		r1 = memw(r30+##-8752)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#168)
		r1 = memw(r30+##-9008)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#172)
		r1 = memw(r30+##-9264)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#176)
		r1 = memw(r30+##-9520)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#180)
		r1 = memw(r30+##-9776)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#184)
		r1 = memw(r30+##-10032)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#188)
		r1 = memw(r30+##-12080)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-8752)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r20)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r30+##-17584)
		memw(r30+##-5680) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r1 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-3376)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16560)
		r1 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3376)
		r2 = add(r30,#-2864)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16432)
		r1 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2864)
		r2 = add(r30,#-2608)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16304)
		r1 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2608)
		r2 = add(r30,#-1840)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16176)
		r1 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1840)
		r2 = add(r30,#-1328)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16048)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1328)
		r2 = add(r30,#-1072)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15920)
		r1 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1072)
		r2 = add(r30,#-816)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15792)
		r1 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-816)
		r2 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15664)
		r1 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#124)
		memw(r30+#-560) = r0.new
	}
	{
		r0 = memw(r17+#120)
		memw(r30+#-816) = r0.new
	}
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r17+#116)
		memw(r30+#-1328) = r0.new
	}
	{
		r0 = memw(r17+#112)
		memw(r30+#-1840) = r0.new
	}
	{
		r0 = memw(r17+#108)
		memw(r30+#-2608) = r0.new
	}
	{
		r0 = memw(r17+#104)
		memw(r30+#-2864) = r0.new
	}
	{
		r0 = memw(r17+#100)
		memw(r30+#-3376) = r0.new
	}
	{
		r0 = memw(r17+#96)
		memw(r30+##-5680) = r0.new
	}
	{
		r0 = memw(r17+#92)
		memw(r30+##-6960) = r0.new
	}
	{
		r0 = memw(r17+#88)
		memw(r30+##-7216) = r0.new
	}
	{
		r0 = memw(r17+#84)
		memw(r30+##-7472) = r0.new
	}
	{
		r0 = memw(r17+#80)
		memw(r30+##-7728) = r0.new
	}
	{
		r0 = memw(r17+#76)
		memw(r30+##-7984) = r0.new
	}
	{
		r0 = memw(r17+#72)
		memw(r30+##-8240) = r0.new
	}
	{
		r0 = memw(r17+#64)
		memw(r30+##-8496) = r0.new
	}
	{
		r0 = memw(r17+#4)
		r1 = memw(r30+##-15152)
	}
	{
		r2 = memw(r17+#68)
		memw(r30+##-9008) = r2.new
	}
	{
		r2 = add(r30,#-8752)
	}
	{
		r2 = add(r30,#-1072)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vor(v1,v0)
	}
	{
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r2 = memw(r17+#60)
		memw(r30+##-8752) = r2.new
	}
	{
		r2 = memw(r17+#56)
		memw(r30+##-9264) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = memw(r30+##-12592)
		memw(r30+##-9520) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r0 = memw(r17+#0)
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#8)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#12)
		r1 = memw(r30+##-12976)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#16)
		r1 = memw(r30+##-13104)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#20)
		r1 = memw(r30+##-13232)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#24)
		r1 = memw(r30+##-13360)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#28)
		r1 = memw(r30+##-13744)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#32)
		r1 = memw(r30+##-14128)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#36)
		r1 = memw(r30+##-14256)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#40)
		r1 = memw(r30+##-14384)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#44)
		r1 = memw(r30+##-14512)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#48)
		r1 = memw(r30+##-14640)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r17+#52)
		r1 = memw(r30+##-14896)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9264)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9264)
		r2 = add(r30,#-8752)
	}
	{
		r1 = r27
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8752)
		r2 = add(r30,#-8752)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r20)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r17 = r0
		r1 = r26
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-8240)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r17)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8240)
		r2 = add(r30,#-7984)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7984)
		r2 = add(r30,#-7728)
	}
	{
		r1 = r19
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7728)
		r2 = add(r30,#-7472)
	}
	{
		r1 = r21
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7472)
		r2 = add(r30,#-7216)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7216)
		r2 = add(r30,#-5168)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5168)
		r2 = add(r30,#-4656)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-4656)
		r2 = add(r30,#-3376)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-3376)
		r1 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3376)
		r2 = add(r30,#-2864)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-2864)
		r1 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2864)
		r2 = add(r30,#-2608)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-2608)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-2608)
		r2 = add(r30,#-1840)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-1840)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1840)
		r2 = add(r30,#-1328)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-1328)
		r1 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1328)
		r2 = add(r30,#-816)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-816)
		r1 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-816)
		r2 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-560)
		r1 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5424)
		r2 = #0
	}
	{
		r1 = add(r30,#-5296)
		v10 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-304)
		v11 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = memw(r30+##-24664)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
		r1 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		r1 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
		r1 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = add(r30,#-8752)
		r1 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-1072)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r0 = add(r30,#-13616)
		v0 = vor(v1,v0)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-13488)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-15024)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-15280)
		v3:2.w = vadd(v3:2.w,v1:0.w)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = vand(v2,v0)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vand(v3,v0)
		r0 = memw(r30+##-24704)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r1,r0)
		r1 = memw(r30+##-23608)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_61:                             // %"for resampled_input.s0.y.rebased9.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_62 Depth 4
	{
		r7 = r19
		r3 = memw(r30+##-23496)
		r11 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = #64
		r16 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		loop0(.LBB131_62,r3)
		r3 = r1
		r17 = memw(r30+##-23536)
	}                                       // 4-byte Folded Reload
	.p2align	4
.Ltmp21:                                // Block address taken
.LBB131_62:                             // %"for resampled_input.s0.x.rebased12.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        //       Parent Loop BB131_61 Depth=3
                                        // =>      This Inner Loop Header: Depth=4
	{
		v4 = vsplat(r3)
		r8 = add(r11,##-36096)
		v6 = v7
	}
	{
		r9 = setbit(r8,#7)
		r5 = add(r7,#-64)
		v2 = vror(v7,r0)
		v5 = v4
	}
	{
		r4 = add(r11,##-36352)
		q0 = vsetq(r0)
		v9:8.w = vadd(v5:4.w,v11:10.w)
	}
	{
		r6 = setbit(r4,#7)
		r3 = add(r3,r16)
		v5:4.w = vadd(v5:4.w,v1:0.w)
		vmem(r8+#0) = v8
	}
	{
		v3 = v7
		r12 = memw(r8+#0)
		vmem(r9+#0) = v9
	}
	{
		r14 = memw(r8+#8)
		r13 = memw(r8+#4)
	}
	{
		r27 = memw(r8+#12)
		r15 = memw(r8+#16)
	}
	{
		r23 = memw(r8+#24)
		r12 = memub(r17+r12<<#0)
	}
	{
		r13 = memub(r17+r13<<#0)
		r14 = memub(r17+r14<<#0)
	}
	{
		r12 |= asl(r13,#8)
		r9 = memub(r17+r27<<#0)
		r22 = memw(r8+#20)
	}
	{
		r14 |= asl(r9,#8)
		r28 = memw(r8+#28)
		r24 = memw(r8+#32)
	}
	{
		r12 = combine(r14.l,r12.l)
		r13 = memub(r17+r22<<#0)
		r15 = memub(r17+r15<<#0)
	}
	{
		r15 |= asl(r13,#8)
		v6.w = vinsert(r12)
		r26 = memub(r17+r28<<#0)
		r28 = memw(r8+#40)
	}
	{
		r9 = memub(r17+r23<<#0)
		r25 = memw(r8+#36)
	}
	{
		r9 |= asl(r26,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#44)
		r14 = memub(r17+r24<<#0)
	}
	{
		r9 = combine(r9.l,r15.l)
		r22 = memw(r8+#56)
		r12 = memub(r17+r25<<#0)
	}
	{
		r14 |= asl(r12,#8)
		v6.w = vinsert(r9)
		r24 = memw(r8+#48)
		r28 = memub(r17+r28<<#0)
	}
	{
		r13 = memub(r17+r27<<#0)
		r23 = memw(r8+#60)
	}
	{
		r28 |= asl(r13,#8)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#52)
		r15 = memub(r17+r22<<#0)
	}
	{
		r14 = combine(r28.l,r14.l)
		r28 = memw(r8+#64)
		r9 = memub(r17+r23<<#0)
	}
	{
		r15 |= asl(r9,#8)
		v6.w = vinsert(r14)
		r22 = memw(r8+#72)
		r12 = memub(r17+r24<<#0)
	}
	{
		r27 = memub(r17+r25<<#0)
		r26 = memw(r8+#68)
	}
	{
		r12 |= asl(r27,#8)
		v6 = valign(v6,v6,#4)
		r23 = memw(r8+#76)
		r24 = memw(r8+#80)
	}
	{
		r12 = combine(r15.l,r12.l)
		r14 = memub(r17+r26<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r28 |= asl(r14,#8)
		v6.w = vinsert(r12)
		r9 = memub(r17+r23<<#0)
		r13 = memub(r17+r22<<#0)
	}
	{
		r13 |= asl(r9,#8)
		r26 = memw(r8+#88)
		r25 = memw(r8+#84)
	}
	{
		r13 = combine(r13.l,r28.l)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#92)
		r28 = memw(r8+#96)
	}
	{
		v6.w = vinsert(r13)
		r12 = memub(r17+r25<<#0)
		r15 = memub(r17+r24<<#0)
	}
	{
		r15 |= asl(r12,#8)
		r14 = memub(r17+r26<<#0)
		r23 = memw(r8+#104)
	}
	{
		v6 = valign(v6,v6,#4)
		r9 = memub(r17+r27<<#0)
		r22 = memw(r8+#100)
	}
	{
		r14 |= asl(r9,#8)
		r24 = memw(r8+#108)
		r26 = memw(r8+#112)
	}
	{
		r14 = combine(r14.l,r15.l)
		r25 = memw(r8+#124)
		r13 = memub(r17+r22<<#0)
	}
	{
		v6.w = vinsert(r14)
		r28 = memub(r17+r28<<#0)
		r9 = memub(r17+r24<<#0)
	}
	{
		r28 |= asl(r13,#8)
		r12 = memub(r17+r23<<#0)
		r27 = memw(r8+#116)
	}
	{
		r12 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r22 = memw(r8+#120)
		r13 = memub(r17+r26<<#0)
	}
	{
		r12 = combine(r12.l,r28.l)
		r28 = memw(r8+#128)
		r14 = memub(r17+r27<<#0)
	}
	{
		r13 |= asl(r14,#8)
		v6.w = vinsert(r12)
		r9 = memub(r17+r22<<#0)
		r15 = memub(r17+r25<<#0)
	}
	{
		r9 |= asl(r15,#8)
		r24 = memw(r8+#136)
		r23 = memw(r8+#132)
	}
	{
		r9 = combine(r9.l,r13.l)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#140)
		r26 = memw(r8+#144)
	}
	{
		v6.w = vinsert(r9)
		r28 = memub(r17+r28<<#0)
		r12 = memub(r17+r23<<#0)
	}
	{
		r28 |= asl(r12,#8)
		r22 = memw(r8+#152)
		r15 = memub(r17+r24<<#0)
	}
	{
		v6 = valign(v6,v6,#4)
		r14 = memub(r17+r25<<#0)
		r27 = memw(r8+#148)
	}
	{
		r15 |= asl(r14,#8)
		r23 = memw(r8+#156)
		r13 = memub(r17+r26<<#0)
	}
	{
		r15 = combine(r15.l,r28.l)
		r28 = memw(r8+#160)
		r9 = memub(r17+r27<<#0)
	}
	{
		r13 |= asl(r9,#8)
		v6.w = vinsert(r15)
		r26 = memw(r8+#168)
		r12 = memub(r17+r22<<#0)
	}
	{
		r25 = memub(r17+r23<<#0)
		r24 = memw(r8+#164)
	}
	{
		r12 |= asl(r25,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#172)
		r22 = memw(r8+#176)
	}
	{
		r12 = combine(r12.l,r13.l)
		r15 = memub(r17+r24<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r28 |= asl(r15,#8)
		v6.w = vinsert(r12)
		r9 = memub(r17+r27<<#0)
		r14 = memub(r17+r26<<#0)
	}
	{
		r14 |= asl(r9,#8)
		r24 = memw(r8+#184)
		r23 = memw(r8+#180)
	}
	{
		r14 = combine(r14.l,r28.l)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#188)
		r28 = memw(r8+#192)
	}
	{
		v6.w = vinsert(r14)
		r12 = memub(r17+r23<<#0)
		r13 = memub(r17+r22<<#0)
	}
	{
		r13 |= asl(r12,#8)
		r9 = memub(r17+r25<<#0)
		r15 = memub(r17+r24<<#0)
	}
	{
		r15 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#200)
		r26 = memw(r8+#196)
	}
	{
		r13 = combine(r15.l,r13.l)
		r22 = memw(r8+#204)
		r23 = memw(r8+#208)
	}
	{
		v6.w = vinsert(r13)
		r28 = memub(r17+r28<<#0)
		r14 = memub(r17+r26<<#0)
	}
	{
		r28 |= asl(r14,#8)
		r9 = memub(r17+r22<<#0)
		r12 = memub(r17+r27<<#0)
	}
	{
		r12 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#216)
		r24 = memw(r8+#212)
	}
	{
		r12 = combine(r12.l,r28.l)
		r26 = memw(r8+#220)
		r28 = memw(r8+#224)
	}
	{
		v6.w = vinsert(r12)
		r13 = memub(r17+r24<<#0)
		r15 = memub(r17+r23<<#0)
	}
	{
		r15 |= asl(r13,#8)
		r9 = memub(r17+r26<<#0)
		r14 = memub(r17+r25<<#0)
	}
	{
		r14 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r22 = memw(r8+#232)
		r27 = memw(r8+#228)
	}
	{
		r14 = combine(r14.l,r15.l)
		r23 = memw(r8+#236)
		r26 = memw(r8+#248)
	}
	{
		v6.w = vinsert(r14)
		r24 = memw(r8+#240)
		r12 = memub(r17+r27<<#0)
	}
	{
		r28 = memub(r17+r28<<#0)
		r13 = memub(r17+r22<<#0)
	}
	{
		r28 |= asl(r12,#8)
		v6 = valign(v6,v6,#4)
		r9 = memub(r17+r23<<#0)
		r25 = memw(r8+#244)
	}
	{
		r13 |= asl(r9,#8)
		r10 = memw(r8+#252)
		r27 = memub(r17+r24<<#0)
	}
	{
		r9 = combine(r13.l,r28.l)
		r12 = memub(r17+r25<<#0)
		r15 = memub(r17+r26<<#0)
	}
	{
		r27 |= asl(r12,#8)
		v6.w = vinsert(r9)
		r22 = memub(r17+r10<<#0)
	}
	{
		r15 |= asl(r22,#8)
	}
	{
		r8 = combine(r15.l,r27.l)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r8)
	}
	{
		v6 = vror(v6,r20)
	}
	{
		v6 = vor(v6,v2)
	}
	{
		if (q0) vmem(r5+#0) = v6
	}
	{
		v4 = vand(q0,r18)
		vmem(r4+#0) = v4
	}
	{
		r23 = memw(r4+#8)
		vmem(r6+#0) = v5
	}
	{
		r5 = memw(r4+#0)
		r24 = memw(r4+#12)
	}
	{
		r6 = memw(r4+#4)
		r25 = memw(r4+#16)
	}
	{
		r27 = memw(r4+#24)
		r8 = memub(r17+r23<<#0)
	}
	{
		r9 = memub(r17+r24<<#0)
		r5 = memub(r17+r5<<#0)
	}
	{
		r8 |= asl(r9,#8)
		r6 = memub(r17+r6<<#0)
		r26 = memw(r4+#20)
	}
	{
		r5 |= asl(r6,#8)
		r22 = memw(r4+#28)
		r6 = memw(r4+#32)
	}
	{
		r5 = combine(r8.l,r5.l)
		r24 = memub(r17+r26<<#0)
		r12 = memub(r17+r25<<#0)
	}
	{
		r12 |= asl(r24,#8)
		v3.w = vinsert(r5)
		r23 = memw(r4+#40)
		r5 = memub(r17+r27<<#0)
	}
	{
		r26 = memub(r17+r22<<#0)
		r28 = memw(r4+#36)
	}
	{
		r5 |= asl(r26,#8)
		v3 = valign(v3,v3,#4)
		r25 = memw(r4+#44)
		r27 = memw(r4+#48)
	}
	{
		r5 = combine(r5.l,r12.l)
		r14 = memw(r4+#56)
		r22 = memub(r17+r28<<#0)
	}
	{
		v3.w = vinsert(r5)
		r6 = memub(r17+r6<<#0)
		r5 = memub(r17+r23<<#0)
	}
	{
		r6 |= asl(r22,#8)
		r23 = memub(r17+r25<<#0)
		r28 = memw(r4+#52)
	}
	{
		r5 |= asl(r23,#8)
		v3 = valign(v3,v3,#4)
		r15 = memw(r4+#60)
		r8 = memub(r17+r27<<#0)
	}
	{
		r5 = combine(r5.l,r6.l)
		r6 = memub(r17+r28<<#0)
		r24 = memw(r4+#64)
	}
	{
		r8 |= asl(r6,#8)
		v3.w = vinsert(r5)
		r26 = memw(r4+#72)
		r5 = memub(r17+r14<<#0)
	}
	{
		r6 = memub(r17+r15<<#0)
		r25 = memw(r4+#68)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#76)
		r14 = memw(r4+#80)
	}
	{
		r5 = combine(r5.l,r8.l)
		r6 = memw(r4+#88)
		r12 = memub(r17+r24<<#0)
	}
	{
		v3.w = vinsert(r5)
		r22 = memub(r17+r25<<#0)
		r5 = memub(r17+r26<<#0)
	}
	{
		r12 |= asl(r22,#8)
		r24 = memub(r17+r28<<#0)
		r27 = memw(r4+#84)
	}
	{
		r5 |= asl(r24,#8)
		v3 = valign(v3,v3,#4)
		r23 = memw(r4+#92)
		r25 = memw(r4+#96)
	}
	{
		r5 = combine(r5.l,r12.l)
		r26 = memw(r4+#104)
		r14 = memub(r17+r14<<#0)
	}
	{
		v3.w = vinsert(r5)
		r27 = memub(r17+r27<<#0)
		r5 = memub(r17+r6<<#0)
	}
	{
		r14 |= asl(r27,#8)
		r6 = memub(r17+r23<<#0)
		r28 = memw(r4+#100)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r22 = memw(r4+#108)
		r23 = memw(r4+#112)
	}
	{
		r5 = combine(r5.l,r14.l)
		r6 = memw(r4+#116)
		r9 = memub(r17+r25<<#0)
	}
	{
		v3.w = vinsert(r5)
		r24 = memub(r17+r28<<#0)
		r12 = memw(r4+#124)
	}
	{
		r9 |= asl(r24,#8)
		r5 = memub(r17+r26<<#0)
		r25 = memub(r17+r22<<#0)
	}
	{
		r5 |= asl(r25,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#120)
		r13 = memub(r17+r23<<#0)
	}
	{
		r5 = combine(r5.l,r9.l)
		r6 = memub(r17+r6<<#0)
		r26 = memw(r4+#128)
	}
	{
		r13 |= asl(r6,#8)
		v3.w = vinsert(r5)
		r22 = memw(r4+#136)
		r6 = memub(r17+r12<<#0)
	}
	{
		r5 = memub(r17+r28<<#0)
		r27 = memw(r4+#132)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r23 = memw(r4+#140)
		r12 = memw(r4+#144)
	}
	{
		r5 = combine(r5.l,r13.l)
		r6 = memw(r4+#152)
		r14 = memub(r17+r26<<#0)
	}
	{
		v3.w = vinsert(r5)
		r24 = memub(r17+r27<<#0)
		r26 = memub(r17+r23<<#0)
	}
	{
		r14 |= asl(r24,#8)
		r5 = memub(r17+r22<<#0)
		r28 = memw(r4+#148)
	}
	{
		r5 |= asl(r26,#8)
		v3 = valign(v3,v3,#4)
		r25 = memw(r4+#156)
		r27 = memw(r4+#160)
	}
	{
		r5 = combine(r5.l,r14.l)
		r23 = memw(r4+#168)
		r24 = memub(r17+r28<<#0)
	}
	{
		v3.w = vinsert(r5)
		r12 = memub(r17+r12<<#0)
		r5 = memub(r17+r6<<#0)
	}
	{
		r12 |= asl(r24,#8)
		r6 = memub(r17+r25<<#0)
		r22 = memw(r4+#164)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#172)
		r14 = memw(r4+#176)
	}
	{
		r5 = combine(r5.l,r12.l)
		r26 = memub(r17+r22<<#0)
		r9 = memub(r17+r27<<#0)
	}
	{
		r9 |= asl(r26,#8)
		v3.w = vinsert(r5)
		r6 = memw(r4+#184)
		r5 = memub(r17+r23<<#0)
	}
	{
		r22 = memub(r17+r28<<#0)
		r25 = memw(r4+#180)
	}
	{
		r5 |= asl(r22,#8)
		v3 = valign(v3,v3,#4)
		r27 = memw(r4+#188)
		r23 = memw(r4+#192)
	}
	{
		r5 = combine(r5.l,r9.l)
		r24 = memw(r4+#200)
		r14 = memub(r17+r14<<#0)
	}
	{
		v3.w = vinsert(r5)
		r25 = memub(r17+r25<<#0)
		r5 = memub(r17+r6<<#0)
	}
	{
		r14 |= asl(r25,#8)
		r6 = memub(r17+r27<<#0)
		r28 = memw(r4+#196)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r26 = memw(r4+#204)
		r9 = memw(r4+#208)
	}
	{
		r5 = combine(r5.l,r14.l)
		r6 = memw(r4+#216)
		r12 = memub(r17+r23<<#0)
	}
	{
		v3.w = vinsert(r5)
		r22 = memub(r17+r28<<#0)
		r5 = memub(r17+r24<<#0)
	}
	{
		r12 |= asl(r22,#8)
		r23 = memub(r17+r26<<#0)
		r27 = memw(r4+#212)
	}
	{
		r5 |= asl(r23,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#220)
		r24 = memw(r4+#224)
	}
	{
		r5 = combine(r5.l,r12.l)
		r26 = memw(r4+#232)
		r9 = memub(r17+r9<<#0)
	}
	{
		v3.w = vinsert(r5)
		r27 = memub(r17+r27<<#0)
		r5 = memub(r17+r28<<#0)
	}
	{
		r9 |= asl(r27,#8)
		r6 = memub(r17+r6<<#0)
		r25 = memw(r4+#228)
	}
	{
		r6 |= asl(r5,#8)
		v3 = valign(v3,v3,#4)
		r22 = memw(r4+#236)
		r5 = memw(r4+#248)
	}
	{
		r6 = combine(r6.l,r9.l)
		r28 = memw(r4+#240)
		r23 = memub(r17+r24<<#0)
	}
	{
		v3.w = vinsert(r6)
		r24 = memub(r17+r25<<#0)
		r25 = memub(r17+r26<<#0)
	}
	{
		r23 |= asl(r24,#8)
		r26 = memub(r17+r22<<#0)
		r10 = memw(r4+#244)
	}
	{
		r25 |= asl(r26,#8)
		v3 = valign(v3,v3,#4)
		r4 = memw(r4+#252)
		r6 = memub(r17+r28<<#0)
	}
	{
		r9 = combine(r25.l,r23.l)
		r27 = memub(r17+r10<<#0)
		r5 = memub(r17+r5<<#0)
	}
	{
		r6 |= asl(r27,#8)
		v3.w = vinsert(r9)
		r4 = memub(r17+r4<<#0)
	}
	{
		r5 |= asl(r4,#8)
	}
	{
		r4 = combine(r5.l,r6.l)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r4)
	}
	{
		v3 = vror(v3,r20)
	}
	{
		v3 = vlalign(v7,v4,r7)
		v2 = vor(v3,v2)
	}
	{
		q2 = vand(v3,r18)
		v3 = vlalign(v4,v7,r7)
	}
	{
		q3 = vand(v3,r18)
		v5 = vlalign(v7,v2,r7)
	}
	{
		v2 = vlalign(v2,v7,r7)
		if (q2) vmem(r7+#1) = v5
	}
	{
		r7 = add(r7,#128)
		if (q3) vmem(r7+#0) = v2
	} :endloop0
// %bb.63:                              // %"end for resampled_input.s0.x.rebased13.loopexit.us"
                                        //   in Loop: Header=BB131_61 Depth=3
	{
		r3 = memw(r30+##-23392)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r2,r3)
		r2 = add(r2,#1)
		r3 = memw(r30+##-23504)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r19,r3)
		r3 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,r3)
		if (!p0) jump:nt .LBB131_61
	}
.LBB131_64:                             // %"consume resampled_input"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r0 = memw(r30+##-23664)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_41
		r0 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
// %bb.65:                              // %"for output.s0.y.yo.preheader"
                                        //   in Loop: Header=BB131_42 Depth=2
	{
		r6 = #0
		r1 = memw(r30+##-23624)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-23872)
		r2 = memw(r30+##-23592)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r2,r1)
		r7 = memw(r30+#-3896)
		memw(r30+##-23320) = r6
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-23552)
		r2 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r7,#-28032)
		v0 = vmem(r0+#0)
	}
	{
		r4 = memw(r30+##-23640)
		v2 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v1 = vmem(r5+#0)
	}
	{
		r3 += mpyi(r1,r4)
		v3 = vmem(r2+#0)
		memw(r30+##-23384) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = #0 ; jump .LBB131_67
		memw(r30+##-23328) = r3.new
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_66:                             // %"end for output.s0.x.xo"
                                        //   in Loop: Header=BB131_67 Depth=3
	{
		r1 = memw(r30+##-23328)
		r2 = memw(r30+##-23320)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r0 = memw(r30+##-23424)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		r0 = add(r2,#2)
		memw(r30+##-23328) = r1
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_41
		memw(r30+##-23320) = r0
	}                                       // 4-byte Folded Spill
.LBB131_67:                             // %"for output.s0.y.yo"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_71 Depth 4
                                        //           Child Loop BB131_75 Depth 5
                                        //             Child Loop BB131_76 Depth 6
	{
		r0 = memw(r30+##-23376)
		r11 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_66
	}
// %bb.68:                              // %"for output.s0.x.xo.preheader"
                                        //   in Loop: Header=BB131_67 Depth=3
	{
		r2 = memw(r30+##-23520)
		r0 = memw(r30+##-23320)
	}                                       // 4-byte Folded Reload
	{
		r1 = r2
		r3 = memw(r30+##-23328)
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-23384)
		r6 = memw(r30+##-23440)
	}                                       // 4-byte Folded Reload
	{
		r3 = asl(r3,#1)
		r15 = r8
		r5 = memw(r30+##-23360)
	}                                       // 4-byte Folded Reload
	{
		r3 = min(r3,r6)
		r0 = min(r0,r6)
		r7 = memw(r30+##-23528)
	}                                       // 4-byte Folded Reload
	{
		r28 = r8
		r4 = add(r5,r0)
		r6 = memw(r30+##-23448)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r7,r0)
		r12 = memw(r30+##-23464)
	}                                       // 4-byte Folded Reload
	{
		v4 = vsplat(r6)
		r3 = add(r3,r5)
		r7 = memw(r30+##-23352)
	}                                       // 4-byte Folded Reload
	{
		r13 = memw(r30+##-23432)
		r9 = memw(r30+##-23456)
	}                                       // 4-byte Folded Reload
	{
		r2 += mpyi(r7,r0)
		r6 = add(r30,#-22064)
		r20 = memw(r30+##-23416)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r3,r7)
		r23 = memw(r30+##-23408)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r7,r4)
		r5 = add(r0,r9)
		r14 = memw(r30+##-23480)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r0,r12)
		r4 = add(r3,#1)
		vmemu(r6+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		r5 = mpyi(r5,r13)
		r24 = memw(r30+##-23472)
	}                                       // 4-byte Folded Reload
	{
		r7 = mpyi(r4,r7)
		r25 = add(r5,r24)
		r27 = memw(r30+##-23400)
	}                                       // 4-byte Folded Reload
	{
		r6 = mpyi(r6,r13)
		r26 = add(r5,r14)
		r5 = sub(r5,r27)
		r21 = add(r7,r9)
	}
	{
		r5 = add(r6,r24)
		r22 = add(r7,r12)
		memw(r30+##-22216) = r5
	}                                       // 4-byte Folded Spill
	{
		r8 = mpyi(r21,r13)
		r5 = add(r6,r14)
		memw(r30+##-22224) = r5
	}                                       // 4-byte Folded Spill
	{
		r9 = mpyi(r22,r13)
		r0 = sub(r0,r23)
		memw(r30+##-22232) = r5
	}                                       // 4-byte Folded Spill
	{
		r5 = add(r8,r14)
		r7 = sub(r7,r23)
		memw(r30+##-22280) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r6 = sub(r6,r27)
		r5 = add(r9,r24)
		memw(r30+##-23232) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r0 = mpyi(r0,r13)
		r7 = mpyi(r7,r13)
		r5 = sub(r9,r27)
	}
	{
		memw(r30+##-23248) = r5
		memw(r30+##-22240) = r6
	}                                       // 4-byte Folded Spill
	{
		r6 = add(r0,r24)
		r5 = memw(r30+##-23488)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r0,r14)
		r0 = sub(r0,r27)
		memw(r30+##-22248) = r6
	}                                       // 4-byte Folded Spill
	{
		v4 = vsplat(r5)
		r5 = add(r7,r14)
		memw(r30+##-23264) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r5 = memw(r30+##-23368)
		memw(r30+##-22256) = r6
	}                                       // 4-byte Folded Reload
	{
		r1 = mpyi(r20,r1)
		r6 = add(r8,r24)
		memw(r30+##-22272) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r2 = mpyi(r20,r2)
		r6 = sub(r8,r27)
		memw(r30+##-23224) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r15 += mpyi(r3,r5)
		memw(r30+##-22264) = r0
	}                                       // 4-byte Folded Spill
	{
		r6 = add(r9,r14)
		r3 = memw(r30+##-23512)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r3,r1)
		r0 = add(r3,r2)
		memw(r30+##-23240) = r6
	}                                       // 4-byte Folded Spill
	{
		r3:2 = combine(#0,#0)
		r6 = add(r7,r24)
		memw(r30+##-23256) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r28 += mpyi(r4,r5)
		r6 = add(r30,#-22448)
		memw(r30+##-22200) = r25
	}                                       // 4-byte Folded Spill
	{
		r6 = sub(r7,r27)
		vmemu(r6+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		memw(r30+##-22208) = r26
		memw(r30+##-23272) = r6
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-21816) = r15
		memw(r30+##-22192) = r28
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23280) = r1
		memw(r30+##-23288) = r0
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_71
	}
	.p2align	4
.LBB131_69:                             // %then_bb26
                                        //   in Loop: Header=BB131_71 Depth=4
	{
		r8 = memw(r30+##-23296)
		memw(r30+##-12976) = r8
	}                                       // 4-byte Folded Reload
	{
		r3 = #64
		v8 = vxor(v8,v8)
		r1 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r8,#1792)
		v3:2 = vcombine(v8,v8)
		memw(r30+##-13616) = r14
	}                                       // 4-byte Folded Spill
	{
		r14 = add(r0,r1)
		r0 = add(r30,#-4400)
		v4 = vmem(r8+#4)
	}
	{
		r2 = add(r30,#-560)
		v7 = vror(v8,r3)
		memw(r30+##-7472) = r14
	}                                       // 4-byte Folded Spill
	{
		v27:26 = vcombine(v8,v8)
		memw(r30+##-11184) = r5
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r30,#-3376)
		v29:28 = vcombine(v8,v8)
		vmemu(r0+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		v4 = valign(v4,v4,r3)
		v31:30 = vcombine(v8,v8)
		memw(r30+##-14384) = r28
	}                                       // 4-byte Folded Spill
	{
		r28 = #116
		memw(r30+##-13104) = r12
	}                                       // 4-byte Folded Spill
	{
		v25:24 = vcombine(v8,v8)
		v21:20 = vcombine(v8,v8)
		memw(r30+##-13232) = r13
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r30,#-5168)
		v18 = v8
		vmemu(r0+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		v6 = v8
		v4 = vmem(r8+#6)
		memw(r30+##-13360) = r9
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r30,#-1584)
		v9 = v8
		vmemu(r0+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		v4 = valign(v4,v4,r3)
		memw(r30+##-13744) = r15
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-14256) = r10
	}                                       // 4-byte Folded Spill
	{
		vmemu(r0+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r30+##-22704)
	}                                       // 4-byte Folded Reload
	{
		r1 = r0
	}
	{
		r3 = add(r8,#1536)
		v1 = valign(v0,v0,r3)
		r0 = memw(r30+#-3912)
		v0.cur = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v13 = vmem(r3+#0)
	}
	{
		v23:22.w = vunpack(v1.h)
		r0 = memw(r30+#-3904)
		v14 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-2608)
		q0 = vcmp.eq(v4.b,v8.b)
		v4.cur = vmem(r0+#0)
	}
	{
		r4 = #64
		v5:4.w = vunpack(v0.h)
		v0 = vmem(r4+#0)
	}
	{
		r0 = add(r30,#-2480)
		vmemu(r0+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		v11:10.w = vunpack(v4.h)
		v4.cur = vmem(r8+#0)
	}
	{
		r0 = add(r30,#-8752)
		v5:4 = vcombine(v8,v8)
		vmemu(r0+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-8624)
		vmemu(r0+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-2096)
		vmemu(r0+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-1968)
		vmemu(r0+#0) = v22
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r1,#-1792)
		r1 = add(r8,#1024)
		vmemu(r0+#0) = v23
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-3632)
		v10 = valign(v13,v13,r4)
		v23:22 = vcombine(v8,v8)
		v1 = vmem(r0+#0)
	}
	{
		v19 = valign(v0,v0,r4)
		r1 = memw(r30+##-10304)
		v17 = vmem(r1+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r8,#1280)
		vmemu(r0+#0) = v1
	}                                       // 128-byte Folded Spill
	{
		r18 = mpyi(r14,r1)
		r0 = memw(r30+##-22200)
		v15 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r18,r0)
		v1 = valign(v1,v1,r4)
		r14 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r1 = asl(r0,#7)
		r20 = r18
		vmemu(r2+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r3 = addasl(r14,r0,#7)
		r0 = add(r30,#-5424)
		v10 = valign(v15,v15,r4)
		r7:6 = memd(r14+r1<<#0)
	}
	{
		v3.w = vinsert(r6)
		r19 = r14
	}
	{
		r23:22 = memd(r3+#32)
		r2 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		v28.w = vinsert(r22)
		r0 = add(r30,#-1328)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r22 = r19
		v10 = valign(v17,v17,r4)
		r5:4 = memd(r3+#16)
	}
	{
		v27.w = vinsert(r4)
		r4 = add(r18,r2)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r3+#48)
	}
	{
		v3.w = vinsert(r7)
		r0 = add(r30,#-2352)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		v29.w = vinsert(r26)
		vmemu(r0+#0) = v1
	}                                       // 128-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r3+#8)
		r7:6 = memd(r3+#24)
	}
	{
		v3.w = vinsert(r0)
		r0 = add(r30,#-816)
		v11:10.w = vunpack(v0.h)
		r25:24 = memd(r3+#40)
	}
	{
		v0 = valign(v28,v28,#4)
		r17:16 = memd(r3+#56)
		r2 = memw(r30+##-22224)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r23)
		r9 = add(r18,r2)
		v27 = valign(v27,v27,#4)
	}
	{
		r2 = memw(r30+##-22240)
	}                                       // 4-byte Folded Reload
	{
		v27.w = vinsert(r5)
		r0 = add(r30,#-688)
		vmemu(r0+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		vmemu(r0+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		r0 = memw(r30+##-22208)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r1)
		r0 = add(r18,r0)
		v27 = valign(v27,v27,#4)
	}
	{
		r21 = addasl(r19,r0,#7)
		r5 = asl(r0,#7)
		v28 = valign(v29,v29,#4)
	}
	{
		v27.w = vinsert(r6)
		v28.w = vinsert(r27)
		v0 = valign(v0,v0,#4)
		r1:0 = memd(r19+r5<<#0)
	}
	{
		v0.w = vinsert(r24)
		v30.w = vinsert(r0)
		v3 = vror(v3,r28)
		r13:12 = memd(r21+#8)
	}
	{
		v27 = valign(v27,v27,#4)
		v29 = vor(v3,v7)
		r0 = memw(r30+##-22232)
	}                                       // 4-byte Folded Reload
	{
		v27.w = vinsert(r7)
		r5 = add(r20,r2)
		v3 = valign(v28,v28,#4)
		r7:6 = memd(r21+#16)
	}
	{
		v3.w = vinsert(r16)
		v31.w = vinsert(r6)
		r0 = add(r18,r0)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r25)
		v28 = valign(v30,v30,#4)
		r19:18 = memd(r21+#40)
		r25:24 = memd(r21+#32)
	}
	{
		v2.w = vinsert(r24)
		v28.w = vinsert(r1)
		r16 = r20
		v30 = valign(v3,v3,#4)
	}
	{
		r17 = addasl(r22,r4,#7)
		v30.w = vinsert(r17)
		v27 = vror(v27,r28)
		r15:14 = memd(r21+#24)
	}
	{
		r1 = asl(r4,#7)
		v2 = valign(v2,v2,#4)
		v1 = vor(v27,v7)
		r27:26 = memd(r21+#56)
	}
	{
		r20 = addasl(r22,r9,#7)
		v2.w = vinsert(r25)
		v27 = valign(v28,v28,#4)
		r25:24 = memd(r17+#32)
	}
	{
		v27.w = vinsert(r12)
		v24.w = vinsert(r24)
		r2 = r22
		v0 = vror(v0,r28)
	}
	{
		v28 = vror(v30,r28)
		v10 = vor(v0,v7)
		memw(r30+##-14640) = r16
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v31,v31,#4)
		v31 = vor(v28,v7)
	}
	{
		v0.w = vinsert(r7)
		v27 = valign(v27,v27,#4)
		r7:6 = memd(r21+#48)
	}
	{
		v27.w = vinsert(r13)
		v5.w = vinsert(r6)
		v28 = valign(v2,v2,#4)
		r13:12 = memd(r17+#8)
	}
	{
		v28.w = vinsert(r18)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r14)
		v2 = vror(v27,r28)
	}
	{
		v27 = valign(v28,v28,#4)
		v11 = vor(v2,v7)
	}
	{
		v27.w = vinsert(r19)
		v5 = valign(v5,v5,#4)
		r19:18 = memd(r17+#16)
	}
	{
		v5.w = vinsert(r7)
		v25.w = vinsert(r18)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r22+r1<<#0)
	}
	{
		v0.w = vinsert(r15)
		v26.w = vinsert(r6)
		v24 = valign(v24,v24,#4)
		r15:14 = memd(r20+#8)
	}
	{
		r1 = asl(r9,#7)
		v24.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
		r25:24 = memd(r17+#48)
	}
	{
		v5.w = vinsert(r26)
		v23.w = vinsert(r24)
		v25 = valign(v25,v25,#4)
	}
	{
		r9 = addasl(r22,r5,#7)
		v25.w = vinsert(r19)
		v26 = valign(v26,v26,#4)
	}
	{
		v26.w = vinsert(r7)
		v0 = vror(v0,r28)
		r7:6 = memd(r17+#24)
	}
	{
		v5 = valign(v5,v5,#4)
		v30 = vor(v0,v7)
		r19:18 = memd(r17+#56)
	}
	{
		v5.w = vinsert(r27)
		v25 = valign(v25,v25,#4)
		r27:26 = memd(r17+#40)
	}
	{
		v25.w = vinsert(r6)
		v1:0.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v24,v24,#4)
	}
	{
		v1.w = vinsert(r26)
		v26 = valign(v26,v26,#4)
	}
	{
		r12 = addasl(r22,r0,#7)
		v26.w = vinsert(r12)
		v3:2.uw = vunpack(v0.uh)
	}
	{
		v0 = valign(v25,v25,#4)
	}
	{
		r1 = asl(r0,#7)
		v0.w = vinsert(r7)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r22+r1<<#0)
	}
	{
		v1.w = vinsert(r27)
		v22.w = vinsert(r6)
		v5 = vror(v5,r28)
		r27:26 = memd(r20+#16)
	}
	{
		v4.w = vinsert(r26)
		r0 = add(r30,#-1840)
		v3 = valign(v23,v23,#4)
		v28 = vor(v5,v7)
	}
	{
		v3.w = vinsert(r25)
		v5 = valign(v26,v26,#4)
		r25:24 = memd(r20+#32)
	}
	{
		v5.w = vinsert(r13)
		v21.w = vinsert(r24)
		v22 = valign(v22,v22,#4)
	}
	{
		v22.w = vinsert(r7)
		v1 = vror(v1,r28)
		r7:6 = memd(r20+#24)
	}
	{
		v5 = vror(v5,r28)
		v25 = vor(v1,v7)
	}
	{
		v1 = valign(v3,v3,#4)
		v24 = vor(v5,v7)
	}
	{
		v1.w = vinsert(r18)
		v3 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r27)
		v5:4.uh = vunpack(v29.ub)
		r27:26 = memd(r20+#48)
	}
	{
		v20.w = vinsert(r26)
		v5 = valign(v22,v22,#4)
	}
	{
		v5.w = vinsert(r14)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r19)
		v0 = vror(v0,r28)
		r19:18 = memd(r20+#40)
	}
	{
		v23:22.uw = vunpack(v4.uh)
		v12 = vor(v0,v7)
	}
	{
		v2 = vdelta(v2,v14)
	}
	{
		v3 = valign(v3,v3,#4)
		v0 = vmux(q0,v2,v22)
	}
	{
		r6 = asl(r5,#7)
		v3.w = vinsert(r6)
		v2 = valign(v5,v5,#4)
		r5:4 = memd(r12+#32)
	}
	{
		v2.w = vinsert(r15)
		v9.w = vinsert(r4)
		v4 = valign(v21,v21,#4)
	}
	{
		r4 = memw(r30+##-22248)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r25)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v1 = vror(v1,r28)
		r1:0 = memd(r22+r1<<#0)
		r25:24 = memd(r20+#56)
	}
	{
		v18.w = vinsert(r0)
		v1 = valign(v3,v3,#4)
		v26 = vor(v1,v7)
		r23:22 = memd(r9+#32)
	}
	{
		v1.w = vinsert(r7)
		v5 = valign(v20,v20,#4)
		r7:6 = memd(r2+r6<<#0)
		r15:14 = memd(r9+#56)
	}
	{
		v5.w = vinsert(r27)
		v2 = vror(v2,r28)
		r27:26 = memd(r12+#8)
	}
	{
		v2 = valign(v4,v4,#4)
		v23 = vor(v2,v7)
	}
	{
		v2.w = vinsert(r18)
		v4 = valign(v18,v18,#4)
	}
	{
		v4.w = vinsert(r1)
		v1 = vror(v1,r28)
		r1:0 = memd(r12+#16)
	}
	{
		v6.w = vinsert(r0)
		v1 = valign(v5,v5,#4)
		v16 = vor(v1,v7)
		r11:10 = memd(r12+#48)
	}
	{
		v1.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r26)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r19)
		v21:20.uh = vunpack(v31.ub)
		r19:18 = memd(r12+#24)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r25)
		v4 = valign(v4,v4,#4)
		r25:24 = memd(r12+#40)
	}
	{
		v4.w = vinsert(r27)
		v21:20.uw = vunpack(v20.uh)
		r27:26 = memd(r9+#16)
	}
	{
		v2 = vror(v2,r28)
	}
	{
		v2 = valign(v6,v6,#4)
		v31 = vor(v2,v7)
	}
	{
		v2.w = vinsert(r1)
		v1 = vror(v1,r28)
		r1:0 = memd(r12+#56)
	}
	{
		v5 = vdelta(v20,v14)
		v22 = vor(v1,v7)
	}
	{
		v21:20.uh = vunpack(v10.ub)
	}
	{
		v1 = vror(v4,r28)
	}
	{
		v4 = valign(v9,v9,#4)
		v18 = vor(v1,v7)
		v9 = v14
	}
	{
		v4.w = vinsert(r5)
		r5 = add(r16,r4)
		r4 = add(r30,#-2864)
		v1:0.uh = vunpack(v30.ub)
	}
	{
		r13 = addasl(r2,r5,#7)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r18)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v4 = valign(v4,v4,#4)
		v6 = vmux(q0,v5,v20)
	}
	{
		v4.w = vinsert(r24)
		v21:20.uw = vunpack(v0.uh)
	}
	{
		v11:10.uh = vunpack(v11.ub)
	}
	{
		v0 = valign(v2,v2,#4)
	}
	{
		v0.w = vinsert(r19)
		v3 = vdelta(v20,v14)
		r19:18 = memd(r9+#24)
	}
	{
		v11:10.uw = vunpack(v10.uh)
	}
	{
		v4 = valign(v4,v4,#4)
		v1 = vmux(q0,v3,v10)
	}
	{
		v4.w = vinsert(r25)
		v27 = vror(v27,r28)
		r25:24 = memd(r9+#8)
	}
	{
		v21:20.w = vunpack(v19.h)
		v27 = vor(v27,v7)
	}
	{
		r4 = add(r30,#-304)
		vmemu(r4+#0) = v1
	}                                       // 128-byte Folded Spill
	{
		v0 = vror(v0,r28)
	}
	{
		r4 = add(r30,#-176)
		v20 = vor(v0,v7)
		vmemu(r4+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		v0 = vror(v4,r28)
	}
	{
		v5:4.uh = vunpack(v28.ub)
		v19 = vor(v0,v7)
	}
	{
		v29:28.uh = vunpack(v27.ub)
	}
	{
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v1:0.uw = vunpack(v28.uh)
	}
	{
		v11:10.w = vunpack(v13.h)
	}
	{
		v1 = vdelta(v4,v14)
	}
	{
		r4 = add(r30,#-1072)
		v0 = vmux(q0,v1,v0)
		vmemu(r4+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-944)
		vmemu(r4+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-3120)
		vmemu(r4+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		v29:28.uh = vunpack(v12.ub)
	}
	{
		r4 = add(r30,#-560)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r4 = add(r30,#-560)
		v1 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v5:4.uh = vunpack(v24.ub)
	}
	{
		v11:10.uw = vunpack(v28.uh)
	}
	{
		v3:2.w = vunpack(v1.h)
	}
	{
		v0 = vdelta(v10,v14)
	}
	{
		v5:4.uw = vunpack(v4.uh)
	}
	{
		r4 = add(r30,#-432)
		v2 = vmux(q0,v0,v4)
		vmemu(r4+#0) = v2
	}                                       // 256-byte Folded Spill
	{
		v5:4.w = vunpack(v15.h)
	}
	{
		r4 = add(r30,#-4656)
		v3 = v8
		vmemu(r4+#0) = v3
	}                                       // 256-byte Folded Spill
	{
		v3.w = vinsert(r26)
		v13:12.uh = vunpack(v26.ub)
	}
	{
		v29:28.uh = vunpack(v25.ub)
	}
	{
		r4 = add(r30,#-4528)
		vmemu(r4+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-3888)
		vmemu(r4+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		v5:4.uw = vunpack(v12.uh)
		v12 = v8
	}
	{
		v1:0.uw = vunpack(v28.uh)
	}
	{
		v11:10.uh = vunpack(v16.ub)
	}
	{
		v1 = vdelta(v4,v14)
	}
	{
		v5:4.uh = vunpack(v23.ub)
		v0 = vmux(q0,v1,v0)
	}
	{
		v25:24.uw = vunpack(v10.uh)
	}
	{
		r4 = add(r30,#-5424)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v0 = vdelta(v24,v14)
	}
	{
		v5:4.w = vunpack(v17.h)
		v15 = vmux(q0,v0,v4)
	}
	{
		r4 = add(r30,#-9520)
		v1 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v27:26.uh = vunpack(v22.ub)
	}
	{
		r4 = add(r30,#-9392)
		vmemu(r4+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		v31:30.uh = vunpack(v31.ub)
	}
	{
		r4 = add(r30,#-1328)
		vmemu(r4+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		v5:4.uw = vunpack(v26.uh)
	}
	{
		v29:28.w = vunpack(v1.h)
	}
	{
		v1:0.uw = vunpack(v30.uh)
	}
	{
		v11:10.uh = vunpack(v20.ub)
	}
	{
		v1 = vdelta(v4,v14)
	}
	{
		v5:4.uh = vunpack(v18.ub)
		v18 = vmux(q0,v1,v0)
	}
	{
		v1:0.uw = vunpack(v10.uh)
	}
	{
		r4 = add(r30,#-1328)
		v1 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v21:20.w = vunpack(v1.h)
		v1 = v8
	}
	{
		v1.w = vinsert(r10)
		v0 = vdelta(v0,v14)
	}
	{
		r4 = add(r30,#-1200)
		vmemu(r4+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-5168)
		vmemu(r4+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		v23:22.uh = vunpack(v19.ub)
		v19 = vmux(q0,v0,v4)
	}
	{
		r4 = add(r30,#-9776)
		v0 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r11)
		v5:4.w = vunpack(v0.h)
		v0 = v8
	}
	{
		r6 = asl(r5,#7)
		v0.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r27)
		r4 = add(r30,#-9648)
		vmemu(r4+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-1584)
		vmemu(r4+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		v5:4.uw = vunpack(v22.uh)
		r27:26 = memd(r9+#40)
		r11:10 = memd(r2+r6<<#0)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r0)
		r4 = add(r30,#-1584)
		v5 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-4400)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r7)
		v25:24.w = vunpack(v5.h)
		v5 = v8
		r7:6 = memd(r13+#8)
	}
	{
		v5.w = vinsert(r22)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r18)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r1)
		r4 = add(r30,#-1456)
		vmemu(r4+#0) = v24
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-3376)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r24)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r19)
		vmemu(r4+#0) = v25
	}                                       // 256-byte Folded Spill
	{
		v1 = vror(v1,r28)
		r5:4 = memd(r9+#48)
		r19:18 = memd(r13+#32)
	}
	{
		v12.w = vinsert(r4)
		v0 = valign(v0,v0,#4)
		v1 = vor(v1,v7)
	}
	{
		v0.w = vinsert(r25)
		v3 = vror(v3,r28)
		r25:24 = memd(r13+#16)
	}
	{
		v12 = valign(v12,v12,#4)
		v3 = vor(v3,v7)
	}
	{
		v12.w = vinsert(r5)
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r13+#24)
	}
	{
		v5.w = vinsert(r23)
		v17:16.uh = vunpack(v1.ub)
	}
	{
		v0 = vror(v0,r28)
	}
	{
		v31:30.uh = vunpack(v3.ub)
		v0 = vor(v0,v7)
	}
	{
		v3 = valign(v12,v12,#4)
	}
	{
		v3.w = vinsert(r14)
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v1 = valign(v5,v5,#4)
	}
	{
		v1.w = vinsert(r26)
		v5 = vdelta(v16,v14)
	}
	{
		v3 = valign(v3,v3,#4)
		v12 = vmux(q0,v5,v4)
	}
	{
		v3.w = vinsert(r15)
		v5:4.uh = vunpack(v0.ub)
		v0 = v8
		r15:14 = memd(r13+#40)
	}
	{
		v0.w = vinsert(r10)
		v5 = v8
		v10 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v5.w = vinsert(r24)
		r1 = add(r30,#-7984)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r27)
		v3 = vror(v3,r28)
	}
	{
		v11:10.w = vunpack(v10.h)
		v3 = vor(v3,v7)
	}
	{
		r0 = add(r30,#-9264)
		v13 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = vror(v1,r28)
	}
	{
		v0 = valign(v0,v0,#4)
		v1 = vor(v1,v7)
	}
	{
		v0.w = vinsert(r11)
		v17:16.uw = vunpack(v4.uh)
	}
	{
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r25)
		v21:20.uw = vunpack(v30.uh)
		r25:24 = memd(r13+#48)
	}
	{
		r1 = add(r30,#-7856)
		vmemu(r1+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		v27:26.w = vunpack(v13.h)
	}
	{
		v13 = vdelta(v20,v14)
	}
	{
		r1 = add(r30,#-3632)
		v13 = vmux(q0,v13,v16)
		vmemu(r1+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		v11:10.uh = vunpack(v3.ub)
	}
	{
		v17:16.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v4,v4,#4)
		v5:4 = vcombine(v8,v8)
	}
	{
		v1.w = vinsert(r4)
		v4.w = vinsert(r18)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r6)
		v5.w = vinsert(r24)
		v21:20.uw = vunpack(v10.uh)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r5)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r7)
		v17:16.uw = vunpack(v16.uh)
		r7:6 = memd(r13+#56)
	}
	{
		v3 = vdelta(v20,v14)
	}
	{
		v14 = vmux(q0,v3,v16)
		v3 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vror(v0,r28)
		r1 = memw(r30+##-22264)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r16,r1)
		v1 = vror(v1,r28)
		v0 = vor(v0,v7)
	}
	{
		r23 = addasl(r2,r4,#7)
		r5 = asl(r4,#7)
		v11:10.w = vunpack(v3.h)
		v1 = vor(v1,v7)
	}
	{
		v3 = valign(v4,v4,#4)
		r5:4 = memd(r2+r5<<#0)
	}
	{
		v3.w = vinsert(r19)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r25)
		v17:16.uh = vunpack(v0.ub)
	}
	{
		v0 = valign(v3,v3,#4)
		v3 = v8
	}
	{
		v0.w = vinsert(r14)
		r0 = add(r30,#-9136)
		vmemu(r0+#0) = v26
	}                                       // 256-byte Folded Spill
	{
		v25:24.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v4,v4,#4)
	}
	{
		v1.w = vinsert(r6)
		vmemu(r0+#0) = v27
	}                                       // 256-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r30+##-22256)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r15)
		r0 = add(r16,r0)
		v1 = valign(v1,v1,#4)
		r15:14 = memd(r3+#64)
	}
	{
		r22 = addasl(r2,r0,#7)
		v1.w = vinsert(r7)
		r16 = add(r8,#1152)
		v5:4.uw = vunpack(v24.uh)
	}
	{
		r1 = asl(r0,#7)
		v0 = vror(v0,r28)
		v5 = v8
	}
	{
		v1 = vror(v1,r28)
		v0 = vor(v0,v7)
		r27:26 = memd(r22+#16)
		r1:0 = memd(r2+r1<<#0)
	}
	{
		v5.w = vinsert(r26)
		v3.w = vinsert(r0)
		v4 = vdelta(v4,v9)
		v1 = vor(v1,v7)
	}
	{
		r0 = add(r30,#-2352)
		v17:16.uw = vunpack(v16.uh)
		r25:24 = memd(r22+#24)
		r19:18 = memd(r22+#32)
	}
	{
		v5 = valign(v5,v5,#4)
		v4 = vmux(q0,v4,v16)
		r11:10 = memd(r22+#8)
		r7:6 = memd(r22+#56)
	}
	{
		v5.w = vinsert(r27)
		v17:16.uh = vunpack(v0.ub)
		r27:26 = memd(r22+#48)
	}
	{
		v27:26.uh = vunpack(v1.ub)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v1 = valign(v5,v5,#4)
		v5 = v8
	}
	{
		v1.w = vinsert(r24)
		v5.w = vinsert(r18)
		r18 = add(r8,#2176)
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v21:20.uw = vunpack(v26.uh)
		v17 = v8
	}
	{
		v17.w = vinsert(r26)
		v0 = valign(v3,v3,#4)
	}
	{
		v0.w = vinsert(r10)
		v3 = vdelta(v20,v9)
	}
	{
		v1 = valign(v1,v1,#4)
		v3 = vmux(q0,v3,v16)
		v16 = v8
	}
	{
		v1.w = vinsert(r25)
		v16.w = vinsert(r4)
		v17 = valign(v17,v17,#4)
		r25:24 = memd(r23+#8)
	}
	{
		v17.w = vinsert(r27)
		r4 = add(r8,#64)
		v5 = valign(v5,v5,#4)
		r27:26 = memd(r23+#16)
	}
	{
		v5.w = vinsert(r19)
		v1 = vror(v1,r28)
	}
	{
		v16 = valign(v16,v16,#4)
		v1 = vor(v1,v7)
	}
	{
		v16.w = vinsert(r5)
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r6)
		v11 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		r1:0 = memd(r22+#40)
	}
	{
		v0.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
		r11:10 = memd(r23+#24)
	}
	{
		v5.w = vinsert(r0)
		v25:24.uh = vunpack(v1.ub)
		r0 = memw(r30+##-22704)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r0,#-1664)
		v1 = valign(v16,v16,#4)
	}
	{
		v1.w = vinsert(r24)
		v16 = valign(v17,v17,#4)
	}
	{
		v16.w = vinsert(r7)
		v0 = vror(v0,r28)
		r7:6 = memd(r23+#40)
	}
	{
		v5 = valign(v5,v5,#4)
		v0 = vor(v0,v7)
	}
	{
		v5.w = vinsert(r1)
		r1 = add(r30,#-10032)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r25)
		v16 = vror(v16,r28)
		r25:24 = memd(r23+#32)
	}
	{
		v21:20.uw = vunpack(v24.uh)
		v16 = vor(v16,v7)
	}
	{
		v5 = vror(v5,r28)
	}
	{
		v17 = vdelta(v20,v9)
		v5 = vor(v5,v7)
	}
	{
		v21:20.uh = vunpack(v0.ub)
	}
	{
		v1 = vror(v1,r28)
	}
	{
		v25:24.uh = vunpack(v16.ub)
		v0 = vor(v1,v7)
		v1 = v8
	}
	{
		v1.w = vinsert(r26)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v23:22.uh = vunpack(v5.ub)
		v5 = vmux(q0,v17,v20)
	}
	{
		v21:20.uh = vunpack(v0.ub)
		r5:4 = memd(r23+#48)
		v0 = vmem(r4+#0)
	}
	{
		v17:16.uw = vunpack(v24.uh)
	}
	{
		v25:24.w = vunpack(v0.h)
	}
	{
		v0 = vdelta(v16,v9)
	}
	{
		r1 = add(r30,#-9904)
		v24 = v8
		vmemu(r1+#0) = v24
	}                                       // 256-byte Folded Spill
	{
		v24.w = vinsert(r4)
		r1 = add(r30,#-2608)
		vmemu(r1+#0) = v25
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-2480)
		r4 = add(r0,#-1920)
		v16 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-1840)
		r0 = add(r30,#-1072)
		v17 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v31:30.w = vunpack(v11.h)
	}
	{
		v31.w = vmpyieo(v5.h,v10.h)
		r1 = add(r30,#-4656)
		v11 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v31.w += vmpyie(v5.w,v10.h)
		v23:22.uw = vunpack(v22.uh)
	}
	{
		r1 = add(r30,#-4528)
		v0 = vmux(q0,v0,v22)
		v22 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w = vmpyieo(v11.h,v16.h)
		r1 = add(r30,#-2096)
		v23 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w = vmpyieo(v15.h,v22.h)
		r1 = add(r30,#-1968)
		v26 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v15.w,v22.h)
		r1 = add(r30,#-11568)
		v27 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w += vmpyie(v11.w,v16.h)
		r1 = add(r30,#-11440)
		vmemu(r1+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v16.w = vmpyieo(v6.h,v26.h)
		r1 = add(r30,#-9264)
		vmemu(r1+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v16.w += vmpyie(v6.w,v26.h)
		v15 = valign(v24,v24,#4)
	}
	{
		v22.w = vmpyieo(v18.h,v28.h)
		r1 = add(r30,#-9136)
		v24 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v18.w,v28.h)
		r1 = add(r30,#-12336)
		v25 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v29.w = vmpyieo(v4.h,v24.h)
		r1 = add(r30,#-12208)
		vmemu(r1+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		v29.w += vmpyie(v4.w,v24.h)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		r1 = add(r30,#-7984)
		v21 = v8
		vmemu(r1+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		v21.w = vinsert(r24)
		v30.w = vmpyieo(v0.h,v30.h)
		v1 = valign(v1,v1,#4)
		v4 = v30
	}
	{
		v1.w = vinsert(r27)
		r1 = add(r30,#-7856)
		v10 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v30.w += vmpyie(v0.w,v4.h)
		r1 = add(r30,#-12080)
		v11 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v28.w = vmpyieo(v3.h,v10.h)
		r1 = add(r30,#-11952)
		vmemu(r1+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		v28.w += vmpyie(v3.w,v10.h)
		v6 = valign(v21,v21,#4)
		v21 = v9
		r27:26 = memd(r23+#56)
	}
	{
		v6.w = vinsert(r25)
		v15.w = vinsert(r5)
		v1 = valign(v1,v1,#4)
		r25:24 = memd(r3+#88)
	}
	{
		v1.w = vinsert(r10)
		r1 = add(r30,#-9776)
		vmemu(r1+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-9648)
		r5 = add(r8,#1408)
		v4 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-9520)
		r10 = add(r8,#1920)
		v5 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v13.h,v4.h)
		v0 = valign(v1,v1,#4)
	}
	{
		v0.w = vinsert(r11)
		r11 = add(r8,#1664)
		v1 = valign(v6,v6,#4)
		v6 = v4
	}
	{
		v1.w = vinsert(r6)
		r1 = add(r30,#-9392)
		v4 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w += vmpyie(v13.w,v6.h)
		r1 = add(r30,#-1584)
		v5 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vmpyieo(v19.h,v4.h)
		r1 = add(r30,#-1456)
		v10 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w += vmpyie(v19.w,v4.h)
		v0 = vror(v0,r28)
	}
	{
		v24.w = vmpyieo(v14.h,v10.h)
		v0 = vor(v0,v7)
		v11 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w += vmpyie(v14.w,v10.h)
		r0 = add(r30,#-944)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-1328)
		r0 = add(r30,#-816)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-1200)
		v18 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v27:26.uh = vunpack(v0.ub)
	}
	{
		v4.w = vmpyieo(v12.h,v18.h)
		r0 = add(r30,#-688)
		v0 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w += vmpyie(v12.w,v18.h)
		v6 = valign(v1,v1,#4)
	}
	{
		v6.w = vinsert(r7)
		r0 = add(r30,#-2864)
		v1 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = valign(v15,v15,#4)
		r7:6 = memd(r3+#72)
	}
	{
		v3.w = vinsert(r26)
		r0 = add(r30,#-560)
		v11 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-432)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vmpyieo(v11.h,v0.h)
		v19 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v11.w,v0.h)
		r0 = add(r30,#-3888)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w = vmpyieo(v2.h,v10.h)
		v12 = valign(v3,v3,#4)
	}
	{
		v3.w += vmpyie(v2.w,v10.h)
		r0 = add(r30,#-304)
		v13 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v12.w = vinsert(r27)
		r0 = add(r30,#-176)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vmpyieo(v13.h,v18.h)
		r0 = add(r30,#-3120)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w += vmpyie(v13.w,v18.h)
		r0 = add(r30,#-8752)
		v11 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v15:14.uw = vunpack(v26.uh)
		r27:26 = memd(r3+#80)
	}
	{
		v0.w = vmpyieo(v11.h,v10.h)
		r0 = add(r30,#-8624)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w += vmpyie(v11.w,v10.h)
		v10 = vror(v12,r28)
		v11 = v8
	}
	{
		v11.w = vinsert(r14)
		v9 = vdelta(v14,v9)
		v10 = vor(v10,v7)
		v14 = v8
	}
	{
		v6 = vror(v6,r28)
		v9 = vmux(q0,v9,v20)
	}
	{
		v6 = vor(v6,v7)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27:26.uh = vunpack(v10.ub)
		r1:0 = memd(r3+#96)
	}
	{
		v14.w = vinsert(r0)
		r0 = add(r30,#-10032)
		v20 = valign(v11,v11,#4)
		v27 = v21
	}
	{
		v20.w = vinsert(r15)
		v13:12.uh = vunpack(v6.ub)
		v6 = v8
		r15:14 = memd(r3+#112)
	}
	{
		v13.w = vmpyieo(v9.h,v18.h)
		v6.w = vinsert(r26)
		v11 = valign(v14,v14,#4)
	}
	{
		v13.w += vmpyie(v9.w,v18.h)
		v15:14.uw = vunpack(v26.uh)
	}
	{
		v11.w = vinsert(r1)
		v19:18.uw = vunpack(v12.uh)
	}
	{
		v10 = vdelta(v14,v21)
	}
	{
		r0 = add(r30,#-9904)
		v10 = vmux(q0,v10,v18)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-7216)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v10.h,v14.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v12.w += vmpyie(v10.w,v14.h)
		r0 = add(r30,#-7088)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r27)
		v10 = v8
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v9 = valign(v20,v20,#4)
		v13:12.w = vadd(v13:12.w,v15:14.w)
		r27:26 = memd(r3+#104)
	}
	{
		v6 = valign(v6,v6,#4)
		v13:12.w = vadd(v13:12.w,v31:30.w)
		r0 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r24)
		v9.w = vinsert(r6)
		v13:12.w = vadd(v29:28.w,v13:12.w)
	}
	{
		v11 = valign(v11,v11,#4)
		v13:12.w = vadd(v13:12.w,v25:24.w)
	}
	{
		v6 = valign(v6,v6,#4)
		v5:4.w = vadd(v13:12.w,v5:4.w)
	}
	{
		v6.w = vinsert(r25)
		v11.w = vinsert(r26)
		v5:4.w = vadd(v23:22.w,v5:4.w)
		r25:24 = memd(r3+#120)
	}
	{
		r3 = #64
		v9 = valign(v9,v9,#4)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v9.w = vinsert(r7)
		v10.w = vinsert(r14)
		v6 = vror(v6,r28)
		r7:6 = memd(r21+#64)
	}
	{
		v11 = valign(v11,v11,#4)
		v1:0.w = vadd(v3:2.w,v1:0.w)
		v6 = vor(v6,v7)
	}
	{
		v9 = vror(v9,r28)
		v1:0.w = vadd(v17:16.w,v1:0.w)
	}
	{
		v11.w = vinsert(r27)
		v10 = valign(v10,v10,#4)
		vmem(r0+#0) = v0
	}
	{
		v10.w = vinsert(r15)
		v9 = vor(v9,v7)
		r0 = memw(r30+##-7232)
	}                                       // 4-byte Folded Reload
	{
		v5:4.uh = vunpack(v6.ub)
		r1 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		v1 = vror(v11,r28)
		r0 = memw(r30+##-7472)
		vmem(r0+#0) = v1
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#1)
		v25:24.uw = vunpack(v4.uh)
		v0 = vor(v1,v7)
		v6 = vmem(r18+#0)
	}
	{
		r1 = mpyi(r0,r1)
		v1 = valign(v10,v10,#4)
		memw(r30+##-11056) = r0
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r24)
		v23:22.uh = vunpack(v9.ub)
		r0 = memw(r30+##-22200)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		v4 = vdelta(v24,v21)
		v13 = vmem(r8+#7)
	}
	{
		v3:2.uw = vunpack(v22.uh)
		r15:14 = memd(r21+#88)
		v18 = vmem(r16+#0)
	}
	{
		r1 = asl(r0,#7)
		r8 = r1
		v5 = vmux(q0,v4,v2)
		v11 = vmem(r8+#5)
	}
	{
		r0 = addasl(r2,r0,#7)
		v1 = valign(v1,v1,#4)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r25)
		r0 = add(r30,#-2352)
		v12 = valign(v13,v13,r3)
		v14 = vmem(r10+#0)
	}
	{
		r3 = #64
		v2 = valign(v18,v18,r3)
		r19:18 = memd(r21+#96)
		v3 = vmem(r19+#0)
	}
	{
		r5 = r2
		v20 = valign(v6,v6,r3)
		r25:24 = memd(r21+#72)
		v17 = vmem(r5+#0)
	}
	{
		v29:28.w = vunpack(v6.h)
		v6 = v8
		r11:10 = memd(r2+r1<<#0)
		v4 = vmem(r11+#0)
	}
	{
		v6.w = vinsert(r6)
		v1 = vror(v1,r28)
		r27:26 = memd(r21+#104)
	}
	{
		r0 = add(r30,#-2224)
		v16 = vor(v1,v7)
		vmemu(r0+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v21:20.w = vunpack(v20.h)
	}
	{
		r0 = add(r30,#-1840)
		vmemu(r0+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-1712)
		vmemu(r0+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-3632)
		vmemu(r0+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		v21:20.uh = vunpack(v16.ub)
		v16 = v8
	}
	{
		v9 = valign(v14,v14,r3)
	}
	{
		v1 = valign(v4,v4,r3)
	}
	{
		v19 = valign(v17,v17,r3)
	}
	{
		v10 = valign(v11,v11,r3)
	}
	{
		v15 = valign(v3,v3,r3)
		r3:2 = memd(r21+#80)
	}
	{
		v16.w = vinsert(r2)
		r4 = add(r30,#-3888)
		v29:28.w = vunpack(v14.h)
		v14 = vmem(r4+#0)
	}
	{
		v31:30.uh = vunpack(v0.ub)
	}
	{
		r0 = add(r30,#-3504)
		v28 = v8
		vmemu(r0+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v28.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v16 = valign(v16,v16,#4)
		r7:6 = memd(r21+#120)
	}
	{
		v16.w = vinsert(r3)
		v23:22.uw = vunpack(v30.uh)
		r3:2 = memd(r17+#64)
	}
	{
		v31:30.w = vunpack(v9.h)
	}
	{
		r0 = add(r30,#-3120)
		vmemu(r0+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v29 = valign(v16,v16,#4)
	}
	{
		v29.w = vinsert(r14)
		r0 = add(r30,#-2992)
		vmemu(r0+#0) = v30
	}                                       // 256-byte Folded Spill
	{
		v25:24.w = vunpack(v4.h)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r24)
		v6 = valign(v28,v28,#4)
	}
	{
		v6.w = vinsert(r19)
		v0 = vdelta(v20,v27)
		v20 = v8
		r19:18 = memd(r17+#112)
	}
	{
		v30 = vmux(q0,v0,v22)
		v31 = v8
		vmemu(r0+#0) = v31
	}                                       // 256-byte Folded Spill
	{
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r21+#112)
	}
	{
		v20.w = vinsert(r0)
		v4.w = vinsert(r25)
		r0 = add(r30,#-3376)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v9 = valign(v29,v29,#4)
		r25:24 = memd(r17+#80)
	}
	{
		v9.w = vinsert(r15)
		v31.w = vinsert(r24)
		v16 = valign(v20,v20,#4)
		r15:14 = memd(r20+#80)
	}
	{
		v16.w = vinsert(r1)
		v4 = vror(v4,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r27)
		v9 = vror(v9,r28)
		r27:26 = memd(r17+#104)
	}
	{
		v16 = valign(v16,v16,#4)
		v9 = vor(v9,v7)
	}
	{
		v16.w = vinsert(r6)
		r0 = add(r30,#-3248)
		vmemu(r0+#0) = v24
	}                                       // 256-byte Folded Spill
	{
		vmemu(r0+#0) = v25
	}                                       // 256-byte Folded Spill
	{
		v29:28.uh = vunpack(v4.ub)
		r1:0 = memd(r17+#72)
	}
	{
		v4 = vror(v6,r28)
	}
	{
		v25:24.uh = vunpack(v9.ub)
		v4 = vor(v4,v7)
	}
	{
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r7)
		v1:0.w = vunpack(v1.h)
		r7:6 = memd(r17+#88)
	}
	{
		v23:22.uw = vunpack(v24.uh)
	}
	{
		v21:20.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v4.ub)
		v4 = v8
		v21 = v8
	}
	{
		v4.w = vinsert(r2)
		r4 = add(r30,#-3760)
		vmemu(r4+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		v21.w = vinsert(r18)
		r2 = add(r30,#-14128)
		v6 = vror(v16,r28)
	}
	{
		r4 = r8
		v6 = vor(v6,v7)
		vmemu(r4+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		v1 = vdelta(v22,v27)
		memw(r30+##-14512) = r4
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v4,v4,#4)
		v0 = vmux(q0,v1,v20)
	}
	{
		v1.w = vinsert(r3)
		v4 = valign(v31,v31,#4)
		v31 = v27
	}
	{
		v4.w = vinsert(r25)
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v25:24.uh = vunpack(v6.ub)
		v6 = v8
		r3:2 = memd(r17+#96)
		r25:24 = memd(r20+#104)
	}
	{
		v6.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r6)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r0)
		r0 = add(r30,#-14896)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v23:22.uw = vunpack(v28.uh)
		r3:2 = memd(r17+#120)
		r17:16 = memd(r20+#72)
	}
	{
		v29:28.uw = vunpack(v24.uh)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r20+#64)
	}
	{
		v1.w = vinsert(r1)
		r1 = add(r30,#-4400)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v20 = vdelta(v28,v27)
	}
	{
		v20 = valign(v21,v21,#4)
		v0 = vmux(q0,v20,v22)
	}
	{
		v20.w = vinsert(r19)
		v4 = vror(v4,r28)
		r19:18 = memd(r20+#96)
	}
	{
		v1 = vror(v1,r28)
		v4 = vor(v4,v7)
	}
	{
		v6 = valign(v6,v6,#4)
		v1 = vor(v1,v7)
	}
	{
		v6.w = vinsert(r27)
		v20 = valign(v20,v20,#4)
		r27:26 = memd(r20+#112)
	}
	{
		v20.w = vinsert(r2)
		v29:28.uh = vunpack(v4.ub)
	}
	{
		v23:22.uh = vunpack(v1.ub)
	}
	{
		v1 = vror(v6,r28)
		v6 = v8
	}
	{
		v6.w = vinsert(r14)
		v4 = valign(v20,v20,#4)
		v1 = vor(v1,v7)
	}
	{
		v4.w = vinsert(r3)
		v25:24.uw = vunpack(v28.uh)
		r3:2 = memd(r20+#88)
		r21:20 = memd(r20+#120)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v1 = vdelta(v24,v27)
	}
	{
		v0 = vmux(q0,v1,v22)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v1 = vror(v4,r28)
		v4 = v8
		r0 = memw(r30+##-22208)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r6)
		r0 = add(r8,r0)
		v6 = valign(v6,v6,#4)
		v1 = vor(v1,v7)
	}
	{
		r8 = addasl(r5,r0,#7)
		v6.w = vinsert(r15)
		v17:16.w = vunpack(v17.h)
		r15:14 = memd(r9+#80)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		v25:24.uh = vunpack(v1.ub)
		v1 = v8
		r7:6 = memd(r12+#72)
	}
	{
		v1.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r16)
		v23:22.uw = vunpack(v24.uh)
	}
	{
		r1 = add(r30,#-4272)
		vmemu(r1+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-15024)
		vmemu(r1+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r19)
		v17:16.w = vunpack(v19.h)
		r19:18 = memd(r12+#80)
	}
	{
		v21:20.uw = vunpack(v28.uh)
	}
	{
		v19 = vdelta(v22,v27)
	}
	{
		v0 = vmux(q0,v19,v20)
		v19 = v8
		vmemu(r1+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v19.w = vinsert(r26)
		r1 = add(r30,#-2864)
		r26 = r5
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r17)
		v6 = valign(v6,v6,#4)
		r17:16 = memd(r9+#72)
	}
	{
		v6.w = vinsert(r3)
		v1 = valign(v1,v1,#4)
		r3:2 = memd(r12+#64)
	}
	{
		v1.w = vinsert(r24)
		r1 = add(r30,#-2736)
		vmemu(r1+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-15152)
		vmemu(r1+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v4 = vror(v4,r28)
	}
	{
		v17:16.w = vunpack(v18.h)
		v4 = vor(v4,v7)
	}
	{
		v18 = valign(v19,v19,#4)
	}
	{
		v18.w = vinsert(r27)
		v6 = vror(v6,r28)
	}
	{
		v1 = valign(v1,v1,#4)
		v6 = vor(v6,v7)
	}
	{
		v1.w = vinsert(r25)
		v29:28.uh = vunpack(v4.ub)
		r25:24 = memd(r12+#104)
	}
	{
		v4 = valign(v18,v18,#4)
	}
	{
		v4.w = vinsert(r20)
		v19:18.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r2)
		v1 = vror(v1,r28)
	}
	{
		v4 = valign(v4,v4,#4)
		v1 = vor(v1,v7)
	}
	{
		v4.w = vinsert(r21)
		v19:18.uw = vunpack(v18.uh)
		r21:20 = memd(r12+#88)
	}
	{
		v21:20.uh = vunpack(v1.ub)
	}
	{
		r1 = add(r30,#-5680)
		vmemu(r1+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v1 = vdelta(v18,v27)
		v18 = v8
	}
	{
		v18.w = vinsert(r18)
		r1 = add(r30,#-5552)
		vmemu(r1+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v4 = vror(v4,r28)
	}
	{
		r1 = asl(r0,#7)
		r0 = add(r30,#-8240)
		vmemu(r1+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v17:16.w = vunpack(v2.h)
		v2 = vor(v4,v7)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r3)
		v6 = valign(v18,v18,#4)
		r3:2 = memd(r12+#96)
	}
	{
		v6.w = vinsert(r19)
		v19:18.uh = vunpack(v2.ub)
		v2 = v8
		r19:18 = memd(r12+#112)
	}
	{
		v2.w = vinsert(r2)
		v23:22.uw = vunpack(v28.uh)
		v19 = v8
	}
	{
		v19.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
		v1 = vmux(q0,v1,v22)
	}
	{
		v6.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r6)
		v29:28.uw = vunpack(v18.uh)
	}
	{
		v18 = valign(v2,v2,#4)
	}
	{
		v18.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r12+#120)
	}
	{
		v6.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
		r21:20 = memd(r9+#104)
	}
	{
		v4.w = vinsert(r7)
		v18 = valign(v18,v18,#4)
		r7:6 = memd(r9+#64)
	}
	{
		v18.w = vinsert(r24)
		v19 = valign(v19,v19,#4)
	}
	{
		v19.w = vinsert(r19)
		v6 = vror(v6,r28)
		r19:18 = memd(r9+#96)
	}
	{
		v4 = vror(v4,r28)
		v6 = vor(v6,v7)
	}
	{
		v18 = valign(v18,v18,#4)
		v4 = vor(v4,v7)
	}
	{
		v18.w = vinsert(r25)
		v19 = valign(v19,v19,#4)
		r25:24 = memd(r9+#112)
	}
	{
		v19.w = vinsert(r2)
		r2 = add(r30,#-5424)
		v2 = vdelta(v28,v27)
	}
	{
		v29:28.uh = vunpack(v6.ub)
	}
	{
		v23:22.uh = vunpack(v4.ub)
	}
	{
		v4 = vror(v18,r28)
	}
	{
		v25:24.uw = vunpack(v20.uh)
		v4 = vor(v4,v7)
	}
	{
		v6 = valign(v19,v19,#4)
		v2 = vmux(q0,v2,v24)
	}
	{
		v6.w = vinsert(r3)
		v25:24.uw = vunpack(v28.uh)
	}
	{
		v19:18.uh = vunpack(v4.ub)
	}
	{
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v4 = vdelta(v24,v27)
	}
	{
		r0 = add(r30,#-8112)
		v0 = vmux(q0,v4,v22)
		vmemu(r0+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v4 = vror(v6,r28)
		v6 = v8
	}
	{
		v6.w = vinsert(r6)
		v4 = vor(v4,v7)
		vmemu(r0+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		r6 = add(r30,#-5168)
		v17:16.w = vunpack(v13.h)
		v13 = v8
		r1:0 = memd(r5+r1<<#0)
	}
	{
		v13.w = vinsert(r14)
		v25:24.uh = vunpack(v4.ub)
		v4 = v8
	}
	{
		v4.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		r2 = add(r30,#-5296)
		vmemu(r2+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-15280)
		vmemu(r2+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v17:16.w = vunpack(v12.h)
	}
	{
		v12 = valign(v13,v13,#4)
	}
	{
		v12.w = vinsert(r15)
		v4 = valign(v4,v4,#4)
		r15:14 = memd(r13+#104)
	}
	{
		v4.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
		r19:18 = memd(r9+#120)
	}
	{
		v6.w = vinsert(r16)
		v23:22.uw = vunpack(v24.uh)
	}
	{
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v12 = valign(v12,v12,#4)
		r3:2 = memd(r9+#88)
	}
	{
		v12.w = vinsert(r2)
		r2 = add(r30,#-9008)
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v13 = vdelta(v22,v27)
	}
	{
		v4 = valign(v4,v4,#4)
		v26 = vmux(q0,v13,v18)
		v13 = v8
	}
	{
		v4.w = vinsert(r20)
		v13.w = vinsert(r24)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
		r6 = add(r30,#-5040)
		vmemu(r6+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		vmemu(r6+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v17:16.w = vunpack(v11.h)
		r17:16 = memd(r13+#72)
	}
	{
		v11 = valign(v12,v12,#4)
	}
	{
		v11.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r21)
		v6 = vror(v6,r28)
		r21:20 = memd(r13+#64)
	}
	{
		v12 = valign(v13,v13,#4)
		v6 = vor(v6,v7)
	}
	{
		v12.w = vinsert(r25)
		v11 = vror(v11,r28)
		r25:24 = memd(r13+#88)
	}
	{
		v19:18.uh = vunpack(v6.ub)
		v11 = vor(v11,v7)
	}
	{
		v4 = vror(v4,r28)
	}
	{
		v6 = valign(v12,v12,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r18)
		v29:28.uh = vunpack(v11.ub)
		v11 = v8
	}
	{
		v11.w = vinsert(r20)
		v21:20.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r19)
		v13:12.uw = vunpack(v28.uh)
		r19:18 = memd(r13+#80)
	}
	{
		r2 = add(r30,#-8880)
		vmemu(r2+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v6 = vdelta(v12,v27)
		v12 = v8
	}
	{
		v12.w = vinsert(r18)
		vmemu(r2+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v4 = vror(v4,r28)
		r2 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r4,r2)
		r2 = add(r30,#-8496)
		v17:16.w = vunpack(v10.h)
		v4 = vor(v4,v7)
	}
	{
		r9 = addasl(r5,r6,#7)
		r7 = asl(r6,#7)
		r6 = add(r30,#-7728)
		v10 = valign(v12,v12,#4)
	}
	{
		v10.w = vinsert(r19)
		r2 = add(r30,#-8368)
		vmemu(r2+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v23:22.uh = vunpack(v4.ub)
		v4 = v8
		r3:2 = memd(r13+#96)
		r19:18 = memd(r22+#96)
	}
	{
		v4.w = vinsert(r2)
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v6 = valign(v11,v11,#4)
		v9 = vmux(q0,v6,v18)
		v11 = v8
	}
	{
		v6.w = vinsert(r21)
		v10 = valign(v10,v10,#4)
		r21:20 = memd(r13+#112)
	}
	{
		v10.w = vinsert(r24)
		v11.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r13+#120)
		r13:12 = memd(r22+#64)
	}
	{
		v6.w = vinsert(r16)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r25)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
		v11 = valign(v11,v11,#4)
		r17:16 = memd(r22+#72)
	}
	{
		v11.w = vinsert(r21)
		v10 = vror(v10,r28)
		r21:20 = memd(r22+#104)
	}
	{
		v4 = valign(v4,v4,#4)
		v10 = vor(v10,v7)
	}
	{
		v4.w = vinsert(r15)
		v6 = vror(v6,r28)
		r15:14 = memd(r22+#80)
	}
	{
		v11 = valign(v11,v11,#4)
		v6 = vor(v6,v7)
		r25:24 = memd(r22+#112)
	}
	{
		v11.w = vinsert(r2)
		v19:18.uh = vunpack(v10.ub)
	}
	{
		v13:12.uw = vunpack(v22.uh)
	}
	{
		v4 = vror(v4,r28)
	}
	{
		v29:28.uh = vunpack(v6.ub)
		v4 = vor(v4,v7)
	}
	{
		v6 = valign(v11,v11,#4)
	}
	{
		v6.w = vinsert(r3)
		v19:18.uw = vunpack(v18.uh)
		r3:2 = memd(r22+#88)
	}
	{
		v25:24.uw = vunpack(v20.uh)
	}
	{
		v12 = vdelta(v12,v27)
	}
	{
		v21:20.uh = vunpack(v4.ub)
		v27 = vmux(q0,v12,v24)
	}
	{
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v4 = vdelta(v18,v31)
	}
	{
		v4 = vror(v6,r28)
		v0 = vmux(q0,v4,v12)
		v6 = v8
	}
	{
		v6.w = vinsert(r12)
		v11:10.uw = vunpack(v20.uh)
		v4 = vor(v4,v7)
	}
	{
		v23:22.w = vunpack(v15.h)
		v11 = v8
	}
	{
		v11.w = vinsert(r14)
		v25:24.uh = vunpack(v4.ub)
		v4 = v8
	}
	{
		v4.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r13)
		v11 = valign(v11,v11,#4)
		r13:12 = memd(r5+r7<<#0)
	}
	{
		v11.w = vinsert(r15)
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v13 = valign(v4,v4,#4)
	}
	{
		v13.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
		r19:18 = memd(r22+#120)
	}
	{
		v6.w = vinsert(r16)
		v4 = vdelta(v12,v31)
		v12 = v8
	}
	{
		v12.w = vinsert(r24)
		v11 = valign(v11,v11,#4)
		v4 = vmux(q0,v4,v10)
	}
	{
		v11.w = vinsert(r2)
		r2 = add(r30,#-10288)
		v10 = valign(v13,v13,#4)
	}
	{
		v10.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
		v11 = valign(v11,v11,#4)
		r17:16 = memd(r23+#64)
	}
	{
		v11.w = vinsert(r3)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r21)
		v6 = vror(v6,r28)
		r21:20 = memd(r23+#80)
	}
	{
		v12 = valign(v12,v12,#4)
		v6 = vor(v6,v7)
		r15:14 = memd(r23+#112)
	}
	{
		v12.w = vinsert(r25)
		v11 = vror(v11,r28)
		r25:24 = memd(r23+#104)
	}
	{
		v10 = vror(v10,r28)
		v11 = vor(v11,v7)
	}
	{
		v19:18.uh = vunpack(v6.ub)
		v6 = vor(v10,v7)
	}
	{
		v10 = valign(v12,v12,#4)
		v12 = v8
	}
	{
		v10.w = vinsert(r18)
		v12.w = vinsert(r20)
		v25:24.uh = vunpack(v11.ub)
	}
	{
		r6 = add(r30,#-7600)
		vmemu(r6+#0) = v22
	}                                       // 256-byte Folded Spill
	{
		v29:28.uh = vunpack(v6.ub)
	}
	{
		r6 = #64
		vmemu(r6+#0) = v23
	}                                       // 256-byte Folded Spill
	{
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v6 = valign(v10,v10,#4)
	}
	{
		v6.w = vinsert(r19)
		v13 = valign(v14,v14,r6)
		r7:6 = memd(r23+#72)
		r19:18 = memd(r23+#96)
	}
	{
		v11:10.uw = vunpack(v28.uh)
	}
	{
		v15:14.w = vunpack(v14.h)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v11 = vdelta(v24,v31)
	}
	{
		v14 = vmux(q0,v11,v18)
		v11 = v8
		vmemu(r2+#0) = v14
	}                                       // 256-byte Folded Spill
	{
		v11.w = vinsert(r16)
		r2 = add(r30,#-10160)
		r16 = add(r30,#-3120)
		v6 = vror(v6,r28)
	}
	{
		v12 = valign(v12,v12,#4)
		v6 = vor(v6,v7)
	}
	{
		v12.w = vinsert(r21)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r17)
		v19:18.uh = vunpack(v6.ub)
	}
	{
		v15 = v8
		vmemu(r2+#0) = v15
	}                                       // 256-byte Folded Spill
	{
		v15.w = vinsert(r14)
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r23+#88)
	}
	{
		v11.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
		r21:20 = memd(r23+#120)
	}
	{
		v12.w = vinsert(r2)
		v29:28.w = vunpack(v13.h)
		v13 = v8
	}
	{
		v13.w = vinsert(r18)
		v11 = valign(v11,v11,#4)
		r2 = memw(r30+##-22224)
	}                                       // 4-byte Folded Reload
	{
		v11.w = vinsert(r7)
		r17 = add(r4,r2)
		r2 = add(r30,#-2352)
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v6 = valign(v13,v13,#4)
		r5 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r19)
		v13 = vdelta(v18,v31)
	}
	{
		r23:22 = memd(r5+#16)
		r19:18 = memd(r5+#24)
	}
	{
		r2 = add(r30,#-2224)
		v25 = vmux(q0,v13,v10)
		v20 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v10 = valign(v12,v12,#4)
		r7:6 = memd(r5+#8)
	}
	{
		v10.w = vinsert(r3)
		v12 = vror(v11,r28)
	}
	{
		r2 = add(r30,#-4400)
		v18 = vor(v12,v7)
		v21 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-4272)
		v12 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-1840)
		v13 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w = vmpyieo(v5.h,v20.h)
		v13.w = vmpyieo(v1.h,v12.h)
		v17:16.w = vunpack(v3.h)
	}
	{
		v3.w += vmpyie(v5.w,v20.h)
		r2 = add(r30,#-1712)
		v20 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v1.w,v12.h)
		v19 = vror(v10,r28)
		v11 = v3
	}
	{
		v10.w = vmpyieo(v30.h,v20.h)
		r2 = add(r30,#-2864)
		v21 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w += vmpyie(v30.w,v20.h)
		r2 = add(r30,#-2736)
		v22 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-15664)
		v1 = vor(v19,v7)
		v23 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v2.h,v22.h)
		r2 = add(r30,#-15536)
		vmemu(r2+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		v12.w += vmpyie(v2.w,v22.h)
		r2 = add(r30,#-9008)
		vmemu(r2+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-8880)
		v22 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-5424)
		v23 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v11.w = vmpyieo(v0.h,v22.h)
		v3:2.uh = vunpack(v18.ub)
	}
	{
		v11.w += vmpyie(v0.w,v22.h)
		r2 = add(r30,#-5296)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-11824)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
		r2 = add(r30,#-11696)
		vmemu(r2+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-8496)
		vmemu(r2+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-8368)
		v22 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vmpyieo(v14.h,v16.h)
		v21:20.uh = vunpack(v1.ub)
	}
	{
		v1.w += vmpyie(v14.w,v16.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r25)
		r2 = add(r30,#-7728)
		v23 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w = vmpyieo(v4.h,v22.h)
		r2 = add(r30,#-7600)
		v16 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-5168)
		v30 = v16
		v17 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v25.h,v16.h)
		v5 = vror(v6,r28)
		r25:24 = memd(r5+#48)
	}
	{
		r2 = add(r30,#-5040)
		v5 = vor(v5,v7)
		v16 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w += vmpyie(v4.w,v22.h)
		r2 = add(r30,#-5680)
		v17 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w = vmpyieo(v27.h,v16.h)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v14.w += vmpyie(v27.w,v16.h)
		v3:2.uw = vunpack(v2.uh)
	}
	{
		v0.w += vmpyie(v25.w,v30.h)
		r2 = add(r30,#-5552)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = vdelta(v20,v31)
		v30 = v8
	}
	{
		v30.w = vinsert(r10)
		v21:20.uh = vunpack(v5.ub)
	}
	{
		r2 = add(r30,#-15280)
		v21 = vmux(q0,v3,v2)
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-3376)
		v23 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r2 = add(r30,#-3248)
		v2 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vmpyieo(v23.h,v4.h)
		r2 = add(r30,#-15024)
		v3 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w += vmpyie(v23.w,v4.h)
		r2 = add(r30,#-8240)
		v16 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r2 = add(r30,#-8112)
		v24 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w = vmpyieo(v16.h,v2.h)
		r2 = add(r30,#-3888)
		v25 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w += vmpyie(v16.w,v2.h)
		r2 = add(r30,#-3760)
		v16 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vmpyieo(v26.h,v24.h)
		r2 = add(r30,#-15152)
		v17 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w += vmpyie(v26.w,v24.h)
		v6 = valign(v15,v15,#4)
	}
	{
		v6.w = vinsert(r15)
		r2 = add(r30,#-10288)
		v17 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v15.w = vmpyieo(v9.h,v18.h)
		v23:22.uw = vunpack(v20.uh)
		v20 = v8
	}
	{
		v2.w = vmpyieo(v17.h,v16.h)
		v20.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
	}
	{
		v2.w += vmpyie(v17.w,v16.h)
		r2 = add(r30,#-10160)
		v16 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r20)
		r2 = add(r30,#-3632)
		v17 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w = vmpyieo(v21.h,v16.h)
		r2 = add(r30,#-3504)
		v24 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v21.w,v16.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r21)
		r2 = add(r30,#-14128)
		v25 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w += vmpyie(v9.w,v18.h)
		v27 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v6 = vror(v6,r28)
		r3:2 = memd(r5+#32)
		r21:20 = memd(r5+#40)
	}
	{
		v17.w = vmpyieo(v27.h,v24.h)
		v20 = valign(v20,v20,#4)
		v6 = vor(v6,v7)
	}
	{
		v17.w += vmpyie(v27.w,v24.h)
		r16 = add(r30,#-2992)
		v24 = vmemu(r16+#0)
	}                                       // 256-byte Folded Reload
	{
		v20.w = vinsert(r23)
		r16 = add(r30,#-14896)
		v25 = vmemu(r16+#0)
	}                                       // 256-byte Folded Reload
	{
		r5 = add(r30,#-15024)
		v27:26.uh = vunpack(v6.ub)
		v6 = v8
		r23:22 = memd(r5+#56)
	}
	{
		v6.w = vinsert(r24)
		v21 = vmemu(r16+#0)
	}                                       // 128-byte Folded Reload
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v16.w = vmpyieo(v21.h,v24.h)
		v9 = valign(v30,v30,#4)
		v30 = v31
	}
	{
		v16.w += vmpyie(v21.w,v24.h)
		v25 = vdelta(v26,v31)
		v21 = v8
	}
	{
		v21.w = vinsert(r2)
		v9.w = vinsert(r11)
		r2 = add(r30,#-12592)
		v25 = vmux(q0,v25,v22)
	}
	{
		r2 = add(r30,#-12464)
		vmemu(r2+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v22.w = vmpyieo(v25.h,v28.h)
		r2 = add(r30,#-6960)
		vmemu(r2+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v22.w += vmpyie(v25.w,v28.h)
		r2 = add(r30,#-6832)
		v24 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v21 = valign(v21,v21,#4)
		r11:10 = memd(r8+#24)
	}
	{
		v21.w = vinsert(r3)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r18)
		r2 = add(r30,#-15664)
		v25 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v9 = valign(v9,v9,#4)
		v23:22.w = vadd(v23:22.w,v25:24.w)
	}
	{
		v9.w = vinsert(r6)
		v19:18.w = vadd(v23:22.w,v1:0.w)
	}
	{
		v21 = valign(v21,v21,#4)
		v1:0.w = vadd(v11:10.w,v19:18.w)
	}
	{
		v21.w = vinsert(r20)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r19)
		v9 = valign(v9,v9,#4)
		r19:18 = memd(r8+#16)
	}
	{
		v6 = valign(v6,v6,#4)
		v1:0.w = vadd(v1:0.w,v15:14.w)
	}
	{
		v9.w = vinsert(r7)
		v6.w = vinsert(r25)
		v1:0.w = vadd(v1:0.w,v5:4.w)
		r7:6 = memd(r8+#8)
	}
	{
		v21 = valign(v21,v21,#4)
		v1:0.w = vadd(v13:12.w,v1:0.w)
		r25:24 = memd(r8+#56)
	}
	{
		v21.w = vinsert(r21)
		v20 = vror(v20,r28)
		r21:20 = memd(r8+#48)
	}
	{
		v9 = vror(v9,r28)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v18 = vor(v20,v7)
	}
	{
		v6 = valign(v6,v6,#4)
		v1:0.w = vadd(v1:0.w,v17:16.w)
		v9 = vor(v9,v7)
	}
	{
		v6.w = vinsert(r22)
		r2 = add(r30,#-15536)
		v10 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-14128)
		v11 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v19 = vror(v21,r28)
		v1:0.w = vadd(v11:10.w,v1:0.w)
	}
	{
		v21:20.uh = vunpack(v18.ub)
		v4 = vor(v19,v7)
	}
	{
		v2 = valign(v6,v6,#4)
	}
	{
		v2.w = vinsert(r23)
		v23:22.uh = vunpack(v9.ub)
		r23:22 = memd(r8+#32)
	}
	{
		r2 = add(r30,#-14000)
		vmemu(r2+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		v15:14.uw = vunpack(v20.uh)
	}
	{
		vmemu(r2+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		v5:4.uh = vunpack(v4.ub)
		r3:2 = memd(r8+#40)
	}
	{
		v1:0.uw = vunpack(v22.uh)
	}
	{
		v1 = vdelta(v14,v31)
	}
	{
		v2 = vror(v2,r28)
		v1 = vmux(q0,v1,v0)
	}
	{
		v3:2 = vcombine(v8,v8)
		v0 = vor(v2,v7)
	}
	{
		v2.w = vinsert(r0)
		v3.w = vinsert(r18)
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v25:24.uh = vunpack(v0.ub)
		v5 = v8
		v0 = v8
	}
	{
		v5.w = vinsert(r22)
		v0.w = vinsert(r20)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r9+#24)
	}
	{
		v3.w = vinsert(r19)
		v5 = valign(v5,v5,#4)
		r19:18 = memd(r9+#16)
	}
	{
		v5.w = vinsert(r23)
		v11:10.uw = vunpack(v24.uh)
		v24 = v8
		r23:22 = memd(r9+#32)
	}
	{
		v24.w = vinsert(r22)
		v6 = valign(v2,v2,#4)
	}
	{
		v6.w = vinsert(r6)
		v2 = vdelta(v10,v31)
	}
	{
		v3 = valign(v3,v3,#4)
		v2 = vmux(q0,v2,v4)
	}
	{
		v3.w = vinsert(r10)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r2)
		r2 = add(r30,#-14896)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
		r21:20 = memd(r9+#48)
	}
	{
		v3.w = vinsert(r11)
		v5 = valign(v6,v6,#4)
		v6 = v8
	}
	{
		v5.w = vinsert(r7)
		v6.w = vinsert(r18)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r9+#8)
	}
	{
		v4.w = vinsert(r3)
		v0 = valign(v0,v0,#4)
		r11:10 = memd(r9+#40)
	}
	{
		v0.w = vinsert(r24)
		v3 = vror(v3,r28)
		r3 = memw(r30+##-22232)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r4,r3)
		v5 = vror(v5,r28)
		v3 = vor(v3,v7)
	}
	{
		v4 = vror(v4,r28)
		v5 = vor(v5,v7)
	}
	{
		v0 = valign(v0,v0,#4)
		v4 = vor(v4,v7)
	}
	{
		v0.w = vinsert(r25)
		v29:28.uh = vunpack(v3.ub)
		r25:24 = memd(r9+#56)
	}
	{
		v27:26.uh = vunpack(v5.ub)
	}
	{
		v5:4.uh = vunpack(v4.ub)
	}
	{
		v0 = vror(v0,r28)
	}
	{
		v13:12.uw = vunpack(v28.uh)
		v0 = vor(v0,v7)
	}
	{
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v11:10.uw = vunpack(v26.uh)
		v5 = v8
	}
	{
		r12 = addasl(r26,r3,#7)
		v5.w = vinsert(r12)
		v3 = vdelta(v12,v31)
	}
	{
		v15:14.uh = vunpack(v0.ub)
		v3 = vmux(q0,v3,v10)
		v0 = v8
	}
	{
		v0.w = vinsert(r20)
	}
	{
		r2 = asl(r17,#7)
		vmemu(r2+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v3 = valign(v5,v5,#4)
	}
	{
		v3.w = vinsert(r13)
		v5 = valign(v6,v6,#4)
	}
	{
		v5.w = vinsert(r19)
		v11:10.uw = vunpack(v14.uh)
		r19:18 = memd(r26+r2<<#0)
	}
	{
		v6 = valign(v24,v24,#4)
	}
	{
		v6.w = vinsert(r23)
		v25 = vdelta(v10,v31)
	}
	{
		v3 = valign(v3,v3,#4)
		v4 = vmux(q0,v25,v4)
	}
	{
		v3.w = vinsert(r6)
		v5 = valign(v5,v5,#4)
	}
	{
		r5 = addasl(r26,r17,#7)
		v5.w = vinsert(r0)
		vmemu(r5+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		r0 = asl(r3,#7)
		v4 = valign(v6,v6,#4)
		v6 = v8
	}
	{
		v4.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
		r17:16 = memd(r5+#16)
		r3:2 = memd(r5+#48)
	}
	{
		v3.w = vinsert(r7)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r5+#8)
		r23:22 = memd(r26+r0<<#0)
	}
	{
		v5.w = vinsert(r1)
		r1 = add(r30,#-15152)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v3 = vror(v3,r28)
		r0 = memw(r30+##-22240)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r4,r0)
		v5 = vror(v5,r28)
		v3 = vor(v3,v7)
		r11:10 = memd(r5+#24)
	}
	{
		r13 = addasl(r26,r0,#7)
		v0 = valign(v0,v0,#4)
		v5 = vor(v5,v7)
	}
	{
		v0.w = vinsert(r21)
		v4 = vror(v4,r28)
		r21:20 = memd(r5+#32)
	}
	{
		v6.w = vinsert(r20)
		v21:20.uh = vunpack(v3.ub)
		v3 = vor(v4,v7)
		r15:14 = memd(r13+#16)
	}
	{
		v5:4.uh = vunpack(v5.ub)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r24)
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v23:22.uh = vunpack(v3.ub)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v3 = vdelta(v4,v31)
		v5:4 = vcombine(v8,v8)
	}
	{
		v0.w = vinsert(r25)
		v4.w = vinsert(r18)
		v11:10.uw = vunpack(v20.uh)
		r25:24 = memd(r5+#40)
	}
	{
		v5.w = vinsert(r16)
		v13:12.uw = vunpack(v22.uh)
		v3 = vmux(q0,v3,v10)
	}
	{
		v0 = vror(v0,r28)
	}
	{
		r1 = add(r30,#-15280)
		v0 = vor(v0,v7)
		vmemu(r1+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v3 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r19)
		v4 = valign(v5,v5,#4)
		r19:18 = memd(r12+#8)
	}
	{
		v4.w = vinsert(r17)
		v5 = valign(v6,v6,#4)
		r17:16 = memd(r5+#56)
	}
	{
		v5.w = vinsert(r21)
		v25:24.uh = vunpack(v0.ub)
		v0 = v8
		r21:20 = memd(r12+#16)
	}
	{
		v0.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r10)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r24)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r12+#24)
	}
	{
		v0.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
		r3:2 = memd(r12+#48)
	}
	{
		v4.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
		r11:10 = memd(r13+#8)
	}
	{
		v5.w = vinsert(r25)
		v3 = vror(v3,r28)
		r25:24 = memd(r12+#40)
	}
	{
		v0 = valign(v0,v0,#4)
		v3 = vor(v3,v7)
	}
	{
		v0.w = vinsert(r16)
		v4 = vror(v4,r28)
	}
	{
		v11:10.uw = vunpack(v24.uh)
		v4 = vor(v4,v7)
	}
	{
		v5 = vror(v5,r28)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r17)
		v27:26.uh = vunpack(v3.ub)
		v3 = vor(v5,v7)
		r17:16 = memd(r12+#32)
	}
	{
		v5:4.uh = vunpack(v4.ub)
	}
	{
		v6 = vdelta(v10,v31)
	}
	{
		v5:4.uw = vunpack(v4.uh)
		v6 = vmux(q0,v6,v12)
	}
	{
		v0 = vror(v0,r28)
		v5 = v8
	}
	{
		v5.w = vinsert(r22)
		v6 = v8
		vmemu(r1+#0) = v6
	}                                       // 128-byte Folded Spill
	{
		r1 = asl(r0,#7)
		v6.w = vinsert(r20)
		v29:28.uh = vunpack(v3.ub)
	}
	{
		v11:10.uw = vunpack(v26.uh)
		r0 = memw(r30+##-22256)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r4,r0)
		v3 = vdelta(v4,v31)
		v4 = vor(v0,v7)
		v11 = v8
	}
	{
		v11.w = vinsert(r16)
		v0 = vmux(q0,v3,v10)
		v3 = v8
	}
	{
		v3.w = vinsert(r2)
		v21:20.uh = vunpack(v4.ub)
		r2 = memw(r30+##-22248)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r4,r2)
		v4 = valign(v5,v5,#4)
	}
	{
		r1 = asl(r2,#7)
		v4.w = vinsert(r23)
		v5 = valign(v6,v6,#4)
		r23:22 = memd(r26+r1<<#0)
	}
	{
		v5.w = vinsert(r21)
		v6 = valign(v11,v11,#4)
		r21:20 = memd(r12+#56)
	}
	{
		r4 = addasl(r26,r2,#7)
		v6.w = vinsert(r17)
		r17:16 = combine(r4,r26)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r18)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r19)
		v3 = valign(v3,v3,#4)
		r19:18 = memd(r13+#32)
	}
	{
		v3.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r13+#40)
	}
	{
		r1 = asl(r0,#7)
		v5.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r26+r1<<#0)
	}
	{
		v6.w = vinsert(r25)
		v4 = vror(v4,r28)
		r25:24 = memd(r13+#24)
		r27:26 = memd(r4+#16)
	}
	{
		v3 = valign(v3,v3,#4)
		v4 = vor(v4,v7)
	}
	{
		v3.w = vinsert(r20)
		v5 = vror(v5,r28)
	}
	{
		v6 = vror(v6,r28)
		v5 = vor(v5,v7)
	}
	{
		v15:14.uw = vunpack(v20.uh)
	}
	{
		v23:22.uh = vunpack(v4.ub)
		v4 = vor(v6,v7)
		v6 = v8
	}
	{
		v6.w = vinsert(r22)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r21)
		v25:24.uh = vunpack(v5.ub)
		r21:20 = memd(r13+#48)
	}
	{
		v5:4.uh = vunpack(v4.ub)
	}
	{
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v10 = vdelta(v14,v31)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		v27 = vmux(q0,v10,v12)
	}
	{
		v3 = vror(v3,r28)
		v13 = v8
	}
	{
		v5:4.uw = vunpack(v4.uh)
		v3 = vor(v3,v7)
	}
	{
		v11:10.uw = vunpack(v22.uh)
	}
	{
		v5 = vdelta(v12,v31)
		v11 = v8
	}
	{
		v11.w = vinsert(r14)
		v5 = valign(v6,v6,#4)
		v12 = vmux(q0,v5,v10)
	}
	{
		v5.w = vinsert(r23)
		v29:28.uh = vunpack(v3.ub)
		v3 = v8
		r23:22 = memd(r4+#8)
	}
	{
		v3.w = vinsert(r18)
		v6 = valign(v11,v11,#4)
		v11 = v8
	}
	{
		v6.w = vinsert(r15)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r10)
		v21:20.uw = vunpack(v28.uh)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r19)
		v10 = vdelta(v20,v31)
		r19:18 = memd(r13+#56)
	}
	{
		v6 = valign(v6,v6,#4)
		v9 = vmux(q0,v10,v4)
		v10 = v8
	}
	{
		v6.w = vinsert(r24)
		v10.w = vinsert(r6)
		v4 = valign(v5,v5,#4)
		v5 = v8
	}
	{
		v5.w = vinsert(r20)
		v4.w = vinsert(r11)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
		r25:24 = memd(r4+#40)
	}
	{
		v5.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v6 = vror(v6,r28)
		r3:2 = memd(r4+#48)
	}
	{
		v11.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
		v6 = vor(v6,v7)
	}
	{
		v5.w = vinsert(r18)
		v4 = vror(v4,r28)
		r2 = memw(r30+##-22264)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r17,r2)
		v3 = vror(v3,r28)
		v4 = vor(v4,v7)
	}
	{
		v5 = valign(v5,v5,#4)
		v3 = vor(v3,v7)
	}
	{
		v5.w = vinsert(r19)
		v23:22.uh = vunpack(v6.ub)
		r19:18 = memd(r4+#32)
	}
	{
		v17:16.uh = vunpack(v4.ub)
	}
	{
		v25:24.uw = vunpack(v22.uh)
	}
	{
		v15:14.uh = vunpack(v3.ub)
	}
	{
		v3 = vror(v5,r28)
		v5 = v8
	}
	{
		r26 = addasl(r16,r2,#7)
		v5.w = vinsert(r26)
		v29:28.uw = vunpack(v16.uh)
		v3 = vor(v3,v7)
	}
	{
		v6 = vdelta(v24,v31)
	}
	{
		v10 = valign(v10,v10,#4)
		v21 = vmux(q0,v6,v28)
		v6 = v8
		r11:10 = memd(r26+#8)
	}
	{
		v10.w = vinsert(r7)
		v6.w = vinsert(r18)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r4+#24)
	}
	{
		r27 = addasl(r16,r0,#7)
		v5.w = vinsert(r27)
		v17:16.uh = vunpack(v3.ub)
	}
	{
		v4 = valign(v10,v10,#4)
	}
	{
		v4.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
		r21:20 = memd(r27+#16)
		r15:14 = memd(r27+#32)
	}
	{
		v6.w = vinsert(r19)
		v10 = valign(v11,v11,#4)
		r19:18 = memd(r4+#56)
	}
	{
		v10.w = vinsert(r3)
		v3 = valign(v5,v5,#4)
	}
	{
		r6 = asl(r2,#7)
		v3.w = vinsert(r6)
		v5 = valign(v6,v6,#4)
	}
	{
		v5.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r23)
		v6 = valign(v10,v10,#4)
		r23:22 = memd(r16+r1<<#0)
		r1:0 = memd(r27+#8)
	}
	{
		v6.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r27+#40)
	}
	{
		v3.w = vinsert(r7)
		r7 = r16
		v4 = vror(v4,r28)
		r17:16 = memd(r27+#48)
	}
	{
		v5 = valign(v5,v5,#4)
		v4 = vor(v4,v7)
	}
	{
		v5.w = vinsert(r25)
		v6 = valign(v6,v6,#4)
		r25:24 = memd(r27+#24)
	}
	{
		v6.w = vinsert(r19)
		v3 = vror(v3,r28)
		r19:18 = memd(r27+#56)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v3 = vor(v3,v7)
	}
	{
		v25:24.uh = vunpack(v4.ub)
	}
	{
		v4 = vror(v5,r28)
	}
	{
		v5 = vror(v6,r28)
		v4 = vor(v4,v7)
		v6 = v8
	}
	{
		v6.w = vinsert(r22)
		v10 = vdelta(v16,v31)
		v5 = vor(v5,v7)
	}
	{
		v23:22.uw = vunpack(v14.uh)
	}
	{
		v29:28.uh = vunpack(v3.ub)
		v23 = vmux(q0,v10,v22)
	}
	{
		v11:10.uw = vunpack(v24.uh)
	}
	{
		v17:16.uh = vunpack(v4.ub)
		v11 = v8
	}
	{
		v11.w = vinsert(r20)
		r20 = r7
		v5:4.uh = vunpack(v5.ub)
	}
	{
		v15:14.uw = vunpack(v28.uh)
	}
	{
		v25:24.uw = vunpack(v4.uh)
	}
	{
		v3 = vdelta(v14,v31)
	}
	{
		v11 = valign(v11,v11,#4)
		v4 = vmux(q0,v3,v10)
		v10 = v8
	}
	{
		v10.w = vinsert(r16)
		v11.w = vinsert(r21)
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v3 = vdelta(v24,v31)
		r16 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		v6 = valign(v6,v6,#4)
		v5 = vmux(q0,v3,v16)
		v3 = v8
	}
	{
		v6.w = vinsert(r23)
		v3.w = vinsert(r14)
		v10 = valign(v10,v10,#4)
		r23:22 = memd(r7+r6<<#0)
	}
	{
		v10.w = vinsert(r17)
		v13.w = vinsert(r22)
		v11 = valign(v11,v11,#4)
		r7:6 = memd(r26+#16)
	}
	{
		v11.w = vinsert(r24)
		v3 = valign(v3,v3,#4)
		r17 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r15)
		v6 = valign(v6,v6,#4)
		r15:14 = memd(r13+#88)
	}
	{
		v6.w = vinsert(r0)
		v10 = valign(v10,v10,#4)
		r0 = memw(r30+##-22272)
	}                                       // 4-byte Folded Reload
	{
		v10.w = vinsert(r18)
		r21 = add(r16,r0)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r25)
		v3 = valign(v3,v3,#4)
		r25:24 = memd(r17+#80)
	}
	{
		v3.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
		r1:0 = memd(r26+#24)
	}
	{
		v10.w = vinsert(r19)
		v11 = vror(v11,r28)
		r19:18 = memd(r26+#48)
	}
	{
		v3 = valign(v3,v3,#4)
		v11 = vor(v11,v7)
	}
	{
		v3.w = vinsert(r3)
		v6 = vror(v6,r28)
		r3:2 = memd(r26+#32)
	}
	{
		v10 = vror(v10,r28)
		v6 = vor(v6,v7)
	}
	{
		v29:28.uh = vunpack(v11.ub)
		v10 = vor(v10,v7)
		v11 = v8
	}
	{
		v11.w = vinsert(r2)
		r2 = add(r30,#-2608)
		v3 = vror(v3,r28)
	}
	{
		v17:16.uh = vunpack(v6.ub)
		v3 = vor(v3,v7)
	}
	{
		v15:14.uw = vunpack(v28.uh)
	}
	{
		v25:24.uh = vunpack(v10.ub)
	}
	{
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r23)
		v17:16.uw = vunpack(v16.uh)
		r23:22 = memd(r26+#56)
	}
	{
		v10 = vdelta(v14,v31)
	}
	{
		v17:16.uh = vunpack(v3.ub)
		v10 = vmux(q0,v10,v16)
		v3 = v8
	}
	{
		v3.w = vinsert(r6)
		v29:28.uw = vunpack(v24.uh)
	}
	{
		v6 = valign(v13,v13,#4)
	}
	{
		v6.w = vinsert(r10)
		v13 = vdelta(v28,v31)
	}
	{
		v15:14.uw = vunpack(v16.uh)
	}
	{
		r2 = add(r30,#-2480)
		v13 = vmux(q0,v13,v14)
		v14 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-4656)
		v15 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w = vmpyieo(v1.h,v14.h)
		r2 = add(r30,#-4528)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w += vmpyie(v1.w,v14.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r11)
		v15.w = vmpyieo(v0.h,v18.h)
		v3 = valign(v3,v3,#4)
		r11:10 = memd(r26+#40)
	}
	{
		v3.w = vinsert(r7)
		r2 = add(r30,#-2096)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w += vmpyie(v0.w,v18.h)
		r2 = add(r30,#-1968)
		v0 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v6 = vror(v6,r28)
		r7:6 = memd(r17+#64)
	}
	{
		v16.w = vmpyieo(v2.h,v0.h)
		v3 = valign(v3,v3,#4)
		v6 = vor(v6,v7)
	}
	{
		v3.w = vinsert(r0)
		r2 = add(r30,#-11568)
		v1 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v16.w += vmpyie(v2.w,v0.h)
		r0 = add(r30,#-9264)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9136)
		r2 = add(r30,#-11440)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w = vmpyieo(v27.h,v18.h)
		v0 = valign(v3,v3,#4)
	}
	{
		v0.w = vinsert(r1)
		v25:24.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r18)
		r0 = add(r30,#-12336)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w += vmpyie(v27.w,v18.h)
		r0 = add(r30,#-12208)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v11 = valign(v11,v11,#4)
	}
	{
		v3.w = vmpyieo(v10.h,v18.h)
		v11.w = vinsert(r3)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w += vmpyie(v10.w,v18.h)
		r2 = add(r30,#-9776)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r19)
		r0 = add(r30,#-7984)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-7856)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v11.w = vmpyieo(v4.h,v2.h)
		v1 = valign(v11,v11,#4)
	}
	{
		v11.w += vmpyie(v4.w,v2.h)
		r0 = add(r30,#-12080)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vinsert(r10)
		v10.w = vmpyieo(v5.h,v18.h)
		v0 = vror(v0,r28)
	}
	{
		v10.w += vmpyie(v5.w,v18.h)
		v4 = valign(v6,v6,#4)
		v0 = vor(v0,v7)
	}
	{
		v4.w = vinsert(r22)
		v27:26.uw = vunpack(v24.uh)
	}
	{
		r0 = add(r30,#-11952)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vmpyieo(v13.h,v24.h)
		r2 = add(r30,#-9648)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w += vmpyie(v13.w,v24.h)
		v25:24.uh = vunpack(v0.ub)
		r1:0 = memd(r17+#72)
	}
	{
		v5.w = vmpyieo(v21.h,v18.h)
		v0 = valign(v4,v4,#4)
		v22 = v18
	}
	{
		v0.w = vinsert(r23)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r11)
		r2 = add(r30,#-9520)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w += vmpyie(v21.w,v22.h)
		v0 = vror(v0,r28)
	}
	{
		r2 = add(r30,#-9392)
		v0 = vor(v0,v7)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1 = vror(v1,r28)
		v20 = v18
	}
	{
		v13.w = vmpyieo(v12.h,v18.h)
		v1 = vor(v1,v7)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v12.w,v20.h)
		r2 = add(r30,#-8752)
		v19:18.uh = vunpack(v0.ub)
	}
	{
		v29:28.uw = vunpack(v24.uh)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v25:24.uh = vunpack(v1.ub)
	}
	{
		v0 = vdelta(v28,v31)
	}
	{
		v1 = vdelta(v18,v31)
		v0 = vmux(q0,v0,v26)
	}
	{
		v19:18.uw = vunpack(v24.uh)
	}
	{
		r2 = add(r30,#-8624)
		v4 = vmux(q0,v1,v18)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-10032)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vmpyieo(v0.h,v18.h)
	}
	{
		v1.w += vmpyie(v0.w,v18.h)
		r2 = add(r30,#-9904)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-7216)
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v4.h,v18.h)
	}
	{
		v0.w += vmpyie(v4.w,v18.h)
		r2 = add(r30,#-7088)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v4 = v8
		v19 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vinsert(r24)
		v1:0.w = vadd(v1:0.w,v19:18.w)
		r3:2 = memd(r17+#112)
	}
	{
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v2 = v8
	}
	{
		v2.w = vinsert(r2)
		r2 = add(r30,#-1584)
		v3 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r25)
		v1:0.w = vadd(v11:10.w,v1:0.w)
		r25:24 = memd(r5+#80)
	}
	{
		r2 = add(r30,#-1456)
		v10 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v2 = valign(v2,v2,#4)
	}
	{
		v4.w = vmpyieo(v23.h,v10.h)
		v2.w = vinsert(r3)
		v11 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w += vmpyie(v23.w,v10.h)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r17+#88)
	}
	{
		v3.w = vinsert(r2)
		r2 = add(r30,#-1328)
		v1:0.w = vadd(v1:0.w,v5:4.w)
		r19:18 = memd(r17+#120)
	}
	{
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r18)
		r2 = add(r30,#-1200)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v12.w = vmpyieo(v9.h,v4.h)
		v3.w = vinsert(r3)
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v9.w,v4.h)
		v4 = valign(v2,v2,#4)
		r3:2 = memd(r17+#96)
	}
	{
		v4.w = vinsert(r19)
		v1:0.w = vadd(v1:0.w,v13:12.w)
	}
	{
		v5 = vror(v3,r28)
		v25:24.w = vadd(v15:14.w,v1:0.w)
		v1 = v8
	}
	{
		v1.w = vinsert(r6)
		v4 = vror(v4,r28)
		v0 = vor(v5,v7)
	}
	{
		v4 = vor(v4,v7)
	}
	{
		v15:14.uh = vunpack(v0.ub)
	}
	{
		v5:4.uh = vunpack(v4.ub)
	}
	{
		v0 = valign(v1,v1,#4)
		v1 = v8
	}
	{
		v1.w = vinsert(r2)
		v0.w = vinsert(r7)
		v5:4.uw = vunpack(v4.uh)
		r7:6 = memd(r8+#80)
	}
	{
		v11:10.uw = vunpack(v14.uh)
		v5 = v8
	}
	{
		v5.w = vinsert(r6)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r3)
		v0 = valign(v0,v0,#4)
		r3:2 = memd(r8+#112)
	}
	{
		v0.w = vinsert(r0)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		v6 = vdelta(v10,v31)
		v10 = v8
		r7:6 = memd(r17+#104)
	}
	{
		v10.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r6)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r3)
		v1 = valign(v1,v1,#4)
		r3:2 = memd(r8+#88)
	}
	{
		v1.w = vinsert(r7)
		v5 = valign(v5,v5,#4)
		r1:0 = memd(r8+#120)
		r7:6 = memd(r9+#80)
	}
	{
		v5.w = vinsert(r2)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r0)
		v0 = vror(v0,r28)
	}
	{
		v1 = vror(v1,r28)
		v0 = vor(v0,v7)
	}
	{
		v5 = valign(v5,v5,#4)
		v1 = vor(v1,v7)
	}
	{
		v5.w = vinsert(r3)
		v10 = valign(v10,v10,#4)
		r3:2 = memd(r8+#64)
	}
	{
		v10.w = vinsert(r1)
		v21:20.uh = vunpack(v0.ub)
		r1:0 = memd(r8+#96)
	}
	{
		v23:22.uh = vunpack(v1.ub)
		v1 = v8
	}
	{
		v1.w = vinsert(r2)
		v13:12.uw = vunpack(v20.uh)
	}
	{
		v0 = vror(v10,r28)
	}
	{
		v11:10.uw = vunpack(v22.uh)
		v22 = vmux(q0,v6,v12)
		v6 = v8
	}
	{
		v6.w = vinsert(r0)
		v4 = vdelta(v4,v31)
		v12 = v8
		v0 = vor(v0,v7)
	}
	{
		v1 = valign(v1,v1,#4)
		v20 = vmux(q0,v4,v10)
		v10 = v8
	}
	{
		v1.w = vinsert(r3)
		v10.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r9+#112)
	}
	{
		v12.w = vinsert(r2)
		v6.w = vinsert(r1)
		v5 = vror(v5,r28)
		r1:0 = memd(r8+#72)
	}
	{
		v1 = valign(v1,v1,#4)
		v5 = vor(v5,v7)
	}
	{
		v1.w = vinsert(r0)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r7)
		v12 = valign(v12,v12,#4)
		r7:6 = memd(r8+#104)
	}
	{
		v12.w = vinsert(r3)
		v1 = valign(v1,v1,#4)
		r3:2 = memd(r9+#88)
	}
	{
		v1.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
		r23:22 = memd(r9+#120)
		r1:0 = memd(r9+#64)
	}
	{
		v10.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r22)
		v27:26.uh = vunpack(v5.ub)
	}
	{
		v1 = vror(v1,r28)
	}
	{
		v10 = valign(v10,v10,#4)
		v1 = vor(v1,v7)
	}
	{
		v10.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r5+#112)
	}
	{
		v6.w = vinsert(r7)
		v12 = valign(v12,v12,#4)
		r7:6 = memd(r9+#96)
	}
	{
		v12.w = vinsert(r23)
		v29:28.uw = vunpack(v26.uh)
	}
	{
		v19:18.uh = vunpack(v0.ub)
	}
	{
		v0 = vdelta(v28,v31)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v1 = vror(v10,r28)
	}
	{
		v10 = vror(v12,r28)
		v1 = vor(v1,v7)
	}
	{
		v6 = vror(v6,r28)
		v10 = vor(v10,v7)
	}
	{
		v27:26.uw = vunpack(v18.uh)
		v6 = vor(v6,v7)
	}
	{
		v19:18.uh = vunpack(v1.ub)
		v1 = v8
	}
	{
		v1.w = vinsert(r0)
		v3:2.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r6)
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v10.ub)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v13:12.uw = vunpack(v28.uh)
		v19 = vmux(q0,v0,v12)
	}
	{
		v11 = vdelta(v26,v31)
		v13 = v8
	}
	{
		v13.w = vinsert(r2)
		v15:14.uw = vunpack(v2.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		v10 = vmux(q0,v11,v14)
		v11 = v8
	}
	{
		v11.w = vinsert(r24)
		v6.w = vinsert(r7)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r5+#88)
	}
	{
		v1.w = vinsert(r1)
		v13 = valign(v13,v13,#4)
		r1:0 = memd(r9+#72)
		r11:10 = memd(r5+#120)
	}
	{
		v13.w = vinsert(r3)
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r9+#104)
	}
	{
		v11.w = vinsert(r25)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r0)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v13 = valign(v13,v13,#4)
		r3:2 = memd(r5+#64)
	}
	{
		v13.w = vinsert(r10)
		v11 = valign(v11,v11,#4)
		r19:18 = memd(r5+#96)
	}
	{
		v11.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r12+#64)
	}
	{
		v1 = valign(v1,v1,#4)
		v6 = vor(v6,v7)
	}
	{
		v1.w = vinsert(r1)
		v11 = vror(v11,r28)
		r1:0 = memd(r12+#80)
	}
	{
		v27:26.uh = vunpack(v6.ub)
		v11 = vor(v11,v7)
		v6 = v8
		r23:22 = memd(r12+#112)
	}
	{
		v6.w = vinsert(r2)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r11)
		v1 = vror(v1,r28)
	}
	{
		v3:2.uh = vunpack(v11.ub)
		v1 = vor(v1,v7)
	}
	{
		v12 = vdelta(v12,v31)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v13 = vror(v13,r28)
		v21 = vmux(q0,v12,v26)
		v12 = v8
	}
	{
		v15:14.uh = vunpack(v1.ub)
		v1 = vor(v13,v7)
		v13 = v8
	}
	{
		v12.w = vinsert(r18)
		v13.w = vinsert(r0)
		r18 = r20
		v29:28.uw = vunpack(v2.uh)
	}
	{
		v0 = vdelta(v18,v31)
		v18 = v8
	}
	{
		v31 = vdelta(v28,v31)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v12,v12,#4)
		v12 = v8
	}
	{
		v12.w = vinsert(r6)
		v1.w = vinsert(r19)
		v11 = valign(v6,v6,#4)
	}
	{
		v11.w = vinsert(r3)
		v6 = valign(v13,v13,#4)
		v13 = v8
		r3:2 = memd(r13+#80)
	}
	{
		v6.w = vinsert(r1)
		v13.w = vinsert(r22)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r5+#72)
	}
	{
		v12.w = vinsert(r7)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r12+#88)
		r25:24 = memd(r5+#104)
	}
	{
		v1.w = vinsert(r24)
		v6 = valign(v6,v6,#4)
		r22 = memw(r30+##-23232)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r25)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v15:14.uw = vunpack(v14.uh)
		r7:6 = memd(r12+#72)
	}
	{
		v12.w = vinsert(r6)
		v11 = valign(v11,v11,#4)
		v23 = vmux(q0,v0,v14)
	}
	{
		v11.w = vinsert(r0)
		v15:14.uw = vunpack(v28.uh)
	}
	{
		v12 = valign(v12,v12,#4)
		v15 = v8
	}
	{
		v12.w = vinsert(r7)
		v15.w = vinsert(r2)
		v13 = valign(v13,v13,#4)
		r7:6 = memd(r13+#72)
	}
	{
		v13.w = vinsert(r23)
		v6 = vror(v6,r28)
		r23 = memw(r30+##-23240)
	}                                       // 4-byte Folded Reload
	{
		v1 = vror(v1,r28)
		v6 = vor(v6,v7)
	}
	{
		v12 = vror(v12,r28)
		v1 = vor(v1,v7)
	}
	{
		v11 = valign(v11,v11,#4)
		v12 = vor(v12,v7)
	}
	{
		v11.w = vinsert(r1)
		v15 = valign(v15,v15,#4)
		r1:0 = memd(r12+#120)
	}
	{
		v15.w = vinsert(r3)
		v13 = valign(v13,v13,#4)
		r3:2 = memd(r13+#64)
	}
	{
		v13.w = vinsert(r0)
		v18.w = vinsert(r2)
		r0 = add(r30,#-1072)
		v3:2.uh = vunpack(v6.ub)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v1:0.uh = vunpack(v12.ub)
	}
	{
		v27:26.uw = vunpack(v2.uh)
	}
	{
		r0 = add(r30,#-944)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r14)
		v11 = vror(v11,r28)
	}
	{
		r0 = add(r30,#-15152)
		v11 = vor(v11,v7)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1:0.uw = vunpack(v0.uh)
	}
	{
		r0 = add(r30,#-816)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-688)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6 = valign(v15,v15,#4)
	}
	{
		v6.w = vinsert(r15)
		v15 = vdelta(v26,v30)
	}
	{
		v27:26.uh = vunpack(v11.ub)
		v11 = v8
	}
	{
		r0 = add(r30,#-14896)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r3)
		v3 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v27:26.uw = vunpack(v26.uh)
		r3:2 = memd(r13+#112)
	}
	{
		v11.w = vinsert(r2)
		r2 = add(r30,#-560)
		v29:28.uw = vunpack(v28.uh)
	}
	{
		v15.w = vmpyieo(v3.h,v4.h)
		v0 = valign(v13,v13,#4)
		v29 = vmux(q0,v31,v26)
		v31 = vmux(q0,v15,v0)
	}
	{
		v15.w += vmpyie(v3.w,v4.h)
		r2 = add(r30,#-432)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w = vmpyieo(v1.h,v2.h)
		r2 = add(r30,#-15280)
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v1.w,v2.h)
		r2 = add(r30,#-304)
		v5 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r1)
		r2 = add(r30,#-176)
		v2 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v5.h,v4.h)
		r2 = add(r30,#-15024)
		v3 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v5.w,v4.h)
		v6 = vror(v6,r28)
		v26 = v8
		r1:0 = memd(r13+#120)
	}
	{
		v13:12.w = vadd(v25:24.w,v13:12.w)
		v6 = vor(v6,v7)
		v3 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v14 = vdelta(v14,v30)
	}
	{
		v14.w = vmpyieo(v3.h,v2.h)
		v1 = valign(v18,v18,#4)
		v27 = vmux(q0,v14,v28)
	}
	{
		v14.w += vmpyie(v3.w,v2.h)
		v3:2.uh = vunpack(v6.ub)
	}
	{
		v1.w = vinsert(r6)
		v3 = valign(v11,v11,#4)
		v6 = v8
	}
	{
		v3.w = vinsert(r3)
		v5:4.uw = vunpack(v2.uh)
		r3:2 = memd(r4+#88)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r7)
		v2 = valign(v3,v3,#4)
		r7:6 = memd(r4+#80)
	}
	{
		v2.w = vinsert(r0)
		v0 = vror(v0,r28)
		r0 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r6)
		v1 = vror(v1,r28)
		v0 = vor(v0,v7)
	}
	{
		v2 = valign(v2,v2,#4)
		v1 = vor(v1,v7)
	}
	{
		v3 = vdelta(v4,v30)
		v5:4.w = vadd(v13:12.w,v15:14.w)
	}
	{
		v2.w = vinsert(r1)
		v5:4.w = vadd(v17:16.w,v5:4.w)
		vmem(r0+#4) = v4.new
	}
	{
		v25:24.uh = vunpack(v1.ub)
		vmem(r0+#5) = v5
	}
	{
		v5:4.uh = vunpack(v0.ub)
		r1:0 = memd(r13+#96)
	}
	{
		v0 = valign(v6,v6,#4)
	}
	{
		v0.w = vinsert(r7)
		v1 = vror(v2,r28)
		r7:6 = memd(r4+#64)
	}
	{
		v5:4.uw = vunpack(v4.uh)
		v2 = vor(v1,v7)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		v5 = v8
	}
	{
		v5.w = vinsert(r0)
		v6 = valign(v0,v0,#4)
		v1 = vmux(q0,v3,v12)
	}
	{
		v6.w = vinsert(r2)
		v3:2.uh = vunpack(v2.ub)
	}
	{
		v0 = vdelta(v4,v30)
		v3 = v8
	}
	{
		v3.w = vinsert(r6)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r1)
		v5 = valign(v6,v6,#4)
		v6 = v8
		r1:0 = memd(r13+#104)
	}
	{
		r13 = addasl(r20,r21,#7)
		v5.w = vinsert(r3)
		v25:24.uw = vunpack(v2.uh)
		r3:2 = memd(r4+#112)
	}
	{
		v6.w = vinsert(r2)
		v2 = valign(v3,v3,#4)
	}
	{
		v2.w = vinsert(r7)
		v3 = valign(v4,v4,#4)
		r7:6 = memd(r4+#72)
	}
	{
		v3.w = vinsert(r0)
		v4 = vror(v5,r28)
	}
	{
		v2 = valign(v2,v2,#4)
		v4 = vor(v4,v7)
	}
	{
		v2.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v5 = vdelta(v24,v30)
		r1:0 = memd(r4+#104)
	}
	{
		v25:24.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r27+#80)
	}
	{
		v2.w = vinsert(r7)
		v26.w = vinsert(r2)
		v3 = vror(v3,r28)
		r7:6 = memd(r4+#120)
	}
	{
		v4 = valign(v4,v4,#4)
		v3 = vor(v3,v7)
	}
	{
		v4.w = vinsert(r6)
		v2 = vror(v2,r28)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		v2 = vor(v2,v7)
	}
	{
		v25:24.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r7)
		v6 = vdelta(v12,v30)
		r7:6 = memd(r4+#96)
		r5:4 = memd(r26+#80)
	}
	{
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v2.ub)
		v2 = v8
	}
	{
		v2.w = vinsert(r6)
		v4 = valign(v26,v26,#4)
	}
	{
		v4.w = vinsert(r3)
		v3 = vror(v3,r28)
		r3:2 = memd(r27+#88)
	}
	{
		v11 = valign(v2,v2,#4)
		v2 = vmux(q0,v5,v12)
		v25 = vor(v3,v7)
		v5 = v8
	}
	{
		v11.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r27+#64)
	}
	{
		v4.w = vinsert(r2)
		v5.w = vinsert(r6)
		v15:14.uw = vunpack(v24.uh)
	}
	{
		v6 = valign(v11,v11,#4)
		v3 = vmux(q0,v6,v14)
		v11 = v8
	}
	{
		v6.w = vinsert(r0)
		v11.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r27+#112)
	}
	{
		v5.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r27+#72)
	}
	{
		v6.w = vinsert(r1)
		v4 = vror(v4,r28)
		r1:0 = memd(r27+#120)
	}
	{
		v25:24.uh = vunpack(v25.ub)
		v4 = vor(v4,v7)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r6)
		v6 = vror(v6,r28)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		v6 = vor(v6,v7)
	}
	{
		v25:24.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v11,v11,#4)
	}
	{
		v4.w = vinsert(r5)
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r26+#88)
	}
	{
		v5.w = vinsert(r7)
		v26 = vdelta(v12,v30)
		r7:6 = memd(r26+#64)
	}
	{
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r4)
		v11 = vdelta(v12,v30)
	}
	{
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v5 = vror(v5,r28)
		v13 = v8
		v9 = vmux(q0,v26,v12)
	}
	{
		v13.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
		v5 = vor(v5,v7)
	}
	{
		v4.w = vinsert(r5)
		v6 = valign(v6,v6,#4)
		r5:4 = memd(r26+#96)
	}
	{
		v6.w = vinsert(r3)
		v25:24.uh = vunpack(v5.ub)
		r3:2 = memd(r27+#96)
	}
	{
		v5 = valign(v13,v13,#4)
	}
	{
		v5.w = vinsert(r7)
		v4 = vror(v4,r28)
		r7:6 = memd(r26+#72)
	}
	{
		v6 = valign(v6,v6,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r0)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r6)
		v15:14.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v4.ub)
		v11 = vmux(q0,v11,v14)
		v14 = v8
	}
	{
		v14.w = vinsert(r4)
		v6 = valign(v6,v6,#4)
		v4 = v8
	}
	{
		v6.w = vinsert(r1)
		v4.w = vinsert(r2)
		v13:12.uw = vunpack(v24.uh)
		r1:0 = memd(r26+#120)
	}
	{
		v5 = valign(v5,v5,#4)
		v13 = v8
	}
	{
		v5.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r26+#112)
	}
	{
		v13.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
		v6 = vor(v6,v7)
	}
	{
		v4.w = vinsert(r3)
		v5 = vror(v5,r28)
		r3:2 = memd(r27+#104)
	}
	{
		v13 = valign(v13,v13,#4)
		v5 = vor(v5,v7)
	}
	{
		v13.w = vinsert(r7)
		v17:16.uh = vunpack(v6.ub)
		r7:6 = memd(r12+#96)
	}
	{
		v6 = valign(v14,v14,#4)
	}
	{
		v6.w = vinsert(r5)
		v25:24.uh = vunpack(v5.ub)
		r5:4 = memd(r26+#104)
	}
	{
		v5 = valign(v13,v13,#4)
	}
	{
		v5.w = vinsert(r0)
		r0 = add(r30,#-9008)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		r2 = add(r30,#-2864)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
		v12 = vdelta(v12,v30)
		r5:4 = memd(r13+#16)
	}
	{
		v5 = vror(v5,r28)
	}
	{
		v15:14.uw = vunpack(v24.uh)
		v5 = vor(v5,v7)
	}
	{
		v4 = valign(v4,v4,#4)
		v12 = vmux(q0,v12,v14)
		v14 = v8
	}
	{
		v14.w = vinsert(r6)
		v4.w = vinsert(r3)
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v13 = vdelta(v16,v30)
		v6 = vor(v6,v7)
	}
	{
		v17:16.uh = vunpack(v5.ub)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r7)
		v4 = vror(v4,r28)
		r7:6 = memd(r12+#104)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v4 = vor(v4,v7)
	}
	{
		v25:24.uh = vunpack(v6.ub)
	}
	{
		v5 = valign(v14,v14,#4)
	}
	{
		v5.w = vinsert(r6)
		v15:14.uh = vunpack(v4.ub)
	}
	{
		v4 = vdelta(v16,v30)
	}
	{
		v17:16.uw = vunpack(v24.uh)
	}
	{
		r0 = add(r30,#-8880)
		v4 = vmux(q0,v4,v16)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-10288)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-10160)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15:14.uw = vunpack(v14.uh)
	}
	{
		v13.w = vmpyieo(v12.h,v16.h)
		v6 = vmux(q0,v13,v14)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v12.w,v16.h)
		r0 = add(r30,#-11824)
		v5 = valign(v5,v5,#4)
	}
	{
		v15.w = vmpyieo(v3.h,v24.h)
		r0 = add(r30,#-11696)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w += vmpyie(v3.w,v24.h)
		r0 = add(r30,#-8496)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vinsert(r7)
		r0 = add(r30,#-8368)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w = vmpyieo(v11.h,v16.h)
		r0 = add(r30,#-12592)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w = vmpyieo(v9.h,v24.h)
		v3 = vror(v5,r28)
		r7:6 = memd(r13+#48)
	}
	{
		v14.w += vmpyie(v9.w,v24.h)
		r0 = add(r30,#-12464)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-7728)
		v3 = vor(v3,v7)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v4.h,v24.h)
		v26 = v24
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w += vmpyie(v11.w,v16.h)
		r0 = add(r30,#-7600)
		v18 = v24
	}
	{
		v16.w = vmpyieo(v6.h,v24.h)
		r0 = add(r30,#-6960)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v4.w,v26.h)
		r0 = add(r30,#-6832)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v16.w += vmpyie(v6.w,v18.h)
		v25:24.uh = vunpack(v3.ub)
	}
	{
		r0 = add(r30,#-5424)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13:12.uw = vunpack(v24.uh)
		v5:4.w = vadd(v13:12.w,v5:4.w)
	}
	{
		r0 = add(r30,#-5296)
		v5:4.w = vadd(v5:4.w,v17:16.w)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5680)
		v5:4.w = vadd(v15:14.w,v5:4.w)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5552)
		v0 = vmux(q0,v0,v12)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5168)
		v6 = v14
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w = vmpyieo(v31.h,v14.h)
		r0 = add(r30,#-5040)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w = vmpyieo(v1.h,v24.h)
		r0 = add(r30,#-4400)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v2.h,v14.h)
	}
	{
		v12.w += vmpyie(v2.w,v14.h)
		r0 = add(r30,#-4272)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v1.w,v24.h)
		r0 = add(r30,#-8240)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w += vmpyie(v31.w,v6.h)
		r0 = add(r30,#-8112)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-3376)
		v5:4.w = vadd(v5:4.w,v13:12.w)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vmpyieo(v0.h,v16.h)
		v15.w = vmpyieo(v29.h,v14.h)
		v12 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w += vmpyie(v0.w,v16.h)
		r2 = add(r30,#-2736)
		v0 = v8
		v6 = v14
	}
	{
		r0 = add(r30,#-3248)
		v3:2.w = vadd(v5:4.w,v3:2.w)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vinsert(r4)
		r0 = add(r30,#-2352)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = asl(r21,#7)
		v15.w += vmpyie(v29.w,v14.h)
		v13 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w = vmpyieo(v27.h,v12.h)
		v0 = valign(v0,v0,#4)
		v6 = v8
		r3:2 = memd(r20+r2<<#0)
	}
	{
		v14.w += vmpyie(v27.w,v12.h)
		r0 = add(r30,#-2224)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r2)
		v0.w = vinsert(r5)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vmpyieo(v22.h,v12.h)
		v5.w = vmpyieo(v23.h,v4.h)
		r0 = add(r30,#-3888)
		v24 = v8
	}
	{
		v1.w += vmpyie(v22.w,v12.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		r0 = add(r30,#-3760)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w += vmpyie(v23.w,v4.h)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vmpyieo(v21.h,v12.h)
		v0 = valign(v0,v0,#4)
		r1:0 = memd(r13+#24)
		r3:2 = memd(r13+#8)
	}
	{
		v0.w = vinsert(r0)
		v24.w = vinsert(r6)
		r0 = add(r30,#-3632)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r2)
		r2 = add(r30,#-1840)
		v3:2.w = vadd(v15:14.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v21.w,v12.h)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r1)
		r0 = add(r30,#-3504)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-3120)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w = vmpyieo(v19.h,v12.h)
		r1 = add(r30,#-12848)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		r0 = add(r30,#-2992)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v19.w,v12.h)
		r3 = r16
		v0 = vror(v0,r28)
	}
	{
		v12.w = vmpyieo(v10.h,v14.h)
		v9 = valign(v24,v24,#4)
		v4 = vor(v0,v7)
	}
	{
		v9.w = vinsert(r7)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v10.w,v14.h)
		v6 = vror(v6,r28)
		r0 = memw(r30+##-22280)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r16,r0)
		r7:6 = memd(r13+#56)
	}
	{
		v3:2.w = vadd(v3:2.w,v13:12.w)
		v14 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r16 = addasl(r20,r0,#7)
		v25:24.uh = vunpack(v4.ub)
		v4 = vor(v6,v7)
		v6 = v8
	}
	{
		r0 = asl(r0,#7)
		v0.w = vmpyieo(v20.h,v14.h)
		r2 = add(r30,#-1712)
		v5 = valign(v9,v9,#4)
	}
	{
		v5.w = vinsert(r6)
		v27:26.uh = vunpack(v4.ub)
		v4 = v8
		r5:4 = memd(r16+#16)
	}
	{
		v0.w += vmpyie(v20.w,v14.h)
		v11:10.uw = vunpack(v24.uh)
	}
	{
		v4.w = vinsert(r4)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		r7:6 = memd(r13+#32)
	}
	{
		v6.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r5)
		r1 = add(r30,#-12720)
		vmemu(r1+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		v0 = v8
		vmemu(r1+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		v5 = vror(v5,r28)
		r1:0 = memd(r20+r0<<#0)
		r5:4 = memd(r16+#24)
	}
	{
		v0.w = vinsert(r0)
		v3 = valign(v6,v6,#4)
		v5 = vor(v5,v7)
	}
	{
		v3.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r20 = memw(r30+##-23224)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r4)
		v2 = valign(v0,v0,#4)
		r7:6 = memd(r13+#40)
	}
	{
		v2.w = vinsert(r1)
		v21:20.uh = vunpack(v5.ub)
		v5 = v8
		r1:0 = memd(r16+#48)
	}
	{
		v5.w = vinsert(r0)
		r0 = add(r3,r20)
		v3 = valign(v3,v3,#4)
	}
	{
		r17 = addasl(r18,r0,#7)
		v3.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		r0 = asl(r0,#7)
		v4.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r16+#8)
	}
	{
		v2.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v4 = vror(v4,r28)
		r7:6 = memd(r17+#16)
	}
	{
		v5 = valign(v5,v5,#4)
		v4 = vor(v4,v7)
	}
	{
		v5.w = vinsert(r1)
		r1 = add(r3,r22)
		v15 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r19 = addasl(r18,r1,#7)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r5)
		v25 = vdelta(v10,v30)
		r5:4 = memd(r16+#56)
	}
	{
		v15:14.uw = vunpack(v26.uh)
		r9:8 = memd(r19+#48)
		r25:24 = memd(r19+#8)
	}
	{
		v3 = vror(v3,r28)
		v1 = vmux(q0,v25,v14)
	}
	{
		v25:24.uh = vunpack(v4.ub)
		v3 = vor(v3,v7)
	}
	{
		v4 = valign(v5,v5,#4)
		v5 = v8
	}
	{
		v4.w = vinsert(r4)
		v5.w = vinsert(r6)
		v2 = vror(v2,r28)
	}
	{
		v23:22.uw = vunpack(v20.uh)
	}
	{
		v27:26.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v4,v4,#4)
		v4 = vor(v2,v7)
	}
	{
		v3.w = vinsert(r5)
		v0 = vdelta(v22,v30)
		r5:4 = memd(r17+#48)
	}
	{
		v23:22.uh = vunpack(v4.ub)
		v4 = v8
	}
	{
		v4.w = vinsert(r4)
		v11:10.uw = vunpack(v24.uh)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r17+#24)
	}
	{
		v4.w = vinsert(r5)
		v21:20.uw = vunpack(v26.uh)
		v26 = v8
		r5:4 = memd(r19+#16)
	}
	{
		v3 = vror(v3,r28)
		r11:10 = memd(r17+#56)
	}
	{
		v2 = vdelta(v10,v30)
		v6 = vor(v3,v7)
		v3 = vmux(q0,v0,v20)
	}
	{
		v13:12.uw = vunpack(v22.uh)
	}
	{
		v5 = valign(v5,v5,#4)
		v0 = vmux(q0,v2,v12)
		v2 = v8
	}
	{
		v2.w = vinsert(r4)
		v5.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r10)
		v25:24.uh = vunpack(v6.ub)
	}
	{
		v2 = valign(v2,v2,#4)
	}
	{
		r0 = asl(r1,#7)
		v2.w = vinsert(r5)
		v6 = valign(v5,v5,#4)
		r5:4 = memd(r18+r0<<#0)
	}
	{
		v6.w = vinsert(r7)
		v26.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r19+#24)
	}
	{
		v4.w = vinsert(r11)
		v2 = valign(v2,v2,#4)
		r1:0 = memd(r18+r0<<#0)
		r11:10 = memd(r19+#32)
	}
	{
		v2.w = vinsert(r6)
		v6 = vror(v6,r28)
	}
	{
		v4 = vror(v4,r28)
		v6 = vor(v6,v7)
	}
	{
		v9 = valign(v26,v26,#4)
		v4 = vor(v4,v7)
	}
	{
		v9.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r17+#8)
	}
	{
		v2.w = vinsert(r7)
		v11:10.uw = vunpack(v24.uh)
		r7:6 = memd(r17+#32)
	}
	{
		v27:26.uh = vunpack(v6.ub)
	}
	{
		v21:20.uh = vunpack(v4.ub)
		v27 = v8
	}
	{
		v27.w = vinsert(r6)
		v4 = valign(v9,v9,#4)
	}
	{
		v4.w = vinsert(r4)
		v6 = vror(v2,r28)
	}
	{
		v5 = vdelta(v10,v30)
		v6 = vor(v6,v7)
	}
	{
		v11:10.uw = vunpack(v26.uh)
	}
	{
		v23:22.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r0)
		r0 = add(r3,r23)
		v2 = vdelta(v10,v30)
	}
	{
		r4 = addasl(r18,r0,#7)
		r0 = asl(r0,#7)
		v10 = valign(v4,v4,#4)
	}
	{
		v10.w = vinsert(r5)
		v9 = valign(v27,v27,#4)
	}
	{
		v9.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r17+#40)
		r27:26 = memd(r4+#16)
	}
	{
		v6.w = vinsert(r1)
		v10 = vror(v10,r28)
		r15:14 = memd(r4+#24)
		r1:0 = memd(r18+r0<<#0)
	}
	{
		v13:12.uw = vunpack(v20.uh)
		v11 = vor(v10,v7)
	}
	{
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r6)
		v25:24.uh = vunpack(v11.ub)
		v11 = v8
	}
	{
		v11.w = vinsert(r8)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
		v4 = vdelta(v12,v30)
	}
	{
		v13:12.uw = vunpack(v22.uh)
	}
	{
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r9)
		v10 = vdelta(v12,v30)
	}
	{
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v9 = valign(v9,v9,#4)
		v13 = v8
	}
	{
		v9.w = vinsert(r7)
		v13.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r19+#56)
	}
	{
		v6.w = vinsert(r25)
		v11 = valign(v11,v11,#4)
		r25:24 = memd(r4+#32)
	}
	{
		v11.w = vinsert(r6)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r27)
		v9 = vror(v9,r28)
	}
	{
		v6 = vror(v6,r28)
		v9 = vor(v9,v7)
	}
	{
		v31 = valign(v11,v11,#4)
		v6 = vor(v6,v7)
		v11 = vmux(q0,v2,v12)
	}
	{
		v31.w = vinsert(r7)
		v13 = valign(v13,v13,#4)
		r7:6 = memd(r4+#48)
	}
	{
		v13.w = vinsert(r14)
		v17:16.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r6)
		v27:26.uh = vunpack(v9.ub)
	}
	{
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r15)
		v15:14.uw = vunpack(v26.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		v9 = vmux(q0,v4,v14)
	}
	{
		v6.w = vinsert(r7)
		v15 = vror(v31,r28)
		r7:6 = memd(r4+#56)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v4 = vor(v15,v7)
	}
	{
		v12 = vror(v13,r28)
		v2 = vmux(q0,v10,v16)
		v10 = v8
		v13 = v8
	}
	{
		v10.w = vinsert(r10)
		v13.w = vinsert(r0)
		v15:14.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v6,v6,#4)
		v6 = vor(v12,v7)
		v12 = v8
		v15 = v8
	}
	{
		v4.w = vinsert(r6)
		v12.w = vinsert(r24)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r11)
		v23:22.uh = vunpack(v6.ub)
		r24 = memw(r30+##-23248)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r3,r24)
		v6 = valign(v4,v4,#4)
	}
	{
		r9 = addasl(r18,r2,#7)
		v6.w = vinsert(r7)
		v21:20.uw = vunpack(v14.uh)
		r7:6 = memd(r19+#40)
	}
	{
		v14 = valign(v10,v10,#4)
	}
	{
		v14.w = vinsert(r6)
		v6 = vror(v6,r28)
		r27:26 = memd(r9+#16)
		r11:10 = memd(r9+#32)
	}
	{
		v15.w = vinsert(r26)
		v13 = valign(v13,v13,#4)
		v6 = vor(v6,v7)
		r15:14 = memd(r9+#64)
	}
	{
		v13.w = vinsert(r1)
		v14 = valign(v14,v14,#4)
		r1:0 = memd(r4+#8)
	}
	{
		v14.w = vinsert(r7)
		v17:16.uh = vunpack(v6.ub)
		r7:6 = memd(r4+#40)
	}
	{
		v6 = valign(v12,v12,#4)
	}
	{
		v6.w = vinsert(r25)
		v12 = valign(v13,v13,#4)
		r25 = memw(r30+##-23256)
	}                                       // 4-byte Folded Reload
	{
		v12.w = vinsert(r0)
		v13 = vror(v14,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v13 = vor(v13,v7)
	}
	{
		v6.w = vinsert(r6)
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r9+#48)
	}
	{
		v6.w = vinsert(r7)
		v14 = vdelta(v16,v30)
		r7:6 = memd(r9+#24)
	}
	{
		v17:16.uh = vunpack(v13.ub)
	}
	{
		v13 = valign(v15,v15,#4)
		v15 = v8
	}
	{
		v13.w = vinsert(r27)
		v15.w = vinsert(r0)
		v12 = vror(v12,r28)
	}
	{
		r0 = asl(r2,#7)
		v6 = vror(v6,r28)
		v12 = vor(v12,v7)
	}
	{
		v13 = valign(v13,v13,#4)
		v6 = vor(v6,v7)
	}
	{
		v13.w = vinsert(r6)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r1)
		v25:24.uw = vunpack(v22.uh)
		r1:0 = memd(r18+r0<<#0)
	}
	{
		v19:18.uh = vunpack(v12.ub)
	}
	{
		v27:26.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v13,v13,#4)
	}
	{
		v6.w = vinsert(r7)
		v4 = vdelta(v20,v30)
		r7:6 = memd(r9+#56)
	}
	{
		v10 = vdelta(v24,v30)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v25:24.uw = vunpack(v18.uh)
		v18 = v8
		v13 = vmux(q0,v4,v16)
	}
	{
		v18.w = vinsert(r0)
		v4 = vror(v6,r28)
		v6 = v8
		v17 = vmux(q0,v10,v24)
	}
	{
		v6.w = vinsert(r10)
		v19 = valign(v15,v15,#4)
		v4 = vor(v4,v7)
	}
	{
		v19.w = vinsert(r6)
		v10 = valign(v18,v18,#4)
	}
	{
		v10.w = vinsert(r1)
		v21:20.uw = vunpack(v26.uh)
		r1:0 = memd(r9+#8)
	}
	{
		v12 = valign(v19,v19,#4)
		v15 = vmux(q0,v14,v20)
	}
	{
		v12.w = vinsert(r7)
		v19:18.uh = vunpack(v4.ub)
		r7:6 = memd(r9+#40)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r11)
		v6 = valign(v10,v10,#4)
	}
	{
		v6.w = vinsert(r0)
		r0 = add(r3,r25)
		v10 = vror(v12,r28)
	}
	{
		r5 = addasl(r18,r0,#7)
		r0 = asl(r0,#7)
		v4 = valign(v4,v4,#4)
		v10 = vor(v10,v7)
	}
	{
		v4.w = vinsert(r6)
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r5+#16)
		r11:10 = memd(r5+#24)
	}
	{
		v6.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r18+r0<<#0)
	}
	{
		v4.w = vinsert(r7)
		v12 = vdelta(v18,v30)
		r7:6 = memd(r5+#48)
	}
	{
		v19:18.uh = vunpack(v10.ub)
		v10 = v8
	}
	{
		v10.w = vinsert(r26)
		v6 = vror(v6,r28)
	}
	{
		v4 = vror(v4,r28)
		v6 = vor(v6,v7)
	}
	{
		v10 = valign(v10,v10,#4)
		v4 = vor(v4,v7)
	}
	{
		v10.w = vinsert(r27)
		v19:18.uw = vunpack(v18.uh)
		r27:26 = memd(r5+#56)
	}
	{
		v27:26.uh = vunpack(v4.ub)
	}
	{
		v14 = vdelta(v18,v30)
	}
	{
		v19:18.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r6)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r10)
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r7)
		v21:20.uw = vunpack(v26.uh)
		r7:6 = memd(r5+#32)
	}
	{
		v6 = valign(v10,v10,#4)
		v21 = vmux(q0,v12,v18)
		v10 = v8
		v12 = v8
	}
	{
		v6.w = vinsert(r11)
		v10.w = vinsert(r0)
		v4 = valign(v4,v4,#4)
		v19 = vmux(q0,v14,v20)
	}
	{
		v12.w = vinsert(r6)
		v4.w = vinsert(r26)
		r26 = memw(r30+##-23264)
	}                                       // 4-byte Folded Reload
	{
		v6 = vror(v6,r28)
		v18 = v8
		v20 = v8
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
	}
	{
		v10.w = vinsert(r1)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r5+#8)
	}
	{
		v12.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r5+#40)
	}
	{
		v4.w = vinsert(r27)
		v25:24.uh = vunpack(v6.ub)
		r27 = memw(r30+##-23272)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r3,r27)
		v6 = valign(v10,v10,#4)
	}
	{
		r8 = addasl(r18,r2,#7)
		v6.w = vinsert(r0)
		r0 = add(r3,r26)
		v10 = valign(v12,v12,#4)
	}
	{
		r12 = addasl(r18,r0,#7)
		v10.w = vinsert(r6)
		v4 = vror(v4,r28)
	}
	{
		r0 = asl(r0,#7)
		v6 = valign(v6,v6,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
		r11:10 = memd(r12+#24)
		r1:0 = memd(r18+r0<<#0)
	}
	{
		v10.w = vinsert(r7)
		v23:22.uw = vunpack(v24.uh)
		r7:6 = memd(r12+#16)
	}
	{
		v27:26.uh = vunpack(v4.ub)
		v4 = v8
	}
	{
		v4.w = vinsert(r6)
		v6 = vror(v6,r28)
	}
	{
		v10 = vror(v10,r28)
		v6 = vor(v6,v7)
	}
	{
		v12 = vdelta(v22,v30)
		v10 = vor(v10,v7)
	}
	{
		v23:22.uw = vunpack(v26.uh)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		v25:24.uh = vunpack(v6.ub)
		r7:6 = memd(r12+#48)
	}
	{
		v6 = vdelta(v22,v30)
	}
	{
		v23:22.uh = vunpack(v10.ub)
		v10 = v8
	}
	{
		v10.w = vinsert(r6)
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v14 = valign(v4,v4,#4)
	}
	{
		v14.w = vinsert(r10)
		v16 = valign(v10,v10,#4)
		v10 = vmux(q0,v12,v24)
		v12 = v8
	}
	{
		v12.w = vinsert(r0)
		v16.w = vinsert(r7)
		v23:22.uw = vunpack(v22.uh)
		r7:6 = memd(r12+#32)
	}
	{
		v14 = valign(v14,v14,#4)
		v4 = vmux(q0,v6,v22)
		v6 = v8
	}
	{
		v6.w = vinsert(r6)
		v14.w = vinsert(r11)
		v12 = valign(v12,v12,#4)
		r11:10 = memd(r12+#56)
	}
	{
		v12.w = vinsert(r1)
		v16 = valign(v16,v16,#4)
		r1:0 = memd(r12+#8)
	}
	{
		v16.w = vinsert(r10)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v12 = valign(v12,v12,#4)
		r7:6 = memd(r12+#40)
	}
	{
		v12.w = vinsert(r0)
		v14 = vror(v14,r28)
	}
	{
		v16 = valign(v16,v16,#4)
		v14 = vor(v14,v7)
	}
	{
		v16.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r1)
		v25:24.uh = vunpack(v14.ub)
		r1:0 = memd(r8+#16)
	}
	{
		v14 = vror(v16,r28)
		v16 = v8
	}
	{
		r0 = asl(r2,#7)
		v16.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
		v14 = vor(v14,v7)
	}
	{
		v6.w = vinsert(r7)
		v12 = vror(v12,r28)
		r7:6 = memd(r8+#48)
		r11:10 = memd(r18+r0<<#0)
	}
	{
		v18.w = vinsert(r6)
		v16 = valign(v16,v16,#4)
		v12 = vor(v12,v7)
		r3:2 = memd(r8+#24)
	}
	{
		v16.w = vinsert(r1)
		v23:22.uw = vunpack(v24.uh)
		r1:0 = memd(r8+#32)
	}
	{
		v20.w = vinsert(r0)
		v25:24.uh = vunpack(v14.ub)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v14 = vdelta(v22,v30)
		v6 = vor(v6,v7)
	}
	{
		v23:22.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v12.ub)
	}
	{
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r7)
		v16 = valign(v16,v16,#4)
		r7:6 = memd(r16+#40)
	}
	{
		v16.w = vinsert(r2)
		v12 = vdelta(v22,v30)
	}
	{
		v23:22.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v6.ub)
		v6 = v8
		v14 = vmux(q0,v14,v22)
	}
	{
		v6.w = vinsert(r10)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r1)
		v16 = valign(v16,v16,#4)
		r1:0 = memd(r8+#56)
	}
	{
		v16.w = vinsert(r3)
		v18 = valign(v18,v18,#4)
		r3:2 = memd(r8+#8)
	}
	{
		v18.w = vinsert(r0)
		r0 = add(r30,#-9264)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r11)
		v20 = valign(v20,v20,#4)
		r11:10 = memd(r8+#40)
	}
	{
		v20.w = vinsert(r10)
		v16 = vror(v16,r28)
	}
	{
		v18 = valign(v18,v18,#4)
		v16 = vor(v16,v7)
	}
	{
		v18.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r2)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r11)
		v27:26.uh = vunpack(v16.ub)
		r11:10 = memd(r17+#64)
	}
	{
		v16 = vror(v18,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v16 = vor(v16,v7)
	}
	{
		v6.w = vinsert(r3)
		v18 = vror(v20,r28)
		r3:2 = memd(r16+#32)
	}
	{
		v25:24.uw = vunpack(v24.uh)
		v18 = vor(v18,v7)
	}
	{
		v25:24.uh = vunpack(v16.ub)
		v12 = vmux(q0,v12,v24)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v23:22.uw = vunpack(v26.uh)
		v6 = vor(v6,v7)
	}
	{
		v27:26.uh = vunpack(v18.ub)
		v18 = v8
	}
	{
		v18.w = vinsert(r2)
		r2 = add(r30,#-7984)
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v16 = vdelta(v22,v30)
	}
	{
		v23:22.uh = vunpack(v6.ub)
	}
	{
		v6 = vdelta(v24,v30)
	}
	{
		v25:24.uw = vunpack(v26.uh)
	}
	{
		r0 = add(r30,#-9136)
		v6 = vmux(q0,v6,v24)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-8752)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v23.w = vmpyieo(v10.h,v26.h)
		r0 = add(r30,#-8624)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v10.w,v26.h)
		r0 = add(r30,#-12336)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-12208)
		v16 = vmux(q0,v16,v22)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-10032)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v16.h,v24.h)
		r0 = add(r30,#-9904)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v14.h,v26.h)
		r0 = add(r30,#-12080)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w += vmpyie(v16.w,v24.h)
		v10 = valign(v18,v18,#4)
	}
	{
		v27.w += vmpyie(v14.w,v26.h)
		v14 = v28
	}
	{
		v24.w = vmpyieo(v6.h,v28.h)
		r0 = add(r30,#-11952)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w = vinsert(r3)
		r0 = add(r30,#-7216)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w = vmpyieo(v12.h,v28.h)
	}
	{
		v26.w += vmpyie(v12.w,v28.h)
		r2 = add(r30,#-7856)
		v28 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w += vmpyie(v6.w,v14.h)
		v6 = valign(v10,v10,#4)
		v10 = v28
	}
	{
		v22.w = vmpyieo(v4.h,v28.h)
		v6.w = vinsert(r6)
		v29 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v4.w,v10.h)
		r0 = add(r30,#-7088)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9776)
		r2 = add(r30,#-560)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9648)
		v25:24.w = vadd(v25:24.w,v29:28.w)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-4656)
		v25:24.w = vadd(v25:24.w,v27:26.w)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29.w = vmpyieo(v21.h,v28.h)
		r0 = add(r30,#-4528)
		v20 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29.w += vmpyie(v21.w,v28.h)
		r0 = add(r30,#-1584)
		v21 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w = vmpyieo(v2.h,v20.h)
		r0 = add(r30,#-1456)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w += vmpyie(v2.w,v20.h)
		v4 = valign(v6,v6,#4)
		v6 = v8
	}
	{
		v4.w = vinsert(r7)
		r0 = add(r30,#-9520)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v28.w = vmpyieo(v19.h,v26.h)
		r0 = add(r30,#-9392)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v28.w += vmpyie(v19.w,v26.h)
		r0 = add(r30,#-11568)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2 = vror(v4,r28)
		v23:22.w = vadd(v23:22.w,v25:24.w)
		r7:6 = memd(r13+#64)
	}
	{
		r0 = add(r30,#-11440)
		v2 = vor(v2,v7)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w = vmpyieo(v17.h,v18.h)
		r0 = add(r30,#-1328)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w += vmpyie(v17.w,v18.h)
		r0 = add(r30,#-1200)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v20.w = vmpyieo(v13.h,v24.h)
		v17:16.uh = vunpack(v2.ub)
		v2 = v12
	}
	{
		v18.w = vmpyieo(v15.h,v12.h)
		v23:22.w = vadd(v23:22.w,v29:28.w)
	}
	{
		v20.w += vmpyie(v13.w,v24.h)
		r0 = add(r30,#-1072)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w += vmpyie(v15.w,v2.h)
		v25:24.uw = vunpack(v16.uh)
	}
	{
		r0 = add(r30,#-944)
		v13:12.w = vadd(v23:22.w,v19:18.w)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27:26.w = vadd(v21:20.w,v13:12.w)
		v2 = v8
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v11.h,v16.h)
		v4 = vmux(q0,v5,v24)
		v5 = v8
		r1:0 = memd(r13+#80)
	}
	{
		v2.w = vinsert(r0)
		r2 = add(r30,#-432)
		v12 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vinsert(r6)
		r0 = add(r30,#-2608)
		v13 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w = vmpyieo(v9.h,v12.h)
		r2 = add(r30,#-304)
		v2 = valign(v2,v2,#4)
	}
	{
		v24.w += vmpyie(v9.w,v12.h)
		r0 = add(r30,#-2480)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r1)
		r0 = add(r30,#-816)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w += vmpyie(v11.w,v16.h)
		r0 = add(r30,#-688)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w = vmpyieo(v1.h,v12.h)
		v9 = v30
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v1.w,v12.h)
		v2 = valign(v2,v2,#4)
		r1:0 = memd(r13+#88)
	}
	{
		v2.w = vinsert(r0)
		v1.w = vmpyieo(v0.h,v16.h)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		r2 = add(r30,#-176)
		v14 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v0.w,v16.h)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r13+#112)
	}
	{
		v6.w = vinsert(r6)
		v2.w = vinsert(r1)
		v15 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v4.h,v14.h)
		r2 = add(r30,#-2096)
		v11:10.w = vadd(v27:26.w,v25:24.w)
		r1:0 = memd(r13+#72)
	}
	{
		v0.w += vmpyie(v4.w,v14.h)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r0)
		v5 = valign(v6,v6,#4)
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r7)
		r2 = add(r30,#-1968)
		v14 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v2 = vror(v2,r28)
		v1:0.w = vadd(v11:10.w,v1:0.w)
		r7:6 = memd(r16+#80)
	}
	{
		v12.w = vmpyieo(v3.h,v14.h)
		v2 = vor(v2,v7)
		v15 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v3.w,v14.h)
		v3 = valign(v4,v4,#4)
		r3:2 = memd(r13+#120)
	}
	{
		v3.w = vinsert(r1)
		v4 = valign(v5,v5,#4)
		v5 = v8
	}
	{
		v4.w = vinsert(r2)
		v29:28.uh = vunpack(v2.ub)
		v2 = v8
	}
	{
		v2.w = vinsert(r6)
		v3 = vror(v3,r28)
	}
	{
		v4 = valign(v4,v4,#4)
		v1:0.w = vadd(v13:12.w,v1:0.w)
		v3 = vor(v3,v7)
	}
	{
		v4.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r13+#96)
		vmem(r0+#0) = v0
	}
	{
		v2.w = vinsert(r7)
		v5.w = vinsert(r2)
		v15:14.uh = vunpack(v3.ub)
		r1:0 = memd(r16+#88)
	}
	{
		v3 = vror(v4,r28)
		r2 = memw(r30+##-14384)
		r7:6 = memd(r16+#64)
	}                                       // 4-byte Folded Reload
	{
		v2 = valign(v2,v2,#4)
		v0 = vor(v3,v7)
	}
	{
		v2.w = vinsert(r0)
		v4 = valign(v5,v5,#4)
		vmem(r2+#0) = v1
	}
	{
		v4.w = vinsert(r3)
		v21:20.uh = vunpack(v0.ub)
		v0 = v8
		r3:2 = memd(r13+#104)
	}
	{
		v0.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
		v3 = valign(v4,v4,#4)
		r1:0 = memd(r17+#80)
	}
	{
		v3.w = vinsert(r2)
		v5:4.uw = vunpack(v20.uh)
	}
	{
		v5 = valign(v0,v0,#4)
	}
	{
		v5.w = vinsert(r7)
		v2 = vror(v2,r28)
		r7:6 = memd(r17+#72)
	}
	{
		v0 = vdelta(v4,v30)
		v4 = v8
		v2 = vor(v2,v7)
	}
	{
		v4.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r16+#72)
	}
	{
		v5.w = vinsert(r2)
		v23:22.uh = vunpack(v2.ub)
	}
	{
		v2 = valign(v4,v4,#4)
	}
	{
		v2.w = vinsert(r1)
		v4 = valign(v5,v5,#4)
		v5 = v8
		r1:0 = memd(r17+#88)
	}
	{
		v4.w = vinsert(r3)
		v11:10.uw = vunpack(v28.uh)
		r3:2 = memd(r16+#112)
	}
	{
		v5.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r0)
		v6 = vdelta(v10,v30)
	}
	{
		v11:10.uw = vunpack(v14.uh)
	}
	{
		v4 = vror(v4,r28)
		v1 = vmux(q0,v6,v10)
		v6 = v8
	}
	{
		v6.w = vinsert(r10)
		v2 = valign(v2,v2,#4)
		v4 = vor(v4,v7)
	}
	{
		v2.w = vinsert(r1)
		v5 = valign(v5,v5,#4)
		r1:0 = memd(r16+#120)
	}
	{
		v5.w = vinsert(r3)
		v3 = vror(v3,r28)
		r3:2 = memd(r17+#112)
	}
	{
		v6 = valign(v6,v6,#4)
		v3 = vor(v3,v7)
	}
	{
		v6.w = vinsert(r11)
		v29:28.uh = vunpack(v4.ub)
		r11:10 = memd(r19+#64)
	}
	{
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r0)
		r0 = add(r30,#-14256)
		v2 = vror(v2,r28)
	}
	{
		v25:24.uh = vunpack(v3.ub)
		v2 = vor(v2,v7)
	}
	{
		v5 = valign(v6,v6,#4)
	}
	{
		v5.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r1)
		v21:20.uh = vunpack(v2.ub)
		v2 = v8
	}
	{
		v2.w = vinsert(r2)
		v27:26.uw = vunpack(v24.uh)
	}
	{
		v11:10.uw = vunpack(v22.uh)
		v0 = vmux(q0,v0,v26)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r19+#72)
	}
	{
		v2.w = vinsert(r3)
		v4 = vror(v4,r28)
		r3:2 = memd(r17+#120)
	}
	{
		v3 = vdelta(v10,v30)
		v4 = vor(v4,v7)
	}
	{
		v11:10.uw = vunpack(v20.uh)
		v20 = v8
	}
	{
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v5 = vror(v5,r28)
		r1:0 = memd(r19+#80)
	}
	{
		v20.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
		v5 = vor(v5,v7)
	}
	{
		v2.w = vinsert(r2)
		v23:22.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v20,v20,#4)
	}
	{
		v4.w = vinsert(r1)
		v6 = vdelta(v10,v30)
		r1:0 = memd(r17+#96)
	}
	{
		v11:10.uw = vunpack(v22.uh)
	}
	{
		v2 = valign(v2,v2,#4)
		v11 = v8
	}
	{
		v2.w = vinsert(r3)
		v11.w = vinsert(r0)
		v25:24.uh = vunpack(v5.ub)
		r3:2 = memd(r19+#88)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		v3 = vmux(q0,v3,v12)
	}
	{
		v2 = vror(v2,r28)
		v0 = vmux(q0,v6,v12)
		v6 = v8
	}
	{
		v6.w = vinsert(r10)
		v5 = vdelta(v10,v30)
		v2 = vor(v2,v7)
	}
	{
		v10 = valign(v11,v11,#4)
	}
	{
		v10.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r4+#80)
	}
	{
		v4.w = vinsert(r3)
		v27:26.uh = vunpack(v2.ub)
		r3:2 = memd(r17+#104)
	}
	{
		v2 = valign(v6,v6,#4)
	}
	{
		v2.w = vinsert(r11)
		v6 = valign(v10,v10,#4)
		r11:10 = memd(r4+#88)
	}
	{
		v6.w = vinsert(r2)
		v4 = vror(v4,r28)
	}
	{
		v2 = valign(v2,v2,#4)
		v4 = vor(v4,v7)
	}
	{
		v2.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v29:28.uh = vunpack(v4.ub)
		v4 = v8
		r3:2 = memd(r19+#112)
	}
	{
		v4.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r4+#112)
	}
	{
		v4 = valign(v4,v4,#4)
		v6 = vor(v6,v7)
	}
	{
		v4.w = vinsert(r3)
		v2 = vror(v2,r28)
		r3:2 = memd(r19+#120)
	}
	{
		v21:20.uh = vunpack(v6.ub)
		v6 = v8
		v2 = vor(v2,v7)
	}
	{
		v6.w = vinsert(r0)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v11:10.uw = vunpack(v26.uh)
	}
	{
		v17:16.uh = vunpack(v2.ub)
	}
	{
		v2 = valign(v6,v6,#4)
		v6 = v8
	}
	{
		v2.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r19+#96)
	}
	{
		v4.w = vinsert(r3)
		v6.w = vinsert(r0)
		v10 = vdelta(v10,v30)
		r3:2 = memd(r4+#64)
	}
	{
		v15:14.uw = vunpack(v20.uh)
	}
	{
		v2 = valign(v2,v2,#4)
		v11 = vmux(q0,v10,v14)
		v10 = v8
	}
	{
		v10.w = vinsert(r2)
		v2.w = vinsert(r10)
		v4 = vror(v4,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
		r1:0 = memd(r19+#104)
	}
	{
		v10.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r4+#72)
	}
	{
		v2.w = vinsert(r11)
		v23:22.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v6,v6,#4)
	}
	{
		v4.w = vinsert(r0)
		v6 = valign(v10,v10,#4)
	}
	{
		v6.w = vinsert(r2)
		v2 = vror(v2,r28)
	}
	{
		v4 = valign(v4,v4,#4)
		v2 = vor(v2,v7)
	}
	{
		v4.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r9+#88)
	}
	{
		v6.w = vinsert(r3)
		v25:24.uh = vunpack(v2.ub)
		v2 = v8
		r3:2 = memd(r9+#80)
	}
	{
		v2.w = vinsert(r2)
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v4 = vror(v4,r28)
	}
	{
		v6 = vror(v6,r28)
		v4 = vor(v4,v7)
	}
	{
		v2 = valign(v2,v2,#4)
		v6 = vor(v6,v7)
	}
	{
		v2.w = vinsert(r3)
		v15:14.uw = vunpack(v22.uh)
		r3:2 = memd(r9+#112)
	}
	{
		v12 = vdelta(v12,v30)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v17:16.uh = vunpack(v4.ub)
		v13 = vmux(q0,v12,v16)
		v12 = v8
	}
	{
		v12.w = vinsert(r6)
		v10 = vdelta(v14,v30)
	}
	{
		v15:14.uw = vunpack(v24.uh)
	}
	{
		v27:26.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v2,v2,#4)
	}
	{
		v6.w = vinsert(r0)
		v4 = vdelta(v14,v30)
	}
	{
		v15:14.uw = vunpack(v26.uh)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v15 = vmux(q0,v4,v14)
		v4 = v8
	}
	{
		v4.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
		v2 = vmux(q0,v10,v16)
		v10 = v8
	}
	{
		v10.w = vinsert(r14)
		v6.w = vinsert(r1)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r4+#120)
	}
	{
		v12.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		v14 = v8
		r7:6 = memd(r9+#72)
	}
	{
		v4.w = vinsert(r3)
		v10 = valign(v10,v10,#4)
		r3:2 = memd(r9+#120)
	}
	{
		v10.w = vinsert(r15)
		v6 = vror(v6,r28)
		r15:14 = memd(r8+#80)
	}
	{
		v12 = valign(v12,v12,#4)
		v6 = vor(v6,v7)
	}
	{
		v12.w = vinsert(r0)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v17:16.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v12,v12,#4)
	}
	{
		v6.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r5+#80)
	}
	{
		v4.w = vinsert(r3)
		v14.w = vinsert(r6)
		v6 = vror(v6,r28)
		r1:0 = memd(r5+#112)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v6 = vor(v6,v7)
		r3:2 = memd(r5+#88)
	}
	{
		v4 = vror(v4,r28)
		r11:10 = memd(r5+#120)
	}
	{
		v12 = vdelta(v16,v30)
		v4 = vor(v4,v7)
	}
	{
		v17:16.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v14,v14,#4)
	}
	{
		v6.w = vinsert(r7)
		v29:28.uh = vunpack(v4.ub)
		v4 = v8
		r7:6 = memd(r12+#80)
	}
	{
		v4.w = vinsert(r0)
		v10 = vror(v10,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v10 = vor(v10,v7)
	}
	{
		v6.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r1)
		v19:18.uh = vunpack(v10.ub)
		v10 = v8
		r1:0 = memd(r5+#64)
	}
	{
		v10.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v17:16.uw = vunpack(v16.uh)
		r3:2 = memd(r5+#96)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v4 = valign(v4,v4,#4)
		v17 = vmux(q0,v12,v18)
		v12 = v8
		v18 = v8
	}
	{
		v12.w = vinsert(r2)
		v4.w = vinsert(r10)
		v6 = vror(v6,r28)
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
	}
	{
		v10.w = vinsert(r1)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r5+#72)
	}
	{
		v12.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
		r3:2 = memd(r5+#104)
	}
	{
		v4.w = vinsert(r11)
		v25:24.uh = vunpack(v6.ub)
		r11:10 = memd(r8+#72)
	}
	{
		v6 = valign(v10,v10,#4)
	}
	{
		v6.w = vinsert(r0)
		v10 = valign(v12,v12,#4)
	}
	{
		v10.w = vinsert(r2)
		v4 = vror(v4,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v4 = vor(v4,v7)
	}
	{
		v6.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
		r1:0 = memd(r12+#88)
	}
	{
		v10.w = vinsert(r3)
		v27:26.uh = vunpack(v4.ub)
		v4 = v8
		r3:2 = memd(r8+#64)
	}
	{
		v4.w = vinsert(r6)
		v6 = vror(v6,r28)
	}
	{
		v23:22.uw = vunpack(v24.uh)
		v6 = vor(v6,v7)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		v25:24.uh = vunpack(v6.ub)
		v6 = v8
		r7:6 = memd(r12+#112)
	}
	{
		v6.w = vinsert(r6)
		v10 = vror(v10,r28)
	}
	{
		v14 = vdelta(v22,v30)
		v12 = vor(v10,v7)
	}
	{
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v21:20.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v12.ub)
	}
	{
		v12 = valign(v4,v4,#4)
		v4 = vmux(q0,v14,v24)
		v14 = v8
	}
	{
		v12.w = vinsert(r0)
		v14.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v19 = vdelta(v16,v30)
		v16 = v8
		r7:6 = memd(r8+#112)
	}
	{
		v16.w = vinsert(r2)
		v18.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r1)
		v14 = valign(v14,v14,#4)
		r1:0 = memd(r12+#120)
	}
	{
		v14.w = vinsert(r15)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r8+#88)
	}
	{
		v6.w = vinsert(r0)
		v12 = vror(v12,r28)
	}
	{
		v14 = valign(v14,v14,#4)
		v12 = vor(v12,v7)
	}
	{
		v14.w = vinsert(r2)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r10)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r1)
		v23:22.uw = vunpack(v26.uh)
		r1:0 = memd(r8+#120)
	}
	{
		v27:26.uh = vunpack(v12.ub)
	}
	{
		v12 = valign(v14,v14,#4)
	}
	{
		v12.w = vinsert(r3)
		v14 = valign(v16,v16,#4)
		r3:2 = memd(r8+#96)
	}
	{
		v14.w = vinsert(r11)
		v6 = vror(v6,r28)
	}
	{
		v10 = vdelta(v22,v30)
		v6 = vor(v6,v7)
	}
	{
		v23:22.uw = vunpack(v28.uh)
	}
	{
		v12 = vror(v12,r28)
		v10 = vmux(q0,v10,v22)
	}
	{
		v23:22.uw = vunpack(v26.uh)
		v12 = vor(v12,v7)
	}
	{
		v14 = vror(v14,r28)
	}
	{
		v16 = vdelta(v22,v30)
	}
	{
		v23:22.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v18,v18,#4)
		v18 = v8
	}
	{
		v6.w = vinsert(r7)
		v29:28.uh = vunpack(v12.ub)
		v12 = vor(v14,v7)
		r7:6 = memd(r12+#64)
	}
	{
		v23:22.uw = vunpack(v22.uh)
		v14 = v8
		r15:14 = memd(r12+#96)
	}
	{
		v14.w = vinsert(r6)
		v18.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r0)
		v21 = vdelta(v20,v30)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r1)
		v18 = valign(v18,v18,#4)
		r1:0 = memd(r12+#72)
	}
	{
		v18.w = vinsert(r15)
		v20 = vdelta(v22,v30)
		v22 = v8
		r7:6 = memd(r12+#104)
	}
	{
		v22.w = vinsert(r2)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r0)
		v6 = vror(v6,r28)
	}
	{
		v22 = valign(v22,v22,#4)
		v6 = vor(v6,v7)
	}
	{
		v22.w = vinsert(r3)
		v18 = valign(v18,v18,#4)
		r3:2 = memd(r8+#104)
	}
	{
		v18.w = vinsert(r6)
		v25:24.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v12.ub)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r1)
		v12 = vdelta(v24,v30)
		r1:0 = memd(r4+#96)
	}
	{
		v29:28.uw = vunpack(v28.uh)
	}
	{
		v18 = valign(v18,v18,#4)
		v12 = vmux(q0,v12,v28)
	}
	{
		v18.w = vinsert(r7)
		v29:28.uh = vunpack(v6.ub)
		r7:6 = memd(r16+#104)
	}
	{
		v6 = valign(v22,v22,#4)
	}
	{
		v6.w = vinsert(r2)
		v14 = vror(v14,r28)
	}
	{
		v23:22.uw = vunpack(v28.uh)
		v14 = vor(v14,v7)
	}
	{
		v6 = valign(v6,v6,#4)
		v23 = v8
	}
	{
		v6.w = vinsert(r3)
		v23.w = vinsert(r0)
		r0 = add(r30,#-9008)
		v18 = vror(v18,r28)
	}
	{
		v25:24.uh = vunpack(v14.ub)
		v18 = vor(v18,v7)
		r3:2 = memd(r9+#96)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v27:26.uh = vunpack(v18.ub)
		v6 = vor(v6,v7)
	}
	{
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v23 = valign(v23,v23,#4)
		v14 = vmux(q0,v16,v24)
	}
	{
		v23.w = vinsert(r1)
		r0 = add(r30,#-8880)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-10288)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v4.h,v24.h)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v29:28.uh = vunpack(v6.ub)
		v16 = vmux(q0,v20,v26)
		v6 = v8
	}
	{
		v25.w += vmpyie(v4.w,v24.h)
		r0 = add(r30,#-10160)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r2)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v12.h,v26.h)
		v4 = valign(v23,v23,#4)
		r1:0 = memd(r4+#104)
		r5:4 = memd(r9+#104)
	}
	{
		v4.w = vinsert(r0)
		r0 = add(r30,#-8496)
		v22 = vdelta(v22,v30)
	}
	{
		v27.w += vmpyie(v12.w,v26.h)
		v29:28.uw = vunpack(v28.uh)
	}
	{
		r0 = add(r30,#-8368)
		v18 = vmux(q0,v22,v28)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-12592)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w = vmpyieo(v10.h,v22.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v24.w += vmpyie(v10.w,v22.h)
		r0 = add(r30,#-12464)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r3)
		r0 = add(r30,#-11824)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w = vmpyieo(v18.h,v22.h)
		v4 = valign(v4,v4,#4)
		v10 = v8
		r3:2 = memd(r16+#96)
	}
	{
		v26.w += vmpyie(v18.w,v22.h)
		r0 = add(r30,#-11696)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vinsert(r1)
		v10.w = vinsert(r2)
		r1 = add(r30,#-3888)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		r0 = add(r30,#-7728)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w = vmpyieo(v14.h,v22.h)
		r0 = add(r30,#-7600)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v14.w,v22.h)
		v6 = valign(v6,v6,#4)
		r4 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		v22.w = vmpyieo(v16.h,v28.h)
		v6.w = vinsert(r5)
		r19 = r4
		v4 = vror(v4,r28)
	}
	{
		v22.w += vmpyie(v16.w,v28.h)
		r0 = add(r30,#-6960)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-6832)
		v4 = vor(v4,v7)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5424)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6 = vror(v6,r28)
		v27:26.w = vadd(v27:26.w,v29:28.w)
	}
	{
		v29:28.uh = vunpack(v4.ub)
		v6 = vor(v6,v7)
	}
	{
		v10 = valign(v10,v10,#4)
		v23:22.w = vadd(v27:26.w,v23:22.w)
	}
	{
		v10.w = vinsert(r3)
		r0 = add(r30,#-5296)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29:28.uw = vunpack(v28.uh)
		v27:26.w = vadd(v25:24.w,v23:22.w)
	}
	{
		v31:30.uh = vunpack(v6.ub)
		v6 = vmux(q0,v19,v28)
	}
	{
		r0 = add(r30,#-5680)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w = vmpyieo(v17.h,v18.h)
		v4 = valign(v10,v10,#4)
	}
	{
		v4.w = vinsert(r6)
		r0 = add(r30,#-5552)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w += vmpyie(v17.w,v18.h)
		r0 = add(r30,#-5168)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w = vmpyieo(v15.h,v16.h)
		v31:30.uw = vunpack(v30.uh)
	}
	{
		r0 = add(r30,#-5040)
		v10 = vmux(q0,v21,v30)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w += vmpyie(v15.w,v16.h)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		r0 = add(r30,#-4400)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w = vmpyieo(v10.h,v22.h)
		r0 = add(r30,#-4272)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w += vmpyie(v10.w,v22.h)
		r0 = add(r30,#-8240)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w = vmpyieo(v13.h,v14.h)
		v4 = vror(v4,r28)
	}
	{
		r0 = add(r30,#-8112)
		v4 = vor(v4,v7)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-3376)
		v29:28.w = vadd(v27:26.w,v19:18.w)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v16.w = vmpyieo(v6.h,v22.h)
		r0 = add(r30,#-3248)
		v20 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v16.w += vmpyie(v6.w,v22.h)
		v19:18.uh = vunpack(v4.ub)
	}
	{
		v15.w += vmpyie(v13.w,v14.h)
		r0 = add(r30,#-2864)
		v21 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-3760)
		v13:12.w = vadd(v29:28.w,v17:16.w)
		v16 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w = vmpyieo(v0.h,v20.h)
		v4 = v20
		v20 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-2736)
		v6 = v20
		v17 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w = vmpyieo(v2.h,v20.h)
		r1 = add(r30,#-2352)
		v21 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v14.w += vmpyie(v2.w,v6.h)
		v31:30.uw = vunpack(v18.uh)
		r0 = memw(r30+##-22272)
	}                                       // 4-byte Folded Reload
	{
		v19.w += vmpyie(v0.w,v4.h)
		r1 = add(r30,#-2224)
		v21 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-3632)
		v2 = vmux(q0,v5,v30)
		v31 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-3504)
		v5:4.w = vadd(v15:14.w,v13:12.w)
		v12 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r4,r0)
		r1 = add(r30,#-3120)
		v13 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r7 = addasl(r18,r0,#7)
		v13.w = vmpyieo(v3.h,v12.h)
		v14 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = asl(r0,#7)
		v13.w += vmpyie(v3.w,v12.h)
		r1 = add(r30,#-2992)
		v6 = v8
	}
	{
		v12.w = vmpyieo(v2.h,v14.h)
		v18.w = vmpyieo(v11.h,v16.h)
		r3:2 = memd(r7+#16)
		r15:14 = memd(r18+r0<<#0)
	}
	{
		v6.w = vinsert(r2)
		r0 = add(r30,#-1840)
		v15 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w += vmpyie(v2.w,v14.h)
		r0 = add(r30,#-1712)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w += vmpyie(v11.w,v16.h)
		r0 = add(r30,#-14256)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v11.w = vmpyieo(v1.h,v21.h)
		v3 = v8
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v11.w += vmpyie(v1.w,v21.h)
		v1 = valign(v6,v6,#4)
		r2 = memw(r30+##-22280)
	}                                       // 4-byte Folded Reload
	{
		v1.w = vinsert(r3)
		v10.w = vmpyieo(v0.h,v14.h)
		r2 = add(r4,r2)
		v6 = v8
	}
	{
		r6 = addasl(r18,r2,#7)
		v3.w = vinsert(r14)
		r1:0 = memd(r7+#24)
		r5:4 = memd(r7+#48)
	}
	{
		v1 = valign(v1,v1,#4)
		v5:4.w = vadd(v5:4.w,v19:18.w)
	}
	{
		v10.w += vmpyie(v0.w,v14.h)
		v2 = valign(v3,v3,#4)
		v0 = v8
	}
	{
		v1.w = vinsert(r0)
		v0.w = vinsert(r4)
		r11:10 = memd(r6+#16)
	}
	{
		v2.w = vinsert(r15)
		v5:4.w = vadd(v5:4.w,v13:12.w)
		v3 = v8
		r15:14 = memd(r6+#8)
	}
	{
		v3.w = vinsert(r10)
		v1 = valign(v1,v1,#4)
		v31 = v9
	}
	{
		v1.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r1:0 = memd(r7+#8)
	}
	{
		v0.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r7+#56)
	}
	{
		v2.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r11)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
		v1 = vror(v1,r28)
		r1:0 = memd(r6+#24)
	}
	{
		v3 = valign(v3,v3,#4)
		v5:4.w = vadd(v11:10.w,v5:4.w)
		v1 = vor(v1,v7)
	}
	{
		v3.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r0 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r5)
		v23:22.uh = vunpack(v1.ub)
		r5:4 = memd(r7+#32)
	}
	{
		v1 = vror(v2,r28)
		r3 = memw(r30+##-13616)
		vmem(r0+#0) = v4
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r2,#7)
		v2 = valign(v3,v3,#4)
		v1 = vor(v1,v7)
	}
	{
		v2.w = vinsert(r1)
		v0 = vror(v0,r28)
		r17:16 = memd(r18+r0<<#0)
		vmem(r3+#0) = v5
	}
	{
		r0 = add(r19,r20)
		v11:10.uw = vunpack(v22.uh)
		v0 = vor(v0,v7)
		r3:2 = memd(r6+#48)
	}
	{
		r1 = addasl(r18,r0,#7)
		r0 = asl(r0,#7)
		v25:24.uh = vunpack(v1.ub)
	}
	{
		r0 = add(r19,r22)
		v1 = vror(v2,r28)
		v2 = v8
		r11:10 = memd(r18+r0<<#0)
	}
	{
		v27:26.uh = vunpack(v0.ub)
		v0 = vor(v1,v7)
		v1 = v8
		r21:20 = memd(r1+#16)
	}
	{
		v1.w = vinsert(r4)
		v2.w = vinsert(r2)
		v3 = vdelta(v10,v9)
	}
	{
		v6.w = vinsert(r20)
		v11:10.uw = vunpack(v24.uh)
	}
	{
		v1 = valign(v1,v1,#4)
		v30 = vmux(q0,v3,v10)
		v3 = v8
	}
	{
		v3.w = vinsert(r16)
		v1.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r7+#40)
	}
	{
		v2.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r6+#56)
	}
	{
		v6.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r17)
		v1 = valign(v1,v1,#4)
		r17:16 = memd(r1+#48)
	}
	{
		v1.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r14)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r5)
		v21:20.uh = vunpack(v0.ub)
		r5:4 = memd(r1+#24)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r15)
		v1 = vror(v1,r28)
	}
	{
		v2 = valign(v2,v2,#4)
		v1 = vor(v1,v7)
	}
	{
		v2.w = vinsert(r3)
		v3 = vror(v3,r28)
		r3:2 = memd(r6+#32)
	}
	{
		v6 = valign(v6,v6,#4)
		v3 = vor(v3,v7)
	}
	{
		v6.w = vinsert(r4)
		r4 = add(r30,#-13616)
		v11:10.uw = vunpack(v20.uh)
	}
	{
		v23:22.uh = vunpack(v1.ub)
	}
	{
		v2 = vror(v2,r28)
	}
	{
		v4 = vdelta(v10,v9)
		v2 = vor(v2,v7)
	}
	{
		v11:10.uw = vunpack(v22.uh)
		v22 = v8
	}
	{
		v22.w = vinsert(r2)
		v25:24.uh = vunpack(v3.ub)
		v11 = v8
	}
	{
		v15:14.uw = vunpack(v26.uh)
	}
	{
		v1 = valign(v6,v6,#4)
	}
	{
		v1.w = vinsert(r5)
		v13:12.uw = vunpack(v24.uh)
	}
	{
		v0 = vdelta(v14,v9)
		v3 = vmux(q0,v4,v12)
	}
	{
		v27:26.uh = vunpack(v2.ub)
		v2 = v8
		v0 = vmux(q0,v0,v10)
	}
	{
		v2.w = vinsert(r10)
		v4 = valign(v22,v22,#4)
		v10 = v8
	}
	{
		r3 = addasl(r18,r0,#7)
		v4.w = vinsert(r3)
		v1 = vror(v1,r28)
	}
	{
		r0 = asl(r0,#7)
		v2 = valign(v2,v2,#4)
		v6 = vor(v1,v7)
	}
	{
		v2.w = vinsert(r11)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v4 = valign(v4,v4,#4)
		r21:20 = memd(r3+#16)
		r5:4 = memd(r6+#40)
	}
	{
		v4.w = vinsert(r4)
		v21:20.uh = vunpack(v6.ub)
		v6 = v8
		r11:10 = memd(r1+#8)
	}
	{
		v10.w = vinsert(r20)
		v6.w = vinsert(r16)
		v2 = valign(v2,v2,#4)
		r15:14 = memd(r3+#48)
	}
	{
		v2.w = vinsert(r10)
		v11.w = vinsert(r14)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r5)
		v10 = valign(v10,v10,#4)
		r5:4 = memd(r1+#56)
	}
	{
		v10.w = vinsert(r21)
		r0 = add(r19,r23)
		v6 = valign(v6,v6,#4)
		r21:20 = memd(r18+r0<<#0)
	}
	{
		r8 = addasl(r18,r0,#7)
		v6.w = vinsert(r17)
		v2 = valign(v2,v2,#4)
		r17:16 = memd(r3+#24)
	}
	{
		r0 = asl(r0,#7)
		v2.w = vinsert(r11)
		v4 = vror(v4,r28)
	}
	{
		v10 = valign(v10,v10,#4)
		v4 = vor(v4,v7)
		r11:10 = memd(r8+#16)
		r23:22 = memd(r18+r0<<#0)
	}
	{
		v10.w = vinsert(r16)
		r0 = add(r19,r24)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		v2 = vror(v2,r28)
	}
	{
		v25:24.uh = vunpack(v4.ub)
		v2 = vor(v2,v7)
	}
	{
		v4 = valign(v10,v10,#4)
	}
	{
		v4.w = vinsert(r17)
		v29:28.uw = vunpack(v26.uh)
		r17:16 = memd(r3+#8)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
		v0 = vdelta(v28,v31)
		r5:4 = memd(r1+#32)
	}
	{
		v29:28.uh = vunpack(v2.ub)
	}
	{
		v2 = vror(v4,r28)
		v4 = v8
	}
	{
		v4.w = vinsert(r4)
		v6 = vror(v6,r28)
		v2 = vor(v2,v7)
	}
	{
		v23:22.uw = vunpack(v20.uh)
		v6 = vor(v6,v7)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r5)
		v21:20.uh = vunpack(v6.ub)
		v6 = v8
		r5:4 = memd(r1+#40)
	}
	{
		v6.w = vinsert(r20)
		v23 = vdelta(v22,v31)
	}
	{
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v4 = valign(v4,v4,#4)
		v9 = vmux(q0,v23,v12)
		v12 = v8
	}
	{
		v4.w = vinsert(r4)
		v12.w = vinsert(r10)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r21)
		v11 = valign(v11,v11,#4)
		r21:20 = memd(r8+#24)
	}
	{
		v11.w = vinsert(r15)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r5)
		v12 = valign(v12,v12,#4)
		r5:4 = memd(r3+#56)
	}
	{
		v12.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
		r11:10 = memd(r3+#40)
	}
	{
		v6.w = vinsert(r16)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r4)
		v4 = vror(v4,r28)
	}
	{
		v12 = valign(v12,v12,#4)
		v4 = vor(v4,v7)
	}
	{
		v12.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
		v11 = valign(v11,v11,#4)
		r17:16 = memd(r8+#48)
	}
	{
		v11.w = vinsert(r5)
		v29:28.uh = vunpack(v4.ub)
		r5:4 = memd(r3+#32)
	}
	{
		v4 = valign(v12,v12,#4)
	}
	{
		v4.w = vinsert(r21)
		v6 = vror(v6,r28)
		r21:20 = memd(r8+#8)
	}
	{
		v11 = vror(v11,r28)
		v6 = vor(v6,v7)
	}
	{
		v27:26.uw = vunpack(v24.uh)
		v11 = vor(v11,v7)
	}
	{
		v4 = vror(v4,r28)
		v1 = vmux(q0,v0,v26)
	}
	{
		v25:24.uh = vunpack(v2.ub)
		v4 = vor(v4,v7)
	}
	{
		v23:22.uw = vunpack(v20.uh)
	}
	{
		v21:20.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		r4 = addasl(r18,r0,#7)
		v6.w = vinsert(r4)
		v17:16.uh = vunpack(v11.ub)
	}
	{
		r0 = asl(r0,#7)
		v27:26.uw = vunpack(v24.uh)
	}
	{
		v2 = vdelta(v22,v31)
	}
	{
		v23:22.uw = vunpack(v16.uh)
	}
	{
		v17:16.uh = vunpack(v4.ub)
		v4 = v8
	}
	{
		v4.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
		v13:12.uw = vunpack(v28.uh)
	}
	{
		v10 = vdelta(v26,v31)
		v11 = vmux(q0,v2,v12)
	}
	{
		v15:14.uw = vunpack(v20.uh)
	}
	{
		v4 = valign(v4,v4,#4)
		v13 = vmux(q0,v10,v14)
		v10 = v8
		v14 = v8
	}
	{
		v10.w = vinsert(r16)
		v4.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r4+#16)
	}
	{
		v6.w = vinsert(r10)
		v14.w = vinsert(r22)
		v25:24.uw = vunpack(v16.uh)
	}
	{
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
		r17:16 = memd(r4+#48)
	}
	{
		v4.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r11)
		v14 = valign(v14,v14,#4)
		r11:10 = memd(r8+#56)
	}
	{
		v14.w = vinsert(r23)
		v10 = valign(v10,v10,#4)
		r23:22 = memd(r8+#32)
	}
	{
		v10.w = vinsert(r10)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r21)
		v6 = vror(v6,r28)
		r21:20 = memd(r4+#24)
	}
	{
		v14 = valign(v14,v14,#4)
		v6 = vor(v6,v7)
	}
	{
		v14.w = vinsert(r20)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r11)
		r0 = add(r19,r25)
		v4 = vror(v4,r28)
		r11:10 = memd(r18+r0<<#0)
	}
	{
		r9 = addasl(r18,r0,#7)
		r0 = asl(r0,#7)
		v17:16.uh = vunpack(v6.ub)
		v4 = vor(v4,v7)
	}
	{
		v6 = valign(v14,v14,#4)
	}
	{
		v6.w = vinsert(r21)
		v10 = vror(v10,r28)
		r21:20 = memd(r8+#40)
		r25:24 = memd(r9+#16)
	}
	{
		v15:14.uw = vunpack(v16.uh)
		v10 = vor(v10,v7)
	}
	{
		v17:16.uh = vunpack(v4.ub)
	}
	{
		v4 = vror(v6,r28)
		v6 = v8
	}
	{
		v6.w = vinsert(r22)
		v19:18.uh = vunpack(v10.ub)
		v4 = vor(v4,v7)
	}
	{
		v2 = vdelta(v22,v31)
		v10 = v8
	}
	{
		v10.w = vinsert(r10)
		v12 = vdelta(v24,v31)
		v15 = vmux(q0,v2,v14)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v17:16.uw = vunpack(v18.uh)
		v2 = vmux(q0,v12,v16)
		v12 = v8
	}
	{
		v12.w = vinsert(r16)
		v19:18.uh = vunpack(v4.ub)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r23)
		v4 = vdelta(v16,v31)
		r23:22 = memd(r4+#8)
	}
	{
		v17:16.uw = vunpack(v18.uh)
	}
	{
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
		r11:10 = memd(r4+#56)
	}
	{
		v6.w = vinsert(r20)
		v14 = vdelta(v16,v31)
		v16 = v8
	}
	{
		v16.w = vinsert(r24)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r21)
		v16 = valign(v16,v16,#4)
		r21:20 = memd(r4+#32)
	}
	{
		v16.w = vinsert(r25)
		v12 = valign(v12,v12,#4)
		r25:24 = memd(r9+#48)
	}
	{
		v12.w = vinsert(r17)
		v10 = valign(v10,v10,#4)
		r17:16 = memd(r9+#24)
	}
	{
		v10.w = vinsert(r23)
		r0 = add(r19,r26)
		v6 = vror(v6,r28)
		r23:22 = memd(r18+r0<<#0)
	}
	{
		r2 = addasl(r18,r0,#7)
		r0 = asl(r0,#7)
		v16 = valign(v16,v16,#4)
		v6 = vor(v6,v7)
	}
	{
		v16.w = vinsert(r16)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r10)
		v10 = vror(v10,r28)
		r15:14 = memd(r2+#24)
	}
	{
		v19:18.uh = vunpack(v6.ub)
		v10 = vor(v10,v7)
	}
	{
		v6 = valign(v16,v16,#4)
	}
	{
		v6.w = vinsert(r17)
		v12 = valign(v12,v12,#4)
		r17:16 = memd(r4+#40)
	}
	{
		v12.w = vinsert(r11)
		v17:16.uw = vunpack(v18.uh)
	}
	{
		v19:18.uh = vunpack(v10.ub)
		v10 = v8
		v17 = vmux(q0,v4,v16)
	}
	{
		v10.w = vinsert(r20)
		v6 = vror(v6,r28)
		v16 = v8
	}
	{
		v12 = vror(v12,r28)
		v6 = vor(v6,v7)
	}
	{
		v10 = valign(v10,v10,#4)
		v12 = vor(v12,v7)
	}
	{
		v10.w = vinsert(r21)
		v29:28.uh = vunpack(v6.ub)
		v6 = v8
		r21:20 = memd(r2+#16)
	}
	{
		v6.w = vinsert(r22)
		v27:26.uh = vunpack(v12.ub)
		v12 = v8
	}
	{
		v12.w = vinsert(r24)
		v16.w = vinsert(r20)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r16)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r23)
		v12 = valign(v12,v12,#4)
		r23:22 = memd(r9+#8)
	}
	{
		v12.w = vinsert(r25)
		v10 = valign(v10,v10,#4)
		r25:24 = memd(r9+#56)
	}
	{
		v10.w = vinsert(r17)
		r0 = add(r19,r27)
		v6 = valign(v6,v6,#4)
		r17:16 = memd(r18+r0<<#0)
	}
	{
		r5 = addasl(r18,r0,#7)
		v6.w = vinsert(r22)
		v16 = valign(v16,v16,#4)
	}
	{
		r0 = asl(r0,#7)
		v16.w = vinsert(r21)
		v12 = valign(v12,v12,#4)
		r21:20 = memd(r2+#48)
	}
	{
		v12.w = vinsert(r24)
		r0 = add(r30,#-8752)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r18+r0<<#0)
	}
	{
		v6.w = vinsert(r23)
		v10 = vror(v10,r28)
		r23:22 = memd(r5+#16)
		r11:10 = memd(r5+#24)
	}
	{
		v16 = valign(v16,v16,#4)
		v10 = vor(v10,v7)
		r19:18 = memd(r5+#48)
	}
	{
		v16.w = vinsert(r14)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r25)
		v21:20.uw = vunpack(v26.uh)
		r25:24 = memd(r2+#8)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v27:26.uh = vunpack(v10.ub)
		v6 = vor(v6,v7)
	}
	{
		v10 = valign(v16,v16,#4)
	}
	{
		v10.w = vinsert(r15)
		v12 = vror(v12,r28)
		r15:14 = memd(r7+#120)
	}
	{
		v23:22.uh = vunpack(v6.ub)
		v12 = vor(v12,v7)
	}
	{
		v6 = vror(v10,r28)
		v10 = v8
	}
	{
		v10.w = vinsert(r16)
		v25:24.uw = vunpack(v28.uh)
		v6 = vor(v6,v7)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v29:28.uh = vunpack(v12.ub)
		v0 = vmux(q0,v14,v18)
		v18 = v8
	}
	{
		v18.w = vinsert(r26)
		v4 = vdelta(v20,v31)
	}
	{
		v21:20.uw = vunpack(v26.uh)
	}
	{
		v14 = vdelta(v24,v31)
		v21 = vmux(q0,v4,v20)
	}
	{
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v12 = valign(v10,v10,#4)
		v4 = vmux(q0,v14,v22)
		v14 = v8
	}
	{
		v12.w = vinsert(r17)
		v14.w = vinsert(r22)
		v23:22.uw = vunpack(v28.uh)
		r17:16 = memd(r5+#8)
	}
	{
		v29:28.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r20)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r24)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r2+#32)
	}
	{
		v6.w = vinsert(r21)
		v12 = valign(v12,v12,#4)
		r21:20 = memd(r2+#56)
	}
	{
		v12.w = vinsert(r25)
		v18 = valign(v18,v18,#4)
		r25:24 = memd(r5+#56)
	}
	{
		v18.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r9+#40)
	}
	{
		v6.w = vinsert(r20)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r10)
		v12 = vror(v12,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v12 = vor(v12,v7)
	}
	{
		v6.w = vinsert(r21)
		v18 = valign(v18,v18,#4)
		r21:20 = memd(r9+#32)
	}
	{
		v18.w = vinsert(r16)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r11)
		v6 = vror(v6,r28)
		r11:10 = memd(r7+#64)
	}
	{
		v27:26.uh = vunpack(v12.ub)
		v6 = vor(v6,v7)
	}
	{
		v12 = valign(v18,v18,#4)
		v18 = v8
	}
	{
		v12.w = vinsert(r17)
		v18.w = vinsert(r22)
		v14 = vror(v14,r28)
		r17:16 = memd(r7+#96)
	}
	{
		v25:24.uw = vunpack(v28.uh)
		v14 = vor(v14,v7)
	}
	{
		v10 = vdelta(v22,v31)
	}
	{
		v16 = vdelta(v24,v31)
	}
	{
		v25:24.uh = vunpack(v6.ub)
	}
	{
		v6 = vror(v12,r28)
	}
	{
		v29:28.uh = vunpack(v14.ub)
		v14 = v8
		v6 = vor(v6,v7)
	}
	{
		v14.w = vinsert(r18)
		v23:22.uw = vunpack(v26.uh)
	}
	{
		v23:22.uw = vunpack(v24.uh)
		v12 = vmux(q0,v16,v22)
		v16 = v8
	}
	{
		v16.w = vinsert(r20)
		v25:24.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v14,v14,#4)
	}
	{
		v6.w = vinsert(r19)
		v16 = valign(v16,v16,#4)
		r19:18 = memd(r5+#32)
	}
	{
		v16.w = vinsert(r21)
		v20 = vdelta(v22,v31)
		v22 = v8
		r21:20 = memd(r5+#40)
	}
	{
		v22.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r23)
		v22 = valign(v22,v22,#4)
		r23:22 = memd(r7+#80)
	}
	{
		v22.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
		r19:18 = memd(r2+#40)
	}
	{
		v6.w = vinsert(r25)
		v16 = valign(v16,v16,#4)
		r25:24 = memd(r7+#88)
	}
	{
		v16.w = vinsert(r26)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r18)
		v6 = vror(v6,r28)
	}
	{
		v22 = valign(v22,v22,#4)
		v6 = vor(v6,v7)
	}
	{
		v22.w = vinsert(r20)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r27)
		v18 = valign(v18,v18,#4)
		r27:26 = memd(r7+#112)
	}
	{
		v18.w = vinsert(r19)
		v14 = vdelta(v24,v31)
		r19:18 = memd(r6+#80)
	}
	{
		v29:28.uw = vunpack(v28.uh)
	}
	{
		v16 = vror(v16,r28)
		v14 = vmux(q0,v14,v28)
	}
	{
		v29:28.uh = vunpack(v6.ub)
		v16 = vor(v16,v7)
	}
	{
		v6 = valign(v22,v22,#4)
	}
	{
		v6.w = vinsert(r21)
		v18 = vror(v18,r28)
		r21:20 = memd(r7+#72)
	}
	{
		v25:24.uh = vunpack(v16.ub)
		v18 = vor(v18,v7)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v27:26.uh = vunpack(v18.ub)
		v6 = vor(v6,v7)
	}
	{
		r0 = add(r30,#-8624)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-12336)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v23:22.uw = vunpack(v28.uh)
	}
	{
		v23.w = vmpyieo(v14.h,v18.h)
		v29:28.uh = vunpack(v6.ub)
		v6 = vmux(q0,v10,v24)
	}
	{
		v23.w += vmpyie(v14.w,v18.h)
		r0 = add(r30,#-12208)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9264)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v12.h,v24.h)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		r0 = add(r30,#-9136)
		v10 = vmux(q0,v20,v26)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w += vmpyie(v12.w,v24.h)
		r0 = add(r30,#-10032)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v4.h,v26.h)
		v22 = vdelta(v22,v31)
	}
	{
		v27.w += vmpyie(v4.w,v26.h)
		v29:28.uw = vunpack(v28.uh)
	}
	{
		r0 = add(r30,#-9904)
		v16 = vmux(q0,v22,v28)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-12080)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w = vmpyieo(v16.h,v4.h)
		r0 = add(r30,#-11952)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v16.w,v4.h)
		r0 = add(r30,#-9776)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w = vmpyieo(v10.h,v28.h)
		r0 = add(r30,#-9648)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w += vmpyie(v10.w,v28.h)
		r0 = add(r30,#-7984)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29.w = vmpyieo(v0.h,v4.h)
		v10 = v4
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-7856)
	}
	{
		v26.w = vmpyieo(v6.h,v4.h)
		r0 = add(r30,#-7216)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w += vmpyie(v6.w,v4.h)
		r0 = add(r30,#-7088)
		v18 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29.w += vmpyie(v0.w,v10.h)
		r0 = add(r30,#-2608)
		v19 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-2480)
		v19:18.w = vadd(v23:22.w,v19:18.w)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-1584)
		v23:22.w = vadd(v19:18.w,v25:24.w)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w = vmpyieo(v30.h,v4.h)
		v0 = v4
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-1456)
		v10 = v4
		v6 = v8
	}
	{
		v28.w = vmpyieo(v21.h,v4.h)
		r0 = add(r30,#-4656)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v28.w += vmpyie(v21.w,v10.h)
		r0 = add(r30,#-4528)
		v20 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9520)
		v5:4.w = vadd(v27:26.w,v23:22.w)
		v21 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w = vmpyieo(v13.h,v20.h)
		r0 = add(r30,#-9392)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v20.h)
		r0 = add(r30,#-1328)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w = vmpyieo(v2.h,v22.h)
		r0 = add(r30,#-1200)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v2.w,v22.h)
		r0 = add(r30,#-11568)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w = vmpyieo(v17.h,v12.h)
		v10 = v12
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vinsert(r22)
		r0 = add(r30,#-11440)
		v5:4.w = vadd(v5:4.w,v29:28.w)
		v26 = v8
	}
	{
		v20.w = vmpyieo(v15.h,v12.h)
		r0 = add(r30,#-1072)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v20.w += vmpyie(v15.w,v12.h)
		r0 = add(r30,#-944)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v17.w,v10.h)
		r0 = add(r30,#-560)
		v13 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w = vmpyieo(v9.h,v12.h)
		r0 = add(r30,#-432)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v9.w,v12.h)
		r0 = add(r30,#-816)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v12.w = vmpyieo(v11.h,v14.h)
		v10 = v14
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-688)
		v2 = valign(v6,v6,#4)
		v5:4.w = vadd(v5:4.w,v23:22.w)
		v6 = v8
	}
	{
		v12.w += vmpyie(v11.w,v10.h)
		r0 = add(r30,#-304)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w = vmpyieo(v3.h,v14.h)
		r0 = add(r30,#-176)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r23)
		r0 = add(r30,#-2096)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15.w += vmpyie(v3.w,v14.h)
		v5:4.w = vadd(v21:20.w,v5:4.w)
		r23:22 = memd(r7+#104)
	}
	{
		v14.w = vmpyieo(v1.h,v10.h)
		v6.w = vinsert(r26)
		v2 = valign(v2,v2,#4)
	}
	{
		v14.w += vmpyie(v1.w,v10.h)
		r0 = add(r30,#-1968)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r24)
		r0 = add(r30,#-13616)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v19.w += vmpyie(v30.w,v0.h)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v6 = valign(v6,v6,#4)
		v5:4.w = vadd(v5:4.w,v13:12.w)
	}
	{
		v6.w = vinsert(r27)
		v18.w = vmpyieo(v1.h,v10.h)
		v2 = valign(v2,v2,#4)
		r27:26 = memd(r6+#88)
	}
	{
		v2.w = vinsert(r25)
		v5:4.w = vadd(v5:4.w,v15:14.w)
		r25:24 = memd(r6+#112)
	}
	{
		v18.w += vmpyie(v1.w,v10.h)
		v3 = valign(v6,v6,#4)
		v1 = v8
	}
	{
		v1.w = vinsert(r10)
		v3.w = vinsert(r14)
		v2 = vror(v2,r28)
		v6 = v8
	}
	{
		v6.w = vinsert(r16)
		v26.w = vinsert(r24)
		v2 = vor(v2,v7)
	}
	{
		v1 = valign(v1,v1,#4)
		v5:4.w = vadd(v19:18.w,v5:4.w)
	}
	{
		v1.w = vinsert(r11)
		v0 = valign(v3,v3,#4)
		r11:10 = memd(r8+#88)
	}
	{
		v0.w = vinsert(r15)
		v13:12.uh = vunpack(v2.ub)
		r15:14 = memd(r4+#96)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
		v3:2.uw = vunpack(v12.uh)
		r17:16 = memd(r6+#120)
	}
	{
		v0 = vror(v0,r28)
	}
	{
		v1 = valign(v1,v1,#4)
		v3 = vor(v0,v7)
	}
	{
		v1.w = vinsert(r21)
		v25 = valign(v6,v6,#4)
		r21:20 = memd(r6+#64)
	}
	{
		v25.w = vinsert(r22)
		v21:20.uh = vunpack(v3.ub)
		v3 = v8
	}
	{
		v3.w = vinsert(r18)
		v1 = vror(v1,r28)
	}
	{
		v9 = valign(v26,v26,#4)
		v1 = vor(v1,v7)
	}
	{
		v9.w = vinsert(r25)
		v24 = valign(v3,v3,#4)
		r25:24 = memd(r1+#88)
	}
	{
		v24.w = vinsert(r19)
		v3 = valign(v25,v25,#4)
		r19:18 = memd(r1+#80)
	}
	{
		v3.w = vinsert(r23)
		v27:26.uh = vunpack(v1.ub)
		r23:22 = memd(r6+#96)
	}
	{
		v1 = valign(v9,v9,#4)
	}
	{
		v1.w = vinsert(r16)
		v3 = vror(v3,r28)
	}
	{
		v23:22.uw = vunpack(v20.uh)
		v3 = vor(v3,v7)
	}
	{
		v27 = valign(v1,v1,#4)
	}
	{
		v27.w = vinsert(r17)
		v29:28.uh = vunpack(v3.ub)
		r17:16 = memd(r1+#64)
	}
	{
		v0 = vdelta(v2,v31)
	}
	{
		v3 = vror(v27,r28)
	}
	{
		v11:10.uw = vunpack(v26.uh)
		v3 = vor(v3,v7)
	}
	{
		v13:12.uw = vunpack(v28.uh)
		v1 = vmux(q0,v0,v10)
		v28 = v8
	}
	{
		v28.w = vinsert(r18)
		v6 = vdelta(v22,v31)
		v10 = v8
	}
	{
		v10.w = vinsert(r22)
		v2 = valign(v24,v24,#4)
		v0 = vmux(q0,v6,v12)
		v6 = v8
	}
	{
		v6.w = vinsert(r20)
		v2.w = vinsert(r26)
		v25:24.uh = vunpack(v3.ub)
	}
	{
		v26 = valign(v10,v10,#4)
	}
	{
		v26.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r1+#120)
	}
	{
		v6.w = vinsert(r21)
		v29 = valign(v28,v28,#4)
		r21:20 = memd(r3+#64)
	}
	{
		v29.w = vinsert(r19)
		v11:10.uw = vunpack(v24.uh)
		r19:18 = memd(r1+#112)
	}
	{
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r6+#72)
	}
	{
		v6.w = vinsert(r26)
		v27 = vdelta(v10,v31)
		v10 = v8
		r7:6 = memd(r6+#104)
	}
	{
		v10.w = vinsert(r16)
		v25 = valign(v29,v29,#4)
	}
	{
		v25.w = vinsert(r24)
		v9 = valign(v26,v26,#4)
	}
	{
		v9.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
		v10 = valign(v10,v10,#4)
		r27:26 = memd(r3+#96)
	}
	{
		v10.w = vinsert(r17)
		v2 = vror(v2,r28)
		r17:16 = memd(r3+#104)
	}
	{
		v11 = valign(v25,v25,#4)
		v2 = vor(v2,v7)
	}
	{
		v11.w = vinsert(r25)
		v9 = valign(v9,v9,#4)
		r25:24 = memd(r3+#72)
	}
	{
		v9.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r1+#72)
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
	}
	{
		v10.w = vinsert(r6)
		v11 = vror(v11,r28)
	}
	{
		v21:20.uh = vunpack(v2.ub)
		v11 = vor(v11,v7)
	}
	{
		v9 = vror(v9,r28)
	}
	{
		v23:22.uw = vunpack(v20.uh)
		v9 = vor(v9,v7)
	}
	{
		v21:20.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v10,v10,#4)
	}
	{
		v6.w = vinsert(r7)
		v29:28.uh = vunpack(v11.ub)
		v11 = v8
		r7:6 = memd(r3+#80)
	}
	{
		v11.w = vinsert(r6)
		v25:24.uh = vunpack(v9.ub)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v11 = valign(v11,v11,#4)
		v6 = vor(v6,v7)
	}
	{
		v11.w = vinsert(r7)
		v13:12.uw = vunpack(v28.uh)
		v28 = v8
		r7:6 = memd(r3+#88)
	}
	{
		v28.w = vinsert(r18)
		v17:16.uh = vunpack(v6.ub)
		v13 = v8
	}
	{
		v13.w = vinsert(r20)
		v6 = valign(v11,v11,#4)
	}
	{
		v6.w = vinsert(r6)
		v9 = valign(v28,v28,#4)
	}
	{
		v9.w = vinsert(r19)
		v15:14.uw = vunpack(v24.uh)
		r19:18 = memd(r8+#72)
	}
	{
		v29 = valign(v6,v6,#4)
		v6 = v8
		v3 = vmux(q0,v27,v14)
	}
	{
		v29.w = vinsert(r7)
		v13 = valign(v13,v13,#4)
		r7:6 = memd(r3+#112)
	}
	{
		v13.w = vinsert(r21)
		v6.w = vinsert(r6)
		v15 = valign(v9,v9,#4)
		r21:20 = memd(r8+#96)
	}
	{
		v15.w = vinsert(r22)
		v10 = vdelta(v12,v31)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v10 = valign(v13,v13,#4)
		v11 = vmux(q0,v10,v16)
	}
	{
		v10.w = vinsert(r24)
		v12 = valign(v15,v15,#4)
	}
	{
		v12.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r4+#64)
	}
	{
		v6.w = vinsert(r7)
		v10 = valign(v10,v10,#4)
		r7:6 = memd(r3+#120)
	}
	{
		v10.w = vinsert(r25)
		v30 = vdelta(v22,v31)
		r25:24 = memd(r8+#104)
	}
	{
		v23:22.uw = vunpack(v20.uh)
	}
	{
		v2 = vror(v29,r28)
		v9 = vmux(q0,v30,v22)
	}
	{
		v30 = valign(v6,v6,#4)
		v2 = vor(v2,v7)
	}
	{
		v30.w = vinsert(r6)
		v12 = vror(v12,r28)
	}
	{
		v10 = vror(v10,r28)
		v6 = vor(v12,v7)
		v12 = v8
	}
	{
		v27:26.uh = vunpack(v2.ub)
	}
	{
		v21:20.uh = vunpack(v6.ub)
		v6 = vor(v10,v7)
	}
	{
		v2 = valign(v30,v30,#4)
	}
	{
		v2.w = vinsert(r7)
		v17:16.uh = vunpack(v6.ub)
		r7:6 = memd(r8+#80)
	}
	{
		v12.w = vinsert(r6)
		v29:28.uw = vunpack(v26.uh)
	}
	{
		v2 = vror(v2,r28)
	}
	{
		v6 = valign(v12,v12,#4)
		v12 = vor(v2,v7)
	}
	{
		v6.w = vinsert(r7)
		v10 = vdelta(v28,v31)
		r7:6 = memd(r8+#64)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v15:14.uw = vunpack(v20.uh)
		v2 = vmux(q0,v10,v16)
		v10 = v8
	}
	{
		v10.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
		v15 = v8
	}
	{
		v6.w = vinsert(r10)
		v15.w = vinsert(r26)
		v17:16.uh = vunpack(v12.ub)
	}
	{
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r7)
		v12 = valign(v15,v15,#4)
		v15 = v8
		r7:6 = memd(r8+#112)
	}
	{
		v15.w = vinsert(r6)
		v12.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r4+#72)
	}
	{
		v6.w = vinsert(r11)
		v10 = valign(v10,v10,#4)
		r11:10 = memd(r4+#104)
	}
	{
		v10.w = vinsert(r18)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r8+#120)
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
	}
	{
		v10.w = vinsert(r19)
		v23:22.uw = vunpack(v16.uh)
	}
	{
		v17:16.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v15,v15,#4)
	}
	{
		v6.w = vinsert(r6)
		v10 = vror(v10,r28)
	}
	{
		v12 = valign(v12,v12,#4)
		v10 = vor(v10,v7)
	}
	{
		v12.w = vinsert(r16)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v19:18.uh = vunpack(v10.ub)
		v10 = v8
		r7:6 = memd(r4+#80)
	}
	{
		v10.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r17)
		v6 = vror(v6,r28)
		r17:16 = memd(r9+#88)
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
		r19:18 = memd(r9+#120)
	}
	{
		v10.w = vinsert(r7)
		v17:16.uw = vunpack(v16.uh)
		r7:6 = memd(r4+#88)
	}
	{
		v12 = vror(v12,r28)
	}
	{
		v15 = vdelta(v16,v31)
		v12 = vor(v12,v7)
	}
	{
		v19:18.uw = vunpack(v18.uh)
	}
	{
		v10 = valign(v10,v10,#4)
		v15 = vmux(q0,v15,v18)
	}
	{
		v10.w = vinsert(r6)
		v19:18.uh = vunpack(v6.ub)
		v6 = v8
	}
	{
		v6.w = vinsert(r22)
		v17:16.uh = vunpack(v12.ub)
		v12 = v8
	}
	{
		v12.w = vinsert(r20)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r4+#112)
	}
	{
		v6.w = vinsert(r23)
		v17:16.uw = vunpack(v16.uh)
		r23:22 = memd(r2+#80)
	}
	{
		v12 = valign(v12,v12,#4)
		v17 = v8
	}
	{
		v17.w = vinsert(r6)
		v12.w = vinsert(r21)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v10 = vror(v10,r28)
	}
	{
		v17 = valign(v17,v17,#4)
		v10 = vor(v10,v7)
	}
	{
		v17.w = vinsert(r7)
		v12 = valign(v12,v12,#4)
		r7:6 = memd(r4+#120)
	}
	{
		v12.w = vinsert(r24)
		v19:18.uw = vunpack(v18.uh)
		r4 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
		v13 = vdelta(v14,v31)
		r27:26 = memd(r2+#88)
	}
	{
		v14 = vdelta(v22,v31)
	}
	{
		v14 = vdelta(v18,v31)
		v16 = vmux(q0,v14,v16)
	}
	{
		v19:18.uh = vunpack(v10.ub)
	}
	{
		v10 = valign(v17,v17,#4)
		v17 = v8
	}
	{
		v10.w = vinsert(r6)
		v17.w = vinsert(r14)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r25)
		v6 = vror(v6,r28)
		r25:24 = memd(r2+#112)
	}
	{
		v10 = valign(v10,v10,#4)
		v6 = vor(v6,v7)
	}
	{
		v10.w = vinsert(r7)
		v12 = vror(v12,r28)
		r7:6 = memd(r9+#80)
	}
	{
		v27:26.uh = vunpack(v6.ub)
		v6 = v8
		v12 = vor(v12,v7)
	}
	{
		v6.w = vinsert(r6)
		v10 = vror(v10,r28)
	}
	{
		v19:18.uw = vunpack(v18.uh)
		v10 = vor(v10,v7)
	}
	{
		v25:24.uh = vunpack(v12.ub)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v12 = vdelta(v18,v31)
		r7:6 = memd(r9+#112)
	}
	{
		v19:18.uw = vunpack(v24.uh)
	}
	{
		v25:24.uh = vunpack(v10.ub)
		v10 = v8
	}
	{
		v10.w = vinsert(r6)
		v21 = valign(v17,v17,#4)
		v17 = vmux(q0,v14,v18)
		v18 = v8
	}
	{
		v21.w = vinsert(r15)
		v29:28.uw = vunpack(v26.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		v19 = vmux(q0,v12,v28)
	}
	{
		v6.w = vinsert(r16)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r7)
		v12 = valign(v21,v21,#4)
		r7:6 = memd(r9+#64)
	}
	{
		v12.w = vinsert(r10)
		v18.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
		r21:20 = memd(r9+#96)
	}
	{
		v6.w = vinsert(r17)
		v10 = valign(v10,v10,#4)
		r17:16 = memd(r5+#88)
	}
	{
		v10.w = vinsert(r18)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r11)
		v6 = vror(v6,r28)
		r11:10 = memd(r5+#80)
	}
	{
		v18 = valign(v18,v18,#4)
		v6 = vor(v6,v7)
	}
	{
		v18.w = vinsert(r7)
		v10 = valign(v10,v10,#4)
		r7:6 = memd(r9+#72)
	}
	{
		v10.w = vinsert(r19)
		v12 = vror(v12,r28)
		r19:18 = memd(r2+#64)
	}
	{
		v27:26.uw = vunpack(v24.uh)
		v12 = vor(v12,v7)
	}
	{
		v25:24.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v18,v18,#4)
	}
	{
		v6.w = vinsert(r6)
		v10 = vror(v10,r28)
	}
	{
		v29:28.uh = vunpack(v12.ub)
		v12 = vor(v10,v7)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v14 = vdelta(v26,v31)
		r7:6 = memd(r9+#104)
	}
	{
		v21:20.uw = vunpack(v28.uh)
	}
	{
		v27:26.uh = vunpack(v12.ub)
		v12 = v8
		v20 = vmux(q0,v14,v20)
	}
	{
		v12.w = vinsert(r22)
		v6 = vror(v6,r28)
		v14 = v8
		v21 = v8
	}
	{
		v14.w = vinsert(r20)
		v21.w = vinsert(r10)
		v6 = vor(v6,v7)
	}
	{
		v18 = valign(v12,v12,#4)
	}
	{
		v18.w = vinsert(r23)
		v14 = valign(v14,v14,#4)
		r23:22 = memd(r2+#72)
	}
	{
		v14.w = vinsert(r21)
		v29:28.uh = vunpack(v6.ub)
		v6 = v8
		r21:20 = memd(r5+#112)
	}
	{
		v6.w = vinsert(r24)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r26)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r25)
		v18 = valign(v18,v18,#4)
		r25:24 = memd(r5+#120)
	}
	{
		v18.w = vinsert(r27)
		v21 = valign(v21,v21,#4)
	}
	{
		v21.w = vinsert(r11)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r7)
		v23:22.uw = vunpack(v24.uh)
		r7:6 = memd(r2+#120)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v18 = vror(v18,r28)
	}
	{
		v21 = valign(v21,v21,#4)
		v18 = vor(v18,v7)
	}
	{
		v21.w = vinsert(r16)
		v14 = vror(v14,r28)
	}
	{
		v10 = vdelta(v22,v31)
		v14 = vor(v14,v7)
	}
	{
		v23:22.uw = vunpack(v26.uh)
	}
	{
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v12 = vdelta(v22,v31)
		r7:6 = memd(r1+#96)
		r1:0 = memd(r1+#104)
	}
	{
		v23:22.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v18.ub)
	}
	{
		v18 = valign(v21,v21,#4)
	}
	{
		v18.w = vinsert(r17)
		v25:24.uh = vunpack(v14.ub)
		v14 = vmux(q0,v10,v22)
	}
	{
		v6 = vror(v6,r28)
		v22 = v8
	}
	{
		v22.w = vinsert(r18)
		v18 = vror(v18,r28)
		v6 = vor(v6,v7)
	}
	{
		v25:24.uw = vunpack(v24.uh)
		v18 = vor(v18,v7)
	}
	{
		v27:26.uw = vunpack(v28.uh)
		v10 = vmux(q0,v12,v24)
		v12 = v8
	}
	{
		v12.w = vinsert(r6)
		v29:28.uh = vunpack(v18.ub)
	}
	{
		v25:24.uh = vunpack(v6.ub)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r7)
		v6 = valign(v22,v22,#4)
		r7:6 = memd(r5+#64)
	}
	{
		v6.w = vinsert(r19)
		v23:22.uw = vunpack(v24.uh)
		r27:26 = memd(r5+#96)
	}
	{
		v29:28.uw = vunpack(v28.uh)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r0)
		v18 = vdelta(v22,v31)
		v22 = v8
	}
	{
		v23 = vdelta(v28,v31)
		v29:28 = vcombine(v8,v8)
	}
	{
		v8.w = vinsert(r20)
		v22.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r1)
		v29.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r2+#96)
	}
	{
		v28.w = vinsert(r0)
		v6.w = vinsert(r22)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r21)
		v22 = valign(v22,v22,#4)
	}
	{
		v22.w = vinsert(r7)
		v24 = valign(v28,v28,#4)
		r7:6 = memd(r5+#72)
	}
	{
		v24.w = vinsert(r1)
		v8 = valign(v8,v8,#4)
		r1:0 = memd(r2+#104)
		r3:2 = memd(r5+#104)
	}
	{
		v8.w = vinsert(r24)
		v22 = valign(v22,v22,#4)
	}
	{
		v22.w = vinsert(r6)
		v25 = valign(v29,v29,#4)
		r6 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		v25.w = vinsert(r27)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r25)
		v22 = valign(v22,v22,#4)
	}
	{
		v22.w = vinsert(r7)
		r7 = #64
		v24 = valign(v24,v24,#4)
	}
	{
		v24.w = vinsert(r0)
		r0 = add(r30,#-4400)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r23)
		v8 = vror(v8,r28)
	}
	{
		v25 = valign(v25,v25,#4)
		v8 = vor(v8,v7)
	}
	{
		v25.w = vinsert(r2)
		r2 = add(r30,#-14128)
		v22 = vror(v22,r28)
	}
	{
		v24 = valign(v24,v24,#4)
		v22 = vor(v22,v7)
	}
	{
		v24.w = vinsert(r1)
		v29:28.uh = vunpack(v8.ub)
		r1 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		v6 = vror(v6,r28)
	}
	{
		v8 = valign(v25,v25,#4)
		v6 = vor(v6,v7)
	}
	{
		v8.w = vinsert(r3)
		r3 = add(r30,#-12848)
		v21 = vdelta(v26,v31)
	}
	{
		v27:26.uh = vunpack(v22.ub)
	}
	{
		v22 = vror(v24,r28)
	}
	{
		v29:28.uw = vunpack(v28.uh)
		v22 = vor(v22,v7)
	}
	{
		v12 = vror(v12,r28)
	}
	{
		v25:24.uh = vunpack(v6.ub)
		v12 = vor(v12,v7)
	}
	{
		v8 = vror(v8,r28)
	}
	{
		v8 = vdelta(v28,v31)
		v7 = vor(v8,v7)
	}
	{
		r0 = add(r30,#-4272)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25:24.uw = vunpack(v24.uh)
	}
	{
		r0 = add(r30,#-10288)
		v21 = vmux(q0,v21,v24)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v31:30.uh = vunpack(v22.ub)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		r0 = add(r30,#-10160)
		v23 = vmux(q0,v23,v26)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27:26.uw = vunpack(v30.uh)
	}
	{
		r0 = add(r30,#-9008)
		v18 = vmux(q0,v18,v26)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v23.h,v24.h)
		v7:6.uh = vunpack(v7.ub)
	}
	{
		v25.w += vmpyie(v23.w,v24.h)
		r0 = add(r30,#-8880)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-11824)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v14.h,v26.h)
		v7:6.uw = vunpack(v6.uh)
	}
	{
		v7.w = vmpyieo(v2.h,v28.h)
		r0 = add(r30,#-11696)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7.w += vmpyie(v2.w,v28.h)
		r0 = add(r30,#-12592)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-12464)
		v6 = vmux(q0,v8,v6)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w = vmpyieo(v21.h,v22.h)
		r0 = add(r30,#-7728)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w = vmpyieo(v6.h,v28.h)
		v31:30.uh = vunpack(v12.ub)
	}
	{
		v24.w += vmpyie(v6.w,v28.h)
		r0 = add(r30,#-7600)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v23.w += vmpyie(v21.w,v22.h)
		r0 = add(r30,#-8496)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w = vmpyieo(v18.h,v28.h)
		v6 = v28
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w += vmpyie(v14.w,v26.h)
		r0 = add(r30,#-8368)
		v14 = v28
	}
	{
		v26.w = vmpyieo(v10.h,v28.h)
		r0 = add(r30,#-6960)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v18.w,v6.h)
		r0 = add(r30,#-6832)
		v28 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w += vmpyie(v10.w,v14.h)
		r0 = add(r30,#-2864)
		v29 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25:24.w = vadd(v25:24.w,v29:28.w)
	}
	{
		r0 = add(r30,#-2736)
		v23:22.w = vadd(v25:24.w,v23:22.w)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5424)
		v28 = v24
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vmpyieo(v16.h,v24.h)
		r0 = add(r30,#-5296)
		v24 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w += vmpyie(v16.w,v28.h)
		r0 = add(r30,#-5168)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v25.w = vmpyieo(v19.h,v24.h)
	}
	{
		v25.w += vmpyie(v19.w,v24.h)
		v19:18.w = vadd(v27:26.w,v23:22.w)
	}
	{
		r0 = add(r30,#-5040)
		v22 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-5680)
		v29 = v22
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w = vmpyieo(v20.h,v22.h)
		r0 = add(r30,#-5552)
		v31 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w += vmpyie(v20.w,v29.h)
		r0 = add(r30,#-8240)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-8112)
		v19:18.w = vadd(v19:18.w,v25:24.w)
		v12 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w = vmpyieo(v15.h,v31.h)
		r0 = add(r30,#-3376)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v20.w = vmpyieo(v17.h,v12.h)
		v27:26.uw = vunpack(v30.uh)
	}
	{
		r0 = add(r30,#-3248)
		v23 = vmux(q0,v13,v26)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v20.w += vmpyie(v17.w,v12.h)
		r0 = add(r30,#-2352)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v11.h,v26.h)
		r0 = add(r30,#-2224)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w += vmpyie(v11.w,v26.h)
		r0 = add(r30,#-3888)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v21.w += vmpyie(v15.w,v31.h)
		r0 = add(r30,#-3760)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-3632)
		v24 = v10
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w = vmpyieo(v23.h,v10.h)
		r0 = add(r30,#-3504)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v29:28.w = vadd(v19:18.w,v21:20.w)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v26.w += vmpyie(v23.w,v24.h)
		v7:6.w = vadd(v7:6.w,v29:28.w)
		r0 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		v11.w = vmpyieo(v9.h,v10.h)
		v17.w = vmpyieo(v1.h,v16.h)
		v7:6.w = vadd(v7:6.w,v27:26.w)
	}
	{
		v11.w += vmpyie(v9.w,v10.h)
		r0 = add(r30,#-3120)
		vmem(r0+#0) = v4
	}
	{
		v17.w += vmpyie(v1.w,v16.h)
		r0 = add(r30,#-2992)
		v25 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-1840)
		v4 = v25
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w = vmpyieo(v3.h,v25.h)
		r0 = add(r30,#-1712)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w += vmpyie(v3.w,v4.h)
		v9 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v16.w = vmpyieo(v0.h,v27.h)
		v31:30.w = vadd(v7:6.w,v11:10.w)
		r0 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		v16.w += vmpyie(v0.w,v27.h)
		r2 = add(r30,#-14000)
		v28 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1:0.w = vadd(v17:16.w,v31:30.w)
		r0 = memw(r30+##-13360)
		vmem(r0+#0) = v5
	}                                       // 4-byte Folded Reload
	{
		v29 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = memw(r30+##-13104)
		vmem(r0+#0) = v0
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-10312)
		vmem(r0+#0) = v1
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r30,#-12720)
		v30 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v31 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v16 = vmem(r2+#0)
		vmem(r0+#0) = v28
	}
	{
		v6 = vmem(r0+#0)
		vmem(r2+#6) = v30
	}
	{
		r2 = memw(r30+##-10320)
		vmem(r2+#7) = v31
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-7232)
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		vmem(r2+#0) = v29
	}
	{
		v17 = vmem(r0+#0)
	}
	{
		v7 = vmem(r2+#0)
	}
.LBB131_70:                             // %"consume convolved"
                                        //   in Loop: Header=BB131_71 Depth=4
	{
		r2 = add(r3,##-39808)
		r0 = memw(r30+##-21432)
	}                                       // 4-byte Folded Reload
	{
		v10 = vxor(v10,v10)
		memw(r30+##-13232) = r2
	}                                       // 4-byte Folded Spill
	{
		r12 = add(r3,##-40064)
		r18 = ##2147483647
	}
	{
		r19 = #0
		r14 = ##-2147483648
		v18 = vmem(r0+#0)
	}
	{
		r0 = #32767
		r15 = #-1
		v9 = vror(v10,r7)
	}
	{
		v12 = vsplat(r0)
		r0 = #-32768
		v27:26 = vcombine(v10,v10)
		v19 = v10
	}
	{
		v11 = vsplat(r0)
		r0 = memw(r30+##-21440)
		vmem(r2+#0) = v7
	}                                       // 4-byte Folded Reload
	{
		v29:28 = vcombine(v10,v10)
		v31:30 = vcombine(v10,v10)
	}
	{
		v25:24 = vcombine(v10,v10)
		v15 = vmem(r0+#0)
	}
	{
		v23:22 = vcombine(v10,v10)
		v13 = v10
		r0 = memw(r30+##-21264)
	}                                       // 4-byte Folded Reload
	{
		v21:20 = vcombine(v10,v10)
		v3:2 = vcombine(v10,v10)
	}
	{
		r5 = mpyi(r1,r0)
		r2 = mpyi(r4,r0)
		r1 = memw(r30+##-21448)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-21464)
		memw(r30+#-816) = r5
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r3,##-40192)
		v5:4 = vcombine(v10,v10)
		memw(r30+#-560) = r2
	}                                       // 4-byte Folded Spill
	{
		q0 = vcmp.eq(v0.b,v10.b)
		r1 = memw(r30+##-21456)
		v0.cur = vmem(r1+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-16304)
		v8 = v10
		q1 = vcmp.eq(v0.b,v10.b)
		v0.cur = vmem(r0+#0)
	}
	{
		v7 = v10
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r3,##-39936)
		memw(r30+##-13360) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r30,#-16176)
		vmem(r0+#0) = v6
	}
	{
		r0 = add(r30,#-16048)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r3,##-36608)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r6 = add(r3,##-39168)
		v0 = vmem(r6+#0)
		memw(r30+#-2864) = r6.new
	}                                       // 4-byte Folded Spill
	{
		q2 = vcmp.eq(v0.b,v10.b)
		memw(r30+#-2352) = r0
		vmem(r12+#0) = v17
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r3,##-43264)
		memw(r30+#-2096) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-21816)
		v14 = vmem(r1+#0)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7224)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,r0)
		r0 = add(r30,#-1584)
		vmem(r4+#0) = v16
	}
	{
		r0 = add(r30,#-1072)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r0 = setbit(r6,#7)
		vmemu(r0+#0) = v10
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r1,r5)
		memw(r30+#-2608) = r0
		r7 = memw(r4+#120)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9264) = r7
		r6 = memw(r4+#124)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = add(r1,r2)
		memw(r30+#-1840) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r6,#31)
		memw(r30+#-1328) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r7,#31)
		memw(r30+##-8240) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-9008) = r6
		memw(r30+##-7728) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r4+#112)
		memw(r30+##-9520) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		r1 = memw(r4+#116)
		memw(r30+##-9776) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r3,##-36864)
		memw(r30+##-5680) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r2 = memw(r4+#104)
		memw(r30+#-304) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-10288) = r2
		r3 = memw(r4+#108)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-7472) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-7984) = r0
		memw(r30+##-10032) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r4+#96)
		memw(r30+##-11824) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r2 = memw(r4+#100)
		memw(r30+##-5424) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-8752) = r0
		memw(r30+##-11056) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		r3 = memw(r4+#88)
		memw(r30+##-12080) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r4+#92)
		memw(r30+##-8496) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-11568) = r0
		memw(r30+##-12336) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r3 = memw(r4+#80)
		memw(r30+##-13616) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r2 = memw(r4+#84)
		memw(r30+##-11184) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-12848) = r0
		memw(r30+##-13104) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		r3 = memw(r4+#72)
		memw(r30+##-14384) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r4+#76)
		memw(r30+##-12592) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-13744) = r0
		memw(r30+##-14128) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r2 = memw(r4+#64)
		memw(r30+##-14512) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		r3 = memw(r4+#68)
		memw(r30+##-12976) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-14256) = r0
		memw(r30+##-14896) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r4+#56)
		memw(r30+##-15024) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r2 = memw(r4+#60)
		memw(r30+##-4400) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+#-3888) = r0
		memw(r30+##-15152) = r2
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r4+#48)
		memw(r30+#-3376) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r4+#52)
		memw(r30+##-14640) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+#-3120) = r0
		memw(r30+##-15280) = r1
	}                                       // 4-byte Folded Spill
	{
		r26 = memw(r4+#40)
		r2 = memw(r4+#44)
	}
	{
		r0 = asr(r26,#31)
		memw(r30+#-3632) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-4656) = r0
		memw(r30+##-15792) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		r11 = memw(r4+#32)
		r22 = memw(r4+#36)
	}
	{
		r0 = asr(r11,#31)
		memw(r30+##-15664) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r22,#31)
		memw(r30+##-15920) = r0
		r27 = memw(r4+#24)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r27,#31)
		r10 = memw(r4+#28)
		memw(r30+##-5168) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r10,#31)
		memw(r30+##-16560) = r0
		r25 = memw(r4+#16)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r25,#31)
		r13 = memw(r4+#20)
		memw(r30+##-16432) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r23 = asr(r13,#31)
		memw(r30+##-16688) = r0
		r6 = memw(r4+#8)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r6,#31)
		r7 = memw(r4+#12)
		r9 = memw(r4+#0)
	}
	{
		r24 = asr(r9,#31)
		r16 = memw(r4+#4)
		r28 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r17 = asr(r7,#31)
		r8 = asr(r16,#31)
		r20 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r6)
		r1:0 = mpyu(r28,r9)
	}
	{
		r5 += mpyi(r28,r2)
		r3:2 = mpyu(r28,r16)
	}
	{
		r3 += mpyi(r28,r8)
		r1 += mpyi(r28,r24)
		r8 = ##-1073741825
	}
	{
		r5 += mpyi(r6,r20)
		r1 += mpyi(r9,r20)
		r9 = ##2147483647
	}
	{
		r3 += mpyi(r16,r20)
		r5:4 = min(r5:4,r9:8)
		r6 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r9:8)
		r3:2 = min(r3:2,r9:8)
		r16 = r20
	}
	{
		r8 = ##1073741824
		r9 = #0
	}
	{
		r1:0 = add(r1:0,r9:8)
		r5:4 = add(r5:4,r9:8)
	}
	{
		r3:2 = add(r3:2,r9:8)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = min(r1:0,r19:18)
		r5:4 = min(r5:4,r19:18)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r1:0 = max(r1:0,r15:14)
	}
	{
		r1:0 = mpyu(r28,r7)
		v26.w = vinsert(r0)
	}
	{
		r21:20 = max(r5:4,r15:14)
		r5:4 = mpyu(r28,r25)
	}
	{
		r9:8 = max(r3:2,r15:14)
		r3:2 = mpyu(r28,r13)
		v26 = valign(v26,v26,#4)
	}
	{
		r5 += mpyi(r28,r6)
		r1 += mpyi(r28,r17)
		r6 = ##-1073741825
	}
	{
		r3 += mpyi(r28,r23)
		r1 += mpyi(r7,r16)
		r7 = ##2147483647
	}
	{
		r5 += mpyi(r25,r16)
		r3 += mpyi(r13,r16)
		r25:24 = combine(r7,r6)
	}
	{
		r1:0 = min(r1:0,r7:6)
		r3:2 = min(r3:2,r7:6)
	}
	{
		r5:4 = min(r5:4,r7:6)
		r6 = ##1073741824
		r7 = #0
	}
	{
		r3:2 = add(r3:2,r7:6)
		v26.w = vinsert(r8)
	}
	{
		r1:0 = add(r1:0,r7:6)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r1:0 = asr(r1:0,#31)
		v26 = valign(v26,v26,#4)
	}
	{
		r3:2 = max(r3:2,r15:14)
		v26.w = vinsert(r20)
	}
	{
		r1:0 = min(r1:0,r19:18)
		r5:4 = add(r5:4,r7:6)
		r3 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r27)
		r1:0 = max(r1:0,r15:14)
		v26 = valign(v26,v26,#4)
	}
	{
		r7 += mpyi(r28,r3)
		r5:4 = asr(r5:4,#31)
		r3 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r28,r10)
		v26.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r19:18)
		r1 += mpyi(r28,r3)
		r3 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r28,r11)
		r5:4 = max(r5:4,r15:14)
		v26 = valign(v26,v26,#4)
	}
	{
		r1 += mpyi(r10,r16)
		r21 += mpyi(r28,r3)
	}
	{
		r21 += mpyi(r11,r16)
		v26.w = vinsert(r4)
	}
	{
		r7 += mpyi(r27,r16)
		r5:4 = min(r21:20,r25:24)
	}
	{
		r1:0 = min(r1:0,r25:24)
		r20 = ##1073741824
		r21 = #0
	}
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = add(r1:0,r21:20)
		r11:10 = combine(r21,r20)
		v26 = valign(v26,v26,#4)
	}
	{
		r5:4 = add(r5:4,r21:20)
		r7:6 = add(r7:6,r21:20)
	}
	{
		r1:0 = asr(r1:0,#31)
		v26.w = vinsert(r2)
	}
	{
		r3:2 = asr(r5:4,#31)
		r7:6 = asr(r7:6,#31)
	}
	{
		r1:0 = min(r1:0,r19:18)
		r3:2 = min(r3:2,r19:18)
		v26 = valign(v26,v26,#4)
	}
	{
		r5:4 = min(r7:6,r19:18)
		r7:6 = max(r1:0,r15:14)
	}
	{
		r1:0 = max(r3:2,r15:14)
		r3:2 = max(r5:4,r15:14)
	}
	{
		r5:4 = mpyu(r28,r22)
		r21:20 = mpyu(r28,r26)
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v26.w = vinsert(r2)
	}
	{
		r5 += mpyi(r28,r1)
		r1 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r16)
		v26 = valign(v26,v26,#4)
		r13 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r28,r1)
		r23:22 = combine(r25,r24)
		r1 = memw(r30+#-3376)
		memd(r30+#-5168) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r13)
		v26.w = vinsert(r6)
		r7 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r26,r16)
		r5:4 = mpyu(r28,r1)
	}
	{
		r3 += mpyi(r28,r7)
		v26 = valign(v26,v26,#4)
		r7 = memw(r30+#-3120)
		memd(r30+#-4656) = r21:20
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r13,r16)
		v26.w = vinsert(r0)
	}
	{
		r5 += mpyi(r28,r7)
		r7 = memw(r30+##-15280)
		memd(r30+#-3120) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r1,r16)
		v26 = valign(v26,v26,#4)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = mpyu(r28,r7)
		r9 = memw(r30+##-15024)
		memd(r30+#-3376) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r27 += mpyi(r28,r1)
		r13 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r9)
		r27 += mpyi(r7,r16)
		r1 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r13)
		r27:26 = combine(r11,r10)
		r8 = memw(r30+#-3888)
		memd(r30+#-3632) = r27:26
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r1)
	}
	{
		r5 += mpyi(r28,r8)
		r8 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r9,r16)
	}
	{
		r7 += mpyi(r28,r8)
		r4 = memw(r30+##-4400)
		memd(r30+#-3888) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r13,r16)
	}
	{
		r3 += mpyi(r28,r4)
		r6 = memw(r30+##-14512)
		memd(r30+#-4400) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r1,r16)
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r6)
	}
	{
		r5 += mpyi(r28,r0)
		r1:0 = min(r3:2,r25:24)
	}
	{
		r5 += mpyi(r6,r16)
		r1:0 = add(r1:0,r11:10)
		r6 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		r1:0 = asr(r1:0,#31)
		r7 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r6)
		r1:0 = min(r1:0,r19:18)
	}
	{
		r3 += mpyi(r28,r7)
		r1:0 = max(r1:0,r15:14)
	}
	{
		r3 += mpyi(r6,r16)
		r5:4 = add(r5:4,r11:10)
		r1 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r25:24)
		r5:4 = asr(r5:4,#31)
		r13 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r1)
		r3:2 = add(r3:2,r11:10)
	}
	{
		r5:4 = min(r5:4,r19:18)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r5:4 = max(r5:4,r15:14)
	}
	{
		r3:2 = max(r3:2,r15:14)
		r21:20 = mpyu(r28,r13)
	}
	{
		v27.w = vinsert(r4)
		r3 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r3)
		v27 = valign(v27,v27,#4)
	}
	{
		r7 += mpyi(r1,r16)
		v27.w = vinsert(r0)
		r1 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r7:6,r25:24)
		r3 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r28,r1)
		r5:4 = add(r5:4,r11:10)
		r1 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r3)
		r21 += mpyi(r13,r16)
		v27 = valign(v27,v27,#4)
	}
	{
		r7 += mpyi(r28,r1)
		r1:0 = min(r21:20,r25:24)
	}
	{
		r7 += mpyi(r3,r16)
		r1:0 = add(r1:0,r11:10)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r19:18)
		r7:6 = add(r7:6,r11:10)
		r10 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = asr(r7:6,#31)
		v27.w = vinsert(r2)
		r13 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r7:6,r19:18)
		r1:0 = max(r1:0,r15:14)
	}
	{
		r3:2 = max(r3:2,r15:14)
		r5:4 = asr(r5:4,#31)
		r1 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r19:18)
		r7:6 = mpyu(r28,r10)
		v27 = valign(v27,v27,#4)
	}
	{
		r7 += mpyi(r28,r1)
		r5:4 = max(r5:4,r15:14)
		r3 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r13)
		v27.w = vinsert(r4)
		r1 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r1)
		r21:20 = mpyu(r28,r3)
		r1 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r10,r16)
		r5 += mpyi(r13,r16)
		v27 = valign(v27,v27,#4)
	}
	{
		r21 += mpyi(r28,r1)
		v27.w = vinsert(r0)
	}
	{
		r21 += mpyi(r3,r16)
		r7:6 = min(r7:6,r25:24)
		r3 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r5:4,r25:24)
		r5:4 = add(r7:6,r27:26)
		r8 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r3)
		r1:0 = add(r1:0,r27:26)
		v27 = valign(v27,v27,#4)
	}
	{
		r7 += mpyi(r28,r8)
		r5:4 = asr(r5:4,#31)
		memd(r30+##-8752) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r3,r16)
		v27.w = vinsert(r2)
	}
	{
		r3:2 = min(r5:4,r19:18)
		r1:0 = asr(r1:0,#31)
		memd(r30+##-8496) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r1:0,r19:18)
		r3:2 = max(r3:2,r15:14)
		r6 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r15:14)
		v27 = valign(v27,v27,#4)
		r3 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r6)
		v27.w = vinsert(r2)
		r1 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r3)
		r13 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r6,r16)
		r3:2 = mpyu(r28,r1)
		r6 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		v27 = valign(v27,v27,#4)
		r9 = memw(r30+##-9776)
		memd(r30+#-7984) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r13)
		v27.w = vinsert(r0)
	}
	{
		r3 += mpyi(r28,r6)
	}
	{
		r3 += mpyi(r1,r16)
		r7:6 = mpyu(r28,r9)
		r1 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = combine(r5,r4)
		v27 = valign(v27,v27,#4)
		memd(r30+#-5424) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r1)
		r1 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r13,r16)
		r8 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r1)
		r1 = memw(r30+##-7728)
		memd(r30+#-5680) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r8)
		r7 += mpyi(r9,r16)
	}
	{
		r5 += mpyi(r28,r1)
		memd(r30+#-7472) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r8,r16)
	}
	{
		r5 = memw(r30+##-9008)
		memd(r30+#-7728) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r12+#120)
		memw(r30+##-13744) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = mpyu(r28,r5)
		r2 = memw(r12+#124)
		r4 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-13616) = r2
	}                                       // 4-byte Folded Spill
	{
		r1 += mpyi(r28,r4)
		r4 = asr(r2,#31)
		memw(r30+##-12592) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r1 += mpyi(r5,r16)
		r2 = asr(r3,#31)
		memw(r30+##-12848) = r2.new
	}                                       // 4-byte Folded Spill
	{
		memd(r30+##-8240) = r1:0
		r2 = memw(r12+#112)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		memw(r30+##-14256) = r2
		r0 = memw(r12+#116)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-14128) = r0
		r3 = memw(r12+#104)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-12336) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-12080) = r1
		memw(r30+##-11824) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#108)
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r2 = memw(r12+#96)
	}
	{
		memw(r30+##-14896) = r2
		memw(r30+##-11568) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-14512) = r0
		r0 = memw(r12+#100)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r0,#31)
		memw(r30+##-12976) = r0
		r1 = memw(r12+#88)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-14640) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15280) = r1
		memw(r30+##-13104) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#92)
		memw(r30+##-15152) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		r0 = asr(r1,#31)
		r3 = memw(r12+#80)
	}
	{
		memw(r30+##-16560) = r3
		memw(r30+##-15024) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15664) = r0
		r0 = memw(r12+#84)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-16432) = r0
		r2 = memw(r12+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-17200) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15792) = r1
		memw(r30+##-15920) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#76)
		memw(r30+##-16816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r12+#64)
	}
	{
		memw(r30+##-16944) = r0
		memw(r30+##-17328) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-16688) = r1
		r1 = memw(r12+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-17584) = r1
		r2 = memw(r12+#56)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+##-17072) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-17840) = r2
		memw(r30+##-10288) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#60)
		memw(r30+##-17712) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r25 = memw(r12+#48)
	}
	{
		memw(r30+##-17456) = r1
		memw(r30+##-10032) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#52)
		memw(r30+##-17968) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r25,#31)
		r20 = memw(r12+#40)
	}
	{
		memw(r30+##-9264) = r1
		memw(r30+##-9520) = r0
	}                                       // 4-byte Folded Spill
	{
		r21 = memw(r12+#44)
		r17 = memw(r12+#32)
	}
	{
		r0 = asr(r21,#31)
		memw(r30+##-9008) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r20,#31)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r18 = memw(r12+#36)
		r15 = memw(r12+#24)
	}
	{
		r0 = asr(r18,#31)
		memw(r30+##-11056) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r17,#31)
		memw(r30+##-11184) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r10 = memw(r12+#28)
		r7 = memw(r12+#16)
	}
	{
		r0 = asr(r10,#31)
		r2 = asr(r7,#31)
		memw(r30+##-18096) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r15,#31)
		memw(r30+##-18224) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r14 = memw(r12+#20)
		r24 = memw(r12+#8)
	}
	{
		r0 = asr(r24,#31)
		r19 = asr(r14,#31)
		r6 = memw(r12+#12)
		r8 = memw(r12+#0)
	}
	{
		r1:0 = mpyu(r28,r7)
		r12 = memw(r12+#4)
		memw(r30+##-18480) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 += mpyi(r28,r2)
		r9 = asr(r8,#31)
	}
	{
		r5:4 = mpyu(r28,r8)
		r1 += mpyi(r7,r16)
	}
	{
		r5 += mpyi(r28,r9)
		r13 = asr(r12,#31)
		r7 = #0
	}
	{
		r3:2 = mpyu(r28,r12)
		r5 += mpyi(r8,r16)
	}
	{
		r3 += mpyi(r28,r13)
		r1:0 = min(r1:0,r23:22)
		r9:8 = combine(r23,r22)
	}
	{
		r3 += mpyi(r12,r16)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r5:4 = min(r5:4,r23:22)
		r3:2 = min(r3:2,r23:22)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r22 = ##2147483647
		r23 = #0
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r3:2,r27:26)
		r26 = ##-2147483648
	}
	{
		r1:0 = min(r1:0,r23:22)
		r5:4 = asr(r5:4,#31)
		r27 = #-1
	}
	{
		r1:0 = max(r1:0,r27:26)
		r5:4 = min(r5:4,r23:22)
	}
	{
		r1:0 = max(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
		memd(r30+##-18232) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r24)
		v28.w = vinsert(r0)
		r0 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r23:22)
		r11 = asr(r6,#31)
	}
	{
		r5 += mpyi(r28,r0)
		r1:0 = mpyu(r28,r14)
		v28 = valign(v28,v28,#4)
	}
	{
		r13:12 = max(r3:2,r27:26)
		r3:2 = mpyu(r28,r6)
		r27:26 = combine(r9,r8)
	}
	{
		r1 += mpyi(r28,r19)
		r3 += mpyi(r28,r11)
	}
	{
		r1 += mpyi(r14,r16)
		r3 += mpyi(r6,r16)
		r6 = ##1073741824
	}
	{
		r1:0 = min(r1:0,r9:8)
		r5 += mpyi(r24,r16)
	}
	{
		r5:4 = min(r5:4,r9:8)
		r1:0 = add(r1:0,r7:6)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r1:0 = asr(r1:0,#31)
		r8 = ##-2147483648
	}
	{
		r1:0 = min(r1:0,r23:22)
		r5:4 = add(r5:4,r7:6)
		r9 = #-1
	}
	{
		r1:0 = max(r1:0,r9:8)
		v28.w = vinsert(r12)
	}
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = add(r3:2,r7:6)
		r1 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r23:22)
		r7:6 = mpyu(r28,r15)
		v28 = valign(v28,v28,#4)
	}
	{
		r7 += mpyi(r28,r1)
		r5:4 = max(r5:4,r9:8)
		r1 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		v28.w = vinsert(r4)
	}
	{
		r5:4 = mpyu(r28,r10)
		r3:2 = min(r3:2,r23:22)
	}
	{
		r5 += mpyi(r28,r1)
		r13:12 = mpyu(r28,r17)
		r1 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r9:8)
		r7 += mpyi(r15,r16)
		r15:14 = combine(r27,r26)
		v28 = valign(v28,v28,#4)
	}
	{
		r13 += mpyi(r28,r1)
		v28.w = vinsert(r2)
		r1 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r10,r16)
		r13 += mpyi(r17,r16)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r3:2 = min(r5:4,r27:26)
		r26 = ##1073741824
	}
	{
		r27 = #0
	}
	{
		r7:6 = mpyu(r28,r18)
		r5:4 = add(r7:6,r27:26)
		memd(r30+##-11184) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r28,r1)
		v28 = valign(v28,v28,#4)
		r13:12 = memd(r30+##-18232)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r3:2,r27:26)
		v28.w = vinsert(r12)
		r1 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r18,r16)
		r3:2 = asr(r3:2,#31)
		r13:12 = combine(r9,r8)
	}
	{
		r3:2 = min(r3:2,r23:22)
		v28 = valign(v28,v28,#4)
		memd(r30+##-11056) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7 += mpyi(r28,r1)
		v28.w = vinsert(r0)
	}
	{
		r1:0 = max(r3:2,r9:8)
		r3:2 = mpyu(r28,r21)
	}
	{
		r7 += mpyi(r20,r16)
		r5:4 = min(r5:4,r23:22)
		r1 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r25)
		v28 = valign(v28,v28,#4)
		memd(r30+##-9776) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = max(r5:4,r9:8)
		r3 += mpyi(r28,r1)
		r1 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r21,r16)
		v28.w = vinsert(r4)
	}
	{
		r2 = memw(r30+##-9520)
		memd(r30+##-9008) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r9:8 = mpyu(r28,r1)
		v28 = valign(v28,v28,#4)
	}
	{
		r7 += mpyi(r28,r2)
		v28.w = vinsert(r0)
		r2 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r25,r16)
	}
	{
		r9 += mpyi(r28,r2)
		v28 = valign(v28,v28,#4)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r9 += mpyi(r1,r16)
	}
	{
		r9 = memw(r30+##-17840)
		memd(r30+##-9520) = r9:8
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-17712)
		r1 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r9)
	}
	{
		r7:6 = mpyu(r28,r8)
		r5 += mpyi(r28,r1)
		r1 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r9,r16)
	}
	{
		r4 = memw(r30+##-17456)
		memd(r30+##-10032) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r1)
	}
	{
		r7 += mpyi(r28,r4)
		r4 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r8,r16)
		r9 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r28,r4)
		r8 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r6 = memw(r30+##-17072)
		memd(r30+##-10288) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r1,r16)
		r1:0 = mpyu(r28,r9)
	}
	{
		r5:4 = mpyu(r28,r8)
		r3:2 = min(r3:2,r15:14)
	}
	{
		r1 += mpyi(r28,r6)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1 += mpyi(r9,r16)
		r3:2 = asr(r3:2,#31)
		r9 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r15:14)
		r3:2 = min(r3:2,r23:22)
		r10 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r9)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r5 += mpyi(r8,r16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r1:0 = min(r1:0,r23:22)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r23:22)
		v29.w = vinsert(r0)
		r3 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r5:4,r13:12)
		r5:4 = mpyu(r28,r10)
	}
	{
		r7:6 = mpyu(r28,r3)
		v29 = valign(v29,v29,#4)
		r1 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		v29.w = vinsert(r2)
	}
	{
		r5 += mpyi(r28,r1)
		r1 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r10,r16)
		v29 = valign(v29,v29,#4)
		r10 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r28,r1)
		r5:4 = min(r5:4,r15:14)
	}
	{
		r7 += mpyi(r28,r10)
		v29.w = vinsert(r0)
		r10 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r16)
	}
	{
		r21 += mpyi(r28,r10)
		r3:2 = min(r7:6,r15:14)
		v29 = valign(v29,v29,#4)
	}
	{
		r21 += mpyi(r1,r16)
		r1:0 = add(r5:4,r27:26)
	}
	{
		r7:6 = min(r21:20,r15:14)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1:0 = min(r1:0,r23:22)
		r5:4 = asr(r7:6,#31)
	}
	{
		r5:4 = min(r5:4,r23:22)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r3:2 = min(r3:2,r23:22)
	}
	{
		r5:4 = max(r5:4,r13:12)
		r3:2 = max(r3:2,r13:12)
		r1 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		v29.w = vinsert(r0)
		r5 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r1)
		r3 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r28,r5)
		v29 = valign(v29,v29,#4)
		r0 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r3)
		v29.w = vinsert(r2)
		r3 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r1,r16)
		r25 += mpyi(r28,r0)
	}
	{
		r1:0 = min(r7:6,r15:14)
		r25 += mpyi(r5,r16)
		r5 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r3)
		r1:0 = add(r1:0,r27:26)
		v29 = valign(v29,v29,#4)
	}
	{
		r7 += mpyi(r28,r5)
		r1:0 = asr(r1:0,#31)
		r5 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r23:22)
		r7 += mpyi(r3,r16)
		r8 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r13:12)
		v29.w = vinsert(r4)
		memd(r30+##-13104) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r5)
		r3:2 = min(r25:24,r15:14)
		r1 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r8)
		r3:2 = add(r3:2,r27:26)
		r8 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r5,r16)
		v29 = valign(v29,v29,#4)
		r9 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		v29.w = vinsert(r0)
		memd(r30+##-12976) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r1)
		r5:4 = mpyu(r28,r8)
		r0 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r9)
		r3:2 = min(r3:2,r23:22)
		v29 = valign(v29,v29,#4)
	}
	{
		r5 += mpyi(r28,r0)
		r7 += mpyi(r1,r16)
	}
	{
		r1:0 = max(r3:2,r13:12)
		r13 = ##2147483647
	}
	{
		r1 = memw(r30+##-14256)
		memd(r30+##-12336) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r8,r16)
		v29.w = vinsert(r0)
		r6 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r7 = memw(r30+##-11824)
		memd(r30+##-11568) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r1)
		r5:4 = mpyu(r28,r6)
		v29 = valign(v29,v29,#4)
	}
	{
		r3 += mpyi(r28,r7)
	}
	{
		r3 += mpyi(r1,r16)
		r1:0 = combine(r5,r4)
	}
	{
		r2 = memw(r30+##-12080)
		memd(r30+##-11824) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r2)
		r7 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r6,r16)
		r3:2 = mpyu(r28,r8)
	}
	{
		r0 = memw(r30+##-12848)
		memd(r30+##-12080) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r7)
	}
	{
		r3 += mpyi(r28,r0)
		r0 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r8,r16)
	}
	{
		r5 += mpyi(r28,r0)
		r0 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r7,r16)
	}
	{
		r6 = memw(r0+#120)
		memw(r30+##-16816) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#124)
		memd(r30+##-12592) = r3:2

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r6,#31)
		memw(r30+##-16560) = r1
	}                                       // 4-byte Folded Spill
	{
		memd(r30+##-12848) = r5:4
		memw(r30+##-15664) = r2
	}                                       // 8-byte Folded Spill
	{
		memw(r30+##-15792) = r1
		r3 = memw(r0+#112)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-16944) = r3
		r1 = memw(r0+#116)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		memw(r30+##-16688) = r1
		r4 = memw(r0+#104)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r3,#31)
		memw(r30+##-17200) = r4
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15280) = r2
		memw(r30+##-15024) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#108)
		memw(r30+##-17072) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r4,#31)
		r3 = memw(r0+#96)
	}
	{
		memw(r30+##-17456) = r3
		memw(r30+##-14896) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15152) = r1
		r1 = memw(r0+#100)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		memw(r30+##-17328) = r1
		r4 = memw(r0+#88)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r3,#31)
		memw(r30+##-17840) = r4
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15920) = r2
		memw(r30+##-16432) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#92)
		memw(r30+##-17712) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r4,#31)
		r3 = memw(r0+#80)
	}
	{
		memw(r30+##-18608) = r3
		memw(r30+##-17584) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-17968) = r1
		r1 = memw(r0+#84)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		memw(r30+##-18232) = r1
		r4 = memw(r0+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r3,#31)
		memw(r30+##-19000) = r4
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-18096) = r2
		memw(r30+##-18224) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#76)
		memw(r30+##-18736) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r4,#31)
		r3 = memw(r0+#64)
	}
	{
		memw(r30+##-18864) = r1
		memw(r30+##-19248) = r3
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r3,#31)
		memw(r30+##-18480) = r2
		r2 = memw(r0+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-19760) = r2
		r4 = memw(r0+#56)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r2,#31)
		memw(r30+##-18992) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-19776) = r4
		memw(r30+##-14384) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#60)
		memw(r30+##-19768) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r4,#31)
		r25 = memw(r0+#48)
	}
	{
		memw(r30+##-19504) = r2
		memw(r30+##-14128) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#52)
		memw(r30+##-20272) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		r1 = asr(r25,#31)
		r21 = memw(r0+#40)
	}
	{
		memw(r30+##-13616) = r2
		memw(r30+##-13744) = r1
	}                                       // 4-byte Folded Spill
	{
		r22 = memw(r0+#44)
		r18 = memw(r0+#32)
	}
	{
		r1 = asr(r22,#31)
		memw(r30+##-20528) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r21,#31)
		memw(r30+##-14256) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r19 = memw(r0+#36)
		r7 = memw(r0+#24)
	}
	{
		r1 = asr(r19,#31)
		memw(r30+##-14512) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r18,#31)
		memw(r30+##-14640) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r20 = memw(r0+#28)
		r8 = memw(r0+#16)
	}
	{
		r1 = asr(r20,#31)
		r2 = asr(r8,#31)
		memw(r30+##-20656) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r7,#31)
		memw(r30+##-20664) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r23 = memw(r0+#20)
		r24 = memw(r0+#8)
	}
	{
		r10 = asr(r24,#31)
		r9 = asr(r23,#31)
		r6 = memw(r0+#12)
		r14 = memw(r0+#0)
	}
	{
		r1:0 = mpyu(r28,r8)
		r17 = asr(r14,#31)
		r15 = memw(r0+#4)
	}
	{
		r1 += mpyi(r28,r2)
		r12 = asr(r15,#31)
	}
	{
		r3:2 = mpyu(r28,r15)
		r5:4 = mpyu(r28,r14)
	}
	{
		r3 += mpyi(r28,r12)
		r5 += mpyi(r28,r17)
		r12 = ##-1073741825
	}
	{
		r3 += mpyi(r15,r16)
		r1 += mpyi(r8,r16)
		r15 = #-1
	}
	{
		r5 += mpyi(r14,r16)
		r1:0 = min(r1:0,r13:12)
		r14 = ##-2147483648
	}
	{
		r3:2 = min(r3:2,r13:12)
		r5:4 = min(r5:4,r13:12)
		r12 = ##2147483647
	}
	{
		r1:0 = add(r1:0,r27:26)
		r3:2 = add(r3:2,r27:26)
		r13 = #0
	}
	{
		r1:0 = asr(r1:0,#31)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = min(r3:2,r13:12)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r1:0 = max(r1:0,r15:14)
	}
	{
		r13:12 = max(r3:2,r15:14)
		r11 = asr(r6,#31)
		memd(r30+##-13360) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r28,r6)
		r1:0 = max(r5:4,r15:14)
	}
	{
		r3 += mpyi(r28,r11)
		r5:4 = mpyu(r28,r24)
		r11 = ##2147483647
	}
	{
		r5 += mpyi(r28,r10)
		v31.w = vinsert(r0)
		r10 = ##-1073741825
	}
	{
		r3 += mpyi(r6,r16)
		r1:0 = mpyu(r28,r23)
	}
	{
		r5 += mpyi(r24,r16)
		r1 += mpyi(r28,r9)
		r9:8 = combine(r11,r10)
		v31 = valign(v31,v31,#4)
	}
	{
		r3:2 = min(r3:2,r11:10)
		r1 += mpyi(r23,r16)
	}
	{
		r5:4 = min(r5:4,r11:10)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r5:4 = add(r5:4,r27:26)
		r10 = ##2147483647
	}
	{
		r3:2 = asr(r3:2,#31)
		v31.w = vinsert(r12)
		r11 = #0
	}
	{
		r1:0 = add(r1:0,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r3:2 = min(r3:2,r11:10)
		r1:0 = asr(r1:0,#31)
		v31 = valign(v31,v31,#4)
	}
	{
		r5:4 = min(r5:4,r11:10)
		r3:2 = max(r3:2,r15:14)
	}
	{
		r5:4 = max(r5:4,r15:14)
		r1:0 = min(r1:0,r11:10)
		r3 = memw(r30+##-20656)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r20)
		v31.w = vinsert(r4)
	}
	{
		r15:14 = max(r1:0,r15:14)
		r5 += mpyi(r28,r3)
		r0 = memw(r30+##-20664)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r28,r7)
		v31 = valign(v31,v31,#4)
		r3 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r20,r16)
		r13 += mpyi(r28,r0)
	}
	{
		r1:0 = mpyu(r28,r18)
	}
	{
		r1 += mpyi(r28,r3)
		r13 += mpyi(r7,r16)
		r7:6 = combine(r9,r8)
	}
	{
		r1 += mpyi(r18,r16)
		r9:8 = min(r13:12,r9:8)
		r13:12 = combine(r7,r6)
	}
	{
		r18 = ##-2147483648
		memd(r30+##-14640) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r9:8,r27:26)
		v31.w = vinsert(r2)
		r9 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r5:4,r7:6)
		r5:4 = asr(r1:0,#31)
		r0 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r19)
		r3:2 = add(r3:2,r27:26)
		r8 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		v31 = valign(v31,v31,#4)
		r1:0 = memd(r30+##-13360)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		v31.w = vinsert(r0)
	}
	{
		r7 += mpyi(r19,r16)
		r19 = #-1
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r21)
		r3:2 = min(r3:2,r11:10)
		memd(r30+##-14512) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r28,r0)
		r1:0 = max(r3:2,r19:18)
		v31 = valign(v31,v31,#4)
	}
	{
		r3:2 = mpyu(r28,r22)
		r7 += mpyi(r21,r16)
		r1 = memw(r30+##-20528)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r25)
		v31.w = vinsert(r14)
		memd(r30+##-14256) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r1)
		r5:4 = min(r5:4,r11:10)
		r1 = memw(r30+##-20272)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r22,r16)
		r5:4 = max(r5:4,r19:18)
		v31 = valign(v31,v31,#4)
	}
	{
		r2 = memw(r30+##-13744)
		memd(r30+##-13360) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r28,r1)
		v31.w = vinsert(r4)
	}
	{
		r5:4 = mpyu(r28,r9)
		r7 += mpyi(r28,r2)
		r2 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r25,r16)
		v31 = valign(v31,v31,#4)
	}
	{
		r21 += mpyi(r28,r2)
		v31.w = vinsert(r0)
		memd(r30+##-13616) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r8)
		r21 += mpyi(r1,r16)
		r1 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		v31 = valign(v31,v31,#4)
		memd(r30+##-13744) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r28,r1)
		r1 = memw(r30+##-19760)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r9,r16)
		r9 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-19504)
		memd(r30+##-14128) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r1)
	}
	{
		r7 += mpyi(r28,r4)
		r4 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r8,r16)
		r8 = memw(r30+##-19248)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r28,r4)
		memd(r30+##-14384) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r9)
		r3 += mpyi(r1,r16)
		r6 = memw(r30+##-18992)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r28,r8)
	}
	{
		r1 += mpyi(r28,r6)
		r3:2 = min(r3:2,r13:12)
	}
	{
		r1 += mpyi(r8,r16)
		r3:2 = add(r3:2,r27:26)
		r8 = memw(r30+##-18864)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5 += mpyi(r28,r8)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r5 += mpyi(r9,r16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = min(r3:2,r11:10)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r3:2 = max(r3:2,r19:18)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = max(r1:0,r19:18)
		r5:4 = min(r5:4,r11:10)
		r3 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r5:4,r19:18)
		v30.w = vinsert(r0)
		r14 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r3)
		r1 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r14)
		v30 = valign(v30,v30,#4)
	}
	{
		r5 += mpyi(r28,r1)
		v30.w = vinsert(r2)
		r1 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r3,r16)
		r3 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r28,r1)
		r5:4 = min(r5:4,r13:12)
		v30 = valign(v30,v30,#4)
	}
	{
		r7 += mpyi(r28,r3)
		v30.w = vinsert(r0)
		r3 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r14,r16)
	}
	{
		r25 += mpyi(r28,r3)
		r3:2 = min(r7:6,r13:12)
		v30 = valign(v30,v30,#4)
	}
	{
		r25 += mpyi(r1,r16)
		r1:0 = add(r5:4,r27:26)
	}
	{
		r7:6 = min(r25:24,r13:12)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r5:4 = asr(r7:6,#31)
	}
	{
		r5:4 = min(r5:4,r11:10)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = max(r1:0,r19:18)
		r3:2 = min(r3:2,r11:10)
	}
	{
		r5:4 = max(r5:4,r19:18)
		r3:2 = max(r3:2,r19:18)
		r1 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		v30.w = vinsert(r0)
		r5 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r1)
		r3 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r15:14 = mpyu(r28,r5)
		v30 = valign(v30,v30,#4)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r3)
		v30.w = vinsert(r2)
		r3 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r1,r16)
		r15 += mpyi(r28,r0)
	}
	{
		r1:0 = min(r7:6,r13:12)
		r15 += mpyi(r5,r16)
		r5 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r3)
		r1:0 = add(r1:0,r27:26)
		v30 = valign(v30,v30,#4)
	}
	{
		r7 += mpyi(r28,r5)
		r1:0 = asr(r1:0,#31)
		r5 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r11:10)
		r7 += mpyi(r3,r16)
		r8 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r19:18)
		v30.w = vinsert(r4)
		memd(r30+##-16432) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r5)
		r3:2 = min(r15:14,r13:12)
		r1 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r8)
		r3:2 = add(r3:2,r27:26)
		r8 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r5,r16)
		v30 = valign(v30,v30,#4)
		r9 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		v30.w = vinsert(r0)
		memd(r30+##-15920) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r1)
		r5:4 = mpyu(r28,r8)
		r0 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r9)
		r3:2 = min(r3:2,r11:10)
		v30 = valign(v30,v30,#4)
	}
	{
		r5 += mpyi(r28,r0)
		r7 += mpyi(r1,r16)
	}
	{
		r1:0 = max(r3:2,r19:18)
	}
	{
		r1 = memw(r30+##-16944)
		memd(r30+##-15152) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r8,r16)
		v30.w = vinsert(r0)
		r6 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r7 = memw(r30+##-15024)
		memd(r30+##-14896) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r1)
		r5:4 = mpyu(r28,r6)
		v30 = valign(v30,v30,#4)
	}
	{
		r3 += mpyi(r28,r7)
	}
	{
		r3 += mpyi(r1,r16)
		r1:0 = combine(r5,r4)
	}
	{
		r2 = memw(r30+##-15280)
		memd(r30+##-15024) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r2)
		r7 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r6,r16)
		r3:2 = mpyu(r28,r8)
	}
	{
		r0 = memw(r30+##-15792)
		memd(r30+##-15280) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r28,r7)
	}
	{
		r3 += mpyi(r28,r0)
		r0 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r8,r16)
		r1 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
	}
	{
		r5 += mpyi(r7,r16)
		r6 = memw(r1+#120)
		memw(r30+##-17072) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#124)
		memd(r30+##-15664) = r3:2

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r2 = asr(r0,#31)
		memd(r30+##-15792) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r0 = asr(r6,#31)
		memw(r30+##-16688) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-16560) = r2
		memw(r30+##-16816) = r0
	}                                       // 4-byte Folded Spill
	{
		r5 = r16
		r3 = memw(r1+#112)
		memw(r30+##-18224) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#116)
		memw(r30+##-17840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		r0 = asr(r3,#31)
		r4 = memw(r1+#104)
	}
	{
		memw(r30+##-20912) = r4
		memw(r30+##-17712) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-18096) = r0
		r0 = memw(r1+#108)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		memw(r30+##-19768) = r0
		r3 = memw(r1+#96)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r4,#31)
		memw(r30+##-21048) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-19760) = r2
		memw(r30+##-20664) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#100)
		memw(r30+##-21040) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		r0 = asr(r3,#31)
		r4 = memw(r1+#88)
	}
	{
		memw(r30+##-21072) = r4
		memw(r30+##-19248) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-19776) = r0
		r0 = memw(r1+#92)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		memw(r30+##-21056) = r0
		r3 = memw(r1+#80)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r4,#31)
		memw(r30+##-21096) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-20656) = r2
		memw(r30+##-21064) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#84)
		memw(r30+##-21080) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		r0 = asr(r3,#31)
		r4 = memw(r1+#72)
	}
	{
		memw(r30+##-21120) = r4
		memw(r30+##-20272) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-20528) = r0
		r0 = memw(r1+#76)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		memw(r30+##-21104) = r0
		r3 = memw(r1+#64)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r4,#31)
		memw(r30+##-21112) = r0.new
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-21136) = r3
		memw(r30+##-21088) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r2 = memw(r1+#68)
		memw(r30+##-21152) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		r23 = memw(r1+#56)
		memw(r30+##-21128) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-17584) = r0
		r26 = memw(r1+#60)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r26,#31)
		r2 = memw(r1+#48)
		memw(r30+##-21144) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r23,#31)
		memw(r30+##-21160) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-17328) = r0
		r21 = memw(r1+#52)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r21,#31)
		r19 = memw(r1+#40)
		memw(r30+##-17200) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-16944) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r27 = memw(r1+#44)
		r16 = memw(r1+#32)
	}
	{
		r0 = asr(r27,#31)
		memw(r30+##-21168) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r19,#31)
		memw(r30+##-17456) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r17 = memw(r1+#36)
		r13 = memw(r1+#24)
	}
	{
		r0 = asr(r17,#31)
		r22 = asr(r13,#31)
		memw(r30+##-17968) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r16,#31)
		memw(r30+##-18232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r14 = memw(r1+#28)
		r6 = memw(r1+#16)
	}
	{
		r0 = asr(r14,#31)
		r10 = asr(r6,#31)
		memw(r30+##-18736) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25:24 = mpyu(r28,r6)
		r12 = memw(r1+#20)
		r18 = memw(r1+#8)
	}
	{
		r25 += mpyi(r28,r10)
		r8 = asr(r18,#31)
		r7 = memw(r1+#12)
		r4 = memw(r1+#0)
	}
	{
		r2 = asr(r4,#31)
		r20 = asr(r12,#31)
		r11 = memw(r1+#4)
	}
	{
		r9 = asr(r11,#31)
		r15 = asr(r7,#31)
	}
	{
		r1:0 = mpyu(r28,r11)
		r25 += mpyi(r6,r5)
	}
	{
		r1 += mpyi(r28,r9)
		memd(r30+##-18608) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r11,r5)
		r11:10 = mpyu(r28,r4)
	}
	{
		r1:0 = mpyu(r28,r18)
		r11 += mpyi(r28,r2)
		memd(r30+##-19504) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r28,r8)
		r9:8 = mpyu(r28,r7)
	}
	{
		r9 += mpyi(r28,r15)
		r11 += mpyi(r4,r5)
		r3:2 = combine(r1,r0)
		r1 = r5
	}
	{
		r3 += mpyi(r18,r5)
		r9 += mpyi(r7,r5)
	}
	{
		r3:2 = mpyu(r28,r12)
		r7:6 = mpyu(r28,r13)
		memd(r30+##-18992) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r20)
		r5:4 = mpyu(r28,r14)
		memd(r30+##-19000) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r12,r1)
		r7 += mpyi(r28,r22)
		r9 = r1
	}
	{
		r2 = memw(r30+##-18736)
		memd(r30+##-18480) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r28,r16)
		r7:6 = combine(r7,r6)
	}
	{
		r7 += mpyi(r13,r9)
		r5 += mpyi(r28,r2)
		r2 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r17)
		r5 += mpyi(r14,r9)
		memd(r30+##-18864) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r4 = memw(r30+##-17968)
		memd(r30+##-18736) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r2)
		r2 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r16,r9)
		r7 += mpyi(r28,r4)
	}
	{
		r5:4 = mpyu(r28,r19)
		memd(r30+##-18232) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r28,r2)
		r7 += mpyi(r17,r9)
	}
	{
		r3:2 = mpyu(r28,r27)
		r5:4 = combine(r5,r4)
		memd(r30+##-17968) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r19,r9)
		r6 = memw(r30+##-21160)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-21168)
		memd(r30+##-17456) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r28,r6)
	}
	{
		r3 += mpyi(r28,r4)
		r4 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r27,r9)
	}
	{
		r2 = memw(r30+##-17200)
		memd(r30+##-16944) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r4)
		r5:4 = mpyu(r28,r21)
	}
	{
		r1 += mpyi(r6,r9)
		r7:6 = mpyu(r28,r23)
	}
	{
		r5 += mpyi(r28,r2)
		r2 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r21,r9)
		memd(r30+##-13232) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r0 = memw(r30+##-21144)
		r1 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r2)
		r3:2 = mpyu(r28,r26)
		memd(r30+##-17200) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r0)
		r4 = memw(r30+##-21152)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r23,r9)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r28,r4)
		r3 += mpyi(r26,r9)
		r8 = memw(r30+##-21096)
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r28,r0)
		r26 = r9
		memd(r30+##-17328) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r0 = memw(r30+##-21136)
		memd(r30+##-17584) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r4,r9)
		r23 = #-1
		r2 = memw(r30+##-21120)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r0)
		r4 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r1)
		r21:20 = mpyu(r28,r2)
	}
	{
		r7 += mpyi(r0,r9)
		r19:18 = mpyu(r28,r4)
		r0 = memw(r30+##-21112)
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-21080)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r28,r0)
		r1:0 = mpyu(r28,r8)
		r12 = memw(r30+##-21048)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r2,r9)
		r2 = memw(r30+##-21088)
	}                                       // 4-byte Folded Reload
	{
		r13 = memw(r30+##-21056)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r28,r2)
		r2 = memw(r30+##-20528)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r4,r9)
		r4 = memw(r30+##-20272)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r2)
		r3:2 = mpyu(r28,r5)
	}
	{
		r3 += mpyi(r28,r4)
		r1 += mpyi(r8,r9)
		r4 = memw(r30+##-21072)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-21064)
		memd(r30+##-20528) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r5,r9)
		r15:14 = mpyu(r28,r4)
	}
	{
		r3:2 = mpyu(r28,r12)
		r15 += mpyi(r28,r0)
		memd(r30+##-20272) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r28,r13)
		r15 += mpyi(r4,r9)
		r4 = memw(r30+##-20656)
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-21040)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r4)
		r4 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r13,r9)
		r13 = #0
	}
	{
		r3 += mpyi(r28,r4)
		memd(r30+##-20656) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r8)
		r3 += mpyi(r12,r9)
		r0 = memw(r30+##-19248)
	}                                       // 4-byte Folded Reload
	{
		r12 = memw(r30+##-20912)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
		r0 = memw(r30+##-20664)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r12)
		r5 += mpyi(r8,r9)
		memd(r30+##-19776) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r0)
		r22 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		r4 = ##-1073741825
		memd(r30+##-19248) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r12,r26)
		r5 = ##2147483647
	}
	{
		r11:10 = min(r11:10,r5:4)
		r12 = memw(r30+##-19760)
	}                                       // 4-byte Folded Reload
	{
		r2 = ##1073741824
		memd(r30+##-20664) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r28,r22)
		r3 = #0
	}
	{
		r1 += mpyi(r28,r12)
		r9:8 = add(r11:10,r3:2)
		r17:16 = memd(r30+##-19504)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r17:16,r5:4)
		r11:10 = asr(r9:8,#31)
		r12 = ##2147483647
	}
	{
		r1 += mpyi(r22,r26)
		r17:16 = min(r11:10,r13:12)
		r27 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r22 = ##-2147483648
	}
	{
		r11:10 = max(r17:16,r23:22)
		r9:8 = add(r9:8,r3:2)
		r17:16 = combine(r5,r4)
	}
	{
		r9:8 = asr(r9:8,#31)
		v25.w = vinsert(r10)
		memd(r30+##-19504) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r28,r27)
		r9:8 = min(r9:8,r13:12)
		r10 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = memd(r30+##-18992)
	}                                       // 8-byte Folded Reload
	{
		r1 += mpyi(r28,r10)
		r5:4 = min(r5:4,r17:16)
		v25 = valign(v25,v25,#4)
	}
	{
		r9:8 = max(r9:8,r23:22)
		r1 += mpyi(r27,r26)
	}
	{
		r9:8 = add(r5:4,r3:2)
		v25.w = vinsert(r8)
		r4 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		memd(r30+##-18096) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r9:8 = asr(r9:8,#31)
		r1:0 = add(r7:6,r3:2)
		r5 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r28,r4)
		v25 = valign(v25,v25,#4)
		r7:6 = memd(r30+##-19000)
	}                                       // 8-byte Folded Reload
	{
		r11 += mpyi(r28,r5)
		r1:0 = asr(r1:0,#31)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r1:0 = min(r1:0,r13:12)
	}
	{
		r11 += mpyi(r4,r26)
		r9:8 = max(r9:8,r23:22)
		r4 = r26
	}
	{
		r27:26 = max(r1:0,r23:22)
		r1:0 = min(r7:6,r17:16)
		memd(r30+##-17712) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r25:24 = min(r25:24,r17:16)
		v25.w = vinsert(r8)
		r10 = r4
	}
	{
		r9:8 = add(r25:24,r3:2)
		v24.w = vinsert(r26)
		r27:26 = combine(r3,r2)
	}
	{
		r7:6 = min(r21:20,r17:16)
		r1:0 = add(r1:0,r3:2)
		v25 = valign(v25,v25,#4)
	}
	{
		r9:8 = asr(r9:8,#31)
		r1:0 = asr(r1:0,#31)
		v24 = valign(v24,v24,#4)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r3:2 = add(r7:6,r27:26)
	}
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = asr(r3:2,#31)
	}
	{
		r9:8 = max(r9:8,r23:22)
		r1:0 = max(r1:0,r23:22)
	}
	{
		r7:6 = min(r3:2,r13:12)
		v25.w = vinsert(r0)
	}
	{
		r1:0 = max(r7:6,r23:22)
		v24.w = vinsert(r8)
		r7:6 = memd(r30+##-18608)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r3:2 = min(r19:18,r17:16)
		v25 = valign(v25,v25,#4)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r3:2 = add(r3:2,r27:26)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		v24.w = vinsert(r0)
		r0 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r13:12)
		r3:2 = asr(r3:2,#31)
		r1 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r28,r0)
		r7:6 = max(r7:6,r23:22)
		v24 = valign(v24,v24,#4)
	}
	{
		r3:2 = min(r3:2,r13:12)
		v25.w = vinsert(r6)
		r7:6 = memd(r30+##-18480)
	}                                       // 8-byte Folded Reload
	{
		r25 += mpyi(r28,r1)
		r3:2 = max(r3:2,r23:22)
	}
	{
		r25 += mpyi(r0,r4)
		r5:4 = min(r7:6,r17:16)
		r7:6 = memd(r30+##-20528)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r5:4,r27:26)
		v24.w = vinsert(r2)
		v25 = valign(v25,v25,#4)
	}
	{
		r7:6 = min(r7:6,r17:16)
		r3:2 = asr(r3:2,#31)
		r5:4 = memd(r30+##-18864)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r13:12)
		r1:0 = min(r5:4,r17:16)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r9:8 = add(r1:0,r27:26)
	}
	{
		r3:2 = max(r3:2,r23:22)
		r7:6 = asr(r7:6,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		v25.w = vinsert(r2)
		r2 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r7:6,r13:12)
		r7:6 = min(r9:8,r13:12)
		r3 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		r5:4 = max(r1:0,r23:22)
		v25 = valign(v25,v25,#4)
	}
	{
		v25.w = vinsert(r6)
		v24.w = vinsert(r4)
		r7:6 = memd(r30+##-20272)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = min(r15:14,r17:16)
		r15:14 = memd(r30+##-18736)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r15:14,r17:16)
		r7:6 = add(r7:6,r27:26)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = add(r5:4,r27:26)
		v25 = valign(v25,v25,#4)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r9:8 = add(r9:8,r27:26)
	}
	{
		r7:6 = max(r7:6,r23:22)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		v24.w = vinsert(r6)
	}
	{
		r5:4 = max(r5:4,r23:22)
		r9:8 = asr(r9:8,#31)
	}
	{
		r7:6 = min(r9:8,r13:12)
		r1:0 = mpyu(r28,r2)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = max(r7:6,r23:22)
		v24.w = vinsert(r4)
		r5:4 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		v25.w = vinsert(r6)
		r7:6 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = add(r5:4,r27:26)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
		v25 = valign(v25,v25,#4)
	}
	{
		r1 += mpyi(r28,r3)
		r7:6 = asr(r7:6,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r1 += mpyi(r2,r10)
		r3:2 = memd(r30+##-20656)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r13:12)
		r5:4 = max(r5:4,r23:22)
	}
	{
		r7:6 = max(r7:6,r23:22)
		v27.w = vinsert(r4)
		r5:4 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		v28.w = vinsert(r6)
		r7:6 = memd(r30+##-14640)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = add(r5:4,r27:26)
		v27 = valign(v27,v27,#4)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
		v28 = valign(v28,v28,#4)
	}
	{
		r11:10 = min(r3:2,r17:16)
		r7:6 = asr(r7:6,#31)
		r3:2 = memd(r30+##-16432)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r9:8 = add(r11:10,r27:26)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r5:4 = max(r5:4,r23:22)
	}
	{
		r9:8 = asr(r9:8,#31)
		v29.w = vinsert(r4)
		r5:4 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		r9:8 = min(r9:8,r13:12)
	}
	{
		r5:4 = min(r5:4,r17:16)
		v31.w = vinsert(r6)
		r7:6 = memd(r30+##-19776)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = add(r5:4,r27:26)
		v29 = valign(v29,v29,#4)
	}
	{
		r9:8 = max(r9:8,r23:22)
		r19:18 = min(r3:2,r17:16)
		v31 = valign(v31,v31,#4)
	}
	{
		r9:8 = add(r19:18,r27:26)
		v24.w = vinsert(r8)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		r7:6 = asr(r7:6,#31)
		v24 = valign(v24,v24,#4)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7:6 = min(r7:6,r13:12)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r5:4 = max(r5:4,r23:22)
	}
	{
		r3:2 = max(r9:8,r23:22)
		v26.w = vinsert(r4)
		r5:4 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		v30.w = vinsert(r2)
		r3:2 = memd(r30+##-18232)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		v24.w = vinsert(r6)
		r7:6 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r3:2,r17:16)
		r7:6 = min(r7:6,r17:16)
		v26 = valign(v26,v26,#4)
	}
	{
		r5:4 = add(r5:4,r27:26)
		v30 = valign(v30,v30,#4)
		r3:2 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = add(r21:20,r27:26)
		r7:6 = add(r7:6,r27:26)
		v24 = valign(v24,v24,#4)
	}
	{
		r5:4 = asr(r5:4,#31)
		r9:8 = asr(r9:8,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7:6 = asr(r7:6,#31)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r7:6 = min(r7:6,r13:12)
	}
	{
		r5:4 = max(r5:4,r23:22)
	}
	{
		r9:8 = max(r9:8,r23:22)
		v27.w = vinsert(r4)
		r5:4 = memd(r30+##-15920)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		r3:2 = min(r3:2,r17:16)
	}
	{
		v25.w = vinsert(r8)
		v28.w = vinsert(r6)
		r7:6 = memd(r30+##-14512)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		r9:8 = add(r3:2,r27:26)
		v27 = valign(v27,v27,#4)
	}
	{
		r7:6 = min(r7:6,r17:16)
		r9:8 = asr(r9:8,#31)
		v25 = valign(v25,v25,#4)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r7:6 = add(r7:6,r27:26)
		v28 = valign(v28,v28,#4)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7:6 = asr(r7:6,#31)
	}
	{
		r3:2 = max(r9:8,r23:22)
		r7:6 = min(r7:6,r13:12)
	}
	{
		r5:4 = max(r5:4,r23:22)
		v29.w = vinsert(r2)
		r3:2 = memd(r30+##-17968)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r3:2,r17:16)
		v30.w = vinsert(r4)
		r5:4 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		r9:8 = add(r15:14,r27:26)
		v29 = valign(v29,v29,#4)
	}
	{
		r5:4 = min(r5:4,r17:16)
		v31.w = vinsert(r6)
		v30 = valign(v30,v30,#4)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r9:8 = asr(r9:8,#31)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r7:6 = add(r7:6,r27:26)
		v31 = valign(v31,v31,#4)
	}
	{
		r9:8 = min(r9:8,r13:12)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7:6 = asr(r7:6,#31)
	}
	{
		r3:2 = max(r9:8,r23:22)
		r7:6 = min(r7:6,r13:12)
	}
	{
		r5:4 = max(r5:4,r23:22)
		v25.w = vinsert(r2)
		r3:2 = memd(r30+##-19248)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		v27.w = vinsert(r4)
		r5:4 = memd(r30+##-12336)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		v26.w = vinsert(r6)
		r7:6 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = add(r5:4,r27:26)
		v0 = valign(v25,v25,#4)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
		v25 = valign(v27,v27,#4)
	}
	{
		r11:10 = min(r3:2,r17:16)
		r7:6 = asr(r7:6,#31)
		r3:2 = memd(r30+##-14256)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r9:8 = add(r11:10,r27:26)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r5:4 = max(r5:4,r23:22)
	}
	{
		r9:8 = asr(r9:8,#31)
		v29.w = vinsert(r4)
		r5:4 = memd(r30+##-15152)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r23:22)
		r9:8 = min(r9:8,r13:12)
	}
	{
		r9:8 = max(r9:8,r23:22)
		v28.w = vinsert(r6)
		r7:6 = memd(r30+##-17456)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r19:18 = min(r3:2,r17:16)
		v27 = valign(v29,v29,#4)
	}
	{
		r9:8 = add(r19:18,r27:26)
		v24.w = vinsert(r8)
	}
	{
		r5:4 = min(r5:4,r17:16)
		r7:6 = add(r7:6,r27:26)
	}
	{
		r9:8 = asr(r9:8,#31)
		r5:4 = add(r5:4,r27:26)
		v1 = valign(v24,v24,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = asr(r5:4,#31)
		v24 = valign(v26,v26,#4)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r9:8 = min(r9:8,r13:12)
		v26 = valign(v28,v28,#4)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = max(r9:8,r23:22)
	}
	{
		r7:6 = max(r7:6,r23:22)
		v31.w = vinsert(r2)
		r3:2 = memd(r30+##-20664)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r23:22)
		v0.w = vinsert(r6)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r3:2,r17:16)
		v30.w = vinsert(r4)
		v28 = valign(v31,v31,#4)
		r5:4 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r7:6,r17:16)
		r9:8 = add(r21:20,r27:26)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r1:0 = min(r1:0,r17:16)
		v29 = valign(v30,v30,#4)
		r5:4 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r7:6,r17:16)
		r9:8 = asr(r9:8,#31)
		r7:6 = memd(r30+#-5424)
		memd(r30+#-4656) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r9:8 = min(r9:8,r13:12)
		r5:4 = memd(r30+#-4400)
		memd(r30+#-5168) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r9:8 = max(r9:8,r23:22)
		r5:4 = memd(r30+#-3888)
		memd(r30+#-5680) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r5:4 = memd(r30+#-3632)
		memd(r30+##-8240) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		v1.w = vinsert(r8)
		memd(r30+##-9776) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r7:6,r17:16)
		r5:4 = memd(r30+#-3376)
		memd(r30+##-11056) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r7:6 = add(r7:6,r27:26)
		r5:4 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r5:4,r17:16)
		r5:4 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r7:6 = asr(r7:6,#31)
		memd(r30+##-11184) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r25:24,r17:16)
		r5:4 = memd(r30+##-12592)
		memd(r30+#-5424) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r5:4 = memd(r30+##-12080)
		memd(r30+#-3120) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r13:12)
		r3:2 = min(r5:4,r17:16)
		memd(r30+#-3376) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r7:6 = max(r7:6,r23:22)
		r5:4 = memd(r30+##-11824)
		memd(r30+#-3888) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		v25.w = vinsert(r6)
		r5:4 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r5:4,r17:16)
		r5:4 = memd(r30+##-10288)
		memd(r30+#-7472) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		v25 = valign(v25,v25,#4)
		memd(r30+#-7984) = r1:0
		memd(r30+##-10288) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = min(r5:4,r17:16)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		memd(r30+##-10032) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5:4 = memd(r30+##-9264)
		memd(r30+##-9520) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = memd(r30+##-15792)
		memd(r30+##-11824) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		r3:2 = min(r3:2,r17:16)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r3:2 = memd(r30+##-15664)
		memd(r30+#-3632) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = memd(r30+##-15280)
		memd(r30+#-4400) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r23:22)
		r3:2 = min(r3:2,r17:16)
	}
	{
		v26.w = vinsert(r4)
		r3:2 = memd(r30+##-15024)
		memd(r30+#-7728) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
	}
	{
		v26 = valign(v26,v26,#4)
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-14896)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r3:2,r17:16)
		r3:2 = memd(r30+##-14384)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
	}
	{
		r3:2 = memd(r30+##-14128)
		memd(r30+##-9264) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
	}
	{
		r3:2 = memd(r30+##-13744)
		memd(r30+##-11568) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
	}
	{
		r3:2 = memd(r30+##-13616)
		memd(r30+##-12080) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
	}
	{
		r3:2 = memd(r30+##-13360)
		memd(r30+##-12336) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
		r1:0 = memd(r30+##-17712)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r17:16)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1:0 = memd(r30+##-18096)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r17:16)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r13:12)
		memd(r30+##-9008) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = max(r3:2,r23:22)
		r1:0 = memd(r30+##-19504)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r1:0,r17:16)
		r1:0 = add(r11:10,r27:26)
		r25:24 = memd(r30+##-17584)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r25:24,r17:16)
		r1:0 = asr(r1:0,#31)
		r25:24 = memd(r30+##-17328)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
		v28.w = vinsert(r2)
		memd(r30+##-12592) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = max(r1:0,r23:22)
		r3:2 = add(r19:18,r27:26)
	}
	{
		r3:2 = asr(r3:2,#31)
		v24.w = vinsert(r0)
		r1:0 = memd(r30+##-17200)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r17:16)
		v28 = valign(v28,v28,#4)
		r5:4 = memd(r30+##-16944)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r13:12)
		r1:0 = add(r15:14,r27:26)
		memd(r30+##-12976) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = max(r3:2,r23:22)
		r7:6 = asr(r1:0,#31)
		v24 = valign(v24,v24,#4)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r1:0 = add(r9:8,r27:26)
	}
	{
		r7:6 = max(r7:6,r23:22)
		v29.w = vinsert(r2)
	}
	{
		r11:10 = min(r25:24,r17:16)
		v27.w = vinsert(r6)
	}
	{
		r7:6 = min(r5:4,r17:16)
		v29 = valign(v29,v29,#4)
		memd(r30+##-12848) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r5:4 = add(r7:6,r27:26)
		r7:6 = asr(r1:0,#31)
	}
	{
		r7:6 = min(r7:6,r13:12)
		r5:4 = asr(r5:4,#31)
		v27 = valign(v27,v27,#4)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7:6 = max(r7:6,r23:22)
	}
	{
		r5:4 = max(r5:4,r23:22)
		v1.w = vinsert(r6)
	}
	{
		v0.w = vinsert(r4)
		r5:4 = memd(r30+##-13232)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r5:4,r17:16)
		v1 = valign(v1,v1,#4)
		r3:2 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r3:2,r27:26)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = memd(r30+##-9776)
		memd(r30+##-11184) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r7:6,r27:26)
		r3:2 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r5:4,r27:26)
		r7:6 = memd(r30+#-5680)
		memd(r30+##-11056) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r3:2,r27:26)
		r5:4 = memd(r30+#-5168)
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r7:6,r27:26)
		r3:2 = memd(r30+#-4656)
		memd(r30+##-13104) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r25:24 = add(r5:4,r27:26)
		r1:0 = add(r21:20,r27:26)
		memd(r30+##-13232) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r9:8 = add(r3:2,r27:26)
		r1:0 = asr(r1:0,#31)
		r7:6 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r7:6,r27:26)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r5:4,r27:26)
		r7:6 = memd(r30+##-10032)
		memd(r30+#-4656) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r7:6,r27:26)
		r5:4 = memd(r30+##-10288)
		memd(r30+#-5680) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r5:4,r27:26)
		r7:6 = memd(r30+#-7472)
		memd(r30+##-8240) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r7:6,r27:26)
		r5:4 = memd(r30+#-3888)
		memd(r30+##-10288) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r19:18 = add(r5:4,r27:26)
		r7:6 = memd(r30+#-3376)
		memd(r30+##-13360) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r11:10 = add(r7:6,r27:26)
		r1:0 = asr(r9:8,#31)
		r5:4 = memd(r30+#-3120)
		memd(r30+#-3120) = r1:0

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r3:2 = add(r5:4,r27:26)
		r7:6 = memd(r30+##-12336)
		memd(r30+#-5168) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r25:24,#31)
		r5:4 = add(r7:6,r27:26)
		r7:6 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = add(r7:6,r27:26)
		memd(r30+#-3888) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-11568)
		memd(r30+#-7472) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r7:6,r27:26)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r7:6,r27:26)
		memd(r30+##-9520) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-8496)
		memd(r30+##-11824) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r7:6,r27:26)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = add(r7:6,r27:26)
		r7:6 = memd(r30+#-4400)
		memd(r30+##-12080) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r17:16 = add(r7:6,r27:26)
		r3:2 = asr(r11:10,#31)
		r7:6 = memd(r30+#-3632)
		memd(r30+#-3376) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r7:6,r27:26)
		r7:6 = add(r15:14,r27:26)
	}
	{
		r5:4 = asr(r5:4,#31)
		r7:6 = memd(r30+##-12976)
		memd(r30+#-4400) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r17:16,#31)
		memd(r30+#-3632) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-12848)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r27:26)
	}
	{
		r7:6 = memd(r30+##-12592)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r27:26)
	}
	{
		r7:6 = memd(r30+##-9008)
		memd(r30+##-12336) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r27:26)
		r5:4 = asr(r23:22,#31)
		memd(r30+##-9008) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-8752)
		memd(r30+##-12976) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r21:20 = add(r7:6,r27:26)
		r7:6 = memd(r30+#-7984)
		memd(r30+#-7984) = r1:0

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r15:14 = add(r7:6,r27:26)
		r3:2 = asr(r19:18,#31)
		r7:6 = memd(r30+#-5424)
		memd(r30+#-5424) = r3:2

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r1:0 = memd(r30+##-13232)
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7:6 = asr(r7:6,#31)
		memd(r30+##-10032) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+##-9776)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r25:24 = asr(r1:0,#31)
		r1:0 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = asr(r1:0,#31)
		memd(r30+##-11056) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+##-11184)
		r3:2 = memd(r30+##-13360)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = min(r1:0,r13:12)
		memd(r30+##-11568) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = memd(r30+##-8240)
		memd(r30+##-12592) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r27:26 = asr(r3:2,#31)
		r7:6 = asr(r15:14,#31)
		r3:2 = memd(r30+#-5680)
		memd(r30+#-5680) = r7:6

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r11:10 = asr(r3:2,#31)
		r7:6 = asr(r21:20,#31)
		r3:2 = memd(r30+#-4656)
		memd(r30+#-4656) = r7:6

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r5:4 = memd(r30+##-12080)
		memd(r30+##-11184) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = asr(r5:4,#31)
	}
	{
		r19:18 = min(r3:2,r13:12)
		memd(r30+##-12080) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3:2 = min(r11:10,r13:12)
		r5:4 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = memd(r30+##-9520)
		memd(r30+##-12848) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r23:22 = asr(r5:4,#31)
		r5:4 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = asr(r5:4,#31)
		r5:4 = memd(r30+#-3888)
		r7:6 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r9:8,r13:12)
		r5:4 = asr(r5:4,#31)
		memd(r30+##-12976) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r25:24,r13:12)
		r7:6 = asr(r7:6,#31)
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-12336)
		memd(r30+##-11824) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r5:4,r13:12)
		r5:4 = min(r23:22,r13:12)
	}
	{
		r7:6 = asr(r7:6,#31)
		memd(r30+##-8240) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-9264)
		memd(r30+##-12336) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r21:20 = asr(r7:6,#31)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = asr(r7:6,#31)
		r7:6 = memd(r30+#-4400)
		r1:0 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
		r7:6 = asr(r7:6,#31)
	}
	{
		r1:0 = memd(r30+##-10032)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r1:0,r13:12)
		r3:2 = min(r27:26,r13:12)
		r1:0 = memd(r30+#-7984)
		memd(r30+#-7984) = r3:2

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
		memd(r30+#-4400) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+#-5168)
		memd(r30+##-9264) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
	}
	{
		r1:0 = memd(r30+#-3120)
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = memd(r30+##-12592)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		memd(r30+##-10288) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r3:2,r13:12)
		r3:2 = memd(r30+##-8496)
		memd(r30+#-7728) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		r3:2 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		r3:2 = memd(r30+#-3376)
		memd(r30+##-9520) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r3:2,r13:12)
		r3:2 = min(r17:16,r13:12)
		memd(r30+#-3376) = r5:4
	}                                       // 8-byte Folded Spill
	{
		memd(r30+##-10032) = r1:0
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-12848)
		r5:4 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		r3:2 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r23:22 = min(r3:2,r13:12)
		memd(r30+#-3888) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		r3:2 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r13:12)
		r3:2 = min(r7:6,r13:12)
		memd(r30+#-5168) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r15:14,r13:12)
		memd(r30+#-5424) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r21:20,r13:12)
		memd(r30+##-9008) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-12336)
		memd(r30+#-3120) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r7:6,r13:12)
		r7:6 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r7:6,r13:12)
		r1:0 = memd(r30+##-11184)
		memd(r30+#-3632) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r1:0,r13:12)
		r0 = ##-2147483648
		r1 = #-1
	}
	{
		r7:6 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = max(r7:6,r1:0)
		r5:4 = max(r5:4,r1:0)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r13:12)
		v29.w = vinsert(r4)
		r5:4 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = max(r25:24,r1:0)
		r9:8 = max(r9:8,r1:0)
		r7:6 = combine(r13,r12)
		memd(r30+#-4656) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = min(r5:4,r7:6)
		v25.w = vinsert(r24)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = max(r3:2,r1:0)
		r3:2 = max(r21:20,r1:0)
		v29 = valign(v29,v29,#4)
	}
	{
		r3:2 = max(r7:6,r1:0)
		v1.w = vinsert(r2)
		r7:6 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		v25 = valign(v25,v25,#4)
		memd(r30+#-5680) = r3:2
	}                                       // 8-byte Folded Spill
	{
		v28.w = vinsert(r8)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r7:6,r1:0)
		r11:10 = max(r11:10,r1:0)
		r7:6 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = max(r7:6,r1:0)
		v27.w = vinsert(r10)
		v1 = valign(v1,v1,#4)
	}
	{
		v24.w = vinsert(r16)
		v28 = valign(v28,v28,#4)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = max(r7:6,r1:0)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = max(r7:6,r1:0)
		v0.w = vinsert(r24)
		r7:6 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r19:18,r1:0)
		r25:24 = max(r7:6,r1:0)
		v27 = valign(v27,v27,#4)
	}
	{
		v26.w = vinsert(r18)
		v24 = valign(v24,v24,#4)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r27:26,r1:0)
		r27:26 = max(r7:6,r1:0)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = max(r7:6,r1:0)
		v24.w = vinsert(r20)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r7:6,r1:0)
		v25.w = vinsert(r8)
		v26 = valign(v26,v26,#4)
	}
	{
		v26.w = vinsert(r20)
		v24 = valign(v24,v24,#4)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r1:0)
		r23:22 = max(r23:22,r1:0)
		v25 = valign(v25,v25,#4)
	}
	{
		v28.w = vinsert(r6)
		v29.w = vinsert(r22)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r1:0)
		v25.w = vinsert(r2)
		v26 = valign(v26,v26,#4)
	}
	{
		v0.w = vinsert(r6)
		v27.w = vinsert(r26)
		v28 = valign(v28,v28,#4)
		r7:6 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r1:0)
		r15:14 = max(r15:14,r1:0)
		v29 = valign(v29,v29,#4)
		r21:20 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		v26.w = vinsert(r6)
		v1.w = vinsert(r14)
		v27 = valign(v27,v27,#4)
		r23:22 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = max(r21:20,r1:0)
		v24.w = vinsert(r16)
		v0 = valign(v0,v0,#4)
		r3:2 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = max(r23:22,r1:0)
		v27.w = vinsert(r24)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r1:0)
		v29.w = vinsert(r12)
		v24 = valign(v24,v24,#4)
	}
	{
		r3:2 = max(r3:2,r1:0)
		v28.w = vinsert(r6)
		v26 = valign(v26,v26,#4)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r1:0)
		r5:4 = max(r5:4,r1:0)
		v27 = valign(v27,v27,#4)
		r27:26 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = max(r27:26,r1:0)
		v1.w = vinsert(r6)
		v28 = valign(v28,v28,#4)
		r21:20 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = max(r21:20,r1:0)
		v27.w = vinsert(r18)
		v25 = valign(v25,v25,#4)
		r1:0 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		v0.w = vinsert(r16)
		v25.w = vinsert(r0)
		r0 = #68
		v29 = valign(v29,v29,#4)
	}
	{
		v24.w = vinsert(r10)
		v26.w = vinsert(r8)
		v1 = valign(v1,v1,#4)
	}
	{
		v28.w = vinsert(r2)
		v1.w = vinsert(r4)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r12)
		v29.w = vinsert(r14)
		v24 = vror(v24,r0)
	}
	{
		v25 = valign(v25,v25,#4)
	}
	{
		v26 = vror(v26,r0)
		v24 = vor(v24,v25)
	}
	{
		v27 = valign(v27,v27,#4)
	}
	{
		v28 = vror(v28,r0)
		v26 = vor(v26,v27)
	}
	{
		r0 = add(r30,#-22064)
		v0 = vror(v0,r0)
	}
	{
		v25 = valign(v29,v29,#4)
	}
	{
		r0 = add(r30,#-21424)
		v25 = vor(v28,v25)
		v29 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = valign(v1,v1,#4)
		v24.w = vadd(v24.w,v29.w):sat
		v26.w = vadd(v26.w,v29.w):sat
	}
	{
		v0 = vor(v0,v1)
		v1.w = vadd(v25.w,v29.w):sat
		v27 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-22960)
		v0.w = vadd(v0.w,v29.w):sat
	}
	{
		r0 = add(r30,#-22448)
		v25.w = vasr(v26.w,v27.w)
		v26 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v24.w = vasr(v24.w,v27.w)
		v25.w = vmin(v25.w,v12.w)
		v28 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.w = vasr(v1.w,v27.w)
		v24.w = vmin(v12.w,v24.w)
		v25.w = vmax(v11.w,v25.w)
		r0 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vasr(v0.w,v27.w)
		v1.w = vmin(v12.w,v1.w)
		v24.w = vmax(v11.w,v24.w)
	}
	{
		v0.w = vmin(v0.w,v12.w)
		v1.w = vmax(v11.w,v1.w)
	}
	{
		v24.h = vpacke(v24.w,v25.w)
		v0.w = vmax(v11.w,v0.w)
	}
	{
		v0.h = vpacke(v1.w,v0.w)
		v25:24.w = vsub(v25:24.w,v25:24.w)
		v1.h = vadd(v26.h,v24.h):sat
	}
	{
		v1.h = vmin(v1.h,v28.h)
		v0.h = vadd(v26.h,v0.h):sat
	}
	{
		v1.h = vmax(v24.h,v1.h)
		v0.h = vmin(v28.h,v0.h)
		vmem(r0+#0) = v1.new
	}
	{
		v0.h = vmax(v25.h,v0.h)
		r0 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = memd(r0+#192)
		vmem(r0+#0) = v0
	}
	{
		memd(r30+#-7984) = r5:4
		r3:2 = memd(r0+#200)

	} :mem_noshuf
	{
		memd(r30+##-8240) = r3:2
		r3:2 = memd(r0+#208)

	} :mem_noshuf
	{
		memd(r30+##-8496) = r3:2
		r3:2 = memd(r0+#216)

	} :mem_noshuf
	{
		memd(r30+##-8752) = r3:2
		r3:2 = memd(r0+#224)

	} :mem_noshuf
	{
		memd(r30+##-9008) = r3:2
		r3:2 = memd(r0+#232)

	} :mem_noshuf
	{
		memd(r30+##-9264) = r3:2
		r3:2 = memd(r0+#240)

	} :mem_noshuf
	{
		memd(r30+##-9520) = r3:2
		r3 = memw(r0+#124)

	} :mem_noshuf
	{
		r7:6 = memd(r0+#248)
	}
	{
		memd(r30+##-10288) = r7:6
		r5:4 = memd(r0+#128)

	} :mem_noshuf
	{
		memd(r30+##-10032) = r5:4
		r2 = memw(r0+#120)

	} :mem_noshuf
	{
		memd(r30+#-2608) = r3:2
		r3:2 = memd(r0+#136)

	} :mem_noshuf
	{
		memd(r30+##-11056) = r3:2
		r3:2 = memd(r0+#144)

	} :mem_noshuf
	{
		memd(r30+##-11184) = r3:2
		r15:14 = memd(r0+#160)

	} :mem_noshuf
	{
		r13:12 = memd(r0+#152)
		r3:2 = memd(r0+#64)
	}
	{
		r11:10 = memd(r0+#168)
		r17:16 = memd(r0+#176)
	}
	{
		r19:18 = memd(r0+#184)
		memd(r30+#-3120) = r3:2

	} :mem_noshuf
	{
		r3:2 = memd(r0+#72)
	}
	{
		memd(r30+#-3376) = r3:2
		r3:2 = memd(r0+#80)

	} :mem_noshuf
	{
		memd(r30+#-3632) = r3:2
		r3:2 = memd(r0+#88)

	} :mem_noshuf
	{
		memd(r30+#-3888) = r3:2
		r3:2 = memd(r0+#96)

	} :mem_noshuf
	{
		memd(r30+#-4400) = r3:2
		r3:2 = memd(r0+#104)

	} :mem_noshuf
	{
		memd(r30+#-4656) = r3:2
		r3:2 = memd(r0+#112)

	} :mem_noshuf
	{
		memd(r30+#-5168) = r3:2
		r21:20 = memd(r0+#0)

	} :mem_noshuf
	{
		r23:22 = memd(r0+#8)
		r25:24 = memd(r0+#16)
	}
	{
		r27:26 = memd(r0+#24)
		r9:8 = memd(r0+#32)
	}
	{
		r7:6 = memd(r0+#48)
		r3:2 = memd(r0+#56)
	}
	{
		r5:4 = memd(r0+#40)
		r0 = memw(r30+#-2352)
	}
	{
		memd(r0+#56) = r3:2
		memd(r0+#48) = r7:6
	}
	{
		memd(r0+#40) = r5:4
		memd(r0+#32) = r9:8
	}
	{
		memd(r0+#24) = r27:26
		memd(r0+#16) = r25:24
	}
	{
		memd(r0+#8) = r23:22
		memd(r0+#0) = r21:20
	}
	{
		r0 = memw(r30+#-2096)
		v0 = vmem(r0+#0)
	}
	{
		r1 = r0
	}
	{
		vmem(r0+#0) = v0
	}
	{
		r2 = memw(r0+#56)
		r3 = memw(r0+#60)
	}
	{
		memd(r30+#-2352) = r3:2
		r2 = memw(r0+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#52)
	}
	{
		memd(r30+#-7472) = r3:2
		r2 = memw(r0+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#44)
	}
	{
		memd(r30+#-2864) = r3:2
		r2 = memw(r0+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#36)
	}
	{
		memd(r30+#-5424) = r3:2
		r2 = memw(r0+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#28)
	}
	{
		memd(r30+#-5680) = r3:2
		r2 = memw(r0+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#20)
	}
	{
		memd(r30+#-7728) = r3:2
		r24 = memw(r0+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r25 = memw(r0+#4)
		r2 = memw(r0+#8)
	}
	{
		r0 = memw(r30+#-3896)
		r3 = memw(r1+#12)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+#-304)
		memd(r30+##-9776) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-43392)
	}
	{
		memd(r1+#56) = r19:18
		memd(r1+#48) = r17:16
	}
	{
		memd(r1+#40) = r11:10
		memd(r1+#32) = r15:14
	}
	{
		memd(r1+#24) = r13:12
		r5:4 = memd(r30+##-11184)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#16) = r5:4
		r3:2 = memd(r30+##-11056)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#8) = r3:2
		r3:2 = memd(r30+##-10032)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#0) = r3:2
	}
	{
		v0.cur = vmem(r1+#0)
		vmem(r0+#0) = v0
	}
	{
		r26 = memw(r0+#56)
		r27 = memw(r0+#60)
	}
	{
		r10 = memw(r0+#48)
		r11 = memw(r0+#52)
	}
	{
		r22 = memw(r0+#40)
		r23 = memw(r0+#44)
	}
	{
		r2 = memw(r0+#32)
		r3 = memw(r0+#36)
	}
	{
		memd(r30+#-304) = r3:2
		r2 = memw(r0+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#28)
	}
	{
		memd(r30+#-2096) = r3:2
		r2 = memw(r0+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#20)
	}
	{
		memd(r30+##-10032) = r3:2
		r12 = memw(r0+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r13 = memw(r0+#4)
		r18 = memw(r0+#8)
	}
	{
		r13 = vtrunehb(r13:12)
		r1 = memw(r30+#-3896)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v22.w = vinsert(r13)
		r1 = add(r1,##-36992)
		r19 = memw(r0+#12)
	}
	{
		r3 = add(r2,##-43520)
		r5:4 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		v1 = valign(v22,v22,#4)
		memd(r1+#56) = r5:4
		r5:4 = memd(r30+##-9520)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#48) = r5:4
		r5:4 = memd(r30+##-9264)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#40) = r5:4
		r5:4 = memd(r30+##-9008)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#32) = r5:4
		r5:4 = memd(r30+##-8752)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#24) = r5:4
		r5:4 = memd(r30+##-8496)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#16) = r5:4
		r5:4 = memd(r30+##-8240)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#8) = r5:4
		r5:4 = memd(r30+#-7984)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r5 = vtrunehb(r25:24)
		memd(r1+#0) = r5:4
	}
	{
		v23.w = vinsert(r5)
		v0.cur = vmem(r1+#0)
		vmem(r3+#0) = v0
	}
	{
		r0 = memw(r3+#56)
		r1 = memw(r3+#60)
	}
	{
		memd(r30+#-7984) = r1:0
		r0 = memw(r3+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r3+#52)
	}
	{
		memd(r30+##-8240) = r1:0
		r0 = memw(r3+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r3+#44)
	}
	{
		memd(r30+##-8752) = r1:0
		r0 = memw(r3+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r3+#36)
	}
	{
		memd(r30+##-8496) = r1:0
		r16 = memw(r3+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r17 = memw(r3+#28)
		r14 = memw(r3+#16)
	}
	{
		r0 = memw(r30+#-3896)
		r15 = memw(r3+#20)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r0,##-36736)
		r6 = memw(r3+#0)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-43648)
		r7 = memw(r3+#4)
		r8 = memw(r3+#8)
	}
	{
		r6 = vtrunehb(r7:6)
		r21:20 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r21:20)
		v21.w = vinsert(r6)
		r21:20 = memd(r30+#-7472)
		memw(r30+#-2352) = r1.new
	}                                       // 8-byte Folded Reload
	{
		r21 = vtrunehb(r21:20)
		r9 = memw(r3+#12)
		r3:2 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r6 = vtrunehb(r9:8)
		v21 = valign(v21,v21,#4)
		memd(r4+#48) = r3:2
		r3:2 = memd(r30+#-4656)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		v21.w = vinsert(r6)
		memd(r4+#40) = r3:2
		r3:2 = memd(r30+#-4400)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		v21 = valign(v21,v21,#4)
		memd(r4+#32) = r3:2
		r3:2 = memd(r30+#-3888)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#24) = r3:2
		r3:2 = memd(r30+#-3632)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#16) = r3:2
		r3:2 = memd(r30+#-3376)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#8) = r3:2
		r3:2 = memd(r30+#-3120)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#0) = r3:2
		r3:2 = memd(r30+#-2608)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#56) = r3:2
		r3:2 = memd(r30+#-2864)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		r3:2 = memd(r30+#-5424)
		memw(r30+#-3120) = r1.new
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		memw(r30+#-3376) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v0.cur = vmem(r4+#0)
		vmem(r0+#0) = v0
	}
	{
		v0 = valign(v23,v23,#4)
		r3:2 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r20 = vtrunehb(r3:2)
		r3:2 = memd(r30+##-9776)
		r24 = memw(r0+#56)
	}                                       // 8-byte Folded Reload
	{
		r4 = vtrunehb(r3:2)
		r3:2 = memd(r30+#-7728)
		r25 = memw(r0+#60)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		r2 = vtrunehb(r27:26)
	}
	{
		r2 = vtrunehb(r11:10)
		v0.w = vinsert(r4)
		r4 = memw(r0+#48)
		memw(r30+#-2608) = r2

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = vtrunehb(r23:22)
		memw(r30+#-2864) = r2
		r5 = memw(r0+#52)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r10 = memw(r0+#40)
		r11 = memw(r0+#44)
	}
	{
		r1 = vtrunehb(r19:18)
		v0.w = vinsert(r1)
		r12 = memw(r0+#32)
		memw(r30+#-3632) = r2

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v1.w = vinsert(r1)
		r13 = memw(r0+#36)
	}
	{
		v0 = valign(v0,v0,#4)
		r26 = memw(r0+#24)
		r27 = memw(r0+#28)
	}
	{
		v0.w = vinsert(r20)
		v1 = valign(v1,v1,#4)
		r18 = memw(r0+#16)
		r19 = memw(r0+#20)
	}
	{
		r2 = memw(r0+#0)
		r23:22 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		r3 = memw(r0+#4)
	}
	{
		r1 = vtrunehb(r3:2)
		r7 = vtrunehb(r23:22)
		r2 = memw(r0+#8)
	}
	{
		r1 = vtrunehb(r15:14)
		v20.w = vinsert(r1)
		r3 = memw(r0+#12)
	}
	{
		r0 = vtrunehb(r3:2)
		v1.w = vinsert(r7)
		r7:6 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r7:6)
		v21.w = vinsert(r1)
		v20 = valign(v20,v20,#4)
	}
	{
		r0 = vtrunehb(r19:18)
		v20.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r27:26)
		v1.w = vinsert(r1)
		v21 = valign(v21,v21,#4)
	}
	{
		r26 = ##-1073741825
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r0)
		r27 = ##2147483647
		r0 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r0 = vtrunehb(r17:16)
		v0.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-304)
	}                                       // 8-byte Folded Reload
	{
		v21.w = vinsert(r0)
		r16 = ##2147483647
		v20 = valign(v20,v20,#4)
	}
	{
		r0 = vtrunehb(r7:6)
		v20.w = vinsert(r1)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r2 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r7:6)
		v0.w = vinsert(r2)
		v21 = valign(v21,v21,#4)
	}
	{
		r0 = vtrunehb(r13:12)
		v20 = valign(v20,v20,#4)
		r3:2 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		v21.w = vinsert(r1)
		v20.w = vinsert(r0)
		r17 = #0
		v0 = valign(v0,v0,#4)
	}
	{
		r0 = vtrunehb(r3:2)
		v0.w = vinsert(r21)
		v1 = valign(v1,v1,#4)
		r2 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r11:10)
		v1.w = vinsert(r2)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r10 = ##-2147483648
		v21 = valign(v21,v21,#4)
		r2 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r0 = vtrunehb(r7:6)
		v21.w = vinsert(r0)
		r11 = #-1
		v20 = valign(v20,v20,#4)
	}
	{
		r1 = vtrunehb(r5:4)
		v20.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
		r2 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		v1.w = vinsert(r2)
		r2 = #100
		v21 = valign(v21,v21,#4)
		r5:4 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		v21.w = vinsert(r0)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
		r1 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r25:24)
		v1.w = vinsert(r1)
		v21 = valign(v21,v21,#4)
	}
	{
		v21.w = vinsert(r0)
		r0 = add(r30,#-22832)
		v20 = valign(v20,v20,#4)
	}
	{
		v20.w = vinsert(r1)
		v1 = vror(v1,r2)
	}
	{
		v21 = vror(v21,r2)
		v1 = vor(v1,v9)
	}
	{
		v20 = vror(v20,r2)
		v21 = vor(v21,v9)
	}
	{
		r0 = add(r30,#-23088)
		v20 = vor(v20,v9)
		v22 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vror(v0,r2)
		v1.ub = vmin(v1.ub,v22.ub)
		v21.ub = vmin(v21.ub,v22.ub)
		v20.ub = vmin(v20.ub,v22.ub)
	}
	{
		v0 = vor(v0,v9)
		v23 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.ub = vmax(v1.ub,v23.ub)
		v21.ub = vmax(v21.ub,v23.ub)
		v20.ub = vmax(v20.ub,v23.ub)
		v0.ub = vmin(v0.ub,v22.ub)
	}
	{
		v0.ub = vmax(v0.ub,v23.ub)
		r0 = memw(r30+#-3896)
		r2 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r0,#-27392)
		v20 = vdelta(v20,v14)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-43776)
		v1 = vdelta(v1,v18)
		v0 = vmux(q1,v20,v0)
	}
	{
		r14 = r1
		v21 = vdelta(v21,v15)
	}
	{
		v1 = vmux(q0,v21,v1)
	}
	{
		v0 = vmux(q2,v1,v0)
	}
	{
		vmemu(r2+#0) = v0
	}
	{
		v0.cur = vmem(r1+#4)
		vmem(r0+#0) = v0
	}
	{
		r1 = memw(r0+#120)
		memw(r30+#-2096) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#124)
		memw(r30+#-1840) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#112)
		memw(r30+#-2608) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#116)
		memw(r30+#-2352) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#104)
		memw(r30+#-3120) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#108)
		memw(r30+#-2864) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#96)
		memw(r30+#-3632) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#100)
		memw(r30+#-3376) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#88)
		memw(r30+##-4400) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#92)
		memw(r30+#-3888) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#80)
		memw(r30+##-9264) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#84)
		memw(r30+##-4656) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#72)
		memw(r30+##-11056) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#76)
		memw(r30+##-10032) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#64)
		r2 = memw(r0+#68)
	}
	{
		memw(r30+##-11568) = r2
		r2 = memw(r0+#56)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r9:8 = mpyu(r28,r1)
		memw(r30+##-5424) = r2
		r2 = memw(r0+#60)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-5168) = r2
		r2 = memw(r0+#48)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-7472) = r2
		r2 = memw(r0+#52)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-5680) = r2
		r2 = memw(r0+#40)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-7984) = r2
		r2 = memw(r0+#44)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-7728) = r2
		r2 = memw(r0+#32)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-8496) = r2
		r2 = memw(r0+#36)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-8240) = r2
		r2 = memw(r0+#24)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9008) = r2
		r2 = memw(r0+#28)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-8752) = r2
		r2 = memw(r0+#16)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9776) = r2
		r2 = memw(r0+#20)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9520) = r2
		r2 = memw(r0+#8)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-11184) = r2
		r2 = memw(r0+#12)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-10288) = r2
		r4 = memw(r0+#0)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r5 = asr(r4,#31)
		r24 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r4)
	}
	{
		r7 += mpyi(r28,r5)
	}
	{
		r7 += mpyi(r4,r24)
	}
	{
		r5:4 = min(r7:6,r27:26)
		r7 = #0
		r6 = memw(r30+#-3896)
		r25 = memw(r0+#4)
	}                                       // 4-byte Folded Reload
	{
		r22 = add(r6,##-43904)
		r6 = ##1073741824
	}
	{
		r5:4 = add(r5:4,r7:6)
		v0.cur = vmem(r14+#5)
		vmem(r22+#0) = v0
	}
	{
		r3:2 = asr(r5:4,#31)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r3:2,r17:16)
		r12 = add(r0,##-44032)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r11:10)
		r2 = memw(r22+#120)
		memw(r30+##-17456) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r0,##-44160)
		r2 = memw(r22+#124)
	}
	{
		r5 = asr(r1,#31)
		v2.w = vinsert(r4)
		memw(r30+##-17072) = r2
	}                                       // 4-byte Folded Spill
	{
		r9 += mpyi(r28,r5)
		r2 = memw(r22+#112)
		memw(r30+##-18232) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r9 += mpyi(r1,r24)
		r2 = memw(r22+#116)
		memw(r30+##-17712) = r2.new
	}                                       // 4-byte Folded Spill
	{
		v2 = valign(v2,v2,#4)
		r2 = memw(r22+#104)
		memw(r30+##-18736) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#108)
		memw(r30+##-18608) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#96)
		memw(r30+##-19504) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#100)
		memw(r30+##-19248) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#88)
		memw(r30+##-19776) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#92)
		memw(r30+##-20272) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#80)
		memw(r30+##-21136) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#84)
		memw(r30+##-20528) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#72)
		memw(r30+##-21200) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r22+#76)
		memw(r30+##-21176) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r22+#64)
		r15 = memw(r22+#68)
	}
	{
		r4 = asr(r3,#31)
		r2 = memw(r22+#56)
		memw(r30+##-21040) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r21:20 = mpyu(r28,r3)
		r2 = memw(r22+#60)
		memw(r30+##-20912) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r21 += mpyi(r28,r4)
		r2 = memw(r22+#48)
		memw(r30+##-21056) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r21 += mpyi(r3,r24)
		r3 = #0
		r2 = memw(r22+#52)
	}
	{
		memw(r30+##-21048) = r2
		r2 = memw(r22+#40)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21072) = r2
		r2 = memw(r22+#44)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21064) = r2
		r2 = memw(r22+#32)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21096) = r2
		r2 = memw(r22+#36)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21080) = r2
		r2 = memw(r22+#24)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21112) = r2
		r2 = memw(r22+#28)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21104) = r2
		r2 = memw(r22+#16)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21160) = r2
		r2 = memw(r22+#20)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21128) = r2
		r2 = memw(r22+#8)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21216) = r2
		r2 = memw(r22+#12)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-21184) = r2
		r7 = memw(r22+#0)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = ##1073741824
		r13 = memw(r22+#4)
		memw(r30+#-304) = r14

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r19:18 = mpyu(r28,r7)
		r5 = asr(r7,#31)
		v0.cur = vmem(r14+#6)
		vmem(r12+#0) = v0
	}
	{
		r19 += mpyi(r28,r5)
		r1 = memw(r12+#120)
		memw(r30+##-13232) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = min(r9:8,r27:26)
		r1 = memw(r12+#124)
		memw(r30+##-12976) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r19 += mpyi(r7,r24)
		r5:4 = add(r5:4,r3:2)
		r1 = memw(r12+#112)
	}
	{
		r9:8 = min(r19:18,r27:26)
		r5:4 = asr(r5:4,#31)
		memw(r30+##-14256) = r1
	}                                       // 4-byte Folded Spill
	{
		r5:4 = min(r5:4,r17:16)
		r1 = memw(r12+#116)
		memw(r30+##-13744) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = min(r21:20,r27:26)
		r1 = memw(r12+#104)
		memw(r30+##-14896) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r5:4,r11:10)
		r19:18 = add(r7:6,r3:2)
		r1 = memw(r12+#108)
	}
	{
		r9:8 = add(r9:8,r3:2)
		v5.w = vinsert(r4)
		memw(r30+##-14640) = r1
	}                                       // 4-byte Folded Spill
	{
		r19:18 = asr(r19:18,#31)
		r1 = memw(r12+#96)
		memw(r30+##-16560) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = asr(r9:8,#31)
		r1 = memw(r12+#100)
		memw(r30+##-15024) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = min(r19:18,r17:16)
		r9:8 = min(r9:8,r17:16)
		r1 = memw(r12+#88)
	}
	{
		r19:18 = max(r19:18,r11:10)
		r7:6 = combine(r17,r16)
		memw(r30+##-18864) = r1
	}                                       // 4-byte Folded Spill
	{
		r9:8 = max(r9:8,r11:10)
		r1 = memw(r12+#92)
		memw(r30+##-17584) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v4.w = vinsert(r18)
		r1 = memw(r12+#80)
		memw(r30+##-21120) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r8 = asr(r25,#31)
		v19.w = vinsert(r8)
		r1 = memw(r12+#84)
	}
	{
		r19:18 = mpyu(r28,r25)
		r16 = r14
		memw(r30+##-19768) = r1
	}                                       // 4-byte Folded Spill
	{
		r19 += mpyi(r28,r8)
		r1 = memw(r12+#72)
		memw(r30+##-21232) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v5 = valign(v5,v5,#4)
		r1 = memw(r12+#76)
		memw(r30+##-21192) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r19 += mpyi(r25,r24)
		v1 = valign(v4,v4,#4)
		r1 = memw(r12+#64)
	}
	{
		r19:18 = min(r19:18,r27:26)
		r4 = memw(r12+#68)
		memw(r30+##-21248) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r8 = asr(r1,#31)
		r19:18 = add(r19:18,r3:2)
		r4 = memw(r12+#56)
	}
	{
		r23:22 = mpyu(r28,r1)
		r19:18 = asr(r19:18,#31)
		memw(r30+##-15920) = r4
	}                                       // 4-byte Folded Spill
	{
		r23 += mpyi(r28,r8)
		r4 = memw(r12+#60)
		memw(r30+##-15664) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = combine(r3,r2)
		r4 = memw(r12+#48)
		memw(r30+##-16816) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r23 += mpyi(r1,r24)
		r1 = r24
		r4 = memw(r12+#52)
	}
	{
		r23:22 = min(r23:22,r27:26)
		memw(r30+##-16432) = r4
		r4 = memw(r12+#40)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r23:22 = add(r23:22,r3:2)
		memw(r30+##-17328) = r4
		r4 = memw(r12+#44)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r23:22 = asr(r23:22,#31)
		memw(r30+##-16944) = r4
		r4 = memw(r12+#32)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r23:22 = min(r23:22,r7:6)
		memw(r30+##-17968) = r4
		r4 = memw(r12+#36)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r23:22 = max(r23:22,r11:10)
		memw(r30+##-17840) = r4
		r4 = memw(r12+#24)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v13.w = vinsert(r22)
		memw(r30+##-19000) = r4
		r4 = memw(r12+#28)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-18224) = r4
		r4 = memw(r12+#16)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v4 = valign(v13,v13,#4)
		memw(r30+##-21152) = r4
		r4 = memw(r12+#20)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-20656) = r4
		r17 = memw(r12+#8)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r4 = memw(r12+#12)
		memw(r30+##-21208) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r4 = memw(r12+#0)
	}
	{
		r14 = asr(r4,#31)
	}
	{
		r21:20 = mpyu(r28,r4)
	}
	{
		r21 += mpyi(r28,r14)
	}
	{
		r21 += mpyi(r4,r24)
		r5:4 = combine(r7,r6)
	}
	{
		r21:20 = min(r21:20,r27:26)
		r19:18 = min(r19:18,r5:4)
	}
	{
		r23:22 = max(r19:18,r11:10)
		r21:20 = add(r21:20,r3:2)
	}
	{
		r21:20 = asr(r21:20,#31)
		v2.w = vinsert(r22)
	}
	{
		r21:20 = min(r21:20,r7:6)
		r7 = memw(r12+#4)
		v0 = vmem(r16+#7)
	}
	{
		r21:20 = max(r21:20,r11:10)
		v0 = valign(v19,v19,#4)
		vmem(r0+#0) = v0
	}
	{
		v3.w = vinsert(r20)
		r3 = memw(r0+#120)
		memw(r30+##-11824) = r3.new
	}                                       // 4-byte Folded Spill
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r0+#124)
		memw(r30+##-12080) = r3.new
	}                                       // 4-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r0+#112)
		memw(r30+##-12592) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#116)
		memw(r30+##-12336) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#104)
		memw(r30+##-13104) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#108)
		memw(r30+##-12848) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#96)
		memw(r30+##-16688) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#100)
		memw(r30+##-15280) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#88)
		memw(r30+##-18992) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#92)
		memw(r30+##-18096) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#80)
		memw(r30+##-21144) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#84)
		memw(r30+##-20664) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r0+#72)
		memw(r30+##-21256) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#76)
		memw(r30+##-21224) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r2 = ##-1073741825
		r16 = memw(r0+#64)
		r12 = memw(r0+#68)
	}
	{
		r3 = memw(r0+#56)
		memw(r30+##-13616) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r25:24 = mpyu(r28,r16)
		r3 = memw(r0+#60)
		memw(r30+##-13360) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#48)
		memw(r30+##-14384) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#52)
		memw(r30+##-14128) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#40)
		memw(r30+##-15152) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#44)
		memw(r30+##-14512) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#32)
		memw(r30+##-17200) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#36)
		memw(r30+##-15792) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#24)
		memw(r30+##-19760) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#28)
		memw(r30+##-18480) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r0+#16)
		memw(r30+##-21168) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = memw(r30+##-11568)
		r6 = memw(r0+#20)
	}                                       // 4-byte Folded Reload
	{
		r19 = asr(r3,#31)
		memw(r30+##-21088) = r6
		r14 = memw(r0+#8)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r27:26 = mpyu(r28,r3)
		r6 = memw(r0+#12)
		memw(r30+##-21240) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r27 += mpyi(r28,r19)
		r19 = asr(r16,#31)
		r18 = memw(r0+#0)
	}
	{
		r27 += mpyi(r3,r1)
		r25 += mpyi(r28,r19)
		r3 = ##2147483647
	}
	{
		r25 += mpyi(r16,r1)
		r22 = asr(r18,#31)
	}
	{
		r21:20 = mpyu(r28,r18)
		r16 = asr(r15,#31)
	}
	{
		r21 += mpyi(r28,r22)
		r23:22 = min(r27:26,r3:2)
		r27:26 = combine(r9,r8)
	}
	{
		r21 += mpyi(r18,r1)
		r23:22 = add(r23:22,r9:8)
	}
	{
		r21:20 = min(r21:20,r3:2)
		r19:18 = asr(r23:22,#31)
	}
	{
		r23:22 = min(r25:24,r3:2)
		r21:20 = add(r21:20,r9:8)
	}
	{
		r25:24 = add(r23:22,r9:8)
		r21:20 = asr(r21:20,#31)
	}
	{
		r23:22 = min(r21:20,r5:4)
		r21:20 = asr(r25:24,#31)
		r25:24 = combine(r3,r2)
	}
	{
		r19:18 = min(r19:18,r5:4)
		r23:22 = max(r23:22,r11:10)
	}
	{
		r21:20 = min(r21:20,r5:4)
		v8.w = vinsert(r22)
	}
	{
		r23:22 = max(r21:20,r11:10)
		r21:20 = mpyu(r28,r15)
	}
	{
		r19:18 = max(r19:18,r11:10)
		r21 += mpyi(r28,r16)
		r16 = r1
		v6 = valign(v8,v8,#4)
	}
	{
		r23 = asr(r13,#31)
		v5.w = vinsert(r18)
	}
	{
		r19:18 = mpyu(r28,r13)
		r21 += mpyi(r15,r1)
	}
	{
		r19 += mpyi(r28,r23)
		v7.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
	}
	{
		r23:22 = mpyu(r28,r7)
		r6 = asr(r7,#31)
	}
	{
		r19 += mpyi(r13,r1)
		r23 += mpyi(r28,r6)
		r13 = memw(r30+##-21248)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = min(r21:20,r3:2)
		r23 += mpyi(r7,r1)
		v7 = valign(v7,v7,#4)
	}
	{
		r19:18 = min(r19:18,r3:2)
		r21:20 = add(r21:20,r9:8)
	}
	{
		r23:22 = min(r23:22,r3:2)
		r19:18 = add(r19:18,r9:8)
		r2 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = asr(r21:20,#31)
		r19:18 = asr(r19:18,#31)
		r6 = r2
	}
	{
		r21:20 = min(r21:20,r5:4)
		r23:22 = add(r23:22,r9:8)
	}
	{
		r19:18 = min(r19:18,r5:4)
		r23:22 = asr(r23:22,#31)
	}
	{
		r21:20 = max(r21:20,r11:10)
		r19:18 = max(r19:18,r11:10)
	}
	{
		r23:22 = min(r23:22,r5:4)
		v1.w = vinsert(r20)
	}
	{
		r21:20 = mpyu(r28,r2)
		r1 = asr(r2,#31)
		r2 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = max(r23:22,r11:10)
		v0.w = vinsert(r18)
		v1 = valign(v1,v1,#4)
		r0 = memw(r0+#4)
	}
	{
		r21 += mpyi(r28,r1)
		v3.w = vinsert(r18)
	}
	{
		r19:18 = mpyu(r28,r2)
		r1 = asr(r2,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r21 += mpyi(r6,r16)
		r19 += mpyi(r28,r1)
		v3 = valign(v3,v3,#4)
	}
	{
		r19 += mpyi(r2,r16)
		r21:20 = min(r21:20,r25:24)
		r2 = memw(r30+##-21216)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = min(r19:18,r25:24)
		r21:20 = add(r21:20,r9:8)
	}
	{
		r21:20 = asr(r21:20,#31)
		r19:18 = add(r19:18,r9:8)
	}
	{
		r21:20 = min(r21:20,r5:4)
		r19:18 = asr(r19:18,#31)
	}
	{
		r19:18 = min(r19:18,r5:4)
		r21:20 = max(r21:20,r11:10)
	}
	{
		r23:22 = mpyu(r28,r13)
		r3 = asr(r13,#31)
	}
	{
		r23 += mpyi(r28,r3)
		v2.w = vinsert(r20)
	}
	{
		r19:18 = max(r19:18,r11:10)
		r1 = asr(r0,#31)
	}
	{
		r21:20 = mpyu(r28,r0)
		r3 = asr(r12,#31)
		v20 = valign(v2,v2,#4)
	}
	{
		r7:6 = mpyu(r28,r12)
		r21 += mpyi(r28,r1)
	}
	{
		r7 += mpyi(r28,r3)
		v5.w = vinsert(r18)
	}
	{
		r19:18 = mpyu(r28,r2)
		r1 = asr(r2,#31)
	}
	{
		r19 += mpyi(r28,r1)
		r7 += mpyi(r12,r16)
		v31 = valign(v5,v5,#4)
	}
	{
		r19 += mpyi(r2,r16)
		r21 += mpyi(r0,r16)
		r12 = memw(r30+##-21256)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r7:6,r25:24)
		r19:18 = min(r19:18,r25:24)
	}
	{
		r23 += mpyi(r13,r16)
		r1:0 = add(r1:0,r9:8)
	}
	{
		r7:6 = min(r21:20,r25:24)
		r1:0 = asr(r1:0,#31)
		r21:20 = combine(r5,r4)
	}
	{
		r23:22 = min(r23:22,r25:24)
		r19:18 = add(r19:18,r9:8)
	}
	{
		r1:0 = min(r1:0,r5:4)
		r3:2 = add(r7:6,r9:8)
	}
	{
		r19:18 = asr(r19:18,#31)
		r23:22 = add(r23:22,r9:8)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = asr(r3:2,#31)
	}
	{
		r7:6 = min(r19:18,r5:4)
		v7.w = vinsert(r0)
	}
	{
		r3:2 = min(r3:2,r5:4)
		r23:22 = asr(r23:22,#31)
	}
	{
		r1:0 = max(r7:6,r11:10)
		r23:22 = min(r23:22,r5:4)
		r6 = memw(r30+##-21200)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r11:10)
		v7 = valign(v7,v7,#4)
		r7 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = max(r23:22,r11:10)
		v6.w = vinsert(r2)
	}
	{
		r3:2 = mpyu(r28,r6)
		r1 = asr(r6,#31)
	}
	{
		r3 += mpyi(r28,r1)
		v4.w = vinsert(r22)
	}
	{
		r0 = asr(r7,#31)
		v0.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
	}
	{
		r23:22 = mpyu(r28,r7)
		r1 = asr(r17,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r19:18 = mpyu(r28,r17)
		r3 += mpyi(r6,r16)
		v30 = valign(v0,v0,#4)
	}
	{
		r19 += mpyi(r28,r1)
		r23 += mpyi(r28,r0)
	}
	{
		r3:2 = min(r3:2,r25:24)
		r23 += mpyi(r7,r16)
	}
	{
		r19 += mpyi(r17,r16)
		r1:0 = add(r3:2,r9:8)
	}
	{
		r7:6 = min(r23:22,r25:24)
		r3:2 = min(r19:18,r25:24)
		r23:22 = combine(r11,r10)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r3:2,r9:8)
	}
	{
		r1:0 = min(r1:0,r5:4)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r3:2 = asr(r3:2,#31)
		r7:6 = asr(r7:6,#31)
	}
	{
		r3:2 = min(r3:2,r5:4)
		r1:0 = max(r1:0,r11:10)
	}
	{
		r7:6 = min(r7:6,r5:4)
		v1.w = vinsert(r0)
		r4 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r7:6,r11:10)
		r3:2 = max(r3:2,r11:10)
		r8 = r4
	}
	{
		r1 = asr(r4,#31)
		v3.w = vinsert(r2)
	}
	{
		r3:2 = mpyu(r28,r4)
		v19 = valign(v1,v1,#4)
		r4 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r28,r1)
		r5 = asr(r4,#31)
	}
	{
		r7:6 = mpyu(r28,r4)
		v4.w = vinsert(r0)
		v5 = valign(v3,v3,#4)
	}
	{
		r7 += mpyi(r28,r5)
		r5 = asr(r14,#31)
	}
	{
		r7 += mpyi(r4,r16)
		r3 += mpyi(r8,r16)
		v2 = valign(v4,v4,#4)
	}
	{
		r1:0 = min(r7:6,r25:24)
		r7:6 = mpyu(r28,r14)
	}
	{
		r7 += mpyi(r28,r5)
		r3:2 = min(r3:2,r25:24)
	}
	{
		r7 += mpyi(r14,r16)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r5:4 = min(r7:6,r25:24)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r21:20)
		r3:2 = min(r3:2,r21:20)
	}
	{
		r7:6 = max(r5:4,r11:10)
		r1:0 = min(r1:0,r21:20)
		r4 = memw(r30+##-21184)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		v6.w = vinsert(r6)
		r6 = memw(r30+##-21176)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r11:10)
		v31.w = vinsert(r0)
		r9 = r4
		r8 = r6
	}
	{
		r1 = asr(r4,#31)
		v20.w = vinsert(r2)
		v3 = valign(v6,v6,#4)
	}
	{
		r3:2 = mpyu(r28,r4)
		r0 = asr(r6,#31)
		v4 = valign(v31,v31,#4)
	}
	{
		r5:4 = mpyu(r28,r6)
		r3 += mpyi(r28,r1)
		v0 = valign(v20,v20,#4)
	}
	{
		r5 += mpyi(r28,r0)
		r1 = asr(r12,#31)
	}
	{
		r7:6 = mpyu(r28,r12)
		r5 += mpyi(r8,r16)
	}
	{
		r7 += mpyi(r28,r1)
		r3 += mpyi(r9,r16)
		r9 = memw(r30+##-21192)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r12,r16)
		r5:4 = min(r5:4,r25:24)
	}
	{
		r3:2 = min(r3:2,r25:24)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = add(r3:2,r27:26)
	}
	{
		r3:2 = add(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r21:20)
		r3:2 = min(r3:2,r21:20)
	}
	{
		r1:0 = min(r1:0,r21:20)
		r7:6 = max(r5:4,r11:10)
		r4 = memw(r30+##-21208)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = max(r3:2,r11:10)
		r12 = r4
	}
	{
		r1 = asr(r4,#31)
		v7.w = vinsert(r2)
	}
	{
		r3:2 = mpyu(r28,r4)
		v30.w = vinsert(r0)
	}
	{
		r3 += mpyi(r28,r1)
		v19.w = vinsert(r6)
		r1 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r9)
		r0 = asr(r1,#31)
		r8 = r1
		v1 = valign(v7,v7,#4)
	}
	{
		r5:4 = mpyu(r28,r1)
		r1 = asr(r9,#31)
		v31 = valign(v30,v30,#4)
	}
	{
		r5 += mpyi(r28,r0)
		r7 += mpyi(r28,r1)
		v30 = valign(v19,v19,#4)
	}
	{
		r5 += mpyi(r8,r16)
		r3 += mpyi(r12,r16)
	}
	{
		r7 += mpyi(r9,r16)
		r5:4 = min(r5:4,r25:24)
		r9 = memw(r30+##-21224)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r7:6,r25:24)
		r3:2 = min(r3:2,r25:24)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r3:2 = add(r3:2,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r21:20)
		r3:2 = min(r3:2,r21:20)
	}
	{
		r1:0 = min(r1:0,r21:20)
		r7:6 = max(r5:4,r11:10)
		r4 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		v0.w = vinsert(r6)
		r6 = memw(r30+##-21240)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r11:10)
		r1 = asr(r4,#31)
		r8 = r4
		r12 = r6
	}
	{
		r3:2 = mpyu(r28,r4)
		v5.w = vinsert(r2)
		v0 = valign(v0,v0,#4)
	}
	{
		r3 += mpyi(r28,r1)
		v2.w = vinsert(r0)
	}
	{
		r0 = asr(r9,#31)
		r1 = asr(r6,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r5:4 = mpyu(r28,r6)
		r7:6 = mpyu(r28,r9)
		v2 = valign(v2,v2,#4)
	}
	{
		r3 += mpyi(r8,r16)
		r7 += mpyi(r28,r0)
	}
	{
		r7 += mpyi(r9,r16)
		r3:2 = min(r3:2,r25:24)
	}
	{
		r5 += mpyi(r28,r1)
		r1:0 = add(r3:2,r27:26)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r5 += mpyi(r12,r16)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r7:6,r27:26)
	}
	{
		r5:4 = min(r5:4,r25:24)
		r1:0 = min(r1:0,r21:20)
	}
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = min(r3:2,r21:20)
	}
	{
		r5:4 = asr(r5:4,#31)
		v4.w = vinsert(r0)
	}
	{
		r1:0 = max(r3:2,r11:10)
		r5:4 = min(r5:4,r21:20)
		r2 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r11:10)
		r1 = asr(r2,#31)
		r21:20 = combine(r21,r20)
		v4 = valign(v4,v4,#4)
	}
	{
		r11:10 = mpyu(r28,r2)
		v1.w = vinsert(r0)
		r5 = memw(r30+##-21160)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r28,r1)
		v3.w = vinsert(r4)
		r1 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r2,r16)
		r0 = asr(r1,#31)
		r2 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = mpyu(r28,r1)
		v3 = valign(v3,v3,#4)
		r4 = r1
		r3 = r2
	}
	{
		r9 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = mpyu(r28,r2)
		r9 += mpyi(r1,r16)
		r2 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r15:14 = mpyu(r28,r2)
		memd(r30+##-9008) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r3,r16)
		r15 += mpyi(r28,r0)
		r2 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+##-10288) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r15 += mpyi(r4,r16)
		r2 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r19:18 = mpyu(r28,r2)
		memd(r30+##-11056) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r3,r16)
		r19 += mpyi(r28,r0)
		r2 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+##-8752) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r19 += mpyi(r4,r16)
		r2 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
		memd(r30+#-7984) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r13:12 = mpyu(r28,r2)
		r7 += mpyi(r3,r16)
		r2 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r13 += mpyi(r4,r16)
		r2 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
		memd(r30+#-7472) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r15:14 = mpyu(r28,r2)
		r7 += mpyi(r3,r16)
		r2 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r15 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+#-5680) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r15 += mpyi(r4,r16)
		r2 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
		memd(r30+#-5424) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r9:8 = mpyu(r28,r2)
		r7 += mpyi(r3,r16)
		r2 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		r9 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+#-5168) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r9 += mpyi(r4,r16)
		r2 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r19:18 = mpyu(r28,r2)
		r7 += mpyi(r3,r16)
		r2 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		memd(r30+##-8496) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r19 += mpyi(r4,r16)
		r3 = r2
		r2 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r15:14 = mpyu(r28,r2)
		r2 = memw(r30+#-3120)
		memd(r30+##-21176) = r19:18
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r16)
		r15 += mpyi(r28,r0)
	}
	{
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+##-11568) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r2)
		r15 += mpyi(r4,r16)
		r2 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r19:18 = mpyu(r28,r2)
		r2 = memw(r30+#-2608)
		memd(r30+##-9264) = r15:14
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r16)
		r19 += mpyi(r28,r0)
	}
	{
		r7:6 = mpyu(r28,r2)
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+#-4656) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r4,r16)
		r7 += mpyi(r28,r0)
		r2 = memw(r30+#-2352)
		r1 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r16)
		r0 = asr(r2,#31)
		r4 = r2
	}
	{
		r15:14 = mpyu(r28,r2)
		r12 = r1
		memd(r30+#-3888) = r19:18
		memd(r30+#-2608) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r28,r0)
		r0 = asr(r5,#31)
		r6 = memw(r30+##-21136)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r5)
		r15 += mpyi(r4,r16)
		r7 = r5
	}
	{
		r3 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		r14 = r16
		memd(r30+#-2352) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r19:18 = mpyu(r28,r1)
		r1 = asr(r6,#31)
	}
	{
		r5:4 = mpyu(r28,r6)
		r3 += mpyi(r7,r14)
	}
	{
		r5 += mpyi(r28,r1)
		r19 += mpyi(r28,r0)
	}
	{
		r5 += mpyi(r6,r14)
		r7:6 = min(r3:2,r25:24)
	}
	{
		r3:2 = min(r5:4,r25:24)
		r1:0 = add(r7:6,r27:26)
		r5 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = add(r3:2,r27:26)
		r4 = asr(r5,#31)
	}
	{
		r7:6 = mpyu(r28,r5)
		r3:2 = asr(r3:2,#31)
	}
	{
		r7 += mpyi(r28,r4)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7 += mpyi(r5,r14)
		r3:2 = min(r3:2,r21:20)
	}
	{
		r1:0 = min(r1:0,r21:20)
		r7:6 = max(r3:2,r23:22)
		memd(r30+#-1840) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r12,r14)
		r1:0 = max(r1:0,r23:22)
		r2 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = combine(r21,r20)
		r21:20 = combine(r23,r22)
		r19:18 = combine(r27,r26)
		memd(r30+#-2096) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r27:26 = mpyu(r28,r2)
		r1 = asr(r2,#31)
	}
	{
		r27 += mpyi(r28,r1)
		v31.w = vinsert(r0)
		r1 = memw(r30+##-21112)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r1,#31)
		v30.w = vinsert(r6)
		r3 = r1
	}
	{
		r5:4 = mpyu(r28,r1)
		r27 += mpyi(r2,r14)
		r1 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		r2 = r1
		v6 = valign(v31,v31,#4)
	}
	{
		r7:6 = mpyu(r28,r1)
		v7 = valign(v30,v30,#4)
		r1 = memw(r30+##-21096)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r3,r14)
		r7 += mpyi(r28,r0)
	}
	{
		r0 = asr(r1,#31)
		r3 = r1
		memd(r30+##-9520) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r1)
		r7 += mpyi(r2,r14)
		r1 = memw(r30+##-21080)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		memd(r30+##-21104) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r1)
		r2 = r1
		r1 = memw(r30+##-21072)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r3,r14)
		r7 += mpyi(r28,r0)
	}
	{
		r0 = asr(r1,#31)
		r3 = r1
		memd(r30+##-21096) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r1)
		r7 += mpyi(r2,r14)
		r1 = memw(r30+##-21064)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r1)
		r2 = r1
		r1 = memw(r30+##-21056)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r3,r14)
		r7 += mpyi(r28,r0)
	}
	{
		r0 = asr(r1,#31)
		r3 = r1
		memd(r30+##-8240) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r28,r1)
		r7 += mpyi(r2,r14)
		r1 = memw(r30+##-21048)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		r2 = r1
		memd(r30+#-4400) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r1)
		r5 += mpyi(r3,r14)
		r1 = memw(r30+##-21040)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r0)
		r0 = asr(r1,#31)
		r3 = r1
		memd(r30+#-2864) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r2,r14)
		r17:16 = mpyu(r28,r1)
		r2 = memw(r30+##-20912)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r28,r0)
		r6 = memw(r30+##-21152)
		memd(r30+#-3120) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r2,#31)
		r1 = asr(r6,#31)
	}
	{
		r17 += mpyi(r3,r14)
		r5:4 = mpyu(r28,r6)
	}
	{
		r23:22 = mpyu(r28,r2)
		r5 += mpyi(r28,r1)
		memd(r30+#-3376) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r17:16 = combine(r23,r22)
		r1 = memw(r30+##-21120)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r28,r0)
		r0 = asr(r1,#31)
	}
	{
		r23:22 = mpyu(r28,r1)
		r5 += mpyi(r6,r14)
	}
	{
		r23 += mpyi(r28,r0)
		r5:4 = min(r5:4,r25:24)
		r0 = memw(r30+##-20528)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r2,r14)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r7:6 = mpyu(r28,r0)
		r2 = asr(r0,#31)
		memd(r30+#-3632) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r23 += mpyi(r1,r14)
		r7 += mpyi(r28,r2)
		r2 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = min(r23:22,r25:24)
		r5:4 = asr(r5:4,#31)
		r17:16 = combine(r2,r14)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r7 += mpyi(r0,r14)
		r0 = memw(r30+##-20272)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r23:22 = add(r23:22,r19:18)
	}
	{
		r5:4 = asr(r23:22,#31)
		v5.w = vinsert(r4)
	}
	{
		r23:22 = mpyu(r28,r2)
		r3 = asr(r2,#31)
	}
	{
		r15:14 = mpyu(r28,r0)
		r2 = asr(r0,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r15 += mpyi(r28,r2)
		r23 += mpyi(r28,r3)
	}
	{
		r15 += mpyi(r0,r16)
		r1:0 = min(r11:10,r25:24)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = add(r1:0,r19:18)
		r1 = memw(r30+##-21168)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r0 = asr(r1,#31)
	}
	{
		r11:10 = mpyu(r28,r1)
		v2.w = vinsert(r4)
	}
	{
		r5:4 = min(r9:8,r25:24)
		r3:2 = asr(r3:2,#31)
	}
	{
		r11 += mpyi(r28,r0)
		r5:4 = add(r5:4,r19:18)
		v2 = valign(v2,v2,#4)
	}
	{
		r3:2 = min(r3:2,r13:12)
		r11 += mpyi(r1,r16)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = min(r11:10,r25:24)
		v0.w = vinsert(r2)
	}
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = add(r1:0,r19:18)
		r0 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r3:2 = asr(r3:2,#31)
		r1 = r0
		v0 = valign(v0,v0,#4)
	}
	{
		r9:8 = mpyu(r28,r0)
		r5 = asr(r0,#31)
		r0 = memw(r30+##-19504)
	}                                       // 4-byte Folded Reload
	{
		r9 += mpyi(r28,r5)
		v4.w = vinsert(r4)
	}
	{
		r3:2 = min(r3:2,r13:12)
		r4 = asr(r0,#31)
	}
	{
		r11:10 = mpyu(r28,r0)
		r9 += mpyi(r1,r16)
		v4 = valign(v4,v4,#4)
	}
	{
		r11 += mpyi(r28,r4)
		r3:2 = max(r3:2,r21:20)
	}
	{
		r5:4 = min(r9:8,r25:24)
		v3.w = vinsert(r2)
	}
	{
		r11 += mpyi(r0,r16)
		r3:2 = add(r5:4,r19:18)
		r5 = memw(r30+##-19248)
	}                                       // 4-byte Folded Reload
	{
		r4 = asr(r5,#31)
		r3:2 = asr(r3:2,#31)
		memd(r30+##-19504) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r9:8 = mpyu(r28,r5)
		r23 += mpyi(r17,r16)
		r17 = r5
		v3 = valign(v3,v3,#4)
	}
	{
		r3:2 = min(r3:2,r13:12)
		r1:0 = combine(r9,r8)
	}
	{
		r1 += mpyi(r28,r4)
		r5:4 = min(r27:26,r25:24)
		r27:26 = combine(r13,r12)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r5:4 = asr(r5:4,#31)
		v1.w = vinsert(r2)
	}
	{
		r3:2 = min(r5:4,r13:12)
		r1 += mpyi(r17,r16)
		r5 = memw(r30+##-20656)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r21:20)
		r1:0 = min(r7:6,r25:24)
		memd(r30+##-19248) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r1:0,r19:18)
		r7 = r5
		r3 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r28,r5)
		r4 = asr(r5,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r11 += mpyi(r28,r4)
		r1:0 = asr(r1:0,#31)
		r6 = r3
	}
	{
		r1:0 = min(r1:0,r13:12)
		v6.w = vinsert(r2)
	}
	{
		r5:4 = mpyu(r28,r3)
		r2 = asr(r3,#31)
	}
	{
		r11 += mpyi(r7,r16)
		r5 += mpyi(r28,r2)
		v6 = valign(v6,v6,#4)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = min(r11:10,r25:24)
	}
	{
		r1:0 = add(r3:2,r19:18)
		v7.w = vinsert(r0)
		r3 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r6,r16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r9:8 = mpyu(r28,r3)
		r2 = asr(r3,#31)
		r17 = r3
		v7 = valign(v7,v7,#4)
	}
	{
		r5:4 = min(r5:4,r25:24)
		r9 += mpyi(r28,r2)
	}
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = add(r5:4,r19:18)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r7:6 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r5:4,r25:24)
		v5.w = vinsert(r0)
		r5 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r7:6,r13:12)
		r3:2 = add(r3:2,r19:18)
		r10 = r5
	}
	{
		r1:0 = max(r1:0,r21:20)
		r4 = asr(r5,#31)
	}
	{
		r7:6 = mpyu(r28,r5)
		r3:2 = asr(r3:2,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r1:0 = min(r3:2,r13:12)
		v2.w = vinsert(r0)
	}
	{
		r7 += mpyi(r28,r4)
		r3 = memw(r30+##-21088)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r10,r16)
		r2 = asr(r3,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = mpyu(r28,r3)
		memd(r30+##-9008) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r9 += mpyi(r17,r16)
		r1:0 = max(r1:0,r21:20)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r28,r2)
		r17 = r3
		memd(r30+##-11184) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r3:2 = min(r7:6,r25:24)
		v0.w = vinsert(r0)
	}
	{
		r5 += mpyi(r17,r16)
		r1:0 = add(r3:2,r19:18)
		r3 = memw(r30+##-20664)
	}                                       // 4-byte Folded Reload
	{
		r2 = asr(r3,#31)
		r1:0 = asr(r1:0,#31)
		r6 = r3
		v0 = valign(v0,v0,#4)
	}
	{
		r9:8 = mpyu(r28,r3)
		r5:4 = min(r5:4,r25:24)
	}
	{
		r9 += mpyi(r28,r2)
		r1:0 = min(r1:0,r13:12)
	}
	{
		r9 += mpyi(r6,r16)
		r3:2 = add(r5:4,r19:18)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r9:8,r25:24)
		v4.w = vinsert(r0)
	}
	{
		r1:0 = min(r5:4,r13:12)
		r3:2 = add(r3:2,r19:18)
		r5 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = asr(r3:2,#31)
		r10 = r5
		v4 = valign(v4,v4,#4)
	}
	{
		r1:0 = min(r3:2,r13:12)
		v3.w = vinsert(r0)
		r3 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r4 = asr(r5,#31)
		r11 = r3
	}
	{
		r7:6 = mpyu(r28,r5)
		r2 = asr(r3,#31)
		r1 = memw(r30+##-18864)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r4)
		r5:4 = mpyu(r28,r3)
		v3 = valign(v3,v3,#4)
	}
	{
		r5 += mpyi(r28,r2)
		v1.w = vinsert(r0)
	}
	{
		r3:2 = mpyu(r28,r1)
		r0 = asr(r1,#31)
	}
	{
		r5 += mpyi(r11,r16)
		r7 += mpyi(r10,r16)
		r10 = r1
		v1 = valign(v1,v1,#4)
	}
	{
		r3 += mpyi(r28,r0)
		r1:0 = min(r23:22,r25:24)
		memd(r30+##-8496) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = min(r5:4,r25:24)
		r1:0 = add(r1:0,r19:18)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r3 += mpyi(r10,r16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r3:2 = min(r3:2,r25:24)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = max(r1:0,r21:20)
	}
	{
		r3:2 = add(r3:2,r19:18)
		v7.w = vinsert(r0)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r1:0 = asr(r3:2,#31)
		r3 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r25:24)
		v5.w = vinsert(r4)
		r4 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = add(r7:6,r19:18)
		r2 = asr(r3,#31)
		r5 = r3
		v7 = valign(v7,v7,#4)
	}
	{
		r7:6 = mpyu(r28,r3)
		r3 = asr(r4,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r9:8 = mpyu(r28,r4)
		r7 += mpyi(r28,r2)
	}
	{
		r9 += mpyi(r28,r3)
		r7 += mpyi(r5,r16)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r9 += mpyi(r4,r16)
		memd(r30+##-9520) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = memd(r30+##-21176)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r25:24)
		r1:0 = max(r1:0,r21:20)
		memd(r30+##-9776) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r7:6,r25:24)
		v2.w = vinsert(r0)
		r5 = memw(r30+##-19760)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r1:0,r19:18)
		r4 = asr(r5,#31)
	}
	{
		r11:10 = mpyu(r28,r5)
		r1:0 = asr(r1:0,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r11 += mpyi(r28,r4)
		r3:2 = add(r3:2,r19:18)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r11 += mpyi(r5,r16)
		r5 = memw(r30+##-18992)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = asr(r3:2,#31)
	}
	{
		r7:6 = min(r11:10,r25:24)
		v0.w = vinsert(r0)
	}
	{
		r1:0 = min(r3:2,r27:26)
		r3:2 = add(r7:6,r19:18)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = asr(r3:2,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r3:2,r27:26)
		v4.w = vinsert(r0)
		r3 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r28,r5)
		r4 = asr(r5,#31)
		r10 = r3
	}
	{
		r23 += mpyi(r28,r4)
		r2 = asr(r3,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r7:6 = mpyu(r28,r3)
		r13:12 = asr(r13:12,#31)
		r3 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r2)
		r23 += mpyi(r5,r16)
	}
	{
		r13:12 = min(r13:12,r27:26)
		r1:0 = max(r1:0,r21:20)
	}
	{
		r7 += mpyi(r10,r16)
		r5:4 = min(r23:22,r25:24)
	}
	{
		r1:0 = add(r5:4,r19:18)
		v3.w = vinsert(r0)
		memd(r30+##-10288) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r13:12 = max(r13:12,r21:20)
		r2 = asr(r3,#31)
		r7:6 = memd(r30+##-21104)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = mpyu(r28,r3)
		v6.w = vinsert(r12)
		r12 = r3
		v3 = valign(v3,v3,#4)
	}
	{
		r5 += mpyi(r28,r2)
		r3:2 = min(r7:6,r25:24)
	}
	{
		r7:6 = min(r15:14,r25:24)
		r5 += mpyi(r12,r16)
		r15 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r13:12 = add(r7:6,r19:18)
		r10 = r15
		v6 = valign(v6,v6,#4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r3:2 = add(r3:2,r19:18)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r13:12 = asr(r13:12,#31)
	}
	{
		r7:6 = min(r13:12,r27:26)
		r3:2 = asr(r3:2,#31)
		r13 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r3:2,r27:26)
		v1.w = vinsert(r0)
	}
	{
		r3:2 = mpyu(r28,r15)
		r14 = asr(r15,#31)
	}
	{
		r3 += mpyi(r28,r14)
		r5:4 = min(r5:4,r25:24)
		v1 = valign(v1,v1,#4)
	}
	{
		r9:8 = mpyu(r28,r13)
		r12 = asr(r13,#31)
	}
	{
		r3 += mpyi(r10,r16)
		r9 += mpyi(r28,r12)
		r10 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r5:4 = asr(r5:4,#31)
		v6.w = vinsert(r0)
	}
	{
		r1:0 = max(r7:6,r21:20)
		r9 += mpyi(r13,r16)
	}
	{
		r5:4 = min(r5:4,r27:26)
		v7.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
	}
	{
		r23:22 = mpyu(r28,r10)
		r12 = asr(r10,#31)
	}
	{
		r1:0 = min(r9:8,r25:24)
		r23 += mpyi(r28,r12)
		v7 = valign(v7,v7,#4)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r13:12 = add(r1:0,r19:18)
		r1:0 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = asr(r13:12,#31)
		v5.w = vinsert(r4)
		r15 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r1:0,r25:24)
		r5:4 = min(r5:4,r27:26)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r13:12 = add(r7:6,r19:18)
		v5 = valign(v5,v5,#4)
	}
	{
		r13:12 = asr(r13:12,#31)
		v2.w = vinsert(r4)
	}
	{
		r5:4 = min(r13:12,r27:26)
		r14 = asr(r15,#31)
		r13 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r28,r15)
		r23 += mpyi(r10,r16)
		r10 = r15
		r6 = r13
	}
	{
		r1 += mpyi(r28,r14)
		r12 = asr(r13,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r9:8 = mpyu(r28,r13)
		r1 += mpyi(r10,r16)
		r11:10 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r9 += mpyi(r28,r12)
		r5:4 = max(r5:4,r21:20)
		memd(r30+##-17072) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r13:12 = min(r11:10,r25:24)
		v0.w = vinsert(r4)
	}
	{
		r9 += mpyi(r6,r16)
		r5:4 = add(r13:12,r19:18)
		r13 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r12 = asr(r13,#31)
		r5:4 = asr(r5:4,#31)
		r17 = r13
	}
	{
		r1:0 = mpyu(r28,r13)
		r7:6 = min(r9:8,r25:24)
		r15 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r12)
		r5:4 = min(r5:4,r27:26)
		r10 = r15
		v0 = valign(v0,v0,#4)
	}
	{
		r1 += mpyi(r17,r16)
		r13:12 = add(r7:6,r19:18)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r9:8 = min(r1:0,r25:24)
	}
	{
		r7:6 = asr(r13:12,#31)
		v4.w = vinsert(r4)
	}
	{
		r5:4 = min(r7:6,r27:26)
		r13:12 = add(r9:8,r19:18)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r13:12 = asr(r13:12,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r5:4 = min(r13:12,r27:26)
		v3.w = vinsert(r4)
		r13 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r14 = asr(r15,#31)
	}
	{
		r1:0 = mpyu(r28,r15)
		r12 = asr(r13,#31)
		r5 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r14)
		r7:6 = mpyu(r28,r13)
		r15 = r13
		r14 = r5
	}
	{
		r1 += mpyi(r10,r16)
		v1.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		r7 += mpyi(r28,r12)
		r4 = asr(r5,#31)
		memd(r30+##-16944) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r11:10 = mpyu(r28,r5)
		r7 += mpyi(r15,r16)
		r1:0 = memd(r30+##-21096)
	}                                       // 8-byte Folded Reload
	{
		r11 += mpyi(r28,r4)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r30+##-19504)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r1:0,r25:24)
		r5:4 = min(r5:4,r25:24)
		memd(r30+##-11056) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = add(r7:6,r19:18)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = min(r5:4,r27:26)
	}
	{
		r11 += mpyi(r14,r16)
		r3:2 = min(r3:2,r25:24)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r7:6 = max(r7:6,r21:20)
	}
	{
		r1:0 = min(r11:10,r25:24)
		v6.w = vinsert(r6)
		r5 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = add(r3:2,r19:18)
		r7:6 = add(r1:0,r19:18)
	}
	{
		r3:2 = asr(r3:2,#31)
		r1:0 = asr(r7:6,#31)
		r6 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		v7.w = vinsert(r4)
		r7 = r5
	}
	{
		r3:2 = max(r3:2,r21:20)
		r4 = asr(r5,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r9:8 = mpyu(r28,r5)
		r5 = asr(r6,#31)
	}
	{
		r11:10 = mpyu(r28,r6)
		v5.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		r3:2 = min(r1:0,r27:26)
		r9 += mpyi(r28,r4)
		r1:0 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r11 += mpyi(r28,r5)
		r9 += mpyi(r7,r16)
	}
	{
		r11 += mpyi(r6,r16)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r25:24)
		r3:2 = max(r3:2,r21:20)
		r7 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r1:0,r25:24)
		v2.w = vinsert(r2)
		memd(r30+##-11568) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r3:2 = add(r3:2,r19:18)
		r6 = asr(r7,#31)
		memd(r30+##-15920) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r28,r7)
		r3:2 = asr(r3:2,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r1 += mpyi(r28,r6)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r1 += mpyi(r7,r16)
		r3:2 = min(r3:2,r27:26)
		r7 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = min(r1:0,r25:24)
		v0.w = vinsert(r2)
	}
	{
		r3:2 = min(r5:4,r27:26)
		r5:4 = add(r1:0,r19:18)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = asr(r5:4,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = min(r5:4,r27:26)
		v4.w = vinsert(r2)
		r5 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = mpyu(r28,r7)
		r6 = asr(r7,#31)
		r14 = r5
	}
	{
		r9 += mpyi(r28,r6)
		r4 = asr(r5,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r1:0 = mpyu(r28,r5)
		r9 += mpyi(r7,r16)
		r5 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r28,r4)
		r3:2 = max(r3:2,r21:20)
		r12 = r5
	}
	{
		r7:6 = min(r9:8,r25:24)
		v3.w = vinsert(r2)
	}
	{
		r1 += mpyi(r14,r16)
		r3:2 = add(r7:6,r19:18)
	}
	{
		r3:2 = asr(r3:2,#31)
		r4 = asr(r5,#31)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r5)
		r3:2 = min(r3:2,r27:26)
		r1:0 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r28,r4)
		r5:4 = min(r1:0,r25:24)
		v3 = valign(v3,v3,#4)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = add(r5:4,r19:18)
	}
	{
		r3:2 = asr(r5:4,#31)
		v1.w = vinsert(r2)
		r5:4 = memd(r30+##-19248)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r25:24)
		r15:14 = min(r3:2,r27:26)
		r13 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = max(r15:14,r21:20)
		r5:4 = add(r1:0,r19:18)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = min(r23:22,r25:24)
		r5:4 = asr(r5:4,#31)
		r9 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = add(r1:0,r19:18)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r7 += mpyi(r12,r16)
	}
	{
		r1:0 = asr(r1:0,#31)
		v6.w = vinsert(r8)
		r5 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r27:26)
		v7.w = vinsert(r4)
	}
	{
		r15:14 = mpyu(r28,r9)
		r8 = asr(r9,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r11:10 = mpyu(r28,r5)
		r4 = asr(r5,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r15 += mpyi(r28,r8)
		r8 = r5
	}
	{
		r11 += mpyi(r28,r4)
		r1:0 = max(r1:0,r21:20)
	}
	{
		r5:4 = add(r7:6,r19:18)
		v5.w = vinsert(r0)
		r7:6 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r25:24)
		r1:0 = asr(r5:4,#31)
		r7 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = add(r5:4,r19:18)
		v20 = valign(v5,v5,#4)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r1:0 = min(r5:4,r27:26)
		v2.w = vinsert(r0)
		r5 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r28,r13)
		r12 = asr(r13,#31)
	}
	{
		r23:22 = mpyu(r28,r7)
		r6 = asr(r7,#31)
		v31 = valign(v2,v2,#4)
	}
	{
		r3 += mpyi(r28,r12)
		r11 += mpyi(r8,r16)
		r8 = r7
	}
	{
		r23 += mpyi(r28,r6)
		r4 = asr(r5,#31)
		memd(r30+##-9264) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r28,r5)
		r3 += mpyi(r13,r16)
		r13 = r5
	}
	{
		r7 += mpyi(r28,r4)
		r1:0 = max(r1:0,r21:20)
		r5:4 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		v0.w = vinsert(r0)
	}
	{
		r7 += mpyi(r13,r16)
		r1:0 = add(r5:4,r19:18)
		r5 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		r4 = asr(r5,#31)
		r1:0 = asr(r1:0,#31)
		r12 = r5
		v0 = valign(v0,v0,#4)
	}
	{
		r11:10 = mpyu(r28,r5)
		r7:6 = min(r7:6,r25:24)
	}
	{
		r11 += mpyi(r28,r4)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r11 += mpyi(r12,r16)
		r5:4 = add(r7:6,r19:18)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r7:6 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r11:10,r25:24)
		v4.w = vinsert(r0)
	}
	{
		r1:0 = min(r7:6,r27:26)
		r5:4 = add(r5:4,r19:18)
		r7 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r5:4,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r1:0 = min(r5:4,r27:26)
		v3.w = vinsert(r0)
		r5 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r23 += mpyi(r8,r16)
	}
	{
		r6 = asr(r7,#31)
		r1 = memw(r30+##-15152)
		memd(r30+#-7984) = r23:22
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r28,r7)
		r4 = asr(r5,#31)
	}
	{
		r11:10 = mpyu(r28,r5)
		r23 += mpyi(r28,r6)
		r8 = r1
		v30 = valign(v3,v3,#4)
	}
	{
		r11 += mpyi(r28,r4)
		v1.w = vinsert(r0)
		r6 = r5
	}
	{
		r23 += mpyi(r7,r16)
		r11 += mpyi(r6,r16)
	}
	{
		r0 = asr(r1,#31)
		r7:6 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = mpyu(r28,r1)
		r15 += mpyi(r9,r16)
		memd(r30+##-10032) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r28,r0)
		v19 = valign(v1,v1,#4)
		memd(r30+##-12976) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r7:6,r25:24)
		r23:22 = combine(r21,r20)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = add(r1:0,r19:18)
		r13:12 = memd(r30+##-17072)
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r8,r16)
		r7:6 = add(r7:6,r19:18)
	}
	{
		r9:8 = min(r13:12,r25:24)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7:6 = max(r7:6,r21:20)
		r9:8 = add(r9:8,r19:18)
	}
	{
		r1:0 = min(r1:0,r27:26)
		v6.w = vinsert(r6)
	}
	{
		r3:2 = min(r3:2,r25:24)
		r7:6 = asr(r9:8,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = max(r1:0,r21:20)
		v1 = valign(v6,v6,#4)
	}
	{
		r3:2 = add(r3:2,r19:18)
		v7.w = vinsert(r0)
	}
	{
		r7:6 = max(r7:6,r21:20)
		r1:0 = asr(r3:2,#31)
		r3 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r27:26)
		v20.w = vinsert(r6)
		r6 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r28,r3)
		r2 = asr(r3,#31)
		r7 = r3
		v5 = valign(v7,v7,#4)
	}
	{
		r11:10 = mpyu(r28,r6)
		r3 = asr(r6,#31)
		v2 = valign(v20,v20,#4)
	}
	{
		r13 += mpyi(r28,r2)
		r11 += mpyi(r28,r3)
		r3:2 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r21:20)
		r13 += mpyi(r7,r16)
	}
	{
		r1:0 = min(r3:2,r25:24)
		v31.w = vinsert(r0)
		memd(r30+##-8240) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r11 += mpyi(r6,r16)
		r1:0 = add(r1:0,r19:18)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r7:6,r25:24)
		r1:0 = asr(r1:0,#31)
		r7 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r27:26)
		r3:2 = add(r3:2,r19:18)
		v3 = valign(v31,v31,#4)
		memd(r30+#-4656) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = asr(r3:2,#31)
		v31 = v15
	}
	{
		r1:0 = min(r3:2,r27:26)
		v0.w = vinsert(r0)
	}
	{
		r3:2 = min(r5:4,r25:24)
		r6 = asr(r7,#31)
		r5 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r28,r7)
		r3:2 = add(r3:2,r19:18)
		v0 = valign(v0,v0,#4)
	}
	{
		r13 += mpyi(r28,r6)
		r1:0 = max(r1:0,r21:20)
	}
	{
		r3:2 = asr(r3:2,#31)
		v4.w = vinsert(r0)
	}
	{
		r1:0 = min(r3:2,r27:26)
		r13 += mpyi(r7,r16)
		r3 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r28,r5)
		r4 = asr(r5,#31)
		r9 = r3
		memd(r30+#-3888) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r28,r4)
		r2 = asr(r3,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r7 += mpyi(r5,r16)
		r11:10 = mpyu(r28,r3)
		r3 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r28,r2)
		r1:0 = max(r1:0,r21:20)
		r5:4 = combine(r19,r18)
		r8 = r3
	}
	{
		r7:6 = min(r7:6,r25:24)
		v30.w = vinsert(r0)
	}
	{
		r11 += mpyi(r9,r16)
		r1:0 = add(r7:6,r19:18)
	}
	{
		r7:6 = mpyu(r28,r3)
		r1:0 = asr(r1:0,#31)
		v6 = valign(v30,v30,#4)
		memd(r30+#-7728) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r1:0,r27:26)
		r2 = asr(r3,#31)
		r11:10 = combine(r7,r6)
		r7:6 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = max(r1:0,r21:20)
	}
	{
		r7:6 = add(r7:6,r19:18)
		v19.w = vinsert(r0)
	}
	{
		r11 += mpyi(r28,r2)
		r1:0 = asr(r7:6,#31)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r25:24)
		r1:0 = min(r1:0,r27:26)
		r3:2 = combine(r25,r24)
	}
	{
		r7:6 = add(r7:6,r19:18)
		v7 = valign(v19,v19,#4)
		r9 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r23:22)
		r7:6 = asr(r7:6,#31)
	}
	{
		r1:0 = min(r7:6,r27:26)
		v1.w = vinsert(r0)
		r7 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r8,r16)
		r1:0 = max(r1:0,r23:22)
	}
	{
		r19:18 = mpyu(r28,r7)
		r6 = asr(r7,#31)
		v1 = valign(v1,v1,#4)
		memd(r30+#-4400) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r28,r9)
		r8 = asr(r9,#31)
		r1 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r28,r6)
		r21 += mpyi(r28,r8)
		r17 = r1
	}
	{
		r19 += mpyi(r7,r16)
		v5.w = vinsert(r0)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r21 += mpyi(r9,r16)
		r9:8 = min(r7:6,r25:24)
	}
	{
		r11:10 = mpyu(r28,r1)
		r0 = asr(r1,#31)
		r7:6 = memd(r30+##-16944)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r15:14,r25:24)
		r11 += mpyi(r28,r0)
		v5 = valign(v5,v5,#4)
	}
	{
		r1:0 = min(r7:6,r25:24)
	}
	{
		r1:0 = add(r1:0,r5:4)
		r15:14 = add(r15:14,r5:4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r13:12 = add(r9:8,r5:4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r15:14 = asr(r15:14,#31)
	}
	{
		r1:0 = max(r1:0,r23:22)
		r13:12 = asr(r13:12,#31)
	}
	{
		r15:14 = min(r15:14,r27:26)
		v2.w = vinsert(r0)
		r0 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = min(r13:12,r27:26)
		r15:14 = max(r15:14,r23:22)
		r9 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = max(r13:12,r23:22)
		v3.w = vinsert(r14)
		r8 = r0
		v2 = valign(v2,v2,#4)
	}
	{
		r12 = asr(r0,#31)
		v0.w = vinsert(r6)
	}
	{
		r15:14 = mpyu(r28,r0)
		r13 = asr(r9,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = mpyu(r28,r9)
		r15 += mpyi(r28,r12)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r28,r13)
		r11 += mpyi(r17,r16)
	}
	{
		r7 += mpyi(r9,r16)
		r15 += mpyi(r0,r16)
		r17:16 = memd(r30+#-1840)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r17:16,r25:24)
		r7:6 = min(r7:6,r3:2)
		r27:26 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r3:2)
		r25:24 = add(r25:24,r5:4)
		r1:0 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r27:26,r3:2)
		r25:24 = asr(r25:24,#31)
		r13:12 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		r11:10 = min(r11:10,r3:2)
		r27:26 = memd(r30+#-5168)
		memd(r30+#-1840) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r13:12,r3:2)
		r1:0 = add(r1:0,r5:4)
	}
	{
		r13:12 = min(r27:26,r3:2)
		r1:0 = add(r17:16,r5:4)
		r27:26 = memd(r30+#-5424)
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r27:26,r3:2)
		r17:16 = add(r23:22,r5:4)
		r23:22 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = add(r27:26,r5:4)
		r13:12 = add(r13:12,r5:4)
	}
	{
		r9:8 = add(r9:8,r5:4)
		r27:26 = memd(r30+##-9776)
		memd(r30+#-2096) = r27:26
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r13:12 = memd(r30+##-9520)
		memd(r30+#-5680) = r13:12
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r23:22,r3:2)
		r17:16 = asr(r17:16,#31)
		memd(r30+#-5168) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r9:8 = min(r27:26,r3:2)
		r23:22 = memd(r30+##-8496)
		memd(r30+#-2608) = r9:8
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r13:12,r3:2)
		r27:26 = memd(r30+#-3632)
		memd(r30+#-5424) = r9:8
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r27:26,r3:2)
		r13:12 = memd(r30+#-3376)
		memd(r30+##-9008) = r9:8
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r23:22,r3:2)
		r23:22 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r13:12,r3:2)
		r13:12 = min(r23:22,r3:2)
		memd(r30+##-8496) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r23:22 = memd(r30+#-2864)
		memd(r30+#-3120) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r3:2)
		r1:0 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		memd(r30+#-2352) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+#-5680)
		memd(r30+#-3632) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+#-2096)
		memd(r30+#-5680) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+#-5168)
		memd(r30+#-2096) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r25:24 = asr(r1:0,#31)
		r1:0 = add(r23:22,r5:4)
	}
	{
		r1:0 = add(r13:12,r5:4)
		memd(r30+#-2864) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r9:8,r5:4)
		memd(r30+#-3376) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r27:26,r5:4)
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+##-8496)
		memd(r30+##-9520) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+##-9008)
		memd(r30+##-8496) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+#-5424)
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r9:8 = add(r1:0,r5:4)
		r1:0 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = add(r1:0,r5:4)
		r1:0 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		r23:22 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r23:22,r3:2)
		memd(r30+#-5168) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+#-7984)
		memd(r30+##-9008) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		r23:22 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r23:22,r3:2)
		memd(r30+##-10032) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+##-8752)
		memd(r30+##-11824) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		r23:22 = memd(r30+##-15920)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r23:22,r3:2)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r0 = ##2147483647
		r23:22 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r1 = #0
		r23:22 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
		r23:22 = min(r23:22,r3:2)
	}
	{
		r25:24 = memd(r30+#-2096)
		memd(r30+#-5424) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
	}
	{
		r25:24 = memd(r30+#-5680)
		memd(r30+#-2608) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
	}
	{
		r25:24 = memd(r30+#-3632)
		memd(r30+#-2096) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
	}
	{
		r25:24 = memd(r30+#-3120)
		memd(r30+##-11184) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
	}
	{
		r25:24 = memd(r30+#-2352)
		memd(r30+#-3632) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
	}
	{
		r25:24 = memd(r30+#-1840)
		memd(r30+#-2352) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r1:0)
		r1:0 = asr(r13:12,#31)
	}
	{
		r1:0 = asr(r9:8,#31)
		memd(r30+#-3120) = r1:0
		memd(r30+#-1840) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r30+##-9776)
		memd(r30+#-5680) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r25:24 = asr(r1:0,#31)
		r1:0 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+##-9520)
		memd(r30+##-8496) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+#-7472)
		memd(r30+##-9264) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+#-3376)
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = memd(r30+#-2864)
		memd(r30+##-10288) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r9:8 = asr(r1:0,#31)
		r1:0 = add(r23:22,r5:4)
		memd(r30+#-2864) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r15:14,r3:2)
		r1:0 = add(r17:16,r5:4)
		memd(r30+#-3376) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r27:26,r5:4)
		r26 = ##2147483647
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r27 = #0
		r1:0 = memd(r30+##-8752)
		memd(r30+#-7984) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+##-11824)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+##-10032)
		memd(r30+##-9520) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+##-9008)
		memd(r30+##-10032) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r5:4)
	}
	{
		r1:0 = memd(r30+#-5168)
		memd(r30+##-11056) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r19:18,r3:2)
		r13:12 = add(r1:0,r5:4)
		memd(r30+#-5168) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r21:20,r3:2)
		r7:6 = memd(r30+#-4400)
		memd(r30+##-9008) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r1:0 = add(r1:0,r5:4)
	}
	{
		r7:6 = memd(r30+#-7728)
		memd(r30+#-4400) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
	}
	{
		r7:6 = memd(r30+##-8240)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r7:6,r3:2)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r17:16 = add(r23:22,r5:4)
		r23:22 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = asr(r17:16,#31)
		r3:2 = asr(r1:0,#31)
		r7:6 = combine(r3,r2)
		memd(r30+#-3888) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r17:16 = min(r17:16,r27:26)
		r0 = ##-2147483648
		r1 = #-1
	}
	{
		r3:2 = min(r3:2,r27:26)
		r17:16 = max(r17:16,r1:0)
	}
	{
		r3:2 = max(r3:2,r1:0)
		v6.w = vinsert(r16)
		r17:16 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r17:16,r7:6)
		v7.w = vinsert(r2)
		r7:6 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = max(r23:22,r1:0)
		r3:2 = max(r7:6,r1:0)
		r7:6 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		v4.w = vinsert(r20)
		v7 = valign(v7,v7,#4)
		memd(r30+#-1840) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r19:18 = add(r19:18,r5:4)
		r7:6 = memd(r30+#-3632)
		memd(r30+#-2352) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r30+#-2608)
		memd(r30+#-3632) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		r7:6 = memd(r30+#-5424)
		memd(r30+#-2096) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r7:6,r1:0)
		r7:6 = min(r9:8,r27:26)
		memd(r30+#-2608) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r7:6 = max(r7:6,r1:0)
		r3:2 = memd(r30+##-10288)
		memd(r30+#-4656) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		v1.w = vinsert(r6)
	}
	{
		r3:2 = memd(r30+##-9776)
		memd(r30+#-5424) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = memd(r30+##-9264)
		memd(r30+##-9776) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
	}
	{
		r3:2 = memd(r30+##-8496)
		memd(r30+##-10288) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r3:2,r27:26)
		r3:2 = min(r25:24,r27:26)
	}
	{
		r9:8 = max(r9:8,r1:0)
		r3:2 = memd(r30+#-5680)
		memd(r30+##-11184) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r3:2,r27:26)
		v5.w = vinsert(r8)
		r3:2 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r3:2,r27:26)
		r3:2 = asr(r13:12,#31)
	}
	{
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r30+##-11056)
		memd(r30+#-5680) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = memd(r30+##-10032)
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = memd(r30+##-9520)
		memd(r30+##-9264) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = memd(r30+##-8752)
		memd(r30+##-9520) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r25:24 = asr(r3:2,#31)
		r3:2 = memd(r30+#-7984)
		memd(r30+#-7984) = r19:18

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r21:20 = asr(r3:2,#31)
		r3:2 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = asr(r3:2,#31)
		r3:2 = memd(r30+#-3376)
		r19:18 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		r13:12 = add(r19:18,r5:4)
		r19:18 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r13:12 = add(r19:18,r5:4)
		memd(r30+##-8240) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r3:2 = max(r3:2,r1:0)
		r19:18 = memd(r30+#-4400)
		memd(r30+##-8752) = r13:12
	}                                       // 8-byte Folded Reload
	{
		r13:12 = add(r19:18,r5:4)
		v2.w = vinsert(r2)
		r19:18 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = add(r19:18,r5:4)
		r19:18 = add(r11:10,r5:4)
		r11:10 = memd(r30+#-5168)
		memd(r30+#-4400) = r13:12
	}                                       // 8-byte Folded Reload
	{
		r11:10 = add(r11:10,r5:4)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r30+#-2864)
		memd(r30+#-7728) = r13:12
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r17:16,r1:0)
		r13:12 = add(r7:6,r5:4)
	}
	{
		r7:6 = max(r23:22,r1:0)
		memd(r30+#-2864) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+##-11184)
		memd(r30+#-3376) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r7:6,r1:0)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r7:6,r1:0)
		r7:6 = memd(r30+##-9776)
		memd(r30+#-5168) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r7:6,r1:0)
		r7:6 = memd(r30+#-5424)
		memd(r30+#-3120) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r7:6,r1:0)
		r7:6 = memd(r30+##-9520)
		memd(r30+#-3888) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r15:14,r27:26)
		r5:4 = combine(r27,r26)
		memd(r30+#-5424) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r17:16 = min(r21:20,r5:4)
		r21:20 = min(r25:24,r5:4)
	}
	{
		r25:24 = min(r7:6,r5:4)
		r21:20 = max(r21:20,r1:0)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r7:6,r5:4)
		r25:24 = max(r25:24,r1:0)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r7:6,r5:4)
		v3.w = vinsert(r24)
		r7:6 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r7:6,r5:4)
		r7:6 = asr(r13:12,#31)
	}
	{
		r7:6 = asr(r11:10,#31)
		r11:10 = asr(r19:18,#31)
		v3 = valign(v3,v3,#4)
		memd(r30+#-5680) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r25:24 = max(r9:8,r1:0)
		r9:8 = max(r15:14,r1:0)
		r7:6 = memd(r30+#-7728)
		memd(r30+#-7472) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r21:20 = max(r17:16,r1:0)
		r19:18 = asr(r7:6,#31)
		r7:6 = memd(r30+#-4400)
		memd(r30+#-4400) = r21:20

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r17:16 = max(r27:26,r1:0)
		r7:6 = asr(r7:6,#31)
	}
	{
		r19:18 = min(r19:18,r5:4)
		r7:6 = memd(r30+##-8752)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r23:22 = max(r23:22,r1:0)
		r13:12 = asr(r7:6,#31)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r19:18,r1:0)
		r7:6 = asr(r7:6,#31)
		r3:2 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r7:6,r5:4)
		v7.w = vinsert(r18)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		v0.w = vinsert(r6)
		r7:6 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r5:4)
		v4.w = vinsert(r6)
		v7 = valign(v7,v7,#4)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r3:2,r1:0)
		r27:26 = min(r7:6,r5:4)
		r7:6 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		v6.w = vinsert(r2)
		v1.w = vinsert(r6)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r13:12,r5:4)
		v5.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r11:10,r5:4)
		r19:18 = min(r7:6,r5:4)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r5:4)
		v2.w = vinsert(r16)
		r5:4 = combine(r1,r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = max(r7:6,r1:0)
		v3.w = vinsert(r8)
		v5 = valign(v5,v5,#4)
		r1:0 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = max(r27:26,r5:4)
		v0.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r15:14,r5:4)
		v3 = valign(v3,v3,#4)
		r15:14 = memd(r30+#-2352)
		r27:26 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		v6.w = vinsert(r0)
		v4.w = vinsert(r14)
	}
	{
		r11:10 = max(r11:10,r5:4)
		r1:0 = max(r19:18,r5:4)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = max(r3:2,r5:4)
		v1.w = vinsert(r26)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		v5.w = vinsert(r4)
		v7.w = vinsert(r10)
		r1 = #68
		r5:4 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		v0.w = vinsert(r4)
		v2.w = vinsert(r20)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		v4.w = vinsert(r4)
		v3.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r4)
		v0 = vror(v0,r1)
		r5:4 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		v5.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		v1 = vror(v1,r1)
		v0 = vor(v0,v4)
	}
	{
		v5 = valign(v5,v5,#4)
		v0.w = vadd(v0.w,v29.w):sat
	}
	{
		v7 = valign(v7,v7,#4)
		v1 = vor(v1,v5)
	}
	{
		v7.w = vinsert(r0)
		v0.w = vasr(v0.w,v27.w)
		v6 = valign(v6,v6,#4)
		v1.w = vadd(v1.w,v29.w):sat
	}
	{
		v6.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
		v0.w = vmin(v12.w,v0.w)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r28 = add(r0,##-39680)
		v1.w = vasr(v1.w,v27.w)
		v0.w = vmax(v11.w,v0.w)
	}
	{
		r0 = setbit(r28,#7)
		v2.w = vinsert(r4)
		v4 = valign(v6,v6,#4)
		v1.w = vmin(v1.w,v12.w)
	}
	{
		v4.w = vinsert(r16)
		v3 = valign(v3,v3,#4)
		v1.w = vmax(v11.w,v1.w)
	}
	{
		v3.w = vinsert(r24)
		v2 = vror(v2,r1)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
	}
	{
		v4 = vror(v4,r1)
		v0.h = vadd(v26.h,v0.h):sat
	}
	{
		v5 = valign(v7,v7,#4)
		v0.h = vmin(v0.h,v28.h)
		v7 = v10
	}
	{
		v5.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
		v0.h = vmax(v24.h,v0.h)
		vmem(r0+#0) = v0.new
	}
	{
		v2 = vor(v2,v3)
		r3:2 = memd(r28+#192)
		r0 = memw(r30+#-3896)
	}
	{
		r18 = add(r0,##-37632)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v5,v5,#4)
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r19 = add(r0,##-44288)
		v3 = vor(v4,v3)
		r1:0 = memd(r28+#200)
	}
	{
		v2.w = vadd(v2.w,v29.w):sat
		v3.w = vadd(v3.w,v29.w):sat
		memd(r30+##-9008) = r1:0
	}                                       // 8-byte Folded Spill
	{
		v5 = v10
		r1:0 = memd(r28+#208)
	}
	{
		v1.w = vasr(v2.w,v27.w)
		memd(r30+##-9264) = r1:0
	}                                       // 8-byte Folded Spill
	{
		v2.w = vasr(v3.w,v27.w)
		v1.w = vmin(v12.w,v1.w)
		r1:0 = memd(r28+#216)
	}
	{
		v2.w = vmin(v2.w,v12.w)
		v1.w = vmax(v11.w,v1.w)
		memd(r30+##-9520) = r1:0
	}                                       // 8-byte Folded Spill
	{
		v2.w = vmax(v11.w,v2.w)
		r1:0 = memd(r28+#224)
	}
	{
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Spill
	{
		v0.h = vpacke(v1.w,v2.w)
		r1:0 = memd(r28+#232)
	}
	{
		v0.h = vadd(v26.h,v0.h):sat
		memd(r30+##-10032) = r1:0
	}                                       // 8-byte Folded Spill
	{
		v0.h = vmin(v28.h,v0.h)
		r1:0 = memd(r28+#240)
	}
	{
		v0.h = vmax(v25.h,v0.h)
		memd(r30+##-10288) = r1:0
		r1:0 = memd(r28+#248)

	} :mem_noshuf
	{
		memd(r30+##-11184) = r1:0
		vmem(r28+#0) = v0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = memd(r28+#128)
	}
	{
		memd(r30+#-3888) = r1:0
		r7:6 = memd(r28+#136)

	} :mem_noshuf
	{
		r1 = memw(r28+#124)
		memd(r30+#-5680) = r7:6

	} :mem_noshuf
	{
		r0 = memw(r28+#120)
	}
	{
		memd(r30+#-1840) = r1:0
		r1:0 = memd(r28+#144)

	} :mem_noshuf
	{
		memd(r30+##-8240) = r1:0
		r13:12 = memd(r28+#160)

	} :mem_noshuf
	{
		r9:8 = memd(r28+#152)
		r1:0 = memd(r28+#64)
	}
	{
		r15:14 = memd(r28+#168)
		r11:10 = memd(r28+#176)
	}
	{
		r17:16 = memd(r28+#184)
		memd(r30+#-2096) = r1:0

	} :mem_noshuf
	{
		r1:0 = memd(r28+#72)
	}
	{
		memd(r30+#-2352) = r1:0
		r1:0 = memd(r28+#80)

	} :mem_noshuf
	{
		memd(r30+#-2608) = r1:0
		r1:0 = memd(r28+#88)

	} :mem_noshuf
	{
		memd(r30+#-2864) = r1:0
		r1:0 = memd(r28+#96)

	} :mem_noshuf
	{
		memd(r30+#-3120) = r1:0
		r1:0 = memd(r28+#104)

	} :mem_noshuf
	{
		memd(r30+#-3376) = r1:0
		r1:0 = memd(r28+#112)

	} :mem_noshuf
	{
		memd(r30+#-3632) = r1:0
		r21:20 = memd(r28+#0)

	} :mem_noshuf
	{
		r23:22 = memd(r28+#8)
		r25:24 = memd(r28+#16)
	}
	{
		r27:26 = memd(r28+#24)
		r7:6 = memd(r28+#32)
	}
	{
		r5:4 = memd(r28+#48)
		r1:0 = memd(r28+#56)
	}
	{
		r3:2 = memd(r28+#40)
		memd(r18+#56) = r1:0

	} :mem_noshuf
	{
		memd(r18+#48) = r5:4
		memd(r18+#40) = r3:2
	}
	{
		memd(r18+#32) = r7:6
		memd(r18+#24) = r27:26
	}
	{
		memd(r18+#16) = r25:24
		memd(r18+#8) = r23:22
	}
	{
		memd(r18+#0) = r21:20
	}
	{
		v0.cur = vmem(r18+#0)
		vmem(r19+#0) = v0
	}
	{
		r0 = memw(r19+#56)
		r1 = memw(r19+#60)
	}
	{
		memd(r30+#-4656) = r1:0
		r0 = memw(r19+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#52)
	}
	{
		memd(r30+#-4400) = r1:0
		r0 = memw(r19+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#44)
	}
	{
		memd(r30+#-5168) = r1:0
		r0 = memw(r19+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#36)
	}
	{
		memd(r30+#-5424) = r1:0
		r0 = memw(r19+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#28)
	}
	{
		memd(r30+#-7472) = r1:0
		r0 = memw(r19+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#20)
	}
	{
		memd(r30+#-7728) = r1:0
		r0 = memw(r19+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r19+#4)
	}
	{
		memd(r30+#-7984) = r1:0
		r2 = memw(r19+#8)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r0 = memw(r30+#-3896)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-37888)
		r3 = memw(r19+#12)
	}
	{
		memd(r30+##-8752) = r3:2
		memd(r0+#56) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r1 = add(r1,##-44416)
		memd(r0+#48) = r11:10
		memd(r0+#40) = r15:14
	}
	{
		memd(r0+#32) = r13:12
		memd(r0+#24) = r9:8
	}
	{
		r5:4 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#16) = r5:4
		r3:2 = memd(r30+#-5680)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r3:2
		r3:2 = memd(r30+#-3888)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r3:2
	}
	{
		v0.cur = vmem(r0+#0)
		vmem(r1+#0) = v0
	}
	{
		r20 = memw(r1+#56)
		r21 = memw(r1+#60)
	}
	{
		r2 = memw(r1+#48)
		r3 = memw(r1+#52)
	}
	{
		memd(r30+##-11568) = r3:2
		r2 = memw(r1+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r1+#44)
	}
	{
		memd(r30+#-3888) = r3:2
		r2 = memw(r1+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r1+#36)
	}
	{
		memd(r30+#-5680) = r3:2
		r2 = memw(r1+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r1+#28)
	}
	{
		memd(r30+##-8240) = r3:2
		r2 = memw(r1+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r1+#20)
	}
	{
		memd(r30+##-11056) = r3:2
		r14 = memw(r1+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r15 = memw(r1+#4)
		r24 = memw(r1+#8)
	}
	{
		r0 = memw(r30+#-3896)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-38016)
		r25 = memw(r1+#12)
	}
	{
		r2 = add(r2,##-44544)
		r5:4 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#56) = r5:4
		r5:4 = memd(r30+##-10288)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#48) = r5:4
		r5:4 = memd(r30+##-10032)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#40) = r5:4
		r5:4 = memd(r30+##-9776)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#32) = r5:4
		r5:4 = memd(r30+##-9520)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#24) = r5:4
		r5:4 = memd(r30+##-9264)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#16) = r5:4
		r5:4 = memd(r30+##-9008)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r5:4
		r5:4 = memd(r30+##-8496)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r5:4
	}
	{
		v0.cur = vmem(r0+#0)
		vmem(r2+#0) = v0
	}
	{
		r0 = memw(r2+#56)
		r1 = memw(r2+#60)
	}
	{
		memd(r30+##-8496) = r1:0
		r0 = memw(r2+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r2+#52)
	}
	{
		memd(r30+##-9008) = r1:0
		r26 = memw(r2+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r27 = memw(r2+#44)
		r22 = memw(r2+#32)
	}
	{
		r23 = memw(r2+#36)
		r18 = memw(r2+#24)
	}
	{
		r19 = memw(r2+#28)
		r16 = memw(r2+#16)
	}
	{
		r17 = memw(r2+#20)
		r6 = memw(r2+#0)
	}
	{
		r7 = memw(r2+#4)
		r12 = memw(r2+#8)
	}
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r0,##-37760)
		r0 = memw(r30+#-3896)
		r13 = memw(r2+#12)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-44672)
		r5:4 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		memd(r3+#48) = r5:4
		r5:4 = memd(r30+#-4656)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		r5:4 = memd(r30+#-3376)
		memw(r30+#-3632) = r1.new
	}                                       // 8-byte Folded Reload
	{
		memd(r3+#40) = r5:4
		r5:4 = memd(r30+#-3120)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#32) = r5:4
		r5:4 = memd(r30+#-2864)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#24) = r5:4
		r5:4 = memd(r30+#-2608)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#16) = r5:4
		r5:4 = memd(r30+#-2352)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#8) = r5:4
		r5:4 = memd(r30+#-2096)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#0) = r5:4
		r5:4 = memd(r30+#-1840)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#56) = r5:4
		r5:4 = memd(r30+#-4400)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		r5:4 = memd(r30+#-5168)
		memw(r30+#-1840) = r1.new
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		r5 = vtrunehb(r25:24)
		memw(r30+#-2096) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v0.cur = vmem(r3+#0)
		vmem(r0+#0) = v0
	}
	{
		r3:2 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		r3:2 = memd(r30+#-7472)
		memw(r30+#-2608) = r1.new
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		memw(r30+#-3120) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r10 = memw(r0+#56)
		r3:2 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		r3:2 = memd(r30+#-7984)
		memw(r30+#-3376) = r1.new
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		r11 = memw(r0+#60)
		r8 = memw(r0+#48)
	}
	{
		r3:2 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r28 = vtrunehb(r3:2)
		r2 = vtrunehb(r21:20)
		memw(r30+#-2352) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r30,#-16304)
		r9 = memw(r0+#52)
		r20 = memw(r0+#40)
	}
	{
		v0 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = vtrunehb(r15:14)
		v0.w = vinsert(r1)
		r21 = memw(r0+#44)
		r14 = memw(r0+#32)
	}
	{
		r3:2 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r2 = vtrunehb(r3:2)
		v0 = valign(v0,v0,#4)
		memw(r30+#-2864) = r2.new
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r28)
		r2 = add(r30,#-16176)
	}
	{
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = vtrunehb(r7:6)
		v1.w = vinsert(r1)
		r15 = memw(r0+#36)
		r2 = memw(r0+#24)
	}
	{
		r6 = add(r30,#-16048)
		v0 = valign(v0,v0,#4)
		r3 = memw(r0+#28)
		r24 = memw(r0+#16)
	}
	{
		v1 = valign(v1,v1,#4)
		r25 = memw(r0+#20)
		r4 = memw(r0+#0)
	}
	{
		v1.w = vinsert(r5)
		r5 = memw(r0+#4)
	}
	{
		r6 = add(r30,#-1584)
		v2 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		v2.w = vinsert(r1)
		v3 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vinsert(r1)
		r5:4 = memd(r30+##-11056)
		r6 = memw(r0+#8)
	}                                       // 8-byte Folded Reload
	{
		r4 = vtrunehb(r5:4)
		r5 = vtrunehb(r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		v2 = valign(v2,v2,#4)
		r13:12 = memd(r30+##-8240)
		r7 = memw(r0+#12)
	}                                       // 8-byte Folded Reload
	{
		v2.w = vinsert(r5)
		v1.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r7:6)
		r4 = vtrunehb(r17:16)
	}
	{
		r1 = vtrunehb(r13:12)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r4 = vtrunehb(r25:24)
		v2.w = vinsert(r4)
		v1 = valign(v1,v1,#4)
	}
	{
		v0.w = vinsert(r0)
		v1.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r19:18)
		v3.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		v2.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r2 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		v0.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r23:22)
		v3.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
		r2 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r0 = vtrunehb(r15:14)
		v1.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r7:6)
		v3.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r0 = vtrunehb(r27:26)
		v0.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		r2 = vtrunehb(r21:20)
		v1.w = vinsert(r1)
		v2 = valign(v2,v2,#4)
		r1 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		v2.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v0 = valign(v0,v0,#4)
		r2 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r9:8)
		v0.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r3:2)
		v2.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r11:10)
		v0.w = vinsert(r1)
		v2 = valign(v2,v2,#4)
		r2 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		v2.w = vinsert(r0)
		r0 = #100
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r1)
		v1 = vror(v1,r0)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,##-44800)
		v2 = vror(v2,r0)
		v1 = vor(v1,v9)
	}
	{
		v3 = vror(v3,r0)
		v2 = vor(v2,v9)
		v1.ub = vmin(v1.ub,v22.ub)
	}
	{
		v0 = vror(v0,r0)
		v3 = vor(v3,v9)
		v2.ub = vmin(v2.ub,v22.ub)
		v1.ub = vmax(v1.ub,v23.ub)
	}
	{
		v3.ub = vmin(v3.ub,v22.ub)
		v0 = vor(v0,v9)
		v2.ub = vmax(v2.ub,v23.ub)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v1 = vdelta(v1,v18)
		v3.ub = vmax(v3.ub,v23.ub)
		v0.ub = vmin(v0.ub,v22.ub)
		r2 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#1024)
		v2 = vdelta(v2,v15)
		v0.ub = vmax(v0.ub,v23.ub)
	}
	{
		v3 = vdelta(v3,v14)
		v1 = vmux(q0,v2,v1)
	}
	{
		v0 = vmux(q1,v3,v0)
		v3 = v10
	}
	{
		v0 = vmux(q2,v1,v0)
		v1 = v10
	}
	{
		vmemu(r2+#0) = v0
	}
	{
		v0.cur = vmem(r0+#0)
		vmem(r1+#0) = v0
	}
	{
		r0 = memw(r1+#120)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#124)
		memw(r30+#-1328) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#112)
		memw(r30+#-2352) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#116)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#104)
		memw(r30+#-3888) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#108)
		memw(r30+#-2864) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#96)
		memw(r30+##-9008) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#100)
		memw(r30+##-5168) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#88)
		memw(r30+##-11056) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#92)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r20 = memw(r1+#80)
		r0 = memw(r1+#84)
	}
	{
		memw(r30+##-9776) = r0
		r28 = memw(r1+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r17 = memw(r1+#76)
		r0 = memw(r1+#64)
	}
	{
		memw(r30+##-9264) = r0
		r9 = memw(r1+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r7 = memw(r1+#56)
		r22 = memw(r1+#60)
	}
	{
		r12 = memw(r1+#48)
		r21 = memw(r1+#52)
	}
	{
		r18 = memw(r1+#40)
		r13 = memw(r1+#44)
	}
	{
		r19 = memw(r1+#32)
		r23 = memw(r1+#36)
	}
	{
		r24 = memw(r1+#24)
		r25 = memw(r1+#28)
	}
	{
		r26 = memw(r1+#16)
		r27 = memw(r1+#20)
	}
	{
		r8 = asr(r26,#31)
		r10 = memw(r1+#8)
		r6 = memw(r1+#12)
	}
	{
		r11 = memw(r1+#0)
		r1 = memw(r1+#4)
	}
	{
		r16 = asr(r11,#31)
		r14 = asr(r1,#31)
		r2 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r2,r1)
	}
	{
		r5 += mpyi(r2,r14)
		r15:14 = mpyu(r2,r11)
	}
	{
		r5 += mpyi(r1,r3)
		r1 = asr(r10,#31)
	}
	{
		r5:4 = mpyu(r2,r10)
		r15 += mpyi(r2,r16)
		memd(r30+#-7728) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r2,r1)
		r15 += mpyi(r11,r3)
	}
	{
		r5 += mpyi(r10,r3)
		r11:10 = mpyu(r2,r26)
	}
	{
		r11 += mpyi(r2,r8)
		r8 = asr(r27,#31)
		memd(r30+##-8496) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r2,r27)
		r16 = asr(r6,#31)
	}
	{
		r1:0 = mpyu(r2,r6)
		r5 += mpyi(r2,r8)
	}
	{
		r1 += mpyi(r2,r16)
		r5 += mpyi(r27,r3)
	}
	{
		r1 += mpyi(r6,r3)
		r11 += mpyi(r26,r3)
		memd(r30+#-7984) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r27:26 = mpyu(r2,r25)
		r5 = asr(r25,#31)
		memd(r30+##-8240) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r2,r24)
		r4 = asr(r24,#31)
	}
	{
		r27 += mpyi(r2,r5)
		r1 += mpyi(r2,r4)
	}
	{
		r1 += mpyi(r24,r3)
		r16 = asr(r19,#31)
		r5:4 = combine(r27,r26)
	}
	{
		r5 += mpyi(r25,r3)
		r27:26 = mpyu(r2,r19)
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r2,r23)
		r0 = asr(r23,#31)
		memd(r30+#-5424) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r27 += mpyi(r2,r16)
		r5 += mpyi(r2,r0)
	}
	{
		r25:24 = mpyu(r2,r18)
		r16 = asr(r18,#31)
		r1:0 = combine(r5,r4)
	}
	{
		r25 += mpyi(r2,r16)
		r16 = asr(r13,#31)
	}
	{
		r5:4 = mpyu(r2,r13)
		r1 += mpyi(r23,r3)
	}
	{
		r5 += mpyi(r2,r16)
		r27 += mpyi(r19,r3)
		memd(r30+#-3376) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r25 += mpyi(r18,r3)
		r5 += mpyi(r13,r3)
		memd(r30+#-4400) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r19:18 = mpyu(r2,r12)
		r0 = asr(r12,#31)
		memd(r30+#-2608) = r5:4
		memd(r30+#-2096) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r2,r0)
		r0 = asr(r7,#31)
	}
	{
		r5:4 = mpyu(r2,r7)
		r1 = asr(r21,#31)
	}
	{
		r5 += mpyi(r2,r0)
		r25:24 = mpyu(r2,r21)
	}
	{
		r25 += mpyi(r2,r1)
		r5 += mpyi(r7,r3)
	}
	{
		r7:6 = mpyu(r2,r9)
		r1 = asr(r9,#31)
		memd(r30+#-4656) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r2,r1)
		r0 = asr(r22,#31)
	}
	{
		r5:4 = mpyu(r2,r22)
		r19 += mpyi(r12,r3)
		r13:12 = combine(r7,r6)
	}
	{
		r25 += mpyi(r21,r3)
		r5 += mpyi(r2,r0)
		memd(r30+#-3120) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r2,r28)
		r1 = asr(r28,#31)
		r25:24 = combine(r5,r4)
		memd(r30+#-3632) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r2,r1)
		r0 = asr(r17,#31)
		r5 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r3)
		r4 = asr(r5,#31)
		r28 = r2
	}
	{
		r19:18 = mpyu(r2,r5)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r27:26 = mpyu(r2,r17)
		r19 += mpyi(r2,r4)
		r6 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r5,r3)
		r1 = asr(r6,#31)
	}
	{
		r5:4 = mpyu(r2,r6)
	}
	{
		r5 += mpyi(r2,r1)
		r27 += mpyi(r2,r0)
		r1 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r9,r3)
		r0 = asr(r20,#31)
	}
	{
		r9:8 = mpyu(r2,r20)
		r13 = r6
		memd(r30+##-9520) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r2,r1)
		r9 += mpyi(r2,r0)
	}
	{
		r0 = asr(r1,#31)
		r12 = r1
	}
	{
		r7 += mpyi(r2,r0)
		r5 += mpyi(r13,r3)
	}
	{
		r25 += mpyi(r22,r3)
		r27 += mpyi(r17,r3)
		memd(r30+##-9776) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r9 += mpyi(r20,r3)
		r7 += mpyi(r12,r3)
		r4 = r3
	}
	{
		r3 = memw(r30+##-8752)
		memd(r30+##-10032) = r27:26
	}                                       // 4-byte Folded Reload
	{
		r16 = memw(r30+##-9008)
		memd(r30+##-10288) = r9:8
	}                                       // 4-byte Folded Reload
	{
		r8 = ##-1073741825
		r9 = ##2147483647
	}
	{
		r1:0 = min(r15:14,r9:8)
		memd(r30+##-11056) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r6 = ##1073741824
		r7 = #0
	}
	{
		r1:0 = add(r1:0,r7:6)
		r14 = asr(r3,#31)
		r5 = r3
		memd(r30+#-5680) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r27:26 = mpyu(r2,r3)
		r15 = asr(r16,#31)
		r3 = #0
	}
	{
		r23:22 = mpyu(r2,r16)
		r27 += mpyi(r2,r14)
		r14 = r4
	}
	{
		r23 += mpyi(r2,r15)
		r1:0 = asr(r1:0,#31)
		r2 = ##2147483647
	}
	{
		r27 += mpyi(r5,r4)
		r1:0 = min(r1:0,r3:2)
		r12 = r14
	}
	{
		r4 = ##-2147483648
		r5 = #-1
	}
	{
		r1:0 = max(r1:0,r5:4)
		r23 += mpyi(r16,r14)
		r17:16 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r17:16,r9:8)
		r1 = add(r30,#-1072)
		memd(r30+##-8752) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r11:10 = min(r11:10,r9:8)
		r27 = r28
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1:0 = add(r15:14,r7:6)
		v0.w = vinsert(r0)
		r13 = r27
	}
	{
		r15 = memw(r30+##-5168)
		r17:16 = memd(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r14 = asr(r15,#31)
	}
	{
		r21:20 = mpyu(r28,r15)
		r1:0 = min(r1:0,r3:2)
		r28 = r15
		v0 = valign(v0,v0,#4)
	}
	{
		r21 += mpyi(r27,r14)
		r17:16 = min(r17:16,r9:8)
		r15:14 = combine(r3,r2)
		r2 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r5:4)
		r17:16 = add(r17:16,r7:6)
	}
	{
		r25:24 = mpyu(r27,r2)
		r1 = asr(r2,#31)
	}
	{
		r25 += mpyi(r27,r1)
		v0.w = vinsert(r0)
	}
	{
		r25 += mpyi(r2,r12)
		r17:16 = asr(r17:16,#31)
		r3:2 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r17:16,r15:14)
		r3:2 = min(r3:2,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r21 += mpyi(r28,r12)
	}
	{
		r1:0 = add(r3:2,r7:6)
		v0.w = vinsert(r0)
		r2 = memw(r30+#-2864)
		memd(r30+#-1072) = r21:20
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r28 = asr(r2,#31)
		r26 = r2
	}
	{
		r21:20 = mpyu(r27,r2)
		r3:2 = add(r11:10,r7:6)
		r10 = r12
		v0 = valign(v0,v0,#4)
	}
	{
		r21 += mpyi(r27,r28)
		r1:0 = min(r1:0,r15:14)
		r11 = r13
		r28 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r5:4)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1 = asr(r28,#31)
		v0.w = vinsert(r0)
	}
	{
		r17:16 = mpyu(r27,r28)
		r3:2 = min(r3:2,r15:14)
	}
	{
		r17 += mpyi(r27,r1)
		r1:0 = max(r3:2,r5:4)
		v0 = valign(v0,v0,#4)
		r3:2 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r9:8)
		v0.w = vinsert(r0)
	}
	{
		r1:0 = min(r19:18,r9:8)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r1:0 = add(r1:0,r7:6)
		r3:2 = asr(r3:2,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r21 += mpyi(r26,r12)
		r26 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r5:4)
		r1:0 = max(r1:0,r5:4)
	}
	{
		v0.w = vinsert(r2)
		v1.w = vinsert(r0)
		r3:2 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r9:8)
		r17 += mpyi(r28,r12)
		r3:2 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = mpyu(r27,r26)
		r28 = asr(r26,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r19 += mpyi(r27,r28)
		r3:2 = min(r3:2,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r19 += mpyi(r26,r12)
		r3:2 = add(r3:2,r7:6)
		r27:26 = combine(r5,r4)
		r5:4 = combine(r9,r8)
	}
	{
		r3:2 = asr(r3:2,#31)
		r1:0 = add(r1:0,r7:6)
		r9:8 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r5:4)
		r3:2 = min(r3:2,r15:14)
		r12 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = add(r9:8,r7:6)
		r1:0 = asr(r1:0,#31)
		r28 = r12
	}
	{
		r3:2 = max(r3:2,r27:26)
		r9:8 = asr(r9:8,#31)
	}
	{
		r3:2 = min(r9:8,r15:14)
		v1.w = vinsert(r2)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r3:2 = max(r3:2,r27:26)
	}
	{
		r1:0 = max(r1:0,r27:26)
		v0.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		r9:8 = mpyu(r13,r12)
		r1 = asr(r12,#31)
		r3:2 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r9 += mpyi(r13,r1)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r3:2,r5:4)
		r9 += mpyi(r28,r10)
		r3:2 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r5:4)
		r1:0 = add(r1:0,r7:6)
		v1 = valign(v1,v1,#4)
		r13:12 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r5:4)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r13:12 = add(r13:12,r7:6)
	}
	{
		r3:2 = asr(r3:2,#31)
		v1.w = vinsert(r0)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r13:12 = asr(r13:12,#31)
	}
	{
		r1:0 = min(r13:12,r15:14)
		r3:2 = max(r3:2,r27:26)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r1:0,r27:26)
		v1.w = vinsert(r2)
		r3:2 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r5:4)
		v0.w = vinsert(r0)
		r3:2 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r5:4)
		r1:0 = add(r1:0,r7:6)
		v1 = valign(v1,v1,#4)
		r13:12 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r5:4)
		r3:2 = add(r3:2,r7:6)
		v0 = valign(v0,v0,#4)
		r28 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		r13:12 = add(r13:12,r7:6)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r1:0 = asr(r1:0,#31)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r13:12 = asr(r13:12,#31)
	}
	{
		r3:2 = min(r13:12,r15:14)
		v1.w = vinsert(r2)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r3:2 = max(r3:2,r27:26)
	}
	{
		r1:0 = max(r1:0,r27:26)
		v0.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		r13:12 = mpyu(r11,r28)
		r1 = asr(r28,#31)
		r3:2 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r13 += mpyi(r11,r1)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r3:2,r5:4)
		r3:2 = min(r23:22,r5:4)
		r23:22 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r5:4)
		r1:0 = add(r1:0,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r23:22 = add(r23:22,r7:6)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r23:22 = asr(r23:22,#31)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = min(r23:22,r15:14)
		r3:2 = min(r3:2,r15:14)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r13 += mpyi(r28,r10)
		v1 = valign(v1,v1,#4)
		r28 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r27:26)
		v0.w = vinsert(r0)
	}
	{
		r3:2 = min(r13:12,r5:4)
		v1.w = vinsert(r2)
	}
	{
		r1:0 = min(r19:18,r5:4)
		r3:2 = min(r9:8,r5:4)
		r19:18 = memd(r30+#-1072)
		memd(r30+#-1328) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r21:20,r5:4)
		r1:0 = min(r17:16,r5:4)
		r21:20 = memd(r30+#-5680)
		memd(r30+#-1584) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r21:20,r5:4)
		r15:14 = min(r19:18,r5:4)
		v1 = valign(v1,v1,#4)
		r23:22 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r5:4)
		r13:12 = min(r25:24,r5:4)
		v0 = valign(v0,v0,#4)
		r21:20 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r5:4)
		r13:12 = add(r13:12,r7:6)
		r23:22 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r23:22,r5:4)
		r9:8 = add(r9:8,r7:6)
		r23:22 = memd(r30+#-2096)
		r25:24 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r5:4)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r25:24 = min(r25:24,r5:4)
		r5:4 = add(r21:20,r7:6)
	}
	{
		r5:4 = add(r1:0,r7:6)
		r23:22 = add(r23:22,r7:6)
		r1:0 = memd(r30+#-1584)
		memd(r30+#-1072) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r7:6)
		r25:24 = add(r25:24,r7:6)
		r21:20 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = add(r19:18,r7:6)
		r17:16 = add(r17:16,r7:6)
	}
	{
		r11:10 = add(r11:10,r7:6)
		r15:14 = add(r15:14,r7:6)
	}
	{
		r7:6 = add(r21:20,r7:6)
		r3:2 = asr(r3:2,#31)
	}
	{
		r7:6 = asr(r5:4,#31)
		r5:4 = asr(r15:14,#31)
		memd(r30+#-2352) = r7:6
		memd(r30+#-2608) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r15:14 = asr(r11:10,#31)
		r11:10 = asr(r19:18,#31)
	}
	{
		r19:18 = asr(r25:24,#31)
		r13:12 = asr(r13:12,#31)
		r24 = ##2147483647
	}
	{
		r3:2 = asr(r17:16,#31)
		r17:16 = asr(r23:22,#31)
		r25 = #0
		r23:22 = memd(r30+#-1072)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r25:24)
		r21:20 = asr(r23:22,#31)
	}
	{
		r23:22 = min(r5:4,r25:24)
		r5:4 = min(r15:14,r25:24)
	}
	{
		r17:16 = min(r17:16,r25:24)
		r15:14 = max(r13:12,r27:26)
	}
	{
		r17:16 = max(r17:16,r27:26)
		r9:8 = asr(r9:8,#31)
		r15 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = max(r23:22,r27:26)
		r9:8 = min(r9:8,r25:24)
		r15 = add(r15,#1152)
	}
	{
		v1.w = vinsert(r22)
		r17 = add(r28,##-44928)
		v2 = vmem(r15+#0)
	}
	{
		r9:8 = max(r9:8,r27:26)
		vmem(r17+#0) = v2
	}
	{
		r7:6 = min(r7:6,r25:24)
		r3:2 = min(r3:2,r25:24)
		r9 = memw(r17+#120)
		memw(r30+#-1328) = r9.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r7:6,r27:26)
		v1 = valign(v1,v1,#4)
		r9 = memw(r17+#124)
		memw(r30+#-1072) = r9.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r14)
		v0.w = vinsert(r16)
		r9 = memw(r17+#112)
		memw(r30+#-1840) = r9.new
	}                                       // 4-byte Folded Spill
	{
		r23:22 = max(r3:2,r27:26)
		r1:0 = asr(r1:0,#31)
		r9 = memw(r17+#116)
		memw(r30+#-1584) = r9.new
	}                                       // 4-byte Folded Spill
	{
		r21:20 = min(r21:20,r25:24)
		r1:0 = min(r1:0,r25:24)
		v1 = valign(v1,v1,#4)
		v2 = v10
	}
	{
		r21:20 = max(r21:20,r27:26)
		v1.w = vinsert(r8)
		r8 = memw(r17+#104)
		memw(r30+#-2864) = r8.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = max(r1:0,r27:26)
		v0 = valign(v0,v0,#4)
		r8 = memw(r17+#108)
		memw(r30+#-2096) = r8.new
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r20)
		r7 = memw(r17+#96)
		memw(r30+##-8496) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = min(r19:18,r25:24)
		v1 = valign(v1,v1,#4)
		r7 = memw(r17+#100)
		memw(r30+#-3632) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = max(r19:18,r27:26)
		r2 = memw(r17+#88)
		memw(r30+##-10032) = r2.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r6)
		v0 = valign(v0,v0,#4)
		r2 = memw(r17+#92)
	}
	{
		r11:10 = min(r11:10,r25:24)
		v0.w = vinsert(r18)
		memw(r30+##-8240) = r2
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r2 = memw(r17+#80)
		memw(r30+##-9008) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r21:20 = max(r11:10,r27:26)
		r2 = memw(r17+#84)
		memw(r30+##-9264) = r2.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r1 = memw(r17+#72)
	}
	{
		v0.w = vinsert(r20)
		r9:8 = combine(r25,r24)
		memw(r30+##-9776) = r1
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r1 = memw(r17+#76)
		memw(r30+##-9520) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r5:4,r27:26)
		r1 = memw(r17+#64)
		memw(r30+##-10288) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r10 = memw(r17+#68)
	}
	{
		v0.w = vinsert(r22)
		r28 = memw(r17+#56)
		r0 = memw(r17+#60)
	}
	{
		r3:2 = memd(r30+#-2608)
		memw(r30+##-5424) = r0
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r25:24)
		v0 = valign(v0,v0,#4)
		r13 = memw(r17+#48)
		r12 = memw(r17+#52)
	}
	{
		r7:6 = max(r1:0,r27:26)
		v0.w = vinsert(r4)
		r19 = memw(r17+#40)
		r18 = memw(r17+#44)
	}
	{
		v1.w = vinsert(r6)
		r20 = memw(r17+#32)
		r21 = memw(r17+#36)
	}
	{
		r22 = memw(r17+#24)
		r23 = memw(r17+#28)
	}
	{
		v1 = valign(v1,v1,#4)
		r24 = memw(r17+#16)
		r25 = memw(r17+#20)
	}
	{
		r0 = memw(r17+#8)
		r2 = memw(r17+#12)
	}
	{
		r14 = memw(r17+#0)
		r15 = memw(r17+#4)
	}
	{
		r11 = asr(r15,#31)
		r1 = memw(r30+##-6456)
		r7:6 = memd(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = asr(r7:6,#31)
	}
	{
		r17:16 = mpyu(r1,r15)
		r5:4 = min(r5:4,r9:8)
		r3 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r1,r11)
	}
	{
		r5:4 = max(r5:4,r27:26)
		r11 = asr(r14,#31)
		r7:6 = combine(r17,r16)
	}
	{
		r27:26 = mpyu(r1,r14)
		v1.w = vinsert(r4)
	}
	{
		r7 += mpyi(r15,r3)
		r4 = asr(r2,#31)
	}
	{
		r17:16 = mpyu(r1,r2)
		r27 += mpyi(r1,r11)
		v1 = valign(v1,v1,#4)
		memd(r30+#-7472) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r1,r4)
		r4 = asr(r24,#31)
	}
	{
		r7:6 = mpyu(r1,r24)
		r27 += mpyi(r14,r3)
	}
	{
		r7 += mpyi(r1,r4)
		r11 = asr(r0,#31)
	}
	{
		r15:14 = mpyu(r1,r0)
		r7 += mpyi(r24,r3)
	}
	{
		r15 += mpyi(r1,r11)
		r7:6 = mpyu(r1,r25)
		r11 = #-1
		memd(r30+#-7984) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r0,r3)
		r0 = asr(r25,#31)
	}
	{
		r7 += mpyi(r1,r0)
		r0 = asr(r23,#31)
	}
	{
		r5:4 = mpyu(r1,r23)
		r7 += mpyi(r25,r3)
	}
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r20,#31)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r25:24 = mpyu(r1,r20)
		r17 += mpyi(r2,r3)
	}
	{
		r25 += mpyi(r1,r0)
		r2 = asr(r22,#31)
	}
	{
		r9:8 = mpyu(r1,r22)
		r0 = asr(r19,#31)
	}
	{
		r7:6 = mpyu(r1,r19)
		r9 += mpyi(r1,r2)
	}
	{
		r7 += mpyi(r1,r0)
		r9 += mpyi(r22,r3)
	}
	{
		r5 += mpyi(r23,r3)
		r7 += mpyi(r19,r3)
		memd(r30+#-5680) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r23:22 = mpyu(r1,r21)
		r2 = asr(r21,#31)
		memd(r30+#-2352) = r7:6
		memd(r30+#-5168) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r23 += mpyi(r1,r2)
		r2 = asr(r13,#31)
	}
	{
		r7:6 = mpyu(r1,r13)
		r0 = asr(r18,#31)
	}
	{
		r5:4 = mpyu(r1,r18)
		r7 += mpyi(r1,r2)
	}
	{
		r5 += mpyi(r1,r0)
		r23 += mpyi(r21,r3)
	}
	{
		r7 += mpyi(r13,r3)
		r5 += mpyi(r18,r3)
		memd(r30+#-3376) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r0 = asr(r12,#31)
		r6 = memw(r30+##-5424)
		memd(r30+#-3120) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r1,r12)
	}
	{
		r23 += mpyi(r1,r0)
		r0 = asr(r28,#31)
		r8 = r6
		memd(r30+#-2608) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r28)
		r2 = asr(r6,#31)
	}
	{
		r19:18 = mpyu(r1,r6)
		r5 += mpyi(r1,r0)
	}
	{
		r19 += mpyi(r1,r2)
		r5 += mpyi(r28,r3)
		r2 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r1,r10)
		r0 = asr(r10,#31)
		memd(r30+#-4656) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r1,r0)
		r19 += mpyi(r8,r3)
		r4 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r10,r3)
		r9 = r2
		r8 = r4
		memd(r30+#-5424) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r19:18 = mpyu(r1,r2)
		r0 = asr(r2,#31)
		r10 = ##-2147483648
	}
	{
		r7:6 = mpyu(r1,r4)
		r2 = asr(r4,#31)
		memd(r30+##-8752) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r1,r0)
		r7 += mpyi(r1,r2)
		r2 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r23 += mpyi(r12,r3)
		r0 = asr(r2,#31)
	}
	{
		r7 += mpyi(r8,r3)
		r22 = ##-1073741825
		memd(r30+#-3888) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r2)
		memd(r30+##-9776) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r25 += mpyi(r20,r3)
		r5 += mpyi(r1,r0)
		r23 = ##2147483647
	}
	{
		r24 = ##2147483647
		r25 = #0
		memd(r30+#-4400) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r2,r3)
		r19 += mpyi(r9,r3)
		r2 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-9264)
		memd(r30+##-9520) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r1,r2)
		r0 = asr(r2,#31)
		r13 = r2
	}
	{
		r7:6 = mpyu(r1,r8)
		r2 = asr(r8,#31)
		r28 = r8
	}
	{
		r5 += mpyi(r1,r0)
		r7 += mpyi(r1,r2)
		r2 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r13,r3)
		r7 += mpyi(r28,r3)
	}
	{
		r0 = asr(r2,#31)
		r12 = r2
		memd(r30+##-9008) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r1,r2)
		r5:4 = min(r27:26,r23:22)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r1,r0)
		r6 = ##1073741824
		r7 = #0
	}
	{
		r1:0 = add(r5:4,r7:6)
		r2 = r1
		r5 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r12,r3)
		r9 = r2
		r8 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r4 = asr(r5,#31)
		memd(r30+##-10032) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r27:26 = mpyu(r2,r5)
		r5 = asr(r8,#31)
		r28 = r5
		r12 = r3
	}
	{
		r21:20 = mpyu(r2,r8)
		r27 += mpyi(r2,r4)
	}
	{
		r21 += mpyi(r2,r5)
		r1:0 = min(r1:0,r25:24)
		r5:4 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r5:4 = min(r5:4,r23:22)
	}
	{
		r1:0 = add(r5:4,r7:6)
		v2.w = vinsert(r0)
		r5 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r27 += mpyi(r28,r3)
		r1:0 = asr(r1:0,#31)
		r28 = r5
	}
	{
		r21 += mpyi(r8,r3)
		r1:0 = min(r1:0,r25:24)
		memd(r30+##-8240) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r23:22 = min(r15:14,r23:22)
		r4 = asr(r5,#31)
		r3:2 = combine(r23,r22)
		v2 = valign(v2,v2,#4)
	}
	{
		r27:26 = mpyu(r9,r5)
		r1:0 = max(r1:0,r11:10)
		r15:14 = combine(r25,r24)
		r8 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r27 += mpyi(r9,r4)
		r5:4 = add(r23:22,r7:6)
	}
	{
		r5:4 = asr(r5:4,#31)
		v2.w = vinsert(r0)
	}
	{
		r23:22 = mpyu(r9,r8)
		r1 = asr(r8,#31)
	}
	{
		r5:4 = min(r5:4,r25:24)
		r23 += mpyi(r9,r1)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r5:4,r11:10)
		r5:4 = min(r17:16,r3:2)
	}
	{
		r1:0 = add(r5:4,r7:6)
		v2.w = vinsert(r0)
		r5 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r27 += mpyi(r28,r12)
		r23 += mpyi(r8,r12)
		r28 = r5
	}
	{
		r1:0 = asr(r1:0,#31)
		r27:26 = combine(r23,r22)
		r23:22 = memd(r30+#-7984)
		memd(r30+#-3632) = r27:26
	}                                       // 8-byte Folded Reload
	{
		r25:24 = mpyu(r9,r5)
		r4 = asr(r5,#31)
		v2 = valign(v2,v2,#4)
		r8 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r25 += mpyi(r9,r4)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r5:4 = add(r17:16,r7:6)
		r16 = r9
	}
	{
		r1:0 = max(r1:0,r11:10)
		r5:4 = asr(r5:4,#31)
		r17 = r16
	}
	{
		r23:22 = mpyu(r9,r8)
		r1 = asr(r8,#31)
	}
	{
		r5:4 = min(r5:4,r15:14)
		v2.w = vinsert(r0)
	}
	{
		r23 += mpyi(r9,r1)
		r1:0 = max(r5:4,r11:10)
		r5:4 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r3:2)
		r3:2 = min(r19:18,r3:2)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = add(r5:4,r7:6)
		v2.w = vinsert(r0)
	}
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r1:0 = asr(r3:2,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r23 += mpyi(r8,r12)
		r2 = ##-1073741825
	}
	{
		r5:4 = max(r5:4,r11:10)
		r3 = ##2147483647
		r8 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		v2.w = vinsert(r4)
		r5:4 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r3:2)
		v3.w = vinsert(r0)
		r5:4 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r25 += mpyi(r28,r12)
		r5:4 = min(r5:4,r3:2)
	}
	{
		r19:18 = mpyu(r9,r8)
		r28 = asr(r8,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r19 += mpyi(r9,r28)
		r5:4 = add(r5:4,r7:6)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = asr(r5:4,#31)
		r1:0 = add(r1:0,r7:6)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r19 += mpyi(r8,r12)
		r13:12 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r13:12,r3:2)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = max(r5:4,r11:10)
		r28 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = add(r9:8,r7:6)
		v3.w = vinsert(r4)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r13:12 = asr(r13:12,#31)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r13:12 = min(r13:12,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		r5:4 = mpyu(r16,r28)
		r1 = asr(r28,#31)
	}
	{
		r13:12 = max(r13:12,r11:10)
		v3.w = vinsert(r0)
	}
	{
		r5 += mpyi(r16,r1)
		v2.w = vinsert(r12)
		r1:0 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		v3 = valign(v3,v3,#4)
		r13:12 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r21:20,r3:2)
		r9:8 = min(r13:12,r3:2)
		r13:12 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r3:2)
		r1:0 = add(r1:0,r7:6)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r16 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r15:14)
		r13:12 = add(r13:12,r7:6)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r13:12 = asr(r13:12,#31)
	}
	{
		r9:8 = add(r9:8,r7:6)
		v3.w = vinsert(r0)
	}
	{
		r1:0 = min(r13:12,r15:14)
		r9:8 = asr(r9:8,#31)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r9:8 = min(r9:8,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		r9:8 = max(r9:8,r11:10)
		v2.w = vinsert(r0)
		r1:0 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		v3.w = vinsert(r8)
		r13:12 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r13:12,r3:2)
		r1:0 = add(r1:0,r7:6)
		v2 = valign(v2,v2,#4)
		r13:12 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r3:2)
		r9:8 = add(r9:8,r7:6)
		v3 = valign(v3,v3,#4)
	}
	{
		r9:8 = asr(r9:8,#31)
		r1:0 = asr(r1:0,#31)
	}
	{
		r9:8 = min(r9:8,r15:14)
		r5 += mpyi(r28,r16)
		r28 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = max(r9:8,r11:10)
		r13:12 = add(r13:12,r7:6)
	}
	{
		r1:0 = min(r1:0,r15:14)
		v3.w = vinsert(r8)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r13:12 = asr(r13:12,#31)
	}
	{
		r9:8 = min(r13:12,r15:14)
		r1 = asr(r28,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r13:12 = mpyu(r17,r28)
		v3.w = vinsert(r0)
	}
	{
		r13 += mpyi(r17,r1)
		r9:8 = max(r9:8,r11:10)
		r1:0 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		v2.w = vinsert(r8)
		v3 = valign(v3,v3,#4)
		r9:8 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r3:2)
		r1:0 = add(r1:0,r7:6)
	}
	{
		r1:0 = asr(r1:0,#31)
		r21:20 = add(r21:20,r7:6)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r9:8 = add(r9:8,r7:6)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r21:20 = asr(r21:20,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		v3.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r3:2)
		r1:0 = min(r9:8,r15:14)
	}
	{
		r21:20 = min(r21:20,r15:14)
		r5:4 = min(r19:18,r3:2)
		v3 = valign(v3,v3,#4)
		memd(r30+#-1072) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = max(r1:0,r11:10)
		r9:8 = max(r21:20,r11:10)
		r5:4 = memd(r30+#-3632)
		memd(r30+#-1840) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r13 += mpyi(r28,r16)
		v2.w = vinsert(r0)
	}
	{
		r1:0 = min(r13:12,r3:2)
		v3.w = vinsert(r8)
	}
	{
		r9:8 = min(r25:24,r3:2)
		r1:0 = min(r23:22,r3:2)
		r25:24 = memd(r30+#-5424)
		memd(r30+#-1584) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r27:26,r3:2)
		r5:4 = min(r5:4,r3:2)
		v3 = valign(v3,v3,#4)
		r23:22 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r11:10 = min(r25:24,r3:2)
		v2 = valign(v2,v2,#4)
		r21:20 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r3:2)
		r23:22 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r23:22,r3:2)
		r23:22 = memd(r30+#-2352)
		r27:26 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r3:2)
		r25:24 = min(r27:26,r3:2)
	}
	{
		r3:2 = add(r23:22,r7:6)
		r23:22 = add(r21:20,r7:6)
	}
	{
		r21:20 = add(r19:18,r7:6)
		r19:18 = add(r17:16,r7:6)
		memd(r30+#-1328) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r17:16 = add(r11:10,r7:6)
		r11:10 = add(r5:4,r7:6)
		r5:4 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r13:12,r7:6)
		r13:12 = add(r1:0,r7:6)
		r1:0 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = add(r9:8,r7:6)
		r9:8 = add(r1:0,r7:6)
		r1:0 = memd(r30+#-1072)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r5:4,r7:6)
		r1:0 = add(r1:0,r7:6)
	}
	{
		r5:4 = asr(r5:4,#31)
		r1:0 = asr(r1:0,#31)
	}
	{
		r25:24 = add(r25:24,r7:6)
		r5:4 = asr(r9:8,#31)
		memd(r30+#-2096) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r9:8 = asr(r13:12,#31)
		r13:12 = asr(r3:2,#31)
		r1:0 = memd(r30+#-1328)
		memd(r30+#-1072) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r7:6 = asr(r11:10,#31)
		r3:2 = asr(r21:20,#31)
	}
	{
		r21:20 = asr(r1:0,#31)
		r0 = ##-2147483648
		r1 = #-1
	}
	{
		r7:6 = min(r7:6,r15:14)
		r13:12 = min(r13:12,r15:14)
	}
	{
		r11:10 = asr(r17:16,#31)
		r17:16 = asr(r19:18,#31)
	}
	{
		r19:18 = asr(r23:22,#31)
		r23:22 = asr(r25:24,#31)
	}
	{
		r25:24 = min(r19:18,r15:14)
		r7:6 = max(r7:6,r1:0)
	}
	{
		r13:12 = max(r13:12,r1:0)
		r27:26 = asr(r27:26,#31)
	}
	{
		r21:20 = min(r21:20,r15:14)
		v3.w = vinsert(r6)
		r13 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r27:26,r15:14)
		r19:18 = min(r17:16,r15:14)
		r13 = add(r13,#1280)
	}
	{
		r17:16 = min(r11:10,r15:14)
		r11:10 = max(r25:24,r1:0)
		r25:24 = combine(r15,r14)
		v4 = vmem(r13+#0)
	}
	{
		r23:22 = min(r23:22,r15:14)
		r3:2 = min(r3:2,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		r9:8 = min(r9:8,r15:14)
		r5:4 = min(r5:4,r15:14)
		r14 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = max(r21:20,r1:0)
		r27:26 = max(r7:6,r1:0)
		r14 = add(r14,##-45056)
	}
	{
		v2.w = vinsert(r20)
		v3.w = vinsert(r12)
		r21:20 = combine(r1,r0)
		vmem(r14+#0) = v4
	}
	{
		r5:4 = max(r5:4,r21:20)
		r3:2 = max(r3:2,r21:20)
		v4 = v10
		r7:6 = memd(r30+#-1072)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r7:6,r25:24)
		r7:6 = max(r9:8,r21:20)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
		r7 = memw(r14+#120)
		memw(r30+#-1328) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r27:26 = max(r23:22,r21:20)
		v3.w = vinsert(r26)
		r7 = memw(r14+#124)
		memw(r30+#-1072) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = max(r1:0,r21:20)
		v2 = valign(v2,v2,#4)
		r7 = memw(r14+#112)
		memw(r30+#-1840) = r7.new
	}                                       // 4-byte Folded Spill
	{
		v2.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
		r7 = memw(r14+#116)
		memw(r30+#-1584) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r11:10 = max(r17:16,r21:20)
		v3.w = vinsert(r6)
		r7 = memw(r14+#104)
		memw(r30+#-3120) = r7.new
	}                                       // 4-byte Folded Spill
	{
		v2 = valign(v2,v2,#4)
		r7 = memw(r14+#108)
		memw(r30+#-2608) = r7.new
	}                                       // 4-byte Folded Spill
	{
		v2.w = vinsert(r2)
		r5 = memw(r14+#96)
		memw(r30+##-7728) = r5.new
	}                                       // 4-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		r5 = memw(r14+#100)
		memw(r30+##-4400) = r5.new
	}                                       // 4-byte Folded Spill
	{
		v3.w = vinsert(r4)
		r3 = memw(r14+#88)
		memw(r30+##-9520) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = combine(r21,r20)
		v2 = valign(v2,v2,#4)
		r3 = memw(r14+#92)
	}
	{
		v3 = valign(v3,v3,#4)
		memw(r30+##-8496) = r3
		r2 = memw(r14+#80)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v3.w = vinsert(r8)
		memw(r30+##-9008) = r2
		r2 = memw(r14+#84)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9776) = r2
		r0 = memw(r14+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r3:2 = max(r19:18,r21:20)
		v3 = valign(v3,v3,#4)
		memw(r30+##-8752) = r0
	}                                       // 4-byte Folded Spill
	{
		v2.w = vinsert(r2)
		r28 = memw(r14+#76)
		r15 = memw(r14+#64)
	}
	{
		r13 = memw(r14+#68)
		r0 = memw(r14+#56)
	}
	{
		v2 = valign(v2,v2,#4)
		memw(r30+##-5168) = r0
		r12 = memw(r14+#60)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v2.w = vinsert(r10)
		r7:6 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r7:6,r25:24)
		r24 = memw(r14+#48)
		r6 = memw(r14+#52)
	}
	{
		r19:18 = max(r19:18,r5:4)
		r9 = memw(r14+#40)
		r8 = memw(r14+#44)
	}
	{
		v3.w = vinsert(r18)
		r7 = memw(r14+#32)
		r16 = memw(r14+#36)
	}
	{
		r17 = memw(r14+#24)
		r22 = memw(r14+#28)
	}
	{
		v3 = valign(v3,v3,#4)
		r23 = memw(r14+#16)
		r25 = memw(r14+#20)
	}
	{
		r26 = memw(r14+#8)
		r0 = memw(r14+#12)
	}
	{
		r20 = memw(r14+#0)
		r14 = memw(r14+#4)
	}
	{
		r10 = asr(r20,#31)
		r11 = asr(r14,#31)
		r1 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r27 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r1,r14)
		r19:18 = mpyu(r1,r20)
	}
	{
		r5 += mpyi(r1,r11)
		r19 += mpyi(r1,r10)
	}
	{
		r5 += mpyi(r14,r27)
		r14 = asr(r26,#31)
	}
	{
		r11:10 = mpyu(r1,r26)
		r19 += mpyi(r20,r27)
		memd(r30+#-7984) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r11 += mpyi(r1,r14)
		r14 = asr(r0,#31)
	}
	{
		r21:20 = mpyu(r1,r0)
		r5:4 = mpyu(r1,r23)
	}
	{
		r21 += mpyi(r1,r14)
		r14 = asr(r23,#31)
	}
	{
		r5 += mpyi(r1,r14)
		r21 += mpyi(r0,r27)
	}
	{
		r3:2 = mpyu(r1,r25)
		r0 = asr(r25,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r23,r27)
	}
	{
		r3 += mpyi(r25,r27)
		r0 = asr(r17,#31)
		memd(r30+##-8240) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r17)
		r3:2 = mpyu(r1,r22)
		memd(r30+#-7472) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r22,#31)
	}
	{
		r5 += mpyi(r17,r27)
		r3 += mpyi(r1,r0)
	}
	{
		r3:2 = mpyu(r1,r7)
		r0 = asr(r7,#31)
		r5:4 = combine(r3,r2)
		memd(r30+#-5680) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r22,r27)
		r3 += mpyi(r1,r0)
	}
	{
		r5:4 = mpyu(r1,r16)
		r0 = asr(r16,#31)
		memd(r30+#-4656) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r7,r27)
		r5 += mpyi(r1,r0)
	}
	{
		r5:4 = mpyu(r1,r9)
		r0 = asr(r9,#31)
		r3:2 = combine(r5,r4)
		memd(r30+#-3632) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r3 += mpyi(r16,r27)
	}
	{
		r5 += mpyi(r9,r27)
		r0 = asr(r8,#31)
		memd(r30+#-3376) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r1,r8)
		r5:4 = mpyu(r1,r24)
		memd(r30+#-2096) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r24,#31)
	}
	{
		r5 += mpyi(r1,r0)
		r3 += mpyi(r8,r27)
	}
	{
		r5 += mpyi(r24,r27)
		r0 = asr(r6,#31)
		memd(r30+#-2352) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r1,r6)
		r4 = memw(r30+##-5168)
		memd(r30+#-2864) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r1,r13)
		r3 += mpyi(r1,r0)
	}
	{
		r9:8 = mpyu(r1,r4)
		r0 = asr(r4,#31)
	}
	{
		r3 += mpyi(r6,r27)
		r9 += mpyi(r1,r0)
	}
	{
		r7:6 = mpyu(r1,r12)
		r0 = asr(r12,#31)
		memd(r30+#-3888) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r9 += mpyi(r4,r27)
		r7 += mpyi(r1,r0)
	}
	{
		r7 += mpyi(r12,r27)
		r0 = asr(r13,#31)
		r9 = #0
		memd(r30+#-5168) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r1,r0)
		r6 = memw(r30+##-8752)
		memd(r30+#-5424) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r1,r15)
		r0 = asr(r15,#31)
	}
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r6,#31)
		r8 = r6
	}
	{
		r3:2 = mpyu(r1,r6)
		r7:6 = mpyu(r1,r28)
	}
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r28,#31)
	}
	{
		r3 += mpyi(r8,r27)
		r7 += mpyi(r1,r0)
		r8 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r28,r27)
		r0 = asr(r8,#31)
		memd(r30+##-8752) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r1,r8)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r13,r27)
		r3 += mpyi(r1,r0)
		r6 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r6,#31)
		r7 = r6
	}
	{
		r25:24 = mpyu(r1,r6)
		r3 += mpyi(r8,r27)
		r6 = r1
	}
	{
		r8 = ##1073741824
		memd(r30+##-9008) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r25 += mpyi(r1,r0)
		r2 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r7,r27)
		r3 = ##2147483647
	}
	{
		r0 = asr(r2,#31)
		r28 = r2
		r7 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r1,r2)
		r5 += mpyi(r15,r27)
		r2 = ##-1073741825
	}
	{
		r13:12 = min(r19:18,r3:2)
		r19:18 = mpyu(r6,r7)
	}
	{
		r23 += mpyi(r1,r0)
		r1:0 = add(r13:12,r9:8)
		r15:14 = combine(r19,r18)
		r13 = r7
	}
	{
		r12 = asr(r7,#31)
		r1:0 = asr(r1:0,#31)
		r18 = ##2147483647
	}
	{
		r15 += mpyi(r6,r12)
		r19 = #0
		r7 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r19:18)
		r15 += mpyi(r13,r27)
		r12 = ##-2147483648
	}
	{
		r11 += mpyi(r26,r27)
		r13 = #-1
		memd(r30+##-8496) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r1:0 = max(r1:0,r13:12)
		r23 += mpyi(r28,r27)
		r15:14 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r15:14,r3:2)
		v4.w = vinsert(r0)
		memd(r30+##-9520) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r11:10 = min(r11:10,r3:2)
		r1:0 = add(r15:14,r9:8)
		r28 = r7
	}
	{
		r1:0 = asr(r1:0,#31)
		r26 = asr(r7,#31)
		r14 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r6,r7)
		r1:0 = min(r1:0,r19:18)
		r7 = r27
		v4 = valign(v4,v4,#4)
	}
	{
		r23 += mpyi(r6,r26)
		r11:10 = add(r11:10,r9:8)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r23 += mpyi(r28,r27)
	}
	{
		r1 = asr(r14,#31)
		v4.w = vinsert(r0)
	}
	{
		r27:26 = mpyu(r6,r14)
		r11:10 = asr(r11:10,#31)
	}
	{
		r27 += mpyi(r6,r1)
		r1:0 = min(r11:10,r19:18)
		v4 = valign(v4,v4,#4)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r21:20 = min(r21:20,r3:2)
	}
	{
		r11:10 = add(r21:20,r9:8)
		v4.w = vinsert(r0)
		r0 = memw(r30+#-3120)
		r1 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r27 += mpyi(r14,r7)
		r15:14 = asr(r11:10,#31)
		r10 = r0
	}
	{
		r27:26 = mpyu(r6,r0)
		r28 = asr(r0,#31)
		r11 = r1
		memd(r30+#-4400) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r6,r1)
		r0 = asr(r1,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r15:14 = min(r15:14,r19:18)
		r21 += mpyi(r6,r0)
	}
	{
		r1:0 = max(r15:14,r13:12)
		r27 += mpyi(r6,r28)
		r15:14 = combine(r6,r7)
	}
	{
		r27 += mpyi(r10,r7)
		v4.w = vinsert(r0)
		r1:0 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		r3:2 = min(r5:4,r3:2)
	}
	{
		r21 += mpyi(r11,r7)
		r5:4 = add(r1:0,r9:8)
		v4 = valign(v4,v4,#4)
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r28 = asr(r0,#31)
		r1 = ##2147483647
	}
	{
		r11:10 = mpyu(r6,r0)
		r3:2 = add(r3:2,r9:8)
	}
	{
		r11 += mpyi(r6,r28)
		r5:4 = min(r5:4,r19:18)
	}
	{
		r11 += mpyi(r0,r7)
		r3:2 = asr(r3:2,#31)
		r0 = ##-1073741825
	}
	{
		r5:4 = max(r5:4,r13:12)
		r3:2 = min(r3:2,r19:18)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r1:0)
		v4.w = vinsert(r4)
	}
	{
		r17:16 = min(r7:6,r1:0)
		r3:2 = max(r3:2,r13:12)
	}
	{
		r7:6 = add(r5:4,r9:8)
		r5:4 = add(r17:16,r9:8)
		v4 = valign(v4,v4,#4)
	}
	{
		r3:2 = asr(r7:6,#31)
		v5.w = vinsert(r2)
		r6 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r28 = asr(r6,#31)
	}
	{
		r5:4 = min(r5:4,r19:18)
		r17:16 = mpyu(r15,r6)
		v5 = valign(v5,v5,#4)
	}
	{
		r5:4 = max(r5:4,r13:12)
		r3:2 = min(r3:2,r19:18)
	}
	{
		r17 += mpyi(r15,r28)
		v4.w = vinsert(r4)
		r5:4 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r3:2,r13:12)
		r17 += mpyi(r6,r14)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r1:0)
		v5.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		r5:4 = min(r7:6,r1:0)
		r3:2 = add(r3:2,r9:8)
		r7:6 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r3:2 = asr(r3:2,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = add(r5:4,r9:8)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r7:6 = min(r7:6,r19:18)
	}
	{
		r5:4 = asr(r5:4,#31)
		v5.w = vinsert(r2)
	}
	{
		r3:2 = max(r7:6,r13:12)
		r5:4 = min(r5:4,r19:18)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r5:4 = max(r5:4,r13:12)
		v5 = valign(v5,v5,#4)
	}
	{
		r7:6 = add(r7:6,r9:8)
		v4.w = vinsert(r2)
	}
	{
		r3:2 = asr(r7:6,#31)
		v5.w = vinsert(r4)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r1:0)
		r3:2 = min(r3:2,r19:18)
		v4 = valign(v4,v4,#4)
	}
	{
		r7:6 = min(r25:24,r1:0)
		r3:2 = max(r3:2,r13:12)
		v5 = valign(v5,v5,#4)
	}
	{
		r5:4 = add(r5:4,r9:8)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r3:2 = asr(r5:4,#31)
		v5.w = vinsert(r2)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r5:4 = asr(r7:6,#31)
		r7 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r13:12)
		r6 = asr(r7,#31)
		r28 = r7
		v5 = valign(v5,v5,#4)
	}
	{
		r25:24 = mpyu(r15,r7)
		r5:4 = min(r5:4,r19:18)
	}
	{
		r25 += mpyi(r15,r6)
		v4.w = vinsert(r2)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r7:6,r1:0)
		r5:4 = max(r5:4,r13:12)
		r7:6 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r3:2,r9:8)
		v5.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		r5:4 = min(r7:6,r1:0)
		r3:2 = asr(r3:2,#31)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r3:2 = min(r3:2,r19:18)
		v5 = valign(v5,v5,#4)
	}
	{
		r5:4 = add(r5:4,r9:8)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r5:4 = asr(r5:4,#31)
	}
	{
		r3:2 = min(r5:4,r19:18)
		v5.w = vinsert(r2)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r7:6 = asr(r7:6,#31)
	}
	{
		r5:4 = min(r7:6,r19:18)
		v4.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r5:4,r13:12)
		r5:4 = min(r23:22,r1:0)
	}
	{
		r7:6 = min(r7:6,r1:0)
		v5.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		r3:2 = add(r5:4,r9:8)
		r5:4 = add(r7:6,r9:8)
		r7 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r6 = asr(r7,#31)
		r3:2 = asr(r3:2,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r23:22 = mpyu(r15,r7)
		r3:2 = min(r3:2,r19:18)
	}
	{
		r23 += mpyi(r15,r6)
		r25 += mpyi(r28,r14)
		r28 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r23 += mpyi(r7,r14)
		r7:6 = min(r11:10,r1:0)
	}
	{
		r7:6 = min(r21:20,r1:0)
		r3:2 = max(r3:2,r13:12)
		r21:20 = memd(r30+#-4400)
		memd(r30+#-1328) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r25:24,r1:0)
		v5.w = vinsert(r2)
		r23:22 = memd(r30+#-5424)
		memd(r30+#-1072) = r23:22
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r27:26,r1:0)
		r5:4 = asr(r5:4,#31)
		r27:26 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r23:22,r1:0)
		r5:4 = min(r5:4,r19:18)
		v5 = valign(v5,v5,#4)
		r23:22 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r13:12)
		r13:12 = min(r21:20,r1:0)
		r21:20 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r1:0)
		v4.w = vinsert(r4)
	}
	{
		r17:16 = min(r23:22,r1:0)
		r19:18 = min(r21:20,r1:0)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r23:22,r1:0)
		r11:10 = min(r27:26,r1:0)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r1:0)
		r7:6 = add(r7:6,r9:8)
		r1:0 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r5:4,r9:8)
		r21:20 = add(r21:20,r9:8)
	}
	{
		r19:18 = add(r19:18,r9:8)
		r23:22 = add(r23:22,r9:8)
	}
	{
		r17:16 = add(r17:16,r9:8)
		r11:10 = add(r11:10,r9:8)
	}
	{
		r15:14 = add(r15:14,r9:8)
		r13:12 = add(r13:12,r9:8)
	}
	{
		r27:26 = add(r25:24,r9:8)
		r3:2 = add(r3:2,r9:8)
	}
	{
		r9:8 = add(r1:0,r9:8)
		r1:0 = asr(r3:2,#31)
	}
	{
		r25:24 = asr(r9:8,#31)
		r9:8 = asr(r27:26,#31)
	}
	{
		r27:26 = asr(r13:12,#31)
		r13:12 = asr(r15:14,#31)
	}
	{
		r15:14 = asr(r17:16,#31)
		r17:16 = asr(r21:20,#31)
	}
	{
		r21:20 = asr(r23:22,#31)
		r22 = ##2147483647
		r23 = #0
	}
	{
		r3:2 = asr(r11:10,#31)
		r11:10 = asr(r19:18,#31)
	}
	{
		r19:18 = min(r21:20,r23:22)
		r21:20 = min(r27:26,r23:22)
	}
	{
		r17:16 = min(r17:16,r23:22)
		r26 = ##-2147483648
		r27 = #-1
	}
	{
		r17:16 = max(r17:16,r27:26)
		r9:8 = min(r9:8,r23:22)
	}
	{
		r11:10 = min(r11:10,r23:22)
		v4.w = vinsert(r16)
	}
	{
		r9:8 = max(r9:8,r27:26)
		r7:6 = asr(r7:6,#31)
	}
	{
		r21:20 = max(r21:20,r27:26)
		r11:10 = max(r11:10,r27:26)
		v4 = valign(v4,v4,#4)
		r9 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r20)
		v4.w = vinsert(r10)
		r9 = add(r9,#1408)
	}
	{
		r7:6 = min(r7:6,r23:22)
		r10 = add(r28,##-45184)
		v6 = vmem(r9+#0)
	}
	{
		r7:6 = max(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
		v5 = valign(v5,v5,#4)
		vmem(r10+#0) = v6
	}
	{
		r3:2 = min(r3:2,r23:22)
		r21:20 = combine(r23,r22)
		r7 = memw(r10+#120)
		memw(r30+#-1584) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r17:16 = min(r25:24,r23:22)
		v5.w = vinsert(r8)
		r7 = memw(r10+#124)
		memw(r30+#-1328) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r15:14 = min(r15:14,r23:22)
		r5:4 = min(r5:4,r23:22)
		r7 = memw(r10+#112)
		memw(r30+#-3120) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r23:22 = max(r19:18,r27:26)
		v5 = valign(v5,v5,#4)
		r7 = memw(r10+#116)
		memw(r30+#-1840) = r7.new
	}                                       // 4-byte Folded Spill
	{
		v5.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
		r6 = memw(r10+#104)
	}
	{
		r25:24 = max(r17:16,r27:26)
		v4.w = vinsert(r22)
		memw(r30+##-4656) = r6
	}                                       // 4-byte Folded Spill
	{
		r3:2 = max(r3:2,r27:26)
		r6 = memw(r10+#108)
		memw(r30+##-4400) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r15:14,r27:26)
		v5 = valign(v5,v5,#4)
	}
	{
		r15:14 = max(r5:4,r27:26)
		v4 = valign(v4,v4,#4)
		v6 = v10
		r7 = memw(r10+#96)
	}
	{
		v4.w = vinsert(r6)
		v5.w = vinsert(r24)
		memw(r30+##-10032) = r7
	}                                       // 4-byte Folded Spill
	{
		r17:16 = min(r13:12,r21:20)
		r7 = memw(r10+#100)
		memw(r30+##-9264) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = min(r1:0,r21:20)
		r6 = memw(r10+#88)
		memw(r30+##-11056) = r6.new
	}                                       // 4-byte Folded Spill
	{
		v5 = valign(v5,v5,#4)
		r6 = memw(r10+#92)
		memw(r30+##-8752) = r6.new
	}                                       // 4-byte Folded Spill
	{
		v5.w = vinsert(r14)
		v4 = valign(v4,v4,#4)
		r4 = memw(r10+#80)
	}
	{
		v4.w = vinsert(r2)
		r7:6 = combine(r27,r26)
		memw(r30+##-8496) = r4
	}                                       // 4-byte Folded Spill
	{
		r19:18 = max(r17:16,r7:6)
		r4 = memw(r10+#84)
		memw(r30+##-11184) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r1:0,r7:6)
		v5 = valign(v5,v5,#4)
		r27 = memw(r10+#72)
	}
	{
		v5.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
		r23 = memw(r10+#76)
		r22 = memw(r10+#64)
	}
	{
		v4.w = vinsert(r18)
		r28 = memw(r10+#68)
		r8 = memw(r10+#56)
	}
	{
		v5 = valign(v5,v5,#4)
		r9 = memw(r10+#60)
		r13 = memw(r10+#48)
	}
	{
		r26 = memw(r10+#52)
		r15 = memw(r10+#40)
	}
	{
		r14 = memw(r10+#44)
		r11 = memw(r10+#32)
	}
	{
		r20 = memw(r10+#36)
		r21 = memw(r10+#24)
	}
	{
		r24 = memw(r10+#28)
		r25 = memw(r10+#16)
	}
	{
		r12 = memw(r10+#20)
		r2 = memw(r10+#8)
	}
	{
		r3 = memw(r10+#12)
		r0 = memw(r10+#0)
	}
	{
		r4 = asr(r0,#31)
		r5 = memw(r10+#4)
		r1 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r10 = asr(r5,#31)
	}
	{
		r7:6 = mpyu(r1,r5)
		r19:18 = mpyu(r1,r0)
	}
	{
		r7 += mpyi(r1,r10)
		r19 += mpyi(r1,r4)
		r10 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r1,r3)
		r4 = asr(r3,#31)
	}
	{
		r7 += mpyi(r5,r10)
		r17 += mpyi(r1,r4)
	}
	{
		r7:6 = mpyu(r1,r2)
		r5 = asr(r2,#31)
		memd(r30+##-10288) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r1,r5)
		r17 += mpyi(r3,r10)
	}
	{
		r7 += mpyi(r2,r10)
		r19 += mpyi(r0,r10)
		memd(r30+#-7728) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r17:16 = mpyu(r1,r25)
		r0 = asr(r25,#31)
		memd(r30+##-8240) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r1,r0)
		r2 = asr(r12,#31)
	}
	{
		r7:6 = mpyu(r1,r12)
		r0 = asr(r21,#31)
	}
	{
		r5:4 = mpyu(r1,r21)
		r7 += mpyi(r1,r2)
	}
	{
		r5 += mpyi(r1,r0)
		r7 += mpyi(r12,r10)
	}
	{
		r5 += mpyi(r21,r10)
		r17 += mpyi(r25,r10)
		r6 = r10
		memd(r30+#-5680) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r11)
		r10 = asr(r11,#31)
		memd(r30+#-5424) = r5:4
		memd(r30+#-7984) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r10)
		r0 = asr(r24,#31)
	}
	{
		r3:2 = mpyu(r1,r24)
		r5:4 = mpyu(r1,r20)
		r17:16 = combine(r5,r4)
	}
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r20,#31)
	}
	{
		r5 += mpyi(r1,r0)
		r3 += mpyi(r24,r6)
	}
	{
		r17 += mpyi(r11,r6)
		r5 += mpyi(r20,r6)
		memd(r30+#-5168) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r15)
		r0 = asr(r15,#31)
		memd(r30+#-7472) = r17:16
		memd(r30+#-2608) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r17:16 = mpyu(r1,r14)
		r2 = asr(r14,#31)
	}
	{
		r5 += mpyi(r1,r0)
		r17 += mpyi(r1,r2)
	}
	{
		r3:2 = mpyu(r1,r13)
		r0 = asr(r13,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r15,r6)
		r15 = #0
	}
	{
		r3 += mpyi(r13,r6)
		r0 = asr(r26,#31)
		memd(r30+#-2096) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r26)
		r2 = asr(r8,#31)
		memd(r30+#-2352) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r25:24 = mpyu(r1,r8)
		r5 += mpyi(r1,r0)
	}
	{
		r25 += mpyi(r1,r2)
		r0 = asr(r9,#31)
	}
	{
		r3:2 = mpyu(r1,r9)
		r5 += mpyi(r26,r6)
	}
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r28,#31)
		memd(r30+#-3376) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r9,r6)
		r13:12 = mpyu(r1,r28)
	}
	{
		r5:4 = mpyu(r1,r22)
		r2 = asr(r22,#31)
		memd(r30+#-3888) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r1,r0)
		r5 += mpyi(r1,r2)
	}
	{
		r3:2 = mpyu(r1,r27)
		r0 = asr(r27,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r13 += mpyi(r28,r6)
	}
	{
		r3 += mpyi(r27,r6)
		r0 = asr(r23,#31)
		memd(r30+##-9776) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r3 = memw(r30+##-8496)
		memd(r30+##-9520) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r1,r23)
		r17 += mpyi(r14,r6)
		r12 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r1,r0)
		r2 = asr(r3,#31)
		r7 = r3
	}
	{
		r21:20 = mpyu(r1,r3)
		r0 = asr(r12,#31)
		memd(r30+#-2864) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r1,r2)
		r3:2 = mpyu(r1,r12)
	}
	{
		r3 += mpyi(r1,r0)
		r25 += mpyi(r8,r6)
	}
	{
		r3 += mpyi(r12,r6)
		r11 += mpyi(r23,r6)
		memd(r30+#-3632) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r2 = memw(r30+##-11056)
		memd(r30+##-8496) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r6)
		r21 += mpyi(r7,r6)
		r3 = r6
	}
	{
		r6 = memw(r30+##-8752)
		memd(r30+##-9008) = r11:10
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r1,r2)
		r0 = asr(r2,#31)
		r12 = r2
	}
	{
		r9:8 = mpyu(r1,r6)
		r2 = asr(r6,#31)
		r14 = r6
	}
	{
		r11 += mpyi(r1,r0)
		r9 += mpyi(r1,r2)
		r2 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r12,r3)
		r9 += mpyi(r14,r3)
	}
	{
		r0 = asr(r2,#31)
		memd(r30+##-11056) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r1,r2)
		r10 = ##1073741824
	}
	{
		r8 = ##-1073741825
		memd(r30+##-8752) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r1,r0)
		r9 = ##2147483647
		r11 = #0
	}
	{
		r14 = ##2147483647
		r17:16 = combine(r7,r6)
		r6 = r1
	}
	{
		r1:0 = min(r19:18,r9:8)
		r17 += mpyi(r2,r3)
		r2 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = add(r1:0,r11:10)
	}
	{
		r1:0 = asr(r19:18,#31)
		r28 = asr(r2,#31)
		r13:12 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r1:0,r15:14)
		memd(r30+##-10032) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r23:22 = min(r13:12,r9:8)
		r0 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = mpyu(r6,r2)
		r12 = ##-2147483648
		r13 = #-1
	}
	{
		r27 += mpyi(r6,r28)
		r25:24 = add(r23:22,r11:10)
		r1 = r0
	}
	{
		r17:16 = max(r19:18,r13:12)
		r28 = asr(r0,#31)
	}
	{
		r19:18 = mpyu(r6,r0)
		r0 = r3
	}
	{
		r19 += mpyi(r6,r28)
		r27 += mpyi(r2,r3)
	}
	{
		r19 += mpyi(r1,r3)
		v6.w = vinsert(r16)
		r3:2 = combine(r9,r8)
	}
	{
		r23:22 = asr(r25:24,#31)
		memd(r30+##-9264) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r5:4 = min(r5:4,r3:2)
		r27:26 = memd(r30+##-8240)
		memd(r30+#-4656) = r19:18
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r15:14)
		r23:22 = min(r27:26,r3:2)
		r1 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = max(r17:16,r13:12)
		r19:18 = add(r23:22,r11:10)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = asr(r19:18,#31)
		v6.w = vinsert(r16)
		r7 = r1
		r9:8 = combine(r15,r14)
	}
	{
		r25:24 = mpyu(r6,r1)
		r28 = asr(r1,#31)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r19:18 = min(r19:18,r15:14)
		v6 = valign(v6,v6,#4)
	}
	{
		r25 += mpyi(r6,r28)
		r17:16 = add(r17:16,r11:10)
	}
	{
		r19:18 = max(r19:18,r13:12)
		r28 = asr(r1,#31)
	}
	{
		r23:22 = mpyu(r6,r1)
		v6.w = vinsert(r18)
	}
	{
		r23 += mpyi(r6,r28)
		r17:16 = asr(r17:16,#31)
	}
	{
		r19:18 = min(r17:16,r15:14)
		r25 += mpyi(r7,r0)
		r7 = r0
		v6 = valign(v6,v6,#4)
	}
	{
		r23 += mpyi(r1,r0)
		r19:18 = max(r19:18,r13:12)
		r1:0 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r3:2)
		v6.w = vinsert(r18)
	}
	{
		r19:18 = add(r1:0,r11:10)
		r5:4 = add(r5:4,r11:10)
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = asr(r19:18,#31)
		r27:26 = asr(r5:4,#31)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r19:18,r15:14)
		r27:26 = min(r27:26,r15:14)
		v6 = valign(v6,v6,#4)
	}
	{
		r15:14 = min(r5:4,r3:2)
		r19:18 = max(r19:18,r13:12)
		r5:4 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r5:4,r3:2)
		v6.w = vinsert(r18)
		r5:4 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = max(r27:26,r13:12)
		r19:18 = add(r19:18,r11:10)
	}
	{
		r19:18 = asr(r19:18,#31)
		v7.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		r5:4 = min(r5:4,r3:2)
		r19:18 = min(r19:18,r9:8)
	}
	{
		r19:18 = max(r19:18,r13:12)
		r27:26 = add(r5:4,r11:10)
		v7 = valign(v7,v7,#4)
		r4 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = asr(r27:26,#31)
		v7.w = vinsert(r18)
		r5 = r6
	}
	{
		r19:18 = min(r27:26,r9:8)
		r15:14 = add(r15:14,r11:10)
	}
	{
		r19:18 = max(r19:18,r13:12)
		r28 = asr(r0,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r17:16 = mpyu(r6,r0)
		v6.w = vinsert(r18)
		r19:18 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r17 += mpyi(r6,r28)
		r15:14 = asr(r15:14,#31)
	}
	{
		r17 += mpyi(r0,r7)
		r19:18 = min(r19:18,r3:2)
		r1:0 = combine(r11,r10)
		r11:10 = combine(r9,r8)
	}
	{
		r15:14 = min(r15:14,r9:8)
		r19:18 = add(r19:18,r1:0)
		v6 = valign(v6,v6,#4)
	}
	{
		r9:8 = max(r15:14,r13:12)
		r19:18 = asr(r19:18,#31)
	}
	{
		r27:26 = min(r21:20,r3:2)
		v7.w = vinsert(r8)
		r21:20 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r19:18,r11:10)
		r9:8 = add(r27:26,r1:0)
		r27:26 = combine(r11,r10)
	}
	{
		r21:20 = min(r21:20,r3:2)
		r28 = asr(r4,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r15:14 = mpyu(r6,r4)
		r19:18 = max(r19:18,r13:12)
	}
	{
		r15 += mpyi(r6,r28)
		r21:20 = add(r21:20,r1:0)
		r28 = r7
	}
	{
		r9:8 = asr(r9:8,#31)
		v7.w = vinsert(r18)
	}
	{
		r9:8 = min(r9:8,r11:10)
		r21:20 = asr(r21:20,#31)
	}
	{
		r15 += mpyi(r4,r7)
		r21:20 = min(r21:20,r11:10)
		r7:6 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r9:8,r13:12)
		v7 = valign(v7,v7,#4)
		r11:10 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		v7.w = vinsert(r8)
	}
	{
		r9:8 = min(r11:10,r3:2)
	}
	{
		r19:18 = max(r21:20,r13:12)
		r9:8 = add(r9:8,r1:0)
	}
	{
		r9:8 = asr(r9:8,#31)
		v6.w = vinsert(r18)
		v7 = valign(v7,v7,#4)
		r19:18 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r27:26)
		r7:6 = add(r7:6,r1:0)
		r4 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = min(r19:18,r3:2)
		r7:6 = asr(r7:6,#31)
		r18 = r4
		v6 = valign(v6,v6,#4)
	}
	{
		r9:8 = max(r9:8,r13:12)
		r11:10 = add(r11:10,r1:0)
	}
	{
		r7:6 = min(r7:6,r27:26)
		v7.w = vinsert(r8)
	}
	{
		r7:6 = max(r7:6,r13:12)
		r11:10 = asr(r11:10,#31)
	}
	{
		r21:20 = min(r11:10,r27:26)
		r7 = asr(r4,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r11:10 = mpyu(r5,r4)
		v7.w = vinsert(r6)
	}
	{
		r11 += mpyi(r5,r7)
		r9:8 = max(r21:20,r13:12)
		r7:6 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		v6.w = vinsert(r8)
		r5:4 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r3:2)
		r7:6 = add(r7:6,r1:0)
		v7 = valign(v7,v7,#4)
		r21:20 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r21:20,r3:2)
		r7:6 = asr(r7:6,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r9:8 = add(r9:8,r1:0)
	}
	{
		r7:6 = max(r7:6,r13:12)
		r5:4 = add(r5:4,r1:0)
	}
	{
		r9:8 = asr(r9:8,#31)
		v7.w = vinsert(r6)
	}
	{
		r7:6 = min(r9:8,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r7:6 = max(r7:6,r13:12)
		v7 = valign(v7,v7,#4)
	}
	{
		r5:4 = max(r5:4,r13:12)
		v6.w = vinsert(r6)
		r7:6 = memd(r30+#-1072)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r3:2)
		v7.w = vinsert(r4)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r7:6,r3:2)
		r5:4 = add(r5:4,r1:0)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r9:8 = add(r21:20,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r7:6 = add(r7:6,r1:0)
		r9:8 = asr(r9:8,#31)
	}
	{
		r9:8 = min(r9:8,r27:26)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r9:8 = max(r9:8,r13:12)
		r5:4 = min(r5:4,r27:26)
	}
	{
		r9:8 = min(r23:22,r3:2)
		v7.w = vinsert(r8)
		r23:22 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r11 += mpyi(r18,r28)
		r7:6 = max(r7:6,r13:12)
		r28 = #68
	}
	{
		r5:4 = max(r5:4,r13:12)
		v6.w = vinsert(r6)
		v7 = valign(v7,v7,#4)
	}
	{
		r13:12 = min(r25:24,r3:2)
		r7:6 = min(r11:10,r3:2)
		r25:24 = memd(r30+#-3888)
		memd(r30+#-1328) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r15:14,r3:2)
		r15:14 = min(r23:22,r3:2)
		r23:22 = memd(r30+#-3632)
		memd(r30+#-1072) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r3:2)
		r17:16 = min(r23:22,r3:2)
		v6 = valign(v6,v6,#4)
		r21:20 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r3:2)
		r11:10 = min(r25:24,r3:2)
		v0 = vror(v0,r28)
		r23:22 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r23:22,r3:2)
		r15:14 = add(r15:14,r1:0)
		v2 = vror(v2,r28)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r3:2)
		r15:14 = asr(r15:14,#31)
		v0 = vor(v0,v1)
		r25:24 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r3:2)
		r23:22 = add(r23:22,r1:0)
		v2 = vor(v2,v3)
		v0.w = vadd(v0.w,v29.w):sat
	}
	{
		r3:2 = add(r5:4,r1:0)
		r23:22 = asr(r23:22,#31)
		v2.w = vadd(v2.w,v29.w):sat
	}
	{
		r23:22 = min(r23:22,r27:26)
		r5:4 = asr(r3:2,#31)
		r2 = ##-2147483648
	}
	{
		r15:14 = min(r15:14,r27:26)
		r21:20 = add(r21:20,r1:0)
		r3 = #-1
		v0.w = vasr(v0.w,v27.w)
	}
	{
		r23:22 = max(r23:22,r3:2)
		r13:12 = add(r13:12,r1:0)
		v2.w = vasr(v2.w,v27.w)
		v0.w = vmin(v12.w,v0.w)
	}
	{
		r15:14 = max(r15:14,r3:2)
		r13:12 = asr(r13:12,#31)
		v2.w = vmin(v2.w,v12.w)
		v0.w = vmax(v11.w,v0.w)
	}
	{
		r21:20 = asr(r21:20,#31)
		v6.w = vinsert(r22)
		v2.w = vmax(v11.w,v2.w)
	}
	{
		r21:20 = min(r21:20,r27:26)
		v7.w = vinsert(r14)
	}
	{
		r13:12 = min(r13:12,r27:26)
		r25:24 = add(r25:24,r1:0)
		v6 = valign(v6,v6,#4)
	}
	{
		r21:20 = max(r21:20,r3:2)
		r9:8 = add(r9:8,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r13:12 = max(r13:12,r3:2)
		r9:8 = asr(r9:8,#31)
		v0.h = vpacke(v0.w,v2.w)
		v2 = v10
	}
	{
		v6.w = vinsert(r20)
		v7.w = vinsert(r12)
		v0.h = vadd(v26.h,v0.h):sat
	}
	{
		r25:24 = asr(r25:24,#31)
		r19:18 = add(r19:18,r1:0)
		v0.h = vmin(v0.h,v28.h)
	}
	{
		r17:16 = add(r17:16,r1:0)
		r7:6 = add(r7:6,r1:0)
		v6 = valign(v6,v6,#4)
		v0.h = vmax(v24.h,v0.h)
	}
	{
		r9:8 = min(r9:8,r27:26)
		r11:10 = add(r11:10,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r1:0 = min(r25:24,r27:26)
		r9:8 = max(r9:8,r3:2)
	}
	{
		r15:14 = max(r1:0,r3:2)
		v7.w = vinsert(r8)
		r1:0 = memd(r30+#-1072)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = asr(r19:18,#31)
		v6.w = vinsert(r14)
		r14 = #68
	}
	{
		r5:4 = min(r5:4,r27:26)
		r23:22 = min(r19:18,r27:26)
		v7 = valign(v7,v7,#4)
	}
	{
		r5:4 = max(r5:4,r3:2)
		r13:12 = max(r23:22,r3:2)
		v6 = valign(v6,v6,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		v7.w = vinsert(r4)
		r4 = ##1073741824
	}
	{
		r17:16 = asr(r17:16,#31)
		v6.w = vinsert(r12)
		r5 = #0
		v4 = vror(v4,r14)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r25:24 = min(r17:16,r27:26)
		v7 = valign(v7,v7,#4)
	}
	{
		r7:6 = max(r7:6,r3:2)
		r1:0 = add(r1:0,r5:4)
		v6 = valign(v6,v6,#4)
	}
	{
		r5:4 = max(r25:24,r3:2)
		v7.w = vinsert(r6)
		r7:6 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = asr(r11:10,#31)
		v6.w = vinsert(r4)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = asr(r1:0,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		v5.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		r5:4 = max(r5:4,r3:2)
		r1:0 = max(r1:0,r3:2)
	}
	{
		v6.w = vinsert(r4)
		v7.w = vinsert(r0)
		v1 = valign(v5,v5,#4)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-39424)
		v1 = vor(v4,v1)
		v5 = v10
	}
	{
		r1 = setbit(r0,#7)
		v4 = valign(v7,v7,#4)
		v1.w = vadd(v1.w,v29.w):sat
		v7 = v10
	}
	{
		v3 = vror(v6,r14)
		vmem(r1+#0) = v0
	}
	{
		v1.w = vasr(v1.w,v27.w)
		v3 = vor(v3,v4)
		r5:4 = memd(r0+#192)
	}
	{
		v3.w = vadd(v3.w,v29.w):sat
		v1.w = vmin(v12.w,v1.w)
	}
	{
		v1.w = vmax(v11.w,v1.w)
	}
	{
		v3.w = vasr(v3.w,v27.w)
	}
	{
		v3.w = vmin(v3.w,v12.w)
	}
	{
		v3.w = vmax(v11.w,v3.w)
	}
	{
		v1.h = vpacke(v1.w,v3.w)
		v3 = v10
	}
	{
		v1.h = vadd(v26.h,v1.h):sat
	}
	{
		v1.h = vmin(v28.h,v1.h)
	}
	{
		v0.h = vmax(v25.h,v1.h)
		vmem(r0+#0) = v0.new
	}
	{
		memd(r30+#-3632) = r5:4
		r3:2 = memd(r0+#200)

	} :mem_noshuf
	{
		r1 = memw(r30+#-3896)
		memd(r30+#-3888) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,##-37120)
		r3:2 = memd(r0+#208)
	}
	{
		memd(r30+#-4400) = r3:2
		r3:2 = memd(r0+#216)

	} :mem_noshuf
	{
		memd(r30+#-5168) = r3:2
		r3:2 = memd(r0+#224)

	} :mem_noshuf
	{
		memd(r30+#-5424) = r3:2
		r3:2 = memd(r0+#232)

	} :mem_noshuf
	{
		memd(r30+#-7472) = r3:2
		r3:2 = memd(r0+#240)

	} :mem_noshuf
	{
		memd(r30+#-7728) = r3:2
		r3:2 = memd(r0+#248)

	} :mem_noshuf
	{
		memd(r30+#-7984) = r3:2
		r3:2 = memd(r0+#128)

	} :mem_noshuf
	{
		memd(r30+#-3376) = r3:2
		r7:6 = memd(r0+#136)

	} :mem_noshuf
	{
		r3 = memw(r0+#124)
		memd(r30+#-4656) = r7:6

	} :mem_noshuf
	{
		r5:4 = memd(r0+#144)
	}
	{
		memd(r30+##-8240) = r5:4
		r2 = memw(r0+#120)

	} :mem_noshuf
	{
		memd(r30+#-3120) = r3:2
		r19:18 = memd(r0+#160)

	} :mem_noshuf
	{
		r17:16 = memd(r0+#152)
		r3:2 = memd(r0+#64)
	}
	{
		r25:24 = memd(r0+#168)
		r27:26 = memd(r0+#176)
	}
	{
		r11:10 = memd(r0+#184)
		memd(r30+#-1072) = r3:2

	} :mem_noshuf
	{
		r3:2 = memd(r0+#72)
	}
	{
		memd(r30+#-1328) = r3:2
		r3:2 = memd(r0+#80)

	} :mem_noshuf
	{
		memd(r30+#-1584) = r3:2
		r3:2 = memd(r0+#88)

	} :mem_noshuf
	{
		memd(r30+#-1840) = r3:2
		r3:2 = memd(r0+#96)

	} :mem_noshuf
	{
		memd(r30+#-2096) = r3:2
		r3:2 = memd(r0+#104)

	} :mem_noshuf
	{
		memd(r30+#-2608) = r3:2
		r3:2 = memd(r0+#112)

	} :mem_noshuf
	{
		memd(r30+#-2864) = r3:2
		r15:14 = memd(r0+#0)

	} :mem_noshuf
	{
		r13:12 = memd(r0+#8)
		r9:8 = memd(r0+#16)
	}
	{
		r21:20 = memd(r0+#24)
		r23:22 = memd(r0+#32)
	}
	{
		r7:6 = memd(r0+#48)
		r3:2 = memd(r0+#56)
	}
	{
		r5:4 = memd(r0+#40)
		memd(r1+#56) = r3:2

	} :mem_noshuf
	{
		memd(r1+#48) = r7:6
		memd(r1+#40) = r5:4
	}
	{
		memd(r1+#32) = r23:22
		memd(r1+#24) = r21:20
	}
	{
		memd(r1+#16) = r9:8
		memd(r1+#8) = r13:12
	}
	{
		memd(r1+#0) = r15:14
		r0 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r6 = add(r0,##-45312)
		v0 = vmem(r1+#0)
	}
	{
		vmem(r6+#0) = v0
	}
	{
		r0 = memw(r6+#56)
		r1 = memw(r6+#60)
	}
	{
		memd(r30+#-2352) = r1:0
		r0 = memw(r6+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r6+#52)
	}
	{
		memd(r30+#-5680) = r1:0
		r0 = memw(r6+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r6+#44)
	}
	{
		memd(r30+##-8496) = r1:0
		r22 = memw(r6+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r23 = memw(r6+#36)
		r2 = memw(r6+#24)
	}
	{
		r3 = memw(r6+#28)
		r4 = memw(r6+#16)
	}
	{
		r5 = memw(r6+#20)
		r0 = memw(r6+#0)
	}
	{
		r1 = memw(r6+#4)
		r20 = memw(r6+#8)
	}
	{
		r0 = vtrunehb(r1:0)
		r21 = memw(r6+#12)
		r6 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = vtrunehb(r21:20)
		r6 = add(r6,##-37376)
	}
	{
		memd(r6+#56) = r11:10
		memd(r6+#48) = r27:26
	}
	{
		memd(r6+#40) = r25:24
		memd(r6+#32) = r19:18
	}
	{
		memd(r6+#24) = r17:16
		r9:8 = memd(r30+##-8240)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r6+#16) = r9:8
		r9:8 = memd(r30+#-4656)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r6+#8) = r9:8
		r9:8 = memd(r30+#-3376)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r6+#0) = r9:8
		r7 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r7 = add(r7,##-45440)
		v0 = vmem(r6+#0)
	}
	{
		v0 = v10
		vmem(r7+#0) = v0
	}
	{
		v0.w = vinsert(r0)
		r8 = memw(r7+#56)
		r9 = memw(r7+#60)
	}
	{
		memd(r30+#-3376) = r9:8
	}                                       // 8-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r8 = memw(r7+#48)
	}
	{
		v0.w = vinsert(r1)
		r9 = memw(r7+#52)
	}
	{
		memd(r30+#-4656) = r9:8
	}                                       // 8-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r8 = memw(r7+#40)
		r9 = memw(r7+#44)
	}
	{
		memd(r30+##-8240) = r9:8
		r8 = memw(r7+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r9 = memw(r7+#36)
	}
	{
		memd(r30+##-8752) = r9:8
		r8 = memw(r7+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r9 = memw(r7+#28)
	}
	{
		memd(r30+##-9008) = r9:8
		r8 = memw(r7+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r9 = memw(r7+#20)
	}
	{
		memd(r30+##-9264) = r9:8
		r24 = memw(r7+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r25 = memw(r7+#4)
		r6 = memw(r7+#8)
	}
	{
		r0 = memw(r30+#-3896)
		r7 = memw(r7+#12)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-37504)
		memd(r30+##-9520) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#56) = r7:6
		r7:6 = memd(r30+#-7728)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#48) = r7:6
		r7:6 = memd(r30+#-7472)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#40) = r7:6
		r7:6 = memd(r30+#-5424)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#32) = r7:6
		r7:6 = memd(r30+#-5168)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#24) = r7:6
		r7:6 = memd(r30+#-4400)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#16) = r7:6
		r7:6 = memd(r30+#-3888)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r7:6
		r7:6 = memd(r30+#-3632)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r7:6
		r6 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		r4 = vtrunehb(r25:24)
		v1 = vmem(r0+#0)
	}
	{
		r0 = vtrunehb(r3:2)
		v0.w = vinsert(r0)
		r28 = add(r6,##-45568)
	}
	{
		vmem(r28+#0) = v1
	}
	{
		v0 = valign(v0,v0,#4)
		r6 = memw(r28+#56)
		r7 = memw(r28+#60)
	}
	{
		r0 = vtrunehb(r23:22)
		v0.w = vinsert(r0)
		memd(r30+#-3632) = r7:6
		r26 = memw(r28+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r27 = memw(r28+#52)
	}
	{
		v0.w = vinsert(r0)
		r20 = memw(r28+#40)
		r21 = memw(r28+#44)
	}
	{
		r18 = memw(r28+#32)
		r19 = memw(r28+#36)
	}
	{
		v0 = valign(v0,v0,#4)
		r16 = memw(r28+#24)
		r17 = memw(r28+#28)
	}
	{
		r10 = memw(r28+#16)
		r11 = memw(r28+#20)
	}
	{
		r12 = memw(r28+#0)
		r13 = memw(r28+#4)
	}
	{
		r5 = vtrunehb(r13:12)
		r14 = memw(r28+#8)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-37248)
		r15 = memw(r28+#12)
		r3:2 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		v2.w = vinsert(r5)
		memd(r0+#48) = r3:2
		r3:2 = memd(r30+#-2608)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		v2 = valign(v2,v2,#4)
		memd(r0+#40) = r3:2
		r3:2 = memd(r30+#-2096)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#32) = r3:2
		r3:2 = memd(r30+#-1840)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#24) = r3:2
		r3:2 = memd(r30+#-1584)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#16) = r3:2
		r3:2 = memd(r30+#-1328)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r3:2
		r3:2 = memd(r30+#-1072)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r3:2
		r3:2 = memd(r30+#-3120)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#56) = r3:2
		r1 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r28 = add(r1,##-45696)
		v1 = vmem(r0+#0)
	}
	{
		v1 = v10
		vmem(r28+#0) = v1
	}
	{
		v1.w = vinsert(r4)
		r1:0 = memd(r30+##-8496)
		r22 = memw(r28+#56)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		r23 = memw(r28+#60)
		r8 = memw(r28+#48)
	}
	{
		v0.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
		r9 = memw(r28+#52)
		r6 = memw(r28+#40)
	}
	{
		r1:0 = memd(r30+#-5680)
		r7 = memw(r28+#44)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		v0 = valign(v0,v0,#4)
		r2 = memw(r28+#32)
	}
	{
		v0.w = vinsert(r0)
		r3 = memw(r28+#36)
		r0 = memw(r28+#24)
	}
	{
		r1 = memw(r28+#28)
		r24 = memw(r28+#16)
	}
	{
		r0 = vtrunehb(r1:0)
		v0 = valign(v0,v0,#4)
		r25 = memw(r28+#20)
		r12 = memw(r28+#0)
	}
	{
		r13 = memw(r28+#4)
	}
	{
		r4 = vtrunehb(r13:12)
		r13:12 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r5 = vtrunehb(r13:12)
		v3.w = vinsert(r4)
		r4 = memw(r28+#8)
	}
	{
		r12 = vtrunehb(r15:14)
		v1.w = vinsert(r5)
		r5 = memw(r28+#12)
	}
	{
		r4 = vtrunehb(r5:4)
		v2.w = vinsert(r12)
		r15:14 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
	}
	{
		r12 = vtrunehb(r11:10)
		v3.w = vinsert(r4)
		v1 = valign(v1,v1,#4)
	}
	{
		r5 = vtrunehb(r15:14)
		v2 = valign(v2,v2,#4)
		r11:10 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r4 = vtrunehb(r25:24)
		v2.w = vinsert(r12)
		r25:24 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
	}
	{
		r12 = vtrunehb(r17:16)
		v3.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
	}
	{
		r5 = vtrunehb(r11:10)
		r4 = vtrunehb(r25:24)
		v1 = valign(v1,v1,#4)
	}
	{
		v2.w = vinsert(r12)
		v1.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r19:18)
		v3.w = vinsert(r0)
	}
	{
		v2 = valign(v2,v2,#4)
	}
	{
		r0 = vtrunehb(r3:2)
		v2.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r21:20)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r0 = vtrunehb(r7:6)
		v2.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r1)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r27:26)
		r1 = vtrunehb(r7:6)
		r7:6 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r9:8)
		v2.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r5:4)
		v1.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r3:2)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r0 = vtrunehb(r23:22)
		v2.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r1)
		r1 = #100
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r7:6)
		v3.w = vinsert(r0)
		v2 = vror(v2,r1)
	}
	{
		v0.w = vinsert(r0)
		v1 = vror(v1,r1)
		v2 = vor(v2,v9)
	}
	{
		v3 = vror(v3,r1)
		v1 = vor(v1,v9)
		r0 = memw(r30+##-22192)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r1)
		v2.ub = vmin(v2.ub,v22.ub)
		v3 = vor(v3,v9)
		v1.ub = vmin(v1.ub,v22.ub)
	}
	{
		v2.ub = vmax(v2.ub,v23.ub)
		v3.ub = vmin(v3.ub,v22.ub)
		v1.ub = vmax(v1.ub,v23.ub)
		v0 = vor(v0,v9)
	}
	{
		v3.ub = vmax(v3.ub,v23.ub)
		v0.ub = vmin(v0.ub,v22.ub)
		r1 = memw(r30+##-7224)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,r0)
		v2 = vdelta(v2,v15)
	}
	{
		v1 = vdelta(v1,v18)
		v0.ub = vmax(v0.ub,v23.ub)
		r0 = memw(r30+#-816)
		memw(r30+#-1072) = r1
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		v3 = vdelta(v3,v14)
		v1 = vmux(q0,v2,v1)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,##-45824)
		v0 = vmux(q1,v3,v0)
		v3 = v10
	}
	{
		v0 = vmux(q2,v1,v0)
		v1 = v10
	}
	{
		vmemu(r0+#0) = v0
	}
	{
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#1536)
	}
	{
		v0.cur = vmem(r0+#0)
		vmem(r1+#0) = v0
	}
	{
		v0 = v10
		r0 = memw(r1+#120)
		memw(r30+#-1328) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#124)
		memw(r30+#-816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#112)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#116)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#104)
		memw(r30+#-2352) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#108)
		memw(r30+#-2096) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#96)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#100)
		memw(r30+#-2608) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#88)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#92)
		memw(r30+##-9264) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#80)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#84)
		memw(r30+##-9520) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r19 = memw(r1+#72)
		r23 = memw(r1+#76)
	}
	{
		r28 = memw(r1+#64)
		r6 = memw(r1+#68)
	}
	{
		r7 = memw(r1+#56)
		r3 = memw(r1+#60)
	}
	{
		r9 = memw(r1+#48)
		r8 = memw(r1+#52)
	}
	{
		r13 = memw(r1+#40)
		r12 = memw(r1+#44)
	}
	{
		r15 = memw(r1+#32)
		r14 = memw(r1+#36)
	}
	{
		r20 = memw(r1+#24)
		r18 = memw(r1+#28)
	}
	{
		r22 = memw(r1+#16)
		r21 = memw(r1+#20)
	}
	{
		r2 = asr(r22,#31)
		r24 = memw(r1+#8)
		r25 = memw(r1+#12)
	}
	{
		r1 = asr(r25,#31)
		r10 = memw(r1+#0)
		r11 = memw(r1+#4)
	}
	{
		r0 = asr(r11,#31)
		r26 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r27 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r26,r11)
		r17:16 = mpyu(r26,r10)
	}
	{
		r5 += mpyi(r26,r0)
		r0 = asr(r10,#31)
	}
	{
		r17 += mpyi(r26,r0)
		r5 += mpyi(r11,r27)
	}
	{
		r5:4 = mpyu(r26,r25)
		r17 += mpyi(r10,r27)
		memd(r30+#-5424) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r11:10 = mpyu(r26,r24)
		r0 = asr(r24,#31)
	}
	{
		r11 += mpyi(r26,r0)
		r5 += mpyi(r26,r1)
	}
	{
		r1:0 = mpyu(r26,r22)
		r5 += mpyi(r25,r27)
	}
	{
		r1 += mpyi(r26,r2)
		r11 += mpyi(r24,r27)
		memd(r30+#-7728) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r22,r27)
		r25:24 = mpyu(r26,r20)
	}
	{
		r1 = asr(r20,#31)
		r0 = asr(r21,#31)
		memd(r30+#-5680) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r26,r21)
		r25 += mpyi(r26,r1)
	}
	{
		r5 += mpyi(r26,r0)
		r22 = asr(r18,#31)
		r1:0 = combine(r25,r24)
	}
	{
		r25:24 = mpyu(r26,r18)
		r5 += mpyi(r21,r27)
	}
	{
		r25 += mpyi(r26,r22)
		r1 += mpyi(r20,r27)
		memd(r30+##-8240) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r25 += mpyi(r18,r27)
		r0 = asr(r15,#31)
		memd(r30+#-5168) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r26,r15)
		r18 = asr(r14,#31)
		memd(r30+#-3376) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r25:24 = mpyu(r26,r14)
		r5 += mpyi(r26,r0)
	}
	{
		r25 += mpyi(r26,r18)
		r18 = asr(r13,#31)
		r1:0 = combine(r5,r4)
	}
	{
		r5:4 = mpyu(r26,r13)
		r1 += mpyi(r15,r27)
	}
	{
		r5 += mpyi(r26,r18)
		r25 += mpyi(r14,r27)
		memd(r30+#-4400) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r13,r27)
		r0 = asr(r12,#31)
		memd(r30+#-3120) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r15:14 = mpyu(r26,r12)
		r1 = asr(r9,#31)
		memd(r30+#-2864) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r26,r0)
		r5:4 = mpyu(r26,r9)
	}
	{
		r5 += mpyi(r26,r1)
		r15 += mpyi(r12,r27)
	}
	{
		r5 += mpyi(r9,r27)
		r0 = asr(r8,#31)
		memd(r30+#-3632) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r25:24 = mpyu(r26,r8)
		r1 = asr(r3,#31)
		memd(r30+#-3888) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r15:14 = mpyu(r26,r3)
		r25 += mpyi(r26,r0)
	}
	{
		r15 += mpyi(r26,r1)
		r0 = asr(r7,#31)
	}
	{
		r5:4 = mpyu(r26,r7)
		r15 += mpyi(r3,r27)
	}
	{
		r5 += mpyi(r26,r0)
		r0 = asr(r6,#31)
		memd(r30+#-7984) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r26,r6)
		r5 += mpyi(r7,r27)
	}
	{
		r21 += mpyi(r26,r0)
		r0 = asr(r28,#31)
		memd(r30+#-7472) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r15:14 = mpyu(r26,r28)
		r21 += mpyi(r6,r27)
	}
	{
		r15 += mpyi(r26,r0)
		r0 = asr(r23,#31)
		memd(r30+##-8496) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r26,r23)
		r1 = asr(r19,#31)
	}
	{
		r21:20 = mpyu(r26,r19)
		r5 += mpyi(r26,r0)
	}
	{
		r21 += mpyi(r26,r1)
		r5 += mpyi(r23,r27)
		r1 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-9520)
		memd(r30+##-9008) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r26,r1)
		r0 = asr(r1,#31)
		r12 = r1
	}
	{
		r3:2 = mpyu(r26,r4)
		r1 = asr(r4,#31)
		r5 = r4
	}
	{
		r7 += mpyi(r26,r0)
		r25 += mpyi(r8,r27)
		r4 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r4,#31)
	}
	{
		r9:8 = mpyu(r26,r4)
		r3 += mpyi(r26,r1)
		memd(r30+#-4656) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r12,r27)
		r9 += mpyi(r26,r0)
		r24 = ##-2147483648
	}
	{
		r13:12 = combine(r27,r26)
		r25 = #-1
		memd(r30+##-9776) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r5,r27)
		r7:6 = combine(r9,r8)
		r8 = ##-1073741825
	}
	{
		r9 = ##2147483647
		memd(r30+##-9520) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r17:16,r9:8)
		r2 = ##1073741824
		r3 = #0
	}
	{
		r7 += mpyi(r4,r27)
		r17:16 = add(r1:0,r3:2)
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r19,r27)
		r4 = asr(r0,#31)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r28,r27)
		r7:6 = mpyu(r26,r0)
		r1 = r0
		r27 = #0
	}
	{
		r23:22 = asr(r17:16,#31)
		r16 = r12
		r0 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r26,r4)
		r5 = asr(r0,#31)
	}
	{
		r19:18 = mpyu(r26,r0)
		r7 += mpyi(r1,r13)
	}
	{
		r19 += mpyi(r26,r5)
		r26 = ##2147483647
	}
	{
		r19 += mpyi(r0,r13)
		r7:6 = memd(r30+#-5424)
		memd(r30+##-9264) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r27:26)
		r0 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r23:22,r25:24)
		r19:18 = min(r7:6,r9:8)
		memd(r30+##-8752) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r5:4 = add(r19:18,r3:2)
		v0.w = vinsert(r4)
		r1 = r0
	}
	{
		r7:6 = min(r11:10,r9:8)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r28 = asr(r0,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r23:22 = mpyu(r12,r0)
		r5:4 = max(r5:4,r25:24)
		r0 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r23 += mpyi(r12,r28)
		r7:6 = add(r7:6,r3:2)
	}
	{
		r23:22 = asr(r7:6,#31)
		r5 = asr(r0,#31)
		r11:10 = combine(r23,r22)
	}
	{
		r19:18 = mpyu(r12,r0)
		v0.w = vinsert(r4)
	}
	{
		r19 += mpyi(r12,r5)
		r7:6 = min(r23:22,r27:26)
	}
	{
		r5:4 = max(r7:6,r25:24)
		r19 += mpyi(r0,r13)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r7:6,r9:8)
		v0.w = vinsert(r4)
		r12 = memw(r30+#-2096)
		memd(r30+#-2352) = r19:18
	}                                       // 4-byte Folded Reload
	{
		r5:4 = add(r19:18,r3:2)
		r28 = asr(r12,#31)
		r7:6 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = mpyu(r16,r12)
		r5:4 = asr(r5:4,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r23 += mpyi(r16,r28)
		r5:4 = min(r5:4,r27:26)
		r28 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r11 += mpyi(r1,r13)
	}
	{
		r5:4 = max(r5:4,r25:24)
		r1:0 = add(r7:6,r3:2)
		r7:6 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		v0.w = vinsert(r4)
		memd(r30+#-2608) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r19:18 = mpyu(r16,r28)
		r5 = asr(r28,#31)
	}
	{
		r19 += mpyi(r16,r5)
		r5:4 = min(r7:6,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = add(r5:4,r3:2)
	}
	{
		r1:0 = max(r1:0,r25:24)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7:6 = min(r15:14,r9:8)
		v0.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = add(r7:6,r3:2)
	}
	{
		r23 += mpyi(r12,r13)
		r1:0 = asr(r1:0,#31)
		v0 = valign(v0,v0,#4)
		r12 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r25:24)
		r1:0 = min(r1:0,r27:26)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r9:8)
		v0.w = vinsert(r4)
		r7:6 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r25:24)
		r5:4 = add(r5:4,r3:2)
	}
	{
		r1:0 = min(r21:20,r9:8)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = min(r7:6,r9:8)
		r5:4 = asr(r5:4,#31)
	}
	{
		r21:20 = add(r7:6,r3:2)
		r1:0 = add(r1:0,r3:2)
		v1 = valign(v1,v1,#4)
		r6 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = max(r5:4,r25:24)
		r21:20 = asr(r21:20,#31)
	}
	{
		r5:4 = min(r21:20,r27:26)
		v1.w = vinsert(r4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = max(r5:4,r25:24)
	}
	{
		r1:0 = max(r1:0,r25:24)
		v0.w = vinsert(r4)
		v1 = valign(v1,v1,#4)
	}
	{
		r21:20 = mpyu(r16,r6)
		r1 = asr(r6,#31)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r21 += mpyi(r16,r1)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r5:4,r9:8)
		r19 += mpyi(r28,r13)
		r5:4 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r9:8)
		r1:0 = add(r1:0,r3:2)
		v1 = valign(v1,v1,#4)
		r15:14 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r28 = asr(r12,#31)
	}
	{
		r11:10 = mpyu(r16,r12)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r11 += mpyi(r16,r28)
		r5:4 = add(r5:4,r3:2)
		r28 = r13
	}
	{
		r1:0 = max(r1:0,r25:24)
		r5:4 = asr(r5:4,#31)
	}
	{
		r11 += mpyi(r12,r13)
		v1.w = vinsert(r0)
	}
	{
		r13:12 = min(r15:14,r9:8)
		r5:4 = min(r5:4,r27:26)
	}
	{
		r5:4 = max(r5:4,r25:24)
		r15:14 = add(r13:12,r3:2)
		v1 = valign(v1,v1,#4)
	}
	{
		r15:14 = asr(r15:14,#31)
		v1.w = vinsert(r4)
		r5:4 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r15:14,r27:26)
		r21 += mpyi(r6,r28)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r25:24)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = min(r5:4,r9:8)
		v0.w = vinsert(r0)
	}
	{
		r5:4 = min(r7:6,r9:8)
		r1:0 = add(r1:0,r3:2)
		r7:6 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r5:4 = add(r5:4,r3:2)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = asr(r5:4,#31)
		r13:12 = add(r7:6,r3:2)
		r6 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r27:26)
		r13:12 = asr(r13:12,#31)
		r14 = r6
	}
	{
		r5:4 = max(r5:4,r25:24)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = min(r13:12,r27:26)
		v1.w = vinsert(r4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = max(r5:4,r25:24)
	}
	{
		r1:0 = max(r1:0,r25:24)
		v0.w = vinsert(r4)
		r5:4 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = mpyu(r16,r6)
		r1 = asr(r6,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r13 += mpyi(r16,r1)
		v1.w = vinsert(r0)
		r7:6 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r9:8)
		r5:4 = min(r7:6,r9:8)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r1:0 = add(r1:0,r3:2)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r7:6 = add(r7:6,r3:2)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r7:6 = asr(r7:6,#31)
	}
	{
		r1:0 = max(r1:0,r25:24)
		r7:6 = min(r7:6,r27:26)
	}
	{
		r13 += mpyi(r14,r28)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = max(r7:6,r25:24)
		r7:6 = min(r13:12,r9:8)
	}
	{
		r7:6 = min(r11:10,r9:8)
		r5:4 = add(r5:4,r3:2)
		v1 = valign(v1,v1,#4)
		memd(r30+#-816) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = asr(r5:4,#31)
		v0.w = vinsert(r0)
		r7:6 = memd(r30+#-2352)
		memd(r30+#-1840) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r7:6,r9:8)
		r5:4 = min(r5:4,r27:26)
		r7:6 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r7:6,r9:8)
		r5:4 = max(r5:4,r25:24)
		v0 = valign(v0,v0,#4)
		r7:6 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r7:6,r9:8)
		v1.w = vinsert(r4)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r7:6,r9:8)
		r5:4 = min(r21:20,r9:8)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r19:18,r9:8)
		r19:18 = min(r7:6,r9:8)
		r7:6 = memd(r30+#-3632)
		memd(r30+#-1584) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r7:6,r9:8)
		r1:0 = min(r23:22,r9:8)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r7:6,r9:8)
		r11:10 = add(r11:10,r3:2)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r7:6,r9:8)
		r7:6 = add(r21:20,r3:2)
	}
	{
		r25:24 = add(r9:8,r3:2)
		r9:8 = add(r1:0,r3:2)
		memd(r30+#-1328) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r5:4,r3:2)
		r7:6 = add(r13:12,r3:2)
		r5:4 = memd(r30+#-1584)
		r21:20 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = add(r15:14,r3:2)
		r5:4 = add(r5:4,r3:2)
	}
	{
		r13:12 = add(r21:20,r3:2)
		r23:22 = add(r23:22,r3:2)
		r21:20 = memd(r30+#-816)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = add(r19:18,r3:2)
		r17:16 = add(r17:16,r3:2)
	}
	{
		r3:2 = add(r21:20,r3:2)
		r5:4 = asr(r5:4,#31)
		r21:20 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = asr(r21:20,#31)
		r3:2 = asr(r17:16,#31)
		memd(r30+#-2096) = r3:2
		memd(r30+#-2864) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r17:16 = asr(r19:18,#31)
		r19:18 = asr(r23:22,#31)
	}
	{
		r23:22 = asr(r25:24,#31)
		r5:4 = asr(r13:12,#31)
		r13:12 = combine(r27,r26)
	}
	{
		r19:18 = min(r19:18,r27:26)
		r21:20 = min(r21:20,r27:26)
	}
	{
		r23:22 = min(r23:22,r27:26)
		r17:16 = min(r17:16,r27:26)
	}
	{
		r3:2 = min(r3:2,r27:26)
		r26 = ##-2147483648
		r27 = #-1
	}
	{
		r19:18 = max(r19:18,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		v0.w = vinsert(r18)
	}
	{
		r7:6 = asr(r7:6,#31)
		r15:14 = asr(r15:14,#31)
	}
	{
		r21:20 = max(r21:20,r27:26)
		r11:10 = asr(r11:10,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r25:24 = min(r15:14,r13:12)
		v0.w = vinsert(r20)
	}
	{
		r15:14 = min(r11:10,r13:12)
		r9:8 = min(r9:8,r13:12)
		r21:20 = combine(r13,r12)
	}
	{
		r1:0 = min(r1:0,r13:12)
		r7:6 = min(r7:6,r13:12)
		v0 = valign(v0,v0,#4)
		r12 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = max(r25:24,r27:26)
		r11:10 = max(r7:6,r27:26)
		r28 = add(r12,#1664)
		r12 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = max(r23:22,r27:26)
		r11 = add(r12,##-45952)
		v2 = vmem(r28+#0)
	}
	{
		v0.w = vinsert(r6)
		v1.w = vinsert(r24)
		vmem(r11+#0) = v2
	}
	{
		r1:0 = max(r1:0,r27:26)
		r9:8 = max(r9:8,r27:26)
		v2 = v10
		r6 = memw(r11+#120)
	}
	{
		r17:16 = max(r17:16,r27:26)
		v1 = valign(v1,v1,#4)
		memw(r30+#-1328) = r6
		r6 = memw(r11+#124)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v1.w = vinsert(r10)
		v0 = valign(v0,v0,#4)
		memw(r30+#-816) = r6
		r6 = memw(r11+#112)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r3:2 = max(r3:2,r27:26)
		v0.w = vinsert(r16)
		memw(r30+#-1840) = r6
		r6 = memw(r11+#116)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r5:4 = min(r5:4,r21:20)
		v1 = valign(v1,v1,#4)
		memw(r30+#-1584) = r6
		r6 = memw(r11+#104)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v1.w = vinsert(r8)
		v0 = valign(v0,v0,#4)
		memw(r30+#-2608) = r6
		r6 = memw(r11+#108)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r25:24 = max(r5:4,r27:26)
		v0.w = vinsert(r2)
		memw(r30+#-2352) = r6
		r1 = memw(r11+#96)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r15:14 = max(r15:14,r27:26)
		v1 = valign(v1,v1,#4)
		memw(r30+##-9008) = r1
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r0)
		r17:16 = combine(r27,r26)
		r1 = memw(r11+#100)
		memw(r30+#-3632) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r1 = memw(r11+#88)
		memw(r30+##-9520) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r1 = memw(r11+#92)
		memw(r30+##-9264) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r14)
		r0 = memw(r11+#80)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r24)
		r0 = memw(r11+#84)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r19 = memw(r11+#72)
	}
	{
		v1 = valign(v1,v1,#4)
		r22 = memw(r11+#76)
		r2 = memw(r11+#64)
	}
	{
		memw(r30+##-10032) = r2
		r28 = memw(r11+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r12 = memw(r11+#56)
		r2 = memw(r11+#60)
	}
	{
		r3:2 = combine(r21,r20)
		r7:6 = memd(r30+#-2864)
		memw(r30+##-5680) = r2
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r7:6,r21:20)
		r7 = memw(r11+#48)
		r23 = memw(r11+#52)
	}
	{
		r5:4 = max(r1:0,r27:26)
		r9 = memw(r11+#40)
		r8 = memw(r11+#44)
	}
	{
		v1.w = vinsert(r4)
		r20 = memw(r11+#32)
		r18 = memw(r11+#36)
	}
	{
		r21 = memw(r11+#24)
		r24 = memw(r11+#28)
	}
	{
		v1 = valign(v1,v1,#4)
		r25 = memw(r11+#16)
		r6 = memw(r11+#20)
	}
	{
		r13 = memw(r11+#8)
		r0 = memw(r11+#12)
	}
	{
		r15 = memw(r11+#0)
		r10 = memw(r11+#4)
	}
	{
		r5:4 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = asr(r5:4,#31)
		r14 = asr(r10,#31)
		r1 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = min(r27:26,r3:2)
	}
	{
		r3:2 = max(r27:26,r17:16)
		r27:26 = mpyu(r1,r15)
	}
	{
		r5:4 = mpyu(r1,r10)
		r3 = asr(r15,#31)
	}
	{
		r27 += mpyi(r1,r3)
		r5 += mpyi(r1,r14)
		r3 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r2 = asr(r13,#31)
		v1.w = vinsert(r2)
	}
	{
		r5 += mpyi(r10,r3)
		r27 += mpyi(r15,r3)
	}
	{
		r15:14 = mpyu(r1,r13)
		r10 = asr(r0,#31)
		v1 = valign(v1,v1,#4)
		memd(r30+#-7472) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r1,r2)
		r5:4 = mpyu(r1,r0)
	}
	{
		r17:16 = mpyu(r1,r25)
		r2 = asr(r25,#31)
	}
	{
		r5 += mpyi(r1,r10)
		r17 += mpyi(r1,r2)
	}
	{
		r5 += mpyi(r0,r3)
		r17 += mpyi(r25,r3)
	}
	{
		r17:16 = mpyu(r1,r6)
		r0 = asr(r6,#31)
		memd(r30+#-7984) = r5:4
		memd(r30+#-7728) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r21)
		r2 = asr(r21,#31)
	}
	{
		r17 += mpyi(r1,r0)
		r5 += mpyi(r1,r2)
	}
	{
		r11:10 = mpyu(r1,r24)
		r0 = asr(r24,#31)
	}
	{
		r11 += mpyi(r1,r0)
		r5 += mpyi(r21,r3)
	}
	{
		r11 += mpyi(r24,r3)
		r15 += mpyi(r13,r3)
		r4 = r3
		memd(r30+#-5168) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r6,r3)
		r0 = asr(r20,#31)
		memd(r30+#-4400) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r1,r20)
		r17:16 = mpyu(r1,r9)
		memd(r30+##-8240) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r9,#31)
	}
	{
		r17 += mpyi(r1,r0)
		r3 += mpyi(r20,r4)
	}
	{
		r17 += mpyi(r9,r4)
		r0 = asr(r8,#31)
		memd(r30+#-4656) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r11:10 = mpyu(r1,r8)
		r2 = asr(r7,#31)
		memd(r30+#-2096) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r17:16 = mpyu(r1,r7)
		r11 += mpyi(r1,r0)
	}
	{
		r17 += mpyi(r1,r2)
		r0 = asr(r23,#31)
	}
	{
		r3:2 = mpyu(r1,r23)
		r17 += mpyi(r7,r4)
	}
	{
		r3 += mpyi(r1,r0)
		r0 = asr(r12,#31)
		memd(r30+#-3376) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r23,r4)
		r7:6 = mpyu(r1,r12)
		r23 = #0
	}
	{
		r7 += mpyi(r1,r0)
		r3 = memw(r30+##-5680)
		memd(r30+#-3888) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r28,#31)
		r2 = asr(r3,#31)
		r5 = r3
	}
	{
		r7 += mpyi(r12,r4)
		r21:20 = mpyu(r1,r3)
	}
	{
		r11 += mpyi(r8,r4)
		r21 += mpyi(r1,r2)
		memd(r30+#-5424) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r1,r28)
		memd(r30+#-3120) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r1,r0)
		r9:8 = mpyu(r1,r19)
	}
	{
		r3 += mpyi(r28,r4)
		r21 += mpyi(r5,r4)
	}
	{
		r2 = memw(r30+##-10032)
		memd(r30+##-8496) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r1,r18)
		r13 = asr(r18,#31)
	}
	{
		r21:20 = mpyu(r1,r22)
		r0 = asr(r2,#31)
		r3 = r2
		memd(r30+#-5680) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r1,r2)
		r2 = asr(r19,#31)
	}
	{
		r7 += mpyi(r1,r0)
		r9 += mpyi(r1,r2)
		r2 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r4)
		r0 = asr(r22,#31)
		r3 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r1,r13)
		r21 += mpyi(r1,r0)
		r10 = r2
	}
	{
		r25 += mpyi(r18,r4)
		r11 = r3
		r28 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r9 += mpyi(r19,r4)
		r21 += mpyi(r22,r4)
		r19 = r4
	}
	{
		r0 = asr(r2,#31)
		r22 = ##2147483647
		memd(r30+#-2864) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r2)
		r2 = asr(r3,#31)
	}
	{
		r13:12 = mpyu(r1,r3)
		r5 += mpyi(r1,r0)
	}
	{
		r13 += mpyi(r1,r2)
		r0 = asr(r28,#31)
	}
	{
		r3:2 = mpyu(r1,r28)
		r13 += mpyi(r11,r19)
		r11 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r10,r19)
		memd(r30+##-8752) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r28,r19)
		r10 = r1
		r18 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r12 = ##-1073741825
		r13 = ##2147483647
	}
	{
		r1:0 = min(r27:26,r13:12)
		memd(r30+##-9520) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r28 = asr(r11,#31)
		r2 = ##1073741824
		r3 = #0
	}
	{
		r17:16 = mpyu(r10,r11)
		r1:0 = add(r1:0,r3:2)
		r26 = r10
	}
	{
		r17 += mpyi(r10,r28)
		r27 = asr(r18,#31)
	}
	{
		r25:24 = mpyu(r10,r18)
	}
	{
		r25 += mpyi(r10,r27)
		r1:0 = asr(r1:0,#31)
		r10 = ##-2147483648
	}
	{
		r17 += mpyi(r11,r19)
		r25 += mpyi(r18,r19)
		r11 = #-1
	}
	{
		r1:0 = min(r1:0,r23:22)
		r17 = r19
		memd(r30+##-9264) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r1:0 = max(r1:0,r11:10)
		r19:18 = memd(r30+#-7472)
		r16 = memw(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r19:18,r13:12)
		v2.w = vinsert(r0)
		memd(r30+##-9008) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r19:18,r3:2)
	}
	{
		r1:0 = asr(r1:0,#31)
		r28 = asr(r16,#31)
	}
	{
		r25:24 = mpyu(r26,r16)
		r1:0 = min(r1:0,r23:22)
		v2 = valign(v2,v2,#4)
	}
	{
		r25 += mpyi(r26,r28)
		r1:0 = max(r1:0,r11:10)
		r28 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = min(r15:14,r13:12)
		r1 = asr(r28,#31)
		r15:14 = combine(r23,r22)
	}
	{
		r23:22 = mpyu(r26,r28)
		r19:18 = add(r19:18,r3:2)
	}
	{
		r23 += mpyi(r26,r1)
		v2.w = vinsert(r0)
	}
	{
		r23 += mpyi(r28,r17)
		r19:18 = asr(r19:18,#31)
	}
	{
		r1:0 = min(r19:18,r15:14)
		r25 += mpyi(r16,r17)
		r23:22 = memd(r30+#-7984)
		memd(r30+#-2608) = r23:22
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r23:22,r13:12)
		r1:0 = max(r1:0,r11:10)
		v2 = valign(v2,v2,#4)
		r16 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r19:18,r3:2)
		v2.w = vinsert(r0)
		r23:22 = memd(r30+#-7728)
		memd(r30+#-3632) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r28 = asr(r16,#31)
	}
	{
		r25:24 = mpyu(r26,r16)
		r19:18 = min(r23:22,r13:12)
		v2 = valign(v2,v2,#4)
	}
	{
		r25 += mpyi(r26,r28)
		r1:0 = min(r1:0,r15:14)
		r28 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r23:22 = add(r19:18,r3:2)
	}
	{
		r1 = asr(r28,#31)
		v2.w = vinsert(r0)
	}
	{
		r19:18 = mpyu(r26,r28)
		r23:22 = asr(r23:22,#31)
	}
	{
		r19 += mpyi(r26,r1)
		r1:0 = min(r23:22,r15:14)
		v2 = valign(v2,v2,#4)
	}
	{
		r25 += mpyi(r16,r17)
		r1:0 = max(r1:0,r11:10)
		r23:22 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r19 += mpyi(r28,r17)
		v2.w = vinsert(r0)
		r24 = r17
		memd(r30+#-2352) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r17:16 = min(r23:22,r13:12)
		r1:0 = min(r7:6,r13:12)
	}
	{
		r23:22 = add(r1:0,r3:2)
		r7:6 = add(r17:16,r3:2)
		r1:0 = combine(r3,r2)
		r2 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = asr(r7:6,#31)
		r28 = asr(r2,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r17:16 = mpyu(r26,r2)
		r7:6 = min(r7:6,r15:14)
	}
	{
		r17 += mpyi(r26,r28)
		r23:22 = asr(r23:22,#31)
	}
	{
		r7:6 = max(r7:6,r11:10)
		r17 += mpyi(r2,r24)
		r3:2 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r9:8,r13:12)
		v2.w = vinsert(r6)
	}
	{
		r9:8 = min(r3:2,r13:12)
		r23:22 = min(r23:22,r15:14)
		r3:2 = memd(r30+#-5168)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = max(r23:22,r11:10)
		r9:8 = add(r9:8,r1:0)
		v2 = valign(v2,v2,#4)
	}
	{
		r9:8 = asr(r9:8,#31)
		v3.w = vinsert(r22)
	}
	{
		r9:8 = min(r9:8,r15:14)
		r7:6 = add(r7:6,r1:0)
	}
	{
		r3:2 = min(r3:2,r13:12)
		r9:8 = max(r9:8,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r23:22 = add(r3:2,r1:0)
		r7:6 = asr(r7:6,#31)
		r2 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = asr(r23:22,#31)
		v3.w = vinsert(r8)
	}
	{
		r7:6 = min(r7:6,r15:14)
		r9:8 = min(r23:22,r15:14)
	}
	{
		r23:22 = max(r7:6,r11:10)
		r9:8 = max(r9:8,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r23:22 = min(r21:20,r13:12)
		v3.w = vinsert(r22)
	}
	{
		r9:8 = add(r23:22,r1:0)
		v2.w = vinsert(r8)
		r23:22 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r9:8 = asr(r9:8,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r9:8 = min(r9:8,r15:14)
		r5:4 = add(r5:4,r1:0)
		v2 = valign(v2,v2,#4)
	}
	{
		r9:8 = max(r9:8,r11:10)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r15:14)
		v3.w = vinsert(r8)
	}
	{
		r21:20 = min(r23:22,r13:12)
		r28 = asr(r2,#31)
	}
	{
		r7:6 = mpyu(r26,r2)
		r5:4 = max(r5:4,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r7 += mpyi(r26,r28)
		v3.w = vinsert(r4)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r2,r24)
		r21:20 = add(r21:20,r1:0)
	}
	{
		r3:2 = min(r5:4,r13:12)
		r21:20 = asr(r21:20,#31)
		r5:4 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r21:20 = min(r21:20,r15:14)
		v3 = valign(v3,v3,#4)
		r23:22 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r21:20,r11:10)
		r5:4 = add(r5:4,r1:0)
		r28 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = min(r23:22,r13:12)
		v2.w = vinsert(r8)
	}
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = add(r3:2,r1:0)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r9:8 = add(r9:8,r1:0)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = max(r5:4,r11:10)
		r3:2 = asr(r3:2,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		v3.w = vinsert(r4)
	}
	{
		r5:4 = min(r9:8,r15:14)
		r3:2 = min(r3:2,r15:14)
	}
	{
		r3:2 = max(r3:2,r11:10)
		r5:4 = max(r5:4,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r3 = asr(r28,#31)
		v2.w = vinsert(r4)
		r5:4 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = mpyu(r26,r28)
		v3.w = vinsert(r2)
	}
	{
		r9 += mpyi(r26,r3)
		r3:2 = min(r5:4,r13:12)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r3:2 = add(r3:2,r1:0)
		v2 = valign(v2,v2,#4)
		r23:22 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r23:22,r13:12)
		r3:2 = asr(r3:2,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r5:4 = add(r5:4,r1:0)
	}
	{
		r3:2 = max(r3:2,r11:10)
		r21:20 = add(r21:20,r1:0)
	}
	{
		r5:4 = asr(r5:4,#31)
		v3.w = vinsert(r2)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r21:20 = asr(r21:20,#31)
	}
	{
		r3:2 = min(r21:20,r15:14)
		r9 += mpyi(r28,r24)
		v3 = valign(v3,v3,#4)
	}
	{
		r5:4 = max(r5:4,r11:10)
		r3:2 = max(r3:2,r11:10)
	}
	{
		r5:4 = min(r9:8,r13:12)
		v3.w = vinsert(r4)
	}
	{
		r5:4 = min(r7:6,r13:12)
		v2.w = vinsert(r2)
		r3:2 = memd(r30+#-2352)
		memd(r30+#-1584) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r13:12)
		r7:6 = min(r19:18,r13:12)
		r17:16 = memd(r30+#-2608)
		memd(r30+#-816) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r17:16,r13:12)
		r3:2 = min(r3:2,r13:12)
		v3 = valign(v3,v3,#4)
		r19:18 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r19:18,r13:12)
		v2 = valign(v2,v2,#4)
		r21:20 = memd(r30+#-5680)
		r23:22 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r21:20,r13:12)
		r17:16 = min(r23:22,r13:12)
		r21:20 = memd(r30+#-3888)
		r23:22 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r13:12)
		r21:20 = min(r23:22,r13:12)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r13:12)
		r25:24 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r25:24,r13:12)
		r23:22 = add(r23:22,r1:0)
	}
	{
		r23:22 = add(r21:20,r1:0)
		r21:20 = add(r19:18,r1:0)
		memd(r30+#-1328) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r19:18 = add(r17:16,r1:0)
		r17:16 = add(r11:10,r1:0)
	}
	{
		r11:10 = add(r9:8,r1:0)
		r9:8 = add(r7:6,r1:0)
		r7:6 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = add(r27:26,r1:0)
		r27:26 = add(r3:2,r1:0)
	}
	{
		r3:2 = add(r7:6,r1:0)
		r7:6 = add(r5:4,r1:0)
		r5:4 = memd(r30+#-816)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = add(r5:4,r1:0)
	}
	{
		r25:24 = add(r25:24,r1:0)
		r3:2 = asr(r5:4,#31)
		r1:0 = memd(r30+#-1328)
		memd(r30+#-2096) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = asr(r21:20,#31)
		r5:4 = asr(r7:6,#31)
		memd(r30+#-816) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r7:6 = asr(r9:8,#31)
		r9:8 = asr(r11:10,#31)
	}
	{
		r11:10 = asr(r17:16,#31)
		r21:20 = asr(r1:0,#31)
	}
	{
		r1:0 = asr(r25:24,#31)
		r17:16 = asr(r19:18,#31)
	}
	{
		r23:22 = min(r1:0,r15:14)
		r19:18 = asr(r23:22,#31)
	}
	{
		r1:0 = min(r11:10,r15:14)
		r9:8 = min(r9:8,r15:14)
		r11:10 = combine(r15,r14)
	}
	{
		r0 = ##-2147483648
		r1 = #-1
		memd(r30+#-2864) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r9:8 = max(r9:8,r1:0)
		r13:12 = asr(r13:12,#31)
	}
	{
		r13:12 = min(r13:12,r15:14)
		v3.w = vinsert(r8)
	}
	{
		r25:24 = min(r21:20,r15:14)
		r27:26 = asr(r27:26,#31)
	}
	{
		r21:20 = min(r17:16,r15:14)
		r17:16 = max(r25:24,r1:0)
	}
	{
		r25:24 = min(r27:26,r15:14)
		r13:12 = max(r13:12,r1:0)
		r27:26 = combine(r1,r0)
		v3 = valign(v3,v3,#4)
	}
	{
		r19:18 = min(r19:18,r15:14)
		v3.w = vinsert(r12)
	}
	{
		r13:12 = max(r25:24,r1:0)
		v2.w = vinsert(r16)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r17:16 = max(r19:18,r1:0)
		r13 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r15:14)
		r19:18 = min(r5:4,r15:14)
		r13 = add(r13,#1792)
		v3 = valign(v3,v3,#4)
	}
	{
		v2 = valign(v2,v2,#4)
		r14 = memw(r30+#-3896)
		v4 = vmem(r13+#0)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r12)
		v2.w = vinsert(r16)
		r14 = add(r14,##-46080)
	}
	{
		r3:2 = max(r3:2,r27:26)
		v4 = v10
		vmem(r14+#0) = v4
	}
	{
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r30+#-816)
		r0 = memw(r14+#120)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r5:4,r11:10)
		r5:4 = max(r7:6,r27:26)
		v2 = valign(v2,v2,#4)
		memw(r30+#-1328) = r0
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r23:22,r27:26)
		v3.w = vinsert(r4)
		r0 = memw(r14+#124)
		memw(r30+#-816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25:24 = max(r17:16,r27:26)
		v2.w = vinsert(r6)
		r0 = memw(r14+#112)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r19:18,r27:26)
		v3 = valign(v3,v3,#4)
		r0 = memw(r14+#116)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v3.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
		r0 = memw(r14+#104)
		memw(r30+#-2608) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r21:20,r27:26)
		v2.w = vinsert(r2)
		r0 = memw(r14+#108)
		memw(r30+#-2352) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r3:2 = combine(r27,r26)
		r1 = memw(r14+#96)
		memw(r30+##-8496) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		r1 = memw(r14+#100)
		memw(r30+#-3632) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v3.w = vinsert(r24)
		v2 = valign(v2,v2,#4)
		r1 = memw(r14+#88)
	}
	{
		v2.w = vinsert(r4)
		memw(r30+##-9776) = r1
		r1 = memw(r14+#92)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		memw(r30+##-9520) = r1
		r1 = memw(r14+#80)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v2 = valign(v2,v2,#4)
		memw(r30+##-9264) = r1
		r1 = memw(r14+#84)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-9008) = r1
		r17 = memw(r14+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r19 = memw(r14+#76)
		r16 = memw(r14+#64)
	}
	{
		r28 = memw(r14+#68)
		r0 = memw(r14+#56)
	}
	{
		memw(r30+##-5168) = r0
		r15 = memw(r14+#60)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r7:6 = memd(r30+#-2096)
		r8 = memw(r14+#48)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r7:6,r11:10)
		r7 = memw(r14+#52)
	}
	{
		r27:26 = max(r27:26,r3:2)
		r1:0 = memd(r30+#-2864)
		r12 = memw(r14+#40)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r1:0,r3:2)
		v3.w = vinsert(r26)
		r9 = memw(r14+#44)
	}
	{
		v2.w = vinsert(r4)
		r18 = memw(r14+#32)
		r13 = memw(r14+#36)
	}
	{
		v3 = valign(v3,v3,#4)
		r22 = memw(r14+#24)
		r23 = memw(r14+#28)
	}
	{
		r24 = memw(r14+#16)
		r25 = memw(r14+#20)
	}
	{
		r0 = memw(r14+#8)
		r6 = memw(r14+#12)
	}
	{
		r20 = memw(r14+#0)
		r14 = memw(r14+#4)
	}
	{
		r10 = asr(r20,#31)
		r11 = asr(r14,#31)
		r1 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r21 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r1,r14)
		r27:26 = mpyu(r1,r20)
	}
	{
		r5 += mpyi(r1,r11)
		r27 += mpyi(r1,r10)
	}
	{
		r5 += mpyi(r14,r21)
		r14 = asr(r0,#31)
	}
	{
		r11:10 = mpyu(r1,r0)
		r3:2 = mpyu(r1,r6)
		memd(r30+#-7984) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r11 += mpyi(r1,r14)
		r14 = asr(r6,#31)
	}
	{
		r3 += mpyi(r1,r14)
		r11 += mpyi(r0,r21)
		r14 = r21
	}
	{
		r3 += mpyi(r6,r21)
		r0 = asr(r24,#31)
		r6 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r1,r24)
		r3:2 = mpyu(r1,r25)
		memd(r30+##-8240) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r25,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r24,r21)
	}
	{
		r3 += mpyi(r25,r21)
		r0 = asr(r22,#31)
		memd(r30+#-7728) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r22)
		r3:2 = mpyu(r1,r23)
		memd(r30+#-7472) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r23,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r22,r21)
	}
	{
		r3 += mpyi(r23,r21)
		r0 = asr(r18,#31)
		memd(r30+#-5424) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r18)
		r3:2 = mpyu(r1,r13)
		memd(r30+#-4656) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r13,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r18,r21)
	}
	{
		r3 += mpyi(r13,r14)
		r0 = asr(r12,#31)
		memd(r30+#-4400) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r12)
		r3:2 = mpyu(r1,r9)
		memd(r30+#-3376) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r9,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r27 += mpyi(r20,r21)
	}
	{
		r3 += mpyi(r9,r14)
		r0 = asr(r8,#31)
	}
	{
		r21:20 = mpyu(r1,r8)
		r3:2 = mpyu(r1,r7)
		memd(r30+#-2864) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r1,r0)
		r0 = asr(r7,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r12,r14)
	}
	{
		r3 += mpyi(r7,r14)
		r0 = asr(r6,#31)
		memd(r30+#-2096) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r6)
		r3:2 = mpyu(r1,r15)
		memd(r30+#-3888) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r1,r0)
		r0 = asr(r15,#31)
	}
	{
		r3 += mpyi(r1,r0)
		r5 += mpyi(r6,r14)
	}
	{
		r3 += mpyi(r15,r14)
		r21 += mpyi(r8,r14)
		memd(r30+#-5168) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r13:12 = mpyu(r1,r16)
		r2 = asr(r16,#31)
		memd(r30+#-5680) = r3:2
		memd(r30+#-3120) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r1,r2)
		r0 = asr(r28,#31)
	}
	{
		r21:20 = mpyu(r1,r28)
		r2 = asr(r17,#31)
	}
	{
		r5:4 = mpyu(r1,r17)
		r21 += mpyi(r1,r0)
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r1,r2)
		r2 = asr(r19,#31)
	}
	{
		r7:6 = mpyu(r1,r19)
		r5 += mpyi(r17,r14)
	}
	{
		r7 += mpyi(r1,r2)
		r2 = asr(r0,#31)
		memd(r30+##-8752) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r9:8 = mpyu(r1,r0)
		r5 = r0
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r28,r14)
		r9 += mpyi(r1,r2)
		r28 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r1,r0)
		r4 = asr(r0,#31)
	}
	{
		r9 += mpyi(r5,r14)
		r3 += mpyi(r1,r4)
	}
	{
		r18 = asr(r28,#31)
		memd(r30+##-9264) = r9:8
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r1,r28)
		r3 += mpyi(r0,r14)
		r8 = ##-1073741825
	}
	{
		r9 = ##2147483647
		r0 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r1,r18)
		r3 = r14
		memd(r30+##-9008) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r23:22 = min(r27:26,r9:8)
		r5 += mpyi(r28,r3)
		r8 = ##1073741824
	}
	{
		r9 = #0
		r28 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r19,r14)
		r27:26 = add(r23:22,r9:8)
		r2 = r0
	}
	{
		r13 += mpyi(r16,r14)
		r14 = asr(r0,#31)
		r18 = ##2147483647
	}
	{
		r25:24 = mpyu(r1,r0)
		r23:22 = asr(r27:26,#31)
		r19 = #0
	}
	{
		r25 += mpyi(r1,r14)
		r26 = ##-2147483648
		r27 = #-1
	}
	{
		r15:14 = mpyu(r1,r28)
		r17:16 = min(r23:22,r19:18)
	}
	{
		r25 += mpyi(r2,r3)
		r0 = asr(r28,#31)
		r23:22 = combine(r15,r14)
		r2 = r1
	}
	{
		r17:16 = max(r17:16,r27:26)
		r25:24 = memd(r30+#-7984)
		memd(r30+##-9520) = r25:24
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r1,r0)
		r0 = ##-1073741825
	}
	{
		v4.w = vinsert(r16)
		r1 = ##2147483647
	}
	{
		r15:14 = min(r25:24,r1:0)
		r11:10 = min(r11:10,r1:0)
	}
	{
		r15:14 = add(r15:14,r9:8)
		r11:10 = add(r11:10,r9:8)
		v4 = valign(v4,v4,#4)
	}
	{
		r23 += mpyi(r28,r3)
		r17:16 = asr(r15:14,#31)
		r15:14 = combine(r19,r18)
	}
	{
		r17:16 = min(r17:16,r19:18)
		r11:10 = asr(r11:10,#31)
		memd(r30+#-7984) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r17:16 = max(r17:16,r27:26)
		r11:10 = min(r11:10,r15:14)
	}
	{
		r11:10 = max(r11:10,r27:26)
		v4.w = vinsert(r16)
		r17 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = min(r21:20,r1:0)
		r28 = asr(r17,#31)
		r18 = r17
	}
	{
		r23:22 = mpyu(r2,r17)
		v4 = valign(v4,v4,#4)
		r17:16 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r17:16 = min(r17:16,r1:0)
	}
	{
		r23 += mpyi(r2,r28)
		r17:16 = add(r17:16,r9:8)
	}
	{
		r23 += mpyi(r18,r3)
		v4.w = vinsert(r10)
		r10 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = asr(r17:16,#31)
		r11 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = min(r19:18,r15:14)
		r22 = asr(r11,#31)
		v4 = valign(v4,v4,#4)
		memd(r30+#-3632) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r19:18 = max(r19:18,r27:26)
		r17:16 = mpyu(r2,r11)
	}
	{
		r17 += mpyi(r2,r22)
		v4.w = vinsert(r18)
		r19:18 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r13:12,r1:0)
		r19:18 = min(r19:18,r1:0)
		r13 = r2
		r12 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r25:24 = mpyu(r2,r10)
		r28 = asr(r10,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r25 += mpyi(r2,r28)
		r19:18 = add(r19:18,r9:8)
	}
	{
		r23:22 = add(r23:22,r9:8)
		r19:18 = asr(r19:18,#31)
	}
	{
		r25 += mpyi(r10,r3)
		r17 += mpyi(r11,r3)
	}
	{
		r11:10 = mpyu(r2,r12)
		r28 = asr(r12,#31)
		r24 = r3
		memd(r30+#-2608) = r25:24
	}                                       // 8-byte Folded Spill
	{
		r11 += mpyi(r2,r28)
		r19:18 = min(r19:18,r15:14)
		r25 = r13
	}
	{
		r19:18 = max(r19:18,r27:26)
		r23:22 = asr(r23:22,#31)
	}
	{
		r11 += mpyi(r12,r3)
		r23:22 = min(r23:22,r15:14)
		r3:2 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r23:22,r27:26)
		v4.w = vinsert(r18)
		r12 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = min(r3:2,r1:0)
		r3:2 = add(r21:20,r9:8)
	}
	{
		r21:20 = add(r23:22,r9:8)
		r3:2 = asr(r3:2,#31)
		v4 = valign(v4,v4,#4)
	}
	{
		r21:20 = asr(r21:20,#31)
		v5.w = vinsert(r18)
	}
	{
		r21:20 = min(r21:20,r15:14)
		r23:22 = min(r3:2,r15:14)
		r3:2 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = max(r21:20,r27:26)
		r28 = asr(r12,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r19:18 = mpyu(r13,r12)
		v4.w = vinsert(r20)
	}
	{
		r19 += mpyi(r13,r28)
		r21:20 = min(r3:2,r1:0)
		r3:2 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = max(r23:22,r27:26)
		r19 += mpyi(r12,r24)
		v4 = valign(v4,v4,#4)
	}
	{
		r13:12 = add(r21:20,r9:8)
		v5.w = vinsert(r22)
	}
	{
		r13:12 = asr(r13:12,#31)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r13:12 = min(r13:12,r15:14)
		r23:22 = min(r3:2,r1:0)
		v5 = valign(v5,v5,#4)
	}
	{
		r13:12 = max(r13:12,r27:26)
		r7:6 = asr(r7:6,#31)
		r3:2 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r15:14)
		v5.w = vinsert(r12)
	}
	{
		r3:2 = min(r3:2,r1:0)
		r21:20 = add(r23:22,r9:8)
	}
	{
		r7:6 = max(r7:6,r27:26)
		r3:2 = add(r3:2,r9:8)
		v5 = valign(v5,v5,#4)
	}
	{
		r7:6 = asr(r3:2,#31)
		v5.w = vinsert(r6)
		r3:2 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r15:14)
		r21:20 = asr(r21:20,#31)
	}
	{
		r23:22 = min(r21:20,r15:14)
		r21:20 = min(r3:2,r1:0)
		r3:2 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r1:0)
		r13:12 = max(r23:22,r27:26)
		v5 = valign(v5,v5,#4)
	}
	{
		r3:2 = add(r3:2,r9:8)
		v4.w = vinsert(r12)
	}
	{
		r7:6 = max(r7:6,r27:26)
		r13:12 = add(r21:20,r9:8)
		r20 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = asr(r13:12,#31)
		v5.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		r7:6 = min(r7:6,r15:14)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r7:6 = max(r7:6,r27:26)
		v5 = valign(v5,v5,#4)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r5:4 = min(r5:4,r1:0)
	}
	{
		v4.w = vinsert(r6)
		v5.w = vinsert(r2)
		r7:6 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r1:0)
		r3:2 = add(r5:4,r9:8)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r3:2 = asr(r3:2,#31)
		v5 = valign(v5,v5,#4)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r5:4 = add(r5:4,r9:8)
		v4 = valign(v4,v4,#4)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r5:4 = asr(r5:4,#31)
		v5.w = vinsert(r2)
	}
	{
		r3:2 = min(r5:4,r15:14)
		r7:6 = asr(r7:6,#31)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r5:4 = min(r7:6,r15:14)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r5:4,r27:26)
		v4.w = vinsert(r2)
	}
	{
		r5:4 = min(r7:6,r1:0)
		v5.w = vinsert(r2)
		r7:6 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r1:0)
		r3:2 = add(r5:4,r9:8)
		v4 = valign(v4,v4,#4)
	}
	{
		r5:4 = add(r7:6,r9:8)
		r28 = asr(r20,#31)
		v5 = valign(v5,v5,#4)
		r7 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r25,r20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r13 += mpyi(r25,r28)
		r5:4 = min(r5:4,r15:14)
		r28 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r13 += mpyi(r20,r24)
		v4.w = vinsert(r4)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r5:4 = min(r13:12,r1:0)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r6 = asr(r7,#31)
		r5:4 = memd(r30+#-2608)
		memd(r30+#-1328) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r23:22 = mpyu(r25,r7)
		v5.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
		r3:2 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r25,r6)
		r13:12 = min(r3:2,r1:0)
		r3:2 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r7,r24)
		r7:6 = min(r11:10,r1:0)
		v5 = valign(v5,v5,#4)
	}
	{
		r25:24 = min(r19:18,r1:0)
		r3:2 = min(r3:2,r1:0)
		r23:22 = memd(r30+#-5168)
		memd(r30+#-816) = r23:22
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r23:22,r1:0)
		r7:6 = min(r17:16,r1:0)
		r23:22 = memd(r30+#-3888)
		memd(r30+#-1584) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r1:0)
		r5:4 = min(r5:4,r1:0)
		r21:20 = memd(r30+#-2864)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r1:0)
		r13:12 = add(r13:12,r9:8)
	}
	{
		r21:20 = min(r23:22,r1:0)
		r7:6 = add(r7:6,r9:8)
		r23:22 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r1:0)
		r1:0 = add(r21:20,r9:8)
	}
	{
		r21:20 = add(r19:18,r9:8)
		r19:18 = add(r17:16,r9:8)
		memd(r30+#-1840) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r17:16 = add(r11:10,r9:8)
		r11:10 = add(r3:2,r9:8)
		r3:2 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r3:2,r9:8)
		r1:0 = add(r5:4,r9:8)
	}
	{
		r5:4 = add(r25:24,r9:8)
		r3:2 = asr(r3:2,#31)
		r25:24 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = add(r25:24,r9:8)
		r23:22 = add(r23:22,r9:8)
		memd(r30+#-2096) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r9:8 = asr(r1:0,#31)
		r3:2 = asr(r11:10,#31)
		r1:0 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r15:14)
		r11:10 = asr(r21:20,#31)
		memd(r30+#-2352) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r11:10 = min(r11:10,r15:14)
		r9:8 = max(r9:8,r27:26)
	}
	{
		r11:10 = max(r11:10,r27:26)
		r5:4 = asr(r5:4,#31)
		r9 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r9 = add(r9,#1920)
		r11 = add(r28,##-46208)
	}
	{
		r17:16 = min(r1:0,r15:14)
		r3:2 = asr(r17:16,#31)
		v6 = vmem(r9+#0)
		memd(r30+#-1328) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r17:16 = max(r17:16,r27:26)
		r13:12 = asr(r13:12,#31)
		vmem(r11+#0) = v6
	}
	{
		r13:12 = min(r13:12,r15:14)
		v4.w = vinsert(r16)
		v6 = v10
		r1:0 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = max(r13:12,r27:26)
		r17:16 = min(r1:0,r15:14)
		r0 = memw(r11+#120)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = asr(r7:6,#31)
		v5.w = vinsert(r12)
		r0 = memw(r11+#124)
		memw(r30+#-1328) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v4 = valign(v4,v4,#4)
		r0 = memw(r11+#112)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = min(r7:6,r15:14)
		v5 = valign(v5,v5,#4)
		r0 = memw(r11+#116)
		memw(r30+#-304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v5.w = vinsert(r8)
		v4.w = vinsert(r10)
		r0 = memw(r11+#104)
		memw(r30+#-3632) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r21:20 = asr(r23:22,#31)
		r5:4 = asr(r19:18,#31)
		r0 = memw(r11+#108)
		memw(r30+#-3120) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = min(r21:20,r15:14)
		r7:6 = max(r7:6,r27:26)
		r0 = memw(r11+#96)
	}
	{
		r25:24 = asr(r25:24,#31)
		v5 = valign(v5,v5,#4)
		memw(r30+##-9520) = r0
	}                                       // 4-byte Folded Spill
	{
		v5.w = vinsert(r6)
		r0 = memw(r11+#100)
		memw(r30+##-9264) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = max(r19:18,r27:26)
		r0 = memw(r11+#88)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25:24 = min(r25:24,r15:14)
		v4 = valign(v4,v4,#4)
		r0 = memw(r11+#92)
	}
	{
		r5:4 = min(r5:4,r15:14)
		v4.w = vinsert(r18)
		memw(r30+##-9008) = r0
	}                                       // 4-byte Folded Spill
	{
		r25:24 = max(r25:24,r27:26)
		r0 = memw(r11+#80)
		memw(r30+##-8496) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v5 = valign(v5,v5,#4)
		r0 = memw(r11+#84)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r5:4,r27:26)
		v5.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
		r23 = memw(r11+#72)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r0 = memw(r11+#76)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r17:16,r27:26)
		v4.w = vinsert(r4)
		v5 = valign(v5,v5,#4)
		r22 = memw(r11+#64)
	}
	{
		r3:2 = max(r3:2,r27:26)
		v5.w = vinsert(r4)
		r17 = memw(r11+#68)
		r28 = memw(r11+#56)
	}
	{
		v4 = valign(v4,v4,#4)
		r16 = memw(r11+#60)
	}
	{
		v4.w = vinsert(r2)
		r7:6 = memd(r30+#-2096)
		r12 = memw(r11+#48)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r7:6,r15:14)
		v5 = valign(v5,v5,#4)
		r13 = memw(r11+#52)
	}
	{
		r1:0 = max(r1:0,r27:26)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r5:4,r15:14)
		v5.w = vinsert(r0)
		r15 = memw(r11+#40)
		r14 = memw(r11+#44)
	}
	{
		r19:18 = max(r21:20,r27:26)
		r10 = memw(r11+#32)
		r9 = memw(r11+#36)
	}
	{
		v4.w = vinsert(r18)
		v5 = valign(v5,v5,#4)
		r8 = memw(r11+#24)
		r24 = memw(r11+#28)
	}
	{
		r25 = memw(r11+#16)
		r7 = memw(r11+#20)
	}
	{
		r2 = memw(r11+#8)
		r6 = memw(r11+#12)
	}
	{
		r3 = memw(r11+#0)
		r5 = memw(r11+#4)
	}
	{
		r0 = asr(r3,#31)
		r11 = asr(r5,#31)
		r21 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r20 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = mpyu(r21,r5)
		r19:18 = mpyu(r21,r3)
	}
	{
		r27 += mpyi(r21,r11)
		r11 = asr(r2,#31)
	}
	{
		r27 += mpyi(r5,r20)
		r5:4 = mpyu(r21,r2)
	}
	{
		r19 += mpyi(r21,r0)
		r5 += mpyi(r21,r11)
	}
	{
		r1:0 = mpyu(r21,r6)
		r11 = asr(r6,#31)
	}
	{
		r1 += mpyi(r21,r11)
		r5 += mpyi(r2,r20)
	}
	{
		r1 += mpyi(r6,r20)
		r19 += mpyi(r3,r20)
		memd(r30+#-7984) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r21,r7)
		r3 = asr(r7,#31)
		memd(r30+#-7728) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r21,r25)
		r2 = asr(r25,#31)
	}
	{
		r5 += mpyi(r21,r3)
		r1 += mpyi(r21,r2)
	}
	{
		r5:4 = mpyu(r21,r8)
		r11 = asr(r8,#31)
		r3:2 = combine(r5,r4)
	}
	{
		r5 += mpyi(r21,r11)
		r1 += mpyi(r25,r20)
	}
	{
		r3 += mpyi(r7,r20)
		r5 += mpyi(r8,r20)
		memd(r30+#-7472) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r21,r24)
		r0 = asr(r24,#31)
		memd(r30+#-5680) = r3:2
		memd(r30+#-3888) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r21,r0)
		r6 = asr(r10,#31)
	}
	{
		r5:4 = mpyu(r21,r10)
		r3:2 = mpyu(r21,r9)
		r1:0 = combine(r3,r2)
	}
	{
		r5 += mpyi(r21,r6)
		r6 = asr(r9,#31)
	}
	{
		r3 += mpyi(r21,r6)
		r1 += mpyi(r24,r20)
	}
	{
		r5 += mpyi(r10,r20)
		r3 += mpyi(r9,r20)
		memd(r30+#-4656) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r21,r15)
		r0 = asr(r15,#31)
		memd(r30+#-3376) = r5:4
		memd(r30+#-2352) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r21,r14)
		r1 = asr(r14,#31)
	}
	{
		r3 += mpyi(r21,r0)
		r5 += mpyi(r21,r1)
	}
	{
		r7:6 = mpyu(r21,r12)
		r0 = asr(r12,#31)
	}
	{
		r7 += mpyi(r21,r0)
		r3 += mpyi(r15,r20)
	}
	{
		r5 += mpyi(r14,r20)
		r7 += mpyi(r12,r20)
		memd(r30+#-2096) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r21,r13)
		r0 = asr(r13,#31)
		memd(r30+#-2608) = r5:4
		memd(r30+#-2864) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r21,r28)
		r1 = asr(r28,#31)
	}
	{
		r3 += mpyi(r21,r0)
		r5 += mpyi(r21,r1)
	}
	{
		r7:6 = mpyu(r21,r16)
		r0 = asr(r16,#31)
	}
	{
		r7 += mpyi(r21,r0)
		r3 += mpyi(r13,r20)
	}
	{
		r5 += mpyi(r28,r20)
		r0 = asr(r17,#31)
		memd(r30+#-4400) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r13:12 = mpyu(r21,r17)
		r7 += mpyi(r16,r20)
		memd(r30+#-5168) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r21,r0)
		r0 = asr(r23,#31)
		memd(r30+#-5424) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r21,r23)
		r1 = asr(r22,#31)
	}
	{
		r7:6 = mpyu(r21,r22)
		r5 += mpyi(r21,r0)
	}
	{
		r7 += mpyi(r21,r1)
		r5 += mpyi(r23,r20)
		r1 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r23 = #0
		memd(r30+##-8240) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r5 = r1
		r4 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r21,r1)
		r1 = asr(r4,#31)
	}
	{
		r3:2 = mpyu(r21,r4)
		r11 += mpyi(r21,r0)
	}
	{
		r3 += mpyi(r21,r1)
		r13 += mpyi(r17,r20)
		r1 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r4,r20)
		r0 = asr(r1,#31)
	}
	{
		r15:14 = mpyu(r21,r1)
		r11 += mpyi(r5,r20)
		memd(r30+##-8496) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r21,r0)
		r7 += mpyi(r22,r20)
		r22 = ##2147483647
	}
	{
		r15 += mpyi(r1,r20)
		r1 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r1,#31)
		r17 = r1
		r4 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r21,r1)
		r1 = asr(r4,#31)
		r28 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = mpyu(r21,r4)
		r3 += mpyi(r21,r0)
		r16 = r4
	}
	{
		r5:4 = mpyu(r21,r28)
		r0 = asr(r28,#31)
	}
	{
		r9 += mpyi(r21,r1)
		r5 += mpyi(r21,r0)
	}
	{
		r3 += mpyi(r17,r20)
		r9 += mpyi(r16,r20)
	}
	{
		r5 += mpyi(r28,r20)
		memd(r30+##-8752) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r2 = ##-1073741825
		r3 = ##2147483647
	}
	{
		memd(r30+##-9008) = r9:8
		memd(r30+##-9520) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r19:18,r3:2)
		r8 = ##1073741824
		r9 = #0
	}
	{
		r27:26 = min(r27:26,r3:2)
		r11:10 = min(r11:10,r3:2)
		r5 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = add(r1:0,r9:8)
		r28 = asr(r5,#31)
		r25:24 = combine(r9,r8)
		r4 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r21,r5)
		r19:18 = asr(r19:18,#31)
	}
	{
		r1 += mpyi(r21,r28)
		r27:26 = add(r27:26,r9:8)
		r8 = ##-2147483648
	}
	{
		r19:18 = min(r19:18,r23:22)
		r1 += mpyi(r5,r20)
		r9 = #-1
	}
	{
		r17:16 = max(r19:18,r9:8)
		r28 = asr(r4,#31)
		memd(r30+##-9264) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r19:18 = mpyu(r21,r4)
		v6.w = vinsert(r16)
		r1:0 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r19 += mpyi(r21,r28)
		r17:16 = asr(r27:26,#31)
		r27:26 = combine(r23,r22)
	}
	{
		r17:16 = min(r17:16,r23:22)
		r19 += mpyi(r4,r20)
		v6 = valign(v6,v6,#4)
	}
	{
		r5:4 = min(r1:0,r3:2)
		r17:16 = max(r17:16,r9:8)
		r1:0 = combine(r25,r24)
		memd(r30+#-3632) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r19:18 = add(r5:4,r25:24)
		v6.w = vinsert(r16)
		r4 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = asr(r19:18,#31)
		r28 = asr(r4,#31)
		r5 = r4
		r23:22 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r19:18 = min(r19:18,r27:26)
		v6 = valign(v6,v6,#4)
	}
	{
		r19:18 = max(r19:18,r9:8)
		r17:16 = add(r17:16,r1:0)
	}
	{
		r25:24 = mpyu(r21,r4)
		v6.w = vinsert(r18)
		r4 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r21,r28)
		r17:16 = asr(r17:16,#31)
	}
	{
		r19:18 = min(r17:16,r27:26)
		r17:16 = min(r7:6,r3:2)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r19:18,r9:8)
		r28 = asr(r4,#31)
	}
	{
		r23:22 = mpyu(r21,r4)
		v6.w = vinsert(r18)
	}
	{
		r19:18 = min(r7:6,r3:2)
		r23 += mpyi(r21,r28)
	}
	{
		r25 += mpyi(r5,r20)
		r7:6 = add(r17:16,r1:0)
		v6 = valign(v6,v6,#4)
	}
	{
		r23 += mpyi(r4,r20)
		r7:6 = asr(r7:6,#31)
	}
	{
		r5:4 = min(r13:12,r3:2)
		r7:6 = min(r7:6,r27:26)
		r13:12 = combine(r1,r0)
	}
	{
		r7:6 = max(r7:6,r9:8)
		r5:4 = add(r5:4,r1:0)
	}
	{
		r5:4 = asr(r5:4,#31)
		r19:18 = add(r19:18,r1:0)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r27:26)
		v7.w = vinsert(r6)
	}
	{
		r5:4 = max(r5:4,r9:8)
		r17:16 = mpyu(r21,r0)
	}
	{
		r5 = asr(r0,#31)
		r19:18 = asr(r19:18,#31)
		r1:0 = combine(r13,r12)
		v7 = valign(v7,v7,#4)
	}
	{
		r17 += mpyi(r21,r5)
		v7.w = vinsert(r4)
		r5:4 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r5:4,r3:2)
		r19:18 = min(r19:18,r27:26)
	}
	{
		r19:18 = max(r19:18,r9:8)
		r5:4 = add(r7:6,r13:12)
		v7 = valign(v7,v7,#4)
		r7:6 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		v6.w = vinsert(r18)
	}
	{
		r5:4 = asr(r5:4,#31)
		r19:18 = add(r7:6,r13:12)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r19:18 = asr(r19:18,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r5:4 = max(r5:4,r9:8)
		r11:10 = add(r11:10,r13:12)
	}
	{
		r7:6 = min(r19:18,r27:26)
		v7.w = vinsert(r4)
	}
	{
		r5:4 = max(r7:6,r9:8)
		r11:10 = asr(r11:10,#31)
		r7:6 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r11:10,r27:26)
		r7:6 = min(r7:6,r3:2)
		v7 = valign(v7,v7,#4)
	}
	{
		r11:10 = max(r11:10,r9:8)
		r7:6 = add(r7:6,r13:12)
	}
	{
		r5:4 = asr(r7:6,#31)
		v6.w = vinsert(r4)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r27:26)
		v7.w = vinsert(r10)
	}
	{
		r7:6 = min(r7:6,r3:2)
		r19:18 = min(r15:14,r3:2)
		v6 = valign(v6,v6,#4)
	}
	{
		r11:10 = add(r7:6,r1:0)
		r13:12 = add(r19:18,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r5:4 = max(r5:4,r9:8)
		r7:6 = asr(r11:10,#31)
	}
	{
		r13:12 = asr(r13:12,#31)
		v7.w = vinsert(r4)
		r4 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = min(r7:6,r27:26)
		r13:12 = min(r13:12,r27:26)
		r7:6 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = max(r13:12,r9:8)
		r7:6 = min(r7:6,r3:2)
		v7 = valign(v7,v7,#4)
	}
	{
		r13:12 = add(r7:6,r1:0)
		v7.w = vinsert(r12)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = max(r11:10,r9:8)
		r13:12 = asr(r13:12,#31)
	}
	{
		r11:10 = min(r7:6,r3:2)
		v6.w = vinsert(r10)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r15:14 = add(r11:10,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r13:12 = min(r13:12,r27:26)
		r15:14 = asr(r15:14,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r13:12 = max(r13:12,r9:8)
		r7:6 = add(r7:6,r1:0)
	}
	{
		r11:10 = min(r15:14,r27:26)
		v7.w = vinsert(r12)
	}
	{
		r13:12 = max(r11:10,r9:8)
		r7:6 = asr(r7:6,#31)
	}
	{
		r19:18 = mpyu(r21,r4)
		r28 = asr(r4,#31)
		v7 = valign(v7,v7,#4)
	}
	{
		r19 += mpyi(r21,r28)
		v6.w = vinsert(r12)
		r13:12 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r19 += mpyi(r4,r20)
		r5:4 = combine(r9,r8)
		r15:14 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r9:8)
		r9:8 = min(r13:12,r3:2)
		v6 = valign(v6,v6,#4)
	}
	{
		r7:6 = add(r9:8,r1:0)
		v7.w = vinsert(r6)
	}
	{
		r13:12 = min(r15:14,r3:2)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r11:10 = add(r13:12,r1:0)
		v7 = valign(v7,v7,#4)
		r13 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = max(r7:6,r5:4)
		r12 = asr(r13,#31)
	}
	{
		r15:14 = mpyu(r21,r13)
		v7.w = vinsert(r6)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r15 += mpyi(r21,r12)
		r9:8 = asr(r11:10,#31)
	}
	{
		r15 += mpyi(r13,r20)
		r13:12 = min(r7:6,r3:2)
		v7 = valign(v7,v7,#4)
		r7:6 = memd(r30+#-2352)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r27:26)
		r7:6 = min(r7:6,r3:2)
		r11:10 = memd(r30+#-816)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r9:8,r5:4)
		r7:6 = add(r7:6,r1:0)
	}
	{
		r9:8 = min(r11:10,r3:2)
		v6.w = vinsert(r8)
	}
	{
		r7:6 = asr(r7:6,#31)
		r9:8 = add(r9:8,r1:0)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r9:8 = asr(r9:8,#31)
		v6 = valign(v6,v6,#4)
	}
	{
		r7:6 = max(r7:6,r5:4)
		r13:12 = add(r13:12,r1:0)
	}
	{
		r7:6 = min(r9:8,r27:26)
		v6.w = vinsert(r6)
	}
	{
		r7:6 = max(r7:6,r5:4)
		r9:8 = min(r23:22,r3:2)
	}
	{
		r13:12 = asr(r13:12,#31)
		v6 = valign(v6,v6,#4)
		r6 = memw(r30+#-304)
		memd(r30+#-816) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r13:12 = min(r13:12,r27:26)
		r9:8 = add(r9:8,r1:0)
	}
	{
		r17 += mpyi(r6,r20)
		r7:6 = min(r15:14,r3:2)
		r21:20 = memd(r30+#-3632)
		r23:22 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r21:20,r3:2)
		r7:6 = min(r19:18,r3:2)
		memd(r30+#-304) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r11:10 = min(r23:22,r3:2)
		r19:18 = min(r17:16,r3:2)
		r23:22 = memd(r30+#-5168)
		r21:20 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r23:22,r3:2)
		r15:14 = add(r15:14,r1:0)
		r23:22 = memd(r30+#-2608)
		memd(r30+#-1328) = r19:18
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r21:20,r3:2)
		r13:12 = max(r13:12,r5:4)
	}
	{
		r21:20 = min(r23:22,r3:2)
		v7.w = vinsert(r12)
		r23:22 = memd(r30+#-2096)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r3:2)
		r15:14 = asr(r15:14,#31)
	}
	{
		r13:12 = min(r25:24,r3:2)
		r23:22 = add(r23:22,r1:0)
		v7 = valign(v7,v7,#4)
		r25:24 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r15:14,r27:26)
		r23:22 = asr(r23:22,#31)
	}
	{
		r23:22 = min(r23:22,r27:26)
		r21:20 = add(r21:20,r1:0)
	}
	{
		r23:22 = max(r23:22,r5:4)
		r13:12 = add(r13:12,r1:0)
	}
	{
		r15:14 = max(r15:14,r5:4)
		r13:12 = asr(r13:12,#31)
	}
	{
		r21:20 = asr(r21:20,#31)
		v6.w = vinsert(r22)
	}
	{
		r25:24 = min(r25:24,r3:2)
		v7.w = vinsert(r14)
	}
	{
		r21:20 = min(r21:20,r27:26)
		r13:12 = min(r13:12,r27:26)
		v6 = valign(v6,v6,#4)
	}
	{
		r21:20 = max(r21:20,r5:4)
		r25:24 = add(r25:24,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r13:12 = max(r13:12,r5:4)
		r9:8 = asr(r9:8,#31)
	}
	{
		r25:24 = asr(r25:24,#31)
		v6.w = vinsert(r20)
	}
	{
		r3:2 = add(r7:6,r1:0)
		v7.w = vinsert(r12)
		r7:6 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r27:26)
		r25:24 = min(r25:24,r27:26)
		v6 = valign(v6,v6,#4)
	}
	{
		r19:18 = add(r19:18,r1:0)
		r7:6 = add(r7:6,r1:0)
		v7 = valign(v7,v7,#4)
	}
	{
		r15:14 = max(r25:24,r5:4)
		r9:8 = max(r9:8,r5:4)
	}
	{
		r7:6 = asr(r7:6,#31)
		v6.w = vinsert(r14)
		r14 = #68
	}
	{
		r19:18 = asr(r19:18,#31)
		v7.w = vinsert(r8)
		v4 = vror(v4,r14)
	}
	{
		r17:16 = add(r17:16,r1:0)
		r11:10 = add(r11:10,r1:0)
		r1 = #68
		v6 = valign(v6,v6,#4)
	}
	{
		r23:22 = min(r19:18,r27:26)
		r7:6 = min(r7:6,r27:26)
		v0 = vror(v0,r1)
	}
	{
		r1:0 = max(r7:6,r5:4)
		r13:12 = max(r23:22,r5:4)
		v2 = vror(v2,r1)
	}
	{
		r3:2 = asr(r3:2,#31)
		r6 = ##1073741824
		v7 = valign(v7,v7,#4)
	}
	{
		v6.w = vinsert(r12)
		v7.w = vinsert(r0)
		r7 = #0
	}
	{
		r3:2 = min(r3:2,r27:26)
		r17:16 = asr(r17:16,#31)
		v0 = vor(v0,v1)
		r1:0 = memd(r30+#-304)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = min(r17:16,r27:26)
		r1:0 = add(r1:0,r7:6)
		v6 = valign(v6,v6,#4)
		v2 = vor(v2,v3)
	}
	{
		r7:6 = max(r25:24,r5:4)
		r3:2 = max(r3:2,r5:4)
		v7 = valign(v7,v7,#4)
		v0.w = vadd(v0.w,v29.w):sat
	}
	{
		v6.w = vinsert(r6)
		v7.w = vinsert(r2)
		v2.w = vadd(v2.w,v29.w):sat
	}
	{
		r7:6 = asr(r11:10,#31)
		r1:0 = asr(r1:0,#31)
		v0.w = vasr(v0.w,v27.w)
		r3:2 = memd(r30+#-816)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = min(r1:0,r27:26)
		v6 = valign(v6,v6,#4)
		v2.w = vasr(v2.w,v27.w)
	}
	{
		r3:2 = max(r7:6,r5:4)
		v5.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
		v0.w = vmin(v12.w,v0.w)
	}
	{
		r1:0 = max(r1:0,r5:4)
		v6.w = vinsert(r2)
		v2.w = vmin(v2.w,v12.w)
		v0.w = vmax(v11.w,v0.w)
	}
	{
		v7.w = vinsert(r0)
		v1 = valign(v5,v5,#4)
		v2.w = vmax(v11.w,v2.w)
	}
	{
		v3 = vror(v6,r14)
		v1 = vor(v4,v1)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r0,##-38912)
		v4 = valign(v7,v7,#4)
		v1.w = vadd(v1.w,v29.w):sat
	}
	{
		r0 = setbit(r2,#7)
		v3 = vor(v3,v4)
	}
	{
		v1.w = vasr(v1.w,v27.w)
		v0.h = vpacke(v0.w,v2.w)
		v3.w = vadd(v3.w,v29.w):sat
	}
	{
		v1.w = vmin(v12.w,v1.w)
		v0.h = vadd(v26.h,v0.h):sat
	}
	{
		v3.w = vasr(v3.w,v27.w)
		v1.w = vmax(v11.w,v1.w)
		v0.h = vmin(v0.h,v28.h)
	}
	{
		v0.h = vmax(v24.h,v0.h)
		v3.w = vmin(v3.w,v12.w)
		vmem(r0+#0) = v0.new
	}
	{
		v3.w = vmax(v11.w,v3.w)
		r1:0 = memd(r2+#192)
	}
	{
		v1.h = vpacke(v1.w,v3.w)
	}
	{
		v1.h = vadd(v26.h,v1.h):sat
	}
	{
		v1.h = vmin(v28.h,v1.h)
	}
	{
		v0.h = vmax(v25.h,v1.h)
		vmem(r2+#0) = v0.new
	}
	{
		v1 = v10
		memd(r30+#-5424) = r1:0
		r1:0 = memd(r2+#200)

	} :mem_noshuf
	{
		memd(r30+#-7472) = r1:0
		r1:0 = memd(r2+#208)

	} :mem_noshuf
	{
		memd(r30+#-7728) = r1:0
		r1:0 = memd(r2+#216)

	} :mem_noshuf
	{
		memd(r30+#-7984) = r1:0
		r1:0 = memd(r2+#224)

	} :mem_noshuf
	{
		memd(r30+##-8240) = r1:0
		r1:0 = memd(r2+#232)

	} :mem_noshuf
	{
		memd(r30+##-8496) = r1:0
		r1:0 = memd(r2+#240)

	} :mem_noshuf
	{
		memd(r30+##-8752) = r1:0
		r1:0 = memd(r2+#248)

	} :mem_noshuf
	{
		memd(r30+##-9008) = r1:0
		r1:0 = memd(r2+#128)

	} :mem_noshuf
	{
		memd(r30+#-816) = r1:0
		r1:0 = memd(r2+#136)

	} :mem_noshuf
	{
		memd(r30+#-1584) = r1:0
		r1:0 = memd(r2+#144)

	} :mem_noshuf
	{
		memd(r30+#-3376) = r1:0
		r1 = memw(r2+#124)

	} :mem_noshuf
	{
		r19:18 = memd(r2+#152)
		r0 = memw(r2+#120)
	}
	{
		r0 = memw(r30+#-3896)
		memd(r30+#-304) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r0,##-38144)
		r23:22 = memd(r2+#160)
		r25:24 = memd(r2+#168)
	}
	{
		r5:4 = memd(r2+#64)
		r27:26 = memd(r2+#176)
	}
	{
		r11:10 = memd(r2+#184)
		memd(r30+#-1328) = r5:4

	} :mem_noshuf
	{
		r1:0 = memd(r2+#72)
	}
	{
		memd(r30+#-1840) = r1:0
		r1:0 = memd(r2+#80)

	} :mem_noshuf
	{
		memd(r30+#-2096) = r1:0
		r1:0 = memd(r2+#88)

	} :mem_noshuf
	{
		memd(r30+#-2352) = r1:0
		r1:0 = memd(r2+#96)

	} :mem_noshuf
	{
		memd(r30+#-2608) = r1:0
		r1:0 = memd(r2+#104)

	} :mem_noshuf
	{
		memd(r30+#-2864) = r1:0
		r1:0 = memd(r2+#112)

	} :mem_noshuf
	{
		memd(r30+#-3120) = r1:0
		r15:14 = memd(r2+#0)

	} :mem_noshuf
	{
		r13:12 = memd(r2+#8)
		r17:16 = memd(r2+#16)
	}
	{
		r21:20 = memd(r2+#24)
		r9:8 = memd(r2+#32)
	}
	{
		r7:6 = memd(r2+#48)
		r1:0 = memd(r2+#56)
	}
	{
		r5:4 = memd(r2+#40)
		memd(r3+#56) = r1:0

	} :mem_noshuf
	{
		memd(r3+#48) = r7:6
		memd(r3+#40) = r5:4
	}
	{
		memd(r3+#32) = r9:8
		memd(r3+#24) = r21:20
	}
	{
		memd(r3+#16) = r17:16
		memd(r3+#8) = r13:12
	}
	{
		memd(r3+#0) = r15:14
		r0 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r0 = add(r0,##-46336)
		v0 = vmem(r3+#0)
	}
	{
		vmem(r0+#0) = v0
	}
	{
		r12 = memw(r0+#56)
		r13 = memw(r0+#60)
	}
	{
		r16 = memw(r0+#48)
		r17 = memw(r0+#52)
	}
	{
		r2 = memw(r0+#40)
		r3 = memw(r0+#44)
	}
	{
		memd(r30+#-4400) = r3:2
		r2 = memw(r0+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#36)
	}
	{
		memd(r30+#-3888) = r3:2
		r2 = memw(r0+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#28)
	}
	{
		memd(r30+#-4656) = r3:2
		r20 = memw(r0+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r21 = memw(r0+#20)
		r2 = memw(r0+#0)
	}
	{
		r3 = memw(r0+#4)
		r6 = memw(r0+#8)
	}
	{
		r2 = vtrunehb(r3:2)
		r3 = vtrunehb(r13:12)
		r7 = memw(r0+#12)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,##-38400)
	}
	{
		memd(r0+#56) = r11:10
		r11 = memw(r30+#-3896)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		memd(r0+#48) = r27:26
		memd(r0+#40) = r25:24
	}
	{
		r28 = add(r11,##-46592)
		memd(r0+#32) = r23:22
		memd(r0+#24) = r19:18
	}
	{
		r5:4 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#16) = r5:4
		r5:4 = memd(r30+#-1584)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r5:4
		r5:4 = memd(r30+#-816)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		r4 = add(r11,##-46464)
		memd(r0+#0) = r5:4
	}
	{
		v0.cur = vmem(r0+#0)
		vmem(r4+#0) = v0
	}
	{
		r0 = memw(r4+#56)
		r1 = memw(r4+#60)
	}
	{
		memd(r30+#-816) = r1:0
		r0 = memw(r4+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#52)
	}
	{
		memd(r30+#-1584) = r1:0
		r0 = memw(r4+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#44)
	}
	{
		memd(r30+#-3376) = r1:0
		r0 = memw(r4+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#36)
	}
	{
		memd(r30+#-3632) = r1:0
		r0 = memw(r4+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#28)
	}
	{
		memd(r30+#-5168) = r1:0
		r0 = memw(r4+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#20)
	}
	{
		memd(r30+#-5680) = r1:0
		r0 = memw(r4+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r1 = memw(r4+#4)
		r14 = memw(r4+#8)
	}
	{
		r4 = add(r11,##-38528)
		r15 = memw(r4+#12)
	}
	{
		r0 = vtrunehb(r1:0)
		r9:8 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r0)
		r1 = add(r11,##-38272)
		memd(r4+#56) = r9:8
	}
	{
		r9:8 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		v1 = valign(v1,v1,#4)
		memd(r4+#48) = r9:8
		r9:8 = memd(r30+##-8496)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#40) = r9:8
		r9:8 = memd(r30+##-8240)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#32) = r9:8
		r9:8 = memd(r30+#-7984)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#24) = r9:8
		r9:8 = memd(r30+#-7728)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#16) = r9:8
		r9:8 = memd(r30+#-7472)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#8) = r9:8
		r9:8 = memd(r30+#-5424)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#0) = r9:8
	}
	{
		v0.cur = vmem(r4+#0)
		vmem(r28+#0) = v0
	}
	{
		v0 = v10
		r4 = memw(r28+#56)
	}
	{
		r2 = vtrunehb(r17:16)
		v0.w = vinsert(r2)
		r5 = memw(r28+#60)
	}
	{
		memd(r30+#-5424) = r5:4
	}                                       // 8-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r4 = memw(r28+#48)
		r5 = memw(r28+#52)
	}
	{
		memd(r30+#-7472) = r5:4
		r26 = memw(r28+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r27 = memw(r28+#44)
		r24 = memw(r28+#32)
	}
	{
		r25 = memw(r28+#36)
		r18 = memw(r28+#24)
	}
	{
		r3 = vtrunehb(r21:20)
		memw(r30+##-7728) = r3
		r19 = memw(r28+#28)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = vtrunehb(r7:6)
		r12 = memw(r28+#16)
		memw(r30+##-7984) = r2

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v0.w = vinsert(r2)
		r13 = memw(r28+#20)
		r16 = memw(r28+#0)
	}
	{
		r17 = memw(r28+#4)
		r8 = memw(r28+#8)
	}
	{
		r28 = add(r11,##-46720)
		r9 = memw(r28+#12)
		r7:6 = memd(r30+#-3120)
	}                                       // 8-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		memd(r1+#48) = r7:6
	}
	{
		r8 = vtrunehb(r9:8)
		v0.w = vinsert(r3)
		r5:4 = memd(r30+#-2864)
	}                                       // 8-byte Folded Reload
	{
		memd(r1+#40) = r5:4
	}
	{
		v0 = valign(v0,v0,#4)
		r5:4 = memd(r30+#-2608)
	}                                       // 8-byte Folded Reload
	{
		memd(r1+#32) = r5:4
		r5:4 = memd(r30+#-2352)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#24) = r5:4
		r5:4 = memd(r30+#-2096)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#16) = r5:4
		r5:4 = memd(r30+#-1840)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#8) = r5:4
		r5:4 = memd(r30+#-1328)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#0) = r5:4
		r5:4 = memd(r30+#-304)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#56) = r5:4
	}
	{
		v2.cur = vmem(r1+#0)
		vmem(r28+#0) = v2
	}
	{
		v2 = v10
		r1:0 = memd(r30+#-4400)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		memw(r30+#-304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = vtrunehb(r15:14)
		r22 = memw(r28+#56)
		r23 = memw(r28+#60)
	}
	{
		r0 = vtrunehb(r17:16)
		v1.w = vinsert(r0)
		r14 = memw(r28+#48)
		r7:6 = memd(r30+#-3888)
	}                                       // 8-byte Folded Reload
	{
		r10 = vtrunehb(r7:6)
		v2.w = vinsert(r0)
		r7:6 = memd(r30+#-4656)
	}                                       // 8-byte Folded Reload
	{
		r2 = vtrunehb(r7:6)
		v1 = valign(v1,v1,#4)
		r15 = memw(r28+#52)
		r20 = memw(r28+#40)
	}
	{
		v0.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
		r21 = memw(r28+#44)
		r4 = memw(r28+#32)
	}
	{
		v2.w = vinsert(r8)
		r5 = memw(r28+#36)
		r16 = memw(r28+#24)
	}
	{
		r3:2 = memd(r30+#-5680)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		v2 = valign(v2,v2,#4)
		r17 = memw(r28+#28)
		r2 = memw(r28+#16)
	}
	{
		v1.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r3 = memw(r28+#20)
		r0 = memw(r28+#0)
	}
	{
		v0.w = vinsert(r10)
		r7:6 = memd(r30+#-5168)
		r1 = memw(r28+#4)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		r1 = vtrunehb(r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		v10.w = vinsert(r0)
		v2.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r0 = memw(r28+#8)
	}
	{
		r6 = vtrunehb(r7:6)
		r1 = memw(r28+#12)
		r13:12 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		r1 = vtrunehb(r19:18)
		r18 = #-1
		v3 = valign(v10,v10,#4)
	}
	{
		v3.w = vinsert(r0)
		v1.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r3:2)
		v2.w = vinsert(r1)
	}
	{
		r0 = vtrunehb(r13:12)
		r2 = vtrunehb(r7:6)
		v3 = valign(v3,v3,#4)
	}
	{
		r1 = vtrunehb(r17:16)
		v3.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
	}
	{
		r0 = vtrunehb(r25:24)
		v1.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r3 = vtrunehb(r27:26)
		v2.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r1)
		v0.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r0 = vtrunehb(r5:4)
		v1.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r5:4)
		v2.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r30+#-816)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r7:6)
		v3.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
		r3:2 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r21:20)
		v1.w = vinsert(r1)
		r7 = #100
		v2 = valign(v2,v2,#4)
	}
	{
		r2 = vtrunehb(r3:2)
		r6 = vtrunehb(r23:22)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v2.w = vinsert(r2)
		r1 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r2 = vtrunehb(r15:14)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r30+#-5424)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		v1.w = vinsert(r0)
		r5 = #100
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
		r4 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		v2.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r3 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r4)
		r3 = add(r3,#1)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v1 = vror(v1,r5)
		r6 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v2 = vror(v2,r5)
		v1 = vor(v1,v9)
	}
	{
		v3 = vror(v3,r5)
		v2 = vor(v2,v9)
		v1.ub = vmin(v1.ub,v22.ub)
		r5 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r7)
		v3 = vor(v3,v9)
		v2.ub = vmin(v2.ub,v22.ub)
		v1.ub = vmax(v1.ub,v23.ub)
	}
	{
		v3.ub = vmin(v3.ub,v22.ub)
		v2.ub = vmax(v2.ub,v23.ub)
		v0 = vor(v0,v9)
	}
	{
		v3.ub = vmax(v3.ub,v23.ub)
		v0.ub = vmin(v0.ub,v22.ub)
		r4 = memw(r30+##-23216)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r5,r6)
		v1 = vdelta(v1,v18)
	}
	{
		v2 = vdelta(v2,v15)
		v0.ub = vmax(v0.ub,v23.ub)
		r2 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r3,r4)
		r5 = add(r30,#-7216)
		v3 = vdelta(v3,v14)
		v1 = vmux(q0,v2,v1)
	}
	{
		r7 = add(r30,#-6960)
		r4 = add(r30,#-7088)
		r6 = add(r30,#-6832)
		v0 = vmux(q1,v3,v0)
	}
	{
		r2 = add(r2,#2)
		v0 = vmux(q2,v1,v0)
		v1 = vmemu(r6+#0)
	}                                       // 256-byte Folded Reload
	{
		v2 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		vmemu(r0+#0) = v0
	}
	{
		v0 = vmemu(r7+#0)
	}                                       // 256-byte Folded Reload
	{
		if (p0) jump:nt .LBB131_66
		v3 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
.LBB131_71:                             // %"for output.s0.x.xo"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        //       Parent Loop BB131_67 Depth=3
                                        // =>      This Loop Header: Depth=4
                                        //           Child Loop BB131_75 Depth 5
                                        //             Child Loop BB131_76 Depth 6
	{
		r0 = asl(r3,#1)
		r5 = add(r11,#-27392)
		r7 = memw(r30+##-10320)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-7232)
		r1 = memw(r30+##-21808)
	}                                       // 4-byte Folded Reload
	{
		r10 = add(r5,#1024)
		r28 = add(r5,#1152)
		r6 = memw(r30+##-10312)
	}                                       // 4-byte Folded Reload
	{
		r0 = min(r0,r1)
		r1 = add(r30,#-6960)
		r15 = add(r5,#1280)
		r14 = add(r5,#1408)
	}
	{
		r1 = add(r30,#-6832)
		r13 = add(r5,#1536)
		vmemu(r1+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-7216)
		r8 = add(r5,#1664)
		vmemu(r1+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r9 = add(r5,#1792)
		r12 = add(r5,#1920)
		vmemu(r1+#0) = v2
	}                                       // 256-byte Folded Spill
	{
		r1 = add(r30,#-7088)
		memw(r30+##-10800) = r2
		vmem(r4+#0) = v3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-10928) = r3
		vmem(r6+#0) = v0
	}                                       // 4-byte Folded Spill
	{
		vmem(r7+#0) = v1
	}
	{
		vmem(r5+#0) = v2
	}
	{
		vmem(r5+#4) = v2
	}
	{
		vmem(r5+#5) = v3
	}
	{
		vmem(r5+#6) = v0
	}
	{
		vmem(r5+#7) = v1
	}
	{
		vmem(r10+#0) = v2
	}
	{
		vmem(r28+#0) = v3
	}
	{
		vmem(r15+#0) = v0
	}
	{
		vmem(r14+#0) = v1
	}
	{
		vmem(r13+#0) = v2
	}
	{
		vmem(r8+#0) = v3
	}
	{
		vmem(r9+#0) = v0
	}
	{
		vmem(r12+#0) = v1
	}
	{
		if (p3) jump:nt ##.LBB131_69
		vmemu(r1+#0) = v3
	}                                       // 256-byte Folded Spill
// %bb.72:                              // %next_bb27
                                        //   in Loop: Header=BB131_71 Depth=4
	{
		r1 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r0,r1)
	}
	{
		r4 = add(r1,#1)
		if (!p2) jump:nt .LBB131_79
	}
// %bb.73:                              // %"for convolved.s1.r19$y.preheader"
                                        //   in Loop: Header=BB131_71 Depth=4
	{
		if (!p1) jump:nt .LBB131_79
	}
// %bb.74:                              //   in Loop: Header=BB131_71 Depth=4
	{
		v5:4 = vcombine(v1,v0)
		v7:6 = vcombine(v3,v2)
		r0 = memw(r30+##-21808)
	}                                       // 4-byte Folded Reload
	{
		v19:18 = vcombine(v3,v2)
		v9:8 = vcombine(v5,v4)
		memw(r30+##-7472) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = min(r2,r0)
		v17:16 = vcombine(v3,v2)
		r7 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		v5:4 = vcombine(v3,v2)
		r5 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		v11:10 = vcombine(v7,v6)
		v21:20 = vcombine(v19,v18)
		memw(r30+##-11056) = r4
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r5,r0)
		v19:18 = vcombine(v17,v16)
		r5 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		r6 = #0
		v15:14 = vcombine(v1,v0)
		r4 = memw(r30+##-23312)
	}                                       // 4-byte Folded Reload
	{
		v13:12 = vcombine(v1,v0)
		v7:6 = vcombine(v1,v0)
		r28 = memw(r30+##-23304)
	}                                       // 4-byte Folded Reload
	{
		r1 = mpyi(r5,r1)
		r0 = add(r4,r0)
		v17:16 = vcombine(v5,v4)
	}
	{
		r0 = mpyi(r5,r0)
		r4 = memw(r30+##-23280)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r4,r1)
		r5 = memw(r30+##-23288)
	}                                       // 4-byte Folded Reload
	{
		r2 = asl(r2,#7)
		r3 = add(r5,r0)
		r0 = add(r4,r0)
		r1 = add(r5,r1)
	}
	{
		r4 = asl(r3,#7)
		r0 = asl(r0,#7)
		memw(r30+##-4400) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = asl(r1,#7)
		memw(r30+#-2352) = r4
		memw(r30+##-4656) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+#-2608) = r2
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_75:                             // %"for convolved.s1.r19$y.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        //       Parent Loop BB131_67 Depth=3
                                        //         Parent Loop BB131_71 Depth=4
                                        // =>        This Loop Header: Depth=5
                                        //             Child Loop BB131_76 Depth 6
	{
		r2 = add(r30,#-1584)
		r0 = add(r30,#-2096)
		r4 = add(r30,#-1840)
		r5 = add(r30,#-1968)
	}
	{
		r2 = add(r30,#-1072)
		r3 = add(r30,#-1712)
		vmemu(r2+#0) = v14
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-1328)
		vmemu(r4+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		vmemu(r0+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-560)
		vmemu(r2+#0) = v10
	}                                       // 256-byte Folded Spill
	{
		r5 = add(r30,#-1456)
		vmemu(r5+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		r3 = add(r30,#-1200)
		vmemu(r3+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-816)
		vmemu(r4+#0) = v12
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v6
	}                                       // 256-byte Folded Spill
	{
		r2 = memw(r30+##-5696)
		memw(r30+##-5168) = r6
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r30,#-944)
		vmemu(r5+#0) = v15
	}                                       // 256-byte Folded Spill
	{
		loop0(.LBB131_76,r2)
		r3 = add(r30,#-688)
		vmemu(r3+#0) = v13
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-304)
		vmemu(r4+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		r5 = add(r30,#-432)
		vmemu(r5+#0) = v11
	}                                       // 256-byte Folded Spill
	{
		r3 = add(r30,#-176)
		vmemu(r3+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		vmemu(r4+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		vmemu(r5+#0) = v7
	}                                       // 256-byte Folded Spill
	{
		vmemu(r3+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		memw(r30+##-5424) = r7
		memw(r30+##-5680) = r28
	}                                       // 4-byte Folded Spill
	.p2align	4
.Ltmp22:                                // Block address taken
.LBB131_76:                             // %"for convolved.s1.r19$x.us"
                                        //   Parent Loop BB131_4 Depth=1
                                        //     Parent Loop BB131_42 Depth=2
                                        //       Parent Loop BB131_67 Depth=3
                                        //         Parent Loop BB131_71 Depth=4
                                        //           Parent Loop BB131_75 Depth=5
                                        // =>          This Inner Loop Header: Depth=6
	{
		v20 = vxor(v20,v20)
		r0 = memw(r30+##-4400)
		v0 = vmem(r28+#-1)
	}                                       // 4-byte Folded Reload
	{
		r6 = #64
		r14 = add(r7,r0)
		r2 = memw(r30+#-3912)
	}                                       // 4-byte Folded Reload
	{
		v24 = v20
		r8 = memw(r30+##-4656)
		r5:4 = memd(r7+r0<<#0)
	}                                       // 4-byte Folded Reload
	{
		r13 = add(r7,r8)
		v17:16.w = vunpack(v0.h)
		r1:0 = memd(r14+#8)
	}
	{
		r20 = r7
		v21 = valign(v0,v0,r6)
		v0 = v20
		v12 = vmem(r2+#0)
	}
	{
		v0.w = vinsert(r4)
		v13 = vror(v20,r6)
		v10 = v20
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r15 = add(r2,##-36480)
		r12 = #116
		r2 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v0,v0,#4)
		v23:22 = vcombine(v20,v20)
	}
	{
		v3.w = vinsert(r5)
		v9:8 = vcombine(v20,v20)
		v17 = v20
	}
	{
		q0 = vcmp.eq(v1.b,v20.b)
		r2 = memw(r30+#-2608)
		v1.cur = vmem(r2+#0)
	}                                       // 4-byte Folded Reload
	{
		r21 = add(r7,r2)
		v5:4 = vcombine(v20,v20)
		r5:4 = memd(r14+#24)
		v1 = vmem(r28+#0)
	}
	{
		v3 = valign(v3,v3,#4)
		v27:26 = vcombine(v20,v20)
	}
	{
		v3.w = vinsert(r0)
		r0 = add(r30,#-3632)
		v19:18.w = vunpack(v1.h)
	}
	{
		r0 = add(r30,#-3120)
		v7:6 = vcombine(v20,v20)
		vmemu(r0+#0) = v20
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r30,#-3376)
		v29:28 = vcombine(v20,v20)
		vmemu(r0+#0) = v20
	}                                       // 128-byte Folded Spill
	{
		v15:14 = vcombine(v20,v20)
		v19 = v20
		vmemu(r0+#0) = v20
	}                                       // 128-byte Folded Spill
	{
		v3 = valign(v3,v3,#4)
		v31:30 = vcombine(v20,v20)
		r0 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r1)
		r0 = add(r7,r0)
		r7:6 = memd(r14+#16)
		memw(r30+#-2864) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v24.w = vinsert(r6)
		r3:2 = memd(r14+#48)
		r1:0 = memd(r14+#32)
	}
	{
		v10.w = vinsert(r2)
		v22.w = vinsert(r0)
		r11:10 = memd(r14+#64)
		r19:18 = memd(r14+#80)
	}
	{
		v23.w = vinsert(r18)
		v9.w = vinsert(r10)
		v11 = valign(v24,v24,#4)
	}
	{
		v11.w = vinsert(r7)
		v24 = valign(v10,v10,#4)
		r7:6 = memd(r14+#56)
	}
	{
		v24.w = vinsert(r3)
		v22 = valign(v22,v22,#4)
		r3:2 = memd(r14+#40)
		r23:22 = memd(r14+#88)
	}
	{
		v22.w = vinsert(r1)
		v11 = valign(v11,v11,#4)
		r17:16 = memd(r14+#72)
	}
	{
		v11.w = vinsert(r4)
		v23 = valign(v23,v23,#4)
	}
	{
		v23.w = vinsert(r19)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r11)
		v25 = valign(v11,v11,#4)
	}
	{
		v25.w = vinsert(r5)
		v11 = valign(v24,v24,#4)
		r5:4 = memd(r28+#184)
	}
	{
		v11.w = vinsert(r6)
		v9 = valign(v9,v9,#4)
		memd(r15+#120) = r5:4
		r5:4 = memd(r28+#176)

	} :mem_noshuf
	{
		v9.w = vinsert(r16)
		v10 = vror(v25,r12)
		memd(r15+#112) = r5:4
	}
	{
		v3 = vror(v3,r12)
		v10 = vor(v10,v13)
		r5:4 = memd(r28+#168)
	}
	{
		v9 = valign(v9,v9,#4)
		v3 = vor(v3,v13)
		memd(r15+#104) = r5:4
	}
	{
		v9.w = vinsert(r17)
		v1:0.uh = vunpack(v10.ub)
		r5:4 = memd(r28+#160)
	}
	{
		v10 = valign(v11,v11,#4)
		memd(r15+#96) = r5:4
	}
	{
		v10.w = vinsert(r7)
		v25:24.uw = vunpack(v0.uh)
		r5:4 = memd(r28+#152)
	}
	{
		v9 = vror(v9,r12)
		memd(r15+#88) = r5:4
	}
	{
		v10 = vror(v10,r12)
		v9 = vor(v9,v13)
		r7:6 = memd(r28+#144)
	}
	{
		v11 = vdelta(v24,v12)
		v10 = vor(v10,v13)
		memd(r15+#80) = r7:6
		r1:0 = memd(r28+#136)

	} :mem_noshuf
	{
		v1:0.uh = vunpack(v10.ub)
		memd(r15+#72) = r1:0
	}
	{
		v10 = valign(v22,v22,#4)
		r1:0 = memd(r28+#128)
	}
	{
		v10.w = vinsert(r2)
		v22 = valign(v23,v23,#4)
		memd(r15+#64) = r1:0
	}
	{
		v22.w = vinsert(r22)
		v25:24.uw = vunpack(v0.uh)
		r1:0 = memd(r28+#120)
	}
	{
		v10 = valign(v10,v10,#4)
		memd(r15+#56) = r1:0
	}
	{
		v10.w = vinsert(r3)
		v22 = valign(v22,v22,#4)
		r1:0 = memd(r28+#112)
	}
	{
		v22.w = vinsert(r23)
		v23 = vdelta(v24,v12)
		memd(r15+#48) = r1:0
	}
	{
		v10 = vror(v10,r12)
		r1:0 = memd(r28+#104)
	}
	{
		v22 = vror(v22,r12)
		v10 = vor(v10,v13)
		memd(r15+#40) = r1:0
	}
	{
		v25:24.uh = vunpack(v3.ub)
		v22 = vor(v22,v13)
		r1:0 = memd(r28+#96)
	}
	{
		v3:2.uh = vunpack(v10.ub)
		memd(r15+#32) = r1:0
	}
	{
		v1:0.uh = vunpack(v22.ub)
		r1:0 = memd(r28+#88)
	}
	{
		v3:2.uw = vunpack(v2.uh)
		memd(r15+#24) = r1:0
	}
	{
		v1:0.uw = vunpack(v0.uh)
		v10 = vmux(q0,v23,v2)
		r1:0 = memd(r28+#80)
	}
	{
		v3:2.uh = vunpack(v9.ub)
		memd(r15+#16) = r1:0
	}
	{
		v25:24.uw = vunpack(v24.uh)
		r1:0 = memd(r28+#72)
	}
	{
		v0 = vdelta(v0,v12)
		v1 = vmux(q0,v11,v24)
		memd(r15+#8) = r1:0
	}
	{
		v23:22.w = vunpack(v21.h)
		r1:0 = memd(r28+#64)
	}
	{
		v3.w = vmpyieo(v1.h,v16.h)
		r28 = add(r28,#256)
		v25:24.uw = vunpack(v2.uh)
		memd(r15+#0) = r1:0
	}
	{
		v2.w = vmpyieo(v10.h,v22.h)
		v23 = vmux(q0,v0,v24)
		r17:16 = memd(r20+r8<<#0)
		memw(r30+#-3888) = r20
	}                                       // 4-byte Folded Spill
	{
		v3.w += vmpyie(v1.w,v16.h)
		r1:0 = memd(r13+#16)
		r9:8 = memd(r13+#32)
	}
	{
		v5.w = vinsert(r0)
		v6.w = vinsert(r16)
		r0 = memw(r30+#-2608)
		r11:10 = memd(r13+#64)
	}                                       // 4-byte Folded Reload
	{
		v2.w += vmpyie(v10.w,v22.h)
		r25:24 = memd(r13+#48)
		r3:2 = memd(r13+#96)
	}
	{
		r15 = r1
		v6 = valign(v6,v6,#4)
		r19:18 = memd(r13+#80)
		v0 = vmem(r15+#0)
	}
	{
		v26.w = vinsert(r2)
		v25:24.w = vunpack(v0.h)
		r23:22 = memd(r14+#96)
		r1:0 = memd(r20+r0<<#0)
	}
	{
		v8.w = vinsert(r22)
		v27.w = vinsert(r18)
		r27:26 = memd(r14+#112)
	}
	{
		v17.w = vinsert(r0)
		v7.w = vinsert(r26)
		r0 = add(r30,#-304)
		v9 = valign(v26,v26,#4)
	}
	{
		v6.w = vinsert(r17)
		r0 = add(r30,#-176)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v9.w = vinsert(r3)
		v25 = valign(v8,v8,#4)
		r7:6 = memd(r21+#16)
		r5:4 = memd(r13+#112)
	}
	{
		v25.w = vinsert(r23)
		v15.w = vinsert(r24)
		v8 = valign(v27,v27,#4)
		r3:2 = memd(r14+#104)
	}
	{
		v31.w = vinsert(r6)
		r0 = add(r30,#-304)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = valign(v7,v7,#4)
		v27:26.w = vadd(v3:2.w,v27:26.w)
		r23:22 = memd(r21+#32)
	}
	{
		v7.w = vinsert(r27)
		v19.w = vinsert(r4)
		v10 = valign(v17,v17,#4)
		r27:26 = memd(r21+#48)
	}
	{
		v10.w = vinsert(r1)
		v14.w = vinsert(r10)
		v5 = valign(v5,v5,#4)
		r17:16 = memd(r21+#80)
	}
	{
		v28.w = vinsert(r16)
		r0 = add(r30,#-176)
		vmemu(r0+#0) = v26
	}                                       // 256-byte Folded Spill
	{
		v5.w = vinsert(r15)
		v30.w = vinsert(r22)
		vmemu(r0+#0) = v27
	}                                       // 256-byte Folded Spill
	{
		v8.w = vinsert(r19)
		v29.w = vinsert(r26)
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r14+#120)
	}
	{
		v7.w = vinsert(r0)
		v4.w = vinsert(r8)
		v2 = valign(v28,v28,#4)
	}
	{
		v2.w = vinsert(r17)
		v3.w = vmpyieo(v23.h,v18.h)
		v28 = valign(v25,v25,#4)
	}
	{
		v28.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r1)
		v1 = valign(v15,v15,#4)
	}
	{
		v1.w = vinsert(r25)
		v15 = valign(v19,v19,#4)
	}
	{
		v15.w = vinsert(r5)
		v11 = valign(v31,v31,#4)
		r5:4 = memd(r13+#8)
	}
	{
		v11.w = vinsert(r7)
		v19 = valign(v28,v28,#4)
		r7:6 = memd(r13+#24)
		r15:14 = memd(r13+#40)
	}
	{
		v19.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r19:18 = memd(r13+#72)
	}
	{
		v5.w = vinsert(r6)
		v0 = valign(v14,v14,#4)
		r25:24 = memd(r13+#104)
		r6 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r11)
		v7 = vror(v7,r12)
		r11:10 = memd(r13+#56)
	}
	{
		v3.w += vmpyie(v23.w,v18.h)
		v14 = valign(v30,v30,#4)
		v7 = vor(v7,v13)
	}
	{
		v14.w = vinsert(r23)
		v27 = valign(v29,v29,#4)
		r23:22 = memd(r13+#88)
	}
	{
		v27.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r13+#120)
	}
	{
		v6.w = vinsert(r4)
		r4 = r21
		v8 = valign(v8,v8,#4)
		r21:20 = memd(r21+#8)
	}
	{
		v8.w = vinsert(r22)
		v9 = valign(v9,v9,#4)
		r1:0 = memd(r4+#40)
	}
	{
		v9.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r7)
		v19 = vror(v19,r12)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r10)
		v29:28.uh = vunpack(v7.ub)
		v7 = vor(v19,v13)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r9)
		v6 = valign(v6,v6,#4)
		r9:8 = memd(r4+#24)
	}
	{
		v6.w = vinsert(r5)
		v8 = valign(v8,v8,#4)
		r3:2 = memd(r4+#56)
	}
	{
		v8.w = vinsert(r23)
		v17 = valign(v27,v27,#4)
	}
	{
		v17.w = vinsert(r2)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r25)
		v5 = vror(v5,r12)
	}
	{
		v27:26.uw = vunpack(v28.uh)
		v5 = vor(v5,v13)
	}
	{
		v31:30.uh = vunpack(v7.ub)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r11)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r26)
		r26 = r4
		v29 = vdelta(v26,v12)
	}
	{
		v27:26.uw = vunpack(v30.uh)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r14)
		v6 = vror(v6,r12)
	}
	{
		v7 = vror(v8,r12)
		v6 = vor(v6,v13)
	}
	{
		v8 = vror(v9,r12)
		v9 = vmux(q0,v29,v26)
	}
	{
		v29:28.uh = vunpack(v5.ub)
	}
	{
		v1 = vror(v1,r12)
	}
	{
		v0 = valign(v0,v0,#4)
		v1 = vor(v1,v13)
	}
	{
		v0.w = vinsert(r18)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r8)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r27)
		v27:26.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v6.ub)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r15)
		v6 = vdelta(v26,v12)
	}
	{
		v27:26.uh = vunpack(v1.ub)
		v1 = vor(v7,v13)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r19)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r9)
		v5 = vror(v15,r12)
	}
	{
		v29:28.uw = vunpack(v28.uh)
		v5 = vor(v5,v13)
	}
	{
		v4 = vror(v4,r12)
		v6 = vmux(q0,v6,v28)
	}
	{
		v29:28.uh = vunpack(v1.ub)
		v1 = vor(v4,v13)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r3)
		v0 = vror(v0,r12)
		r3:2 = memd(r4+#88)
	}
	{
		v31:30.uh = vunpack(v5.ub)
		v0 = vor(v0,v13)
	}
	{
		v5:4.uw = vunpack(v28.uh)
	}
	{
		v11 = vror(v11,r12)
	}
	{
		v5 = vdelta(v26,v12)
	}
	{
		v27:26.uh = vunpack(v1.ub)
		v1 = vor(v11,v13)
	}
	{
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r20)
		v29:28.uw = vunpack(v30.uh)
	}
	{
		v31:30.uh = vunpack(v0.ub)
	}
	{
		v7 = vror(v17,r12)
	}
	{
		v1:0.uh = vunpack(v1.ub)
	}
	{
		v10 = valign(v10,v10,#4)
		v1 = vor(v7,v13)
		v7 = vor(v8,v13)
	}
	{
		v10.w = vinsert(r21)
		v15 = vdelta(v28,v12)
	}
	{
		v29:28.uw = vunpack(v30.uh)
	}
	{
		v31:30.uh = vunpack(v1.ub)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r0)
		r0 = add(r30,#-560)
		v1:0.uw = vunpack(v0.uh)
	}
	{
		v31:30.uw = vunpack(v30.uh)
	}
	{
		v10 = vror(v10,r12)
	}
	{
		v11:10.uh = vunpack(v7.ub)
		v8 = vor(v10,v13)
	}
	{
		v2.w = vmpyieo(v9.h,v24.h)
		v7 = valign(v2,v2,#4)
	}
	{
		v2.w += vmpyie(v9.w,v24.h)
		v1 = vdelta(v0,v12)
	}
	{
		v7.w = vinsert(r2)
		r2 = add(r30,#-1840)
		v0 = vdelta(v30,v12)
	}
	{
		r0 = add(r30,#-432)
		v30 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-560)
		v31 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27:26.uw = vunpack(v26.uh)
		v31:30.w = vadd(v3:2.w,v31:30.w)
	}
	{
		v27.w = vmpyieo(v6.h,v16.h)
		v14 = valign(v14,v14,#4)
		v5 = vmux(q0,v5,v26)
	}
	{
		v27.w += vmpyie(v6.w,v16.h)
		r0 = add(r30,#-432)
		vmemu(r0+#0) = v30
	}                                       // 256-byte Folded Spill
	{
		v26.w = vmpyieo(v5.h,v22.h)
		v14.w = vinsert(r1)
		vmemu(r0+#0) = v31
	}                                       // 256-byte Folded Spill
	{
		v26.w += vmpyie(v5.w,v22.h)
		v31:30.uh = vunpack(v8.ub)
		r1:0 = memd(r4+#64)
	}
	{
		r2 = add(r30,#-1712)
		v8 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-1840)
		v9 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5 = valign(v7,v7,#4)
		v9:8.w = vadd(v27:26.w,v9:8.w)
	}
	{
		v5.w = vinsert(r3)
		v3 = vror(v14,r12)
	}
	{
		r2 = add(r30,#-1712)
		v3 = vor(v3,v13)
		vmemu(r2+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-3632)
		vmemu(r2+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v8 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v8.w = vinsert(r0)
		r0 = add(r30,#-3376)
		v7:6.uw = vunpack(v30.uh)
		r3:2 = memd(r4+#112)
	}
	{
		v7 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v7.w = vinsert(r2)
		v2 = valign(v8,v8,#4)
	}
	{
		v2.w = vinsert(r1)
		v27:26.uh = vunpack(v3.ub)
		r1:0 = memd(r4+#72)
	}
	{
		v3 = vror(v5,r12)
	}
	{
		v5 = valign(v7,v7,#4)
		v7 = vor(v3,v13)
	}
	{
		v5.w = vinsert(r3)
		v9 = valign(v2,v2,#4)
		r3:2 = memd(r4+#96)
		r5:4 = memd(r4+#120)
	}
	{
		v9.w = vinsert(r0)
		r0 = add(r30,#-3120)
		v4 = vdelta(v4,v12)
	}
	{
		v3:2.uw = vunpack(v26.uh)
		v4 = vmux(q0,v4,v28)
	}
	{
		v29:28.uh = vunpack(v7.ub)
		v0 = vmux(q0,v0,v2)
	}
	{
		v3 = valign(v5,v5,#4)
	}
	{
		v3.w = vinsert(r4)
		v5 = valign(v9,v9,#4)
	}
	{
		v5.w = vinsert(r1)
		v7 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v7.w = vinsert(r2)
		v9:8.uw = vunpack(v28.uh)
		r1:0 = memd(r6+#16)
		r2 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		v9 = valign(v3,v3,#4)
		r7 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		v9.w = vinsert(r5)
		v3 = vdelta(v8,v12)
		v8 = v20
		r5:4 = memd(r26+#104)
	}
	{
		v8.w = vinsert(r0)
		r0 = add(r30,#-1328)
		v11:10.uw = vunpack(v10.uh)
	}
	{
		v15.w = vmpyieo(v4.h,v18.h)
		v7 = valign(v7,v7,#4)
		v10 = vmux(q0,v15,v10)
	}
	{
		v7.w = vinsert(r3)
		v5 = vror(v5,r12)
		r3:2 = memd(r7+r2<<#0)
	}
	{
		v15.w += vmpyie(v4.w,v18.h)
		v8 = valign(v8,v8,#4)
		v4 = vor(v5,v13)
	}
	{
		v8.w = vinsert(r1)
		v14.w = vmpyieo(v10.h,v24.h)
		v5 = vror(v9,r12)
	}
	{
		v14.w += vmpyie(v10.w,v24.h)
		v7 = valign(v7,v7,#4)
		v5 = vor(v5,v13)
	}
	{
		v7.w = vinsert(r4)
		v31:30.uh = vunpack(v4.ub)
	}
	{
		v29:28.uh = vunpack(v5.ub)
	}
	{
		v5 = valign(v8,v8,#4)
	}
	{
		r0 = add(r30,#-1200)
		v8 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v4 = valign(v7,v7,#4)
		v7 = v20
	}
	{
		v4.w = vinsert(r5)
		v7.w = vinsert(r2)
		v9 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-1328)
		v15:14.uw = vunpack(v30.uh)
		v9:8.w = vadd(v15:14.w,v9:8.w)
		r5:4 = memd(r6+#24)
	}
	{
		v5.w = vinsert(r4)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r3)
		r0 = add(r30,#-1200)
		vmemu(r0+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v5 = valign(v5,v5,#4)
		v8 = v20
	}
	{
		v5.w = vinsert(r5)
		vmemu(r0+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r6+#48)
		r5:4 = memd(r6+#8)
	}
	{
		v8.w = vinsert(r0)
		v7.w = vinsert(r4)
		v4 = vror(v4,r12)
	}
	{
		v5 = vror(v5,r12)
		v9 = vor(v4,v13)
	}
	{
		v15 = valign(v8,v8,#4)
		v11 = vor(v5,v13)
		v5 = vmux(q0,v1,v6)
		v1 = v20
	}
	{
		v15.w = vinsert(r1)
		v6 = valign(v7,v7,#4)
		r1:0 = memd(r6+#32)
	}
	{
		v6.w = vinsert(r5)
		v1.w = vinsert(r0)
		v31:30.uw = vunpack(v28.uh)
		r5:4 = memd(r6+#56)
	}
	{
		v27:26.uh = vunpack(v9.ub)
		r3:2 = memd(r6+#88)
	}
	{
		v29:28.uh = vunpack(v11.ub)
	}
	{
		v7 = valign(v15,v15,#4)
	}
	{
		v7.w = vinsert(r4)
		v9:8.uw = vunpack(v26.uh)
	}
	{
		v11:10.uw = vunpack(v28.uh)
	}
	{
		v9 = valign(v1,v1,#4)
	}
	{
		v9.w = vinsert(r1)
		v1 = vror(v6,r12)
		r1:0 = memd(r6+#80)
	}
	{
		v6 = valign(v7,v7,#4)
		v7 = vor(v1,v13)
	}
	{
		v6.w = vinsert(r5)
		v1 = vdelta(v10,v12)
		v10 = v20
		r5:4 = memd(r6+#40)
	}
	{
		v10.w = vinsert(r0)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r4)
		v2 = vror(v6,r12)
	}
	{
		v6 = valign(v10,v10,#4)
		v2 = vor(v2,v13)
	}
	{
		v6.w = vinsert(r1)
		v4 = vdelta(v30,v12)
		r1:0 = memd(r6+#64)
	}
	{
		v29:28.uh = vunpack(v2.ub)
		v2 = vmux(q0,v3,v14)
		v3 = v20
	}
	{
		v3.w = vinsert(r0)
		v31:30.uh = vunpack(v7.ub)
		v4 = vmux(q0,v4,v8)
	}
	{
		v7 = valign(v9,v9,#4)
		v9 = v20
	}
	{
		v7.w = vinsert(r5)
		v6 = valign(v6,v6,#4)
		r5:4 = memd(r6+#112)
	}
	{
		v6.w = vinsert(r2)
		v20.w = vinsert(r4)
		r2 = add(r30,#-2096)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v15:14.uw = vunpack(v28.uh)
		r1:0 = memd(r6+#96)
	}
	{
		v9.w = vinsert(r0)
		v11:10.uw = vunpack(v30.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		v1 = vmux(q0,v1,v10)
	}
	{
		v6.w = vinsert(r3)
		r3 = add(r30,#-1968)
		v11 = vdelta(v14,v12)
	}
	{
		v14 = valign(v20,v20,#4)
	}
	{
		v14.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r6+#72)
	}
	{
		v3.w = vinsert(r4)
		v7 = vror(v7,r12)
	}
	{
		v6 = vror(v6,r12)
		v7 = vor(v7,v13)
	}
	{
		v9 = valign(v9,v9,#4)
		v6 = vor(v6,v13)
	}
	{
		v9.w = vinsert(r1)
		v14 = valign(v14,v14,#4)
		r1:0 = memd(r6+#120)
	}
	{
		v14.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r5)
		v7.w = vmpyieo(v5.h,v16.h)
		v21:20.uh = vunpack(v7.ub)
		r5:4 = memd(r6+#104)
	}
	{
		v7.w += vmpyie(v5.w,v16.h)
		v31:30.uh = vunpack(v6.ub)
	}
	{
		v6 = valign(v9,v9,#4)
	}
	{
		v6.w = vinsert(r4)
		r4 = add(r30,#-2096)
		v5 = valign(v14,v14,#4)
	}
	{
		v5.w = vinsert(r1)
		v9:8.uw = vunpack(v30.uh)
	}
	{
		v6.w = vmpyieo(v0.h,v22.h)
		v9 = valign(v6,v6,#4)
	}
	{
		v9.w = vinsert(r5)
		r5 = add(r30,#-1968)
		v5 = vror(v5,r12)
	}
	{
		v6.w += vmpyie(v0.w,v22.h)
		v3 = vror(v3,r12)
		v5 = vor(v5,v13)
	}
	{
		v9.w = vmpyieo(v1.h,v16.h)
		v0 = vror(v9,r12)
		v3 = vor(v3,v13)
	}
	{
		v29:28.uh = vunpack(v5.ub)
		v0 = vor(v0,v13)
	}
	{
		v3.w = vmpyieo(v2.h,v18.h)
		v15:14.uh = vunpack(v3.ub)
	}
	{
		v3.w += vmpyie(v2.w,v18.h)
		v27:26.uw = vunpack(v28.uh)
	}
	{
		v9.w += vmpyie(v1.w,v16.h)
		v29:28.uh = vunpack(v0.ub)
	}
	{
		v8 = vdelta(v8,v12)
	}
	{
		v15:14.uw = vunpack(v14.uh)
	}
	{
		v21:20.uw = vunpack(v20.uh)
		v5 = vmux(q0,v8,v14)
	}
	{
		v31:30.uw = vunpack(v28.uh)
		v0 = vmux(q0,v11,v20)
	}
	{
		v31.w = vmpyieo(v5.h,v18.h)
		v2 = vdelta(v26,v12)
	}
	{
		v2.w = vmpyieo(v4.h,v24.h)
		v8.w = vmpyieo(v0.h,v22.h)
		v1 = vmux(q0,v2,v30)
	}
	{
		v31.w += vmpyie(v5.w,v18.h)
		r5 = add(r30,#-1456)
		v5 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		v30.w = vmpyieo(v1.h,v24.h)
	}
	{
		v2.w += vmpyie(v4.w,v24.h)
		r2 = add(r30,#-1584)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v8.w += vmpyie(v0.w,v22.h)
		v5:4.w = vadd(v7:6.w,v5:4.w)
	}
	{
		v30.w += vmpyie(v1.w,v24.h)
		r2 = add(r30,#-1072)
		v0 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r5 = add(r30,#-944)
		v1 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = add(r30,#-1584)
		v1:0.w = vadd(v3:2.w,v1:0.w)
		vmemu(r4+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		r3 = add(r30,#-1456)
		vmemu(r3+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-1072)
		vmemu(r4+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		r3 = add(r30,#-944)
		vmemu(r3+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-816)
		v0 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r5 = add(r30,#-688)
		v1 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		v1:0.w = vadd(v9:8.w,v1:0.w)
	}
	{
		r4 = add(r30,#-816)
		vmemu(r4+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		r3 = add(r30,#-688)
		vmemu(r3+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		v0 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		v1:0.w = vadd(v31:30.w,v1:0.w)
		r2 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r2)
	}
	{
		vmemu(r4+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		nop
		vmemu(r3+#0) = v1
	} :endloop0                             // 256-byte Folded Spill
// %bb.77:                              // %"end for convolved.s1.r19$x.loopexit.us"
                                        //   in Loop: Header=BB131_75 Depth=5
	{
		r3 = add(r30,#-1968)
		r2 = add(r30,#-1840)
		r6 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r30,#-1712)
		r3 = add(r30,#-1456)
		v21 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = add(r30,#-1584)
		r3 = add(r30,#-944)
		v15 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-2096)
		r2 = add(r30,#-1328)
		v18 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r6 = add(r6,#1)
		r3 = add(r30,#-432)
		v11 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-816)
		v12 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r3 = memw(r30+##-5936)
		r28 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r30,#-1200)
		r28 = add(r28,r3)
		v19 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = add(r30,#-1072)
		v14 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-304)
		v8 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r5 = add(r30,#-688)
		v13 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = add(r30,#-560)
		v10 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		v16 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = memw(r30+##-6192)
		r7 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r30,#-176)
		r7 = add(r7,r2)
		v9 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		v6 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		v20 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v17 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = memw(r30+##-5688)
		if (!cmp.eq(r4.new,r6)) jump:t ##.LBB131_75
	}                                       // 4-byte Folded Reload
// %bb.78:                              // %"consume convolved.loopexit.split.us"
                                        //   in Loop: Header=BB131_71 Depth=4
	{
		r7 = #64
		r1 = memw(r30+##-7232)
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r3,#-27392)
		r2 = memw(r30+##-10320)
		r6 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-10312)
		vmem(r1+#0) = v17
	}                                       // 4-byte Folded Reload
	{
		vmem(r0+#0) = v16
	}
	{
		r1 = add(r0,#1024)
		vmem(r1+#0) = v6
	}
	{
		r1 = add(r0,#1152)
		vmem(r1+#0) = v20
	}
	{
		r1 = add(r0,#1280)
		vmem(r1+#0) = v21
	}
	{
		r1 = add(r0,#1408)
		vmem(r1+#0) = v14
	}
	{
		r1 = add(r0,#1536)
		vmem(r1+#0) = v15
	}
	{
		r1 = add(r0,#1664)
		vmem(r1+#0) = v10
	}
	{
		r1 = add(r0,#1792)
		vmem(r1+#0) = v11
	}
	{
		r1 = memw(r30+##-7472)
		vmem(r1+#0) = v8
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-11056)
		vmem(r0+#4) = v18
	}                                       // 4-byte Folded Reload
	{
		vmem(r0+#5) = v19
	}
	{
		vmem(r0+#6) = v12
	}
	{
		r0 = add(r0,#1920)
		vmem(r0+#7) = v13
	}
	{
		vmem(r2+#0) = v7
	}
	{
		jump .LBB131_70
		vmem(r0+#0) = v9
	}
.LBB131_79:                             //   in Loop: Header=BB131_71 Depth=4
	{
		r7 = #64
		v7:6 = vcombine(v1,v0)
		v17:16 = vcombine(v3,v2)
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_70
		r6 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_80:                             //   in Loop: Header=BB131_4 Depth=1
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r0,#-27404)
	}
.LBB131_81:                             // %"end for output.s0.b.rebased"
                                        //   in Loop: Header=BB131_4 Depth=1
	{
		r1 = memw(r30+##-24720)
		r4 = memw(r30+##-24712)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r0 = memw(r30+##-25024)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		memw(r30+##-24720) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r4,#128)
		r17 = memw(r30+##-25032)
	}                                       // 4-byte Folded Reload
	{
		if (!p0) jump:nt ##.LBB131_4
		memw(r30+##-24712) = r0
	}                                       // 4-byte Folded Spill
.LBB131_82:                             // %after_bb
	{
		p0 = cmp.eq(r19,#0)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r0,#-28044)
		if (p0) jump:nt ##.LBB131_722
	}
// %bb.83:                              // %if.then.i
	{
		r1 = memw(r19+#0)
		if (cmp.eq(r1.new,#0)) jump:nt ##.LBB131_721
	}
.LBB131_84:                             // %land.lhs.true.i2032
	{
		r0 = memw(r19+#8)
	}
	{
		r2 = #16384
		if (!cmp.gtu(r0,r2.new)) jump:t ##.LBB131_721
	}
// %bb.85:                              // %if.then.i2033
	{
		call ##halide_free
		r0 = #0
	}
	{
		jump .LBB131_721
	}
.LBB131_86:                             // %next_bb
	{
		if (!p0) jump:nt .LBB131_175
	}
// %bb.87:                              // %then_bb28
	{
		r3 = and(r19,#255)
		r21 = memw(r30+#-3888)
		memw(r30+##-4656) = r24
	}                                       // 4-byte Folded Reload
	{
		r10 = #-1
		r9 = #0
		r22 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		v8.h = vsplat(r3)
		r24 = memw(r30+##-5696)
		r7 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v1 = vsplat(r10)
		r0 = add(r24,#-1)
		r4 = and(r7,#255)
		memw(r30+#-2096) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r6 = mpyi(r0,r21)
		r5 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-23624)
		r7 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r4)
		r4 = add(r30,#-22704)
		r17 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r17,#-1)
		v3:2.w = vsub(v3:2.w,v3:2.w)
		r25 = memw(r30+##-23352)
	}                                       // 4-byte Folded Reload
	{
		r14 = max(r22,r9)
		r27 = mpyi(r7,r0)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r15 = mpyi(r1,r22)
		v0 = v1
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r28 = asr(r22,#31)
		r12 = and(r0,#255)
		r4 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r24,#8)
		p0 = r4
		r7 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r3 = mux(p0,r0,#0)
		r0 = #0
		memw(r30+#-1840) = r0
	}                                       // 4-byte Folded Spill
	{
		r16 = mpyi(r7,r20)
		r7 = max(r17,r9)
		memw(r30+##-22272) = r0
	}                                       // 4-byte Folded Spill
	{
		p0 = cmp.eq(r23,#0)
	}
	{
		r4 = max(r21,r9)
		if (!p0) v1:0 = vcombine(v3,v2)
		r23 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		r7 = mpyi(r3,r7)
		v4.h = vsplat(r12)
		r3 = add(r30,#-21808)
		p1 = cmp.gt(r23,#-1)
	}
	{
		r0 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		r9 = asl(r4,#1)
		r10 = asl(r14,#1)
		vmemu(r3+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		if (p1) r14 = add(r0,#-1)
		r4 = add(r30,#-20656)
		r3 = mux(p1,#1,r0)
		p2 = cmp.gt(r22,#-1)
	}
	{
		r11 = mpyi(r5,r11)
		r13 = asr(r21,#31)
		r0 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r5 = and(r28,r15)
		r7 = add(r30,#-21680)
		if (!p2) r15 = #0
		memw(r30+#-304) = r7
	}                                       // 4-byte Folded Spill
	{
		if (!p1) r14 = #0
		p0 = cmp.gt(r21,#-1)
		vmemu(r4+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		p2 = cmp.eq(r24,#3)
		p1 = cmp.eq(r17,#3)
		r4 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r1 = asl(r21,#1)
		p3 = and(p2,p1)
		r2 = and(r13,r6)
		if (!p0) r6 = #0
	}
	{
		r7 = add(r4,add(r3,#-1))
		p0 = cmp.gt(r25,#-1)
		vmemu(r7+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r26 = r27
		if (p3) r13 = and(r13,r1)
	}
	{
		r26 += add(r11,r16)
		if (p0) r16 = add(r0,#-1)
		r12 = mux(p0,#1,r0)
		if (!p3) r13 = add(r2,#0)
	}
	{
		r18 = mpyi(r7,r23)
		r8 = asl(r22,#1)
		r0 = memw(r30+##-23360)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r0,add(r12,#-1))
		p1 = cmp.gt(r13,r2)
		r11 = add(r18,r2)
		r20 = add(r14,r4)
	}
	{
		if (!p0) r16 = #0
		if (p1) r7 = add(r11,#0)
		if (p3) r28 = and(r28,r8)
		if (!p3) r9 = add(r6,#0)
	}
	{
		if (!p1) r7 = add(r18,r13)
		r16 = add(r16,r0)
		if (!p3) r28 = add(r5,#0)
		if (!p3) r10 = add(r15,#0)
	}
	{
		r18 = mpyi(r19,r25)
		r17 = mpyi(r20,r23)
		p0 = cmp.gt(r9,r6)
	}
	{
		r16 = mpyi(r16,r25)
		r19 = add(r17,r6)
		if (p0) r17 = add(r17,r9)
		p2 = cmp.gt(r10,r15)
	}
	{
		p1 = cmp.gt(r28,r5)
		r20 = add(r18,r5)
		if (!p0) r17 = add(r19,#0)
		memw(r30+#-560) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r16,r15)
		if (p2) r10 = add(r16,r10)
		if (p1) r15 = add(r20,#0)
		memw(r30+#-816) = r8
	}                                       // 4-byte Folded Spill
	{
		if (!p1) r15 = add(r18,r28)
		if (!p2) r10 = add(r1,#0)
		r16 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r16
		r17 = sub(r17,r7)
		r8 = #-1
		r10 = sub(r10,r15)
	}
	{
		r14 = add(r14,sub(#1,r3))
		r16 = max(r17,r8)
	}
	{
		r10 = max(r10,r8)
		r16 = add(#128,asl(r16,#7))
		r8 = #131
		r18 = sub(r19,r11)
	}
	{
		r14 = mpyi(r23,r14)
		r9 = max(r6,r9)
		r10 = add(r10,#1)
		r19 = sub(r1,r20)
	}
	{
		r13 = min(r2,r13)
		r6 = add(r6,r14)
		memw(r30+##-21152) = r19
	}                                       // 4-byte Folded Spill
	{
		r16 = add(r8,mpyi(r16,r10))
		r8 = memw(r30+#-3376)
		memw(r30+##-21256) = r16.new
	}                                       // 4-byte Folded Reload
	{
		v30.b = vsplat(r8)
		r10 = add(r30,#-20912)
		r16 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r9 = add(r9,r14)
		r1 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r9 = add(r9,sub(#1,r13))
		r6 = add(r6,sub(#1,r2))
		memw(r30+##-21168) = r6.new
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-20664) = r7
	}                                       // 4-byte Folded Spill
	{
		r10 = mpyi(r16,r11)
		r11 = mpyi(r1,r20)
		vmemu(r10+#0) = v30
	}                                       // 128-byte Folded Spill
	{
		r16 = r10
		r1 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r16 += add(r27,r11)
		v31.b = vsplat(r1)
		memw(r30+##-21160) = r15
	}                                       // 4-byte Folded Spill
	{
		r11 += add(r27,r10)
		r1 = add(r30,#-21040)
		memw(r30+##-22224) = r18
	}                                       // 4-byte Folded Spill
	{
		r27 = min(r5,r28)
		r10 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r20 = add(r10,#127)
		r5 = sub(r5,r27)
		vmemu(r1+#0) = v31
	}                                       // 128-byte Folded Spill
	{
		r2 += mpyi(r9,r5)
		r1 = asr(r20,#7)
		r5 = sub(#1,r12)
	}
	{
		r5 = mpyi(r25,r5)
		r1 = memw(r30+#-3120)
		memw(r30+##-22832) = r1
	}                                       // 4-byte Folded Reload
	{
		r8 = asr(r1,#31)
		r2 = sub(r2,r13)
		r1 = memw(r30+##-23368)
	}                                       // 4-byte Folded Reload
	{
		r5 = sub(r5,r27)
		r6 = memw(r30+#-2608)
		r27 = memw(r30+##-21264)
	}                                       // 4-byte Folded Reload
	{
		r5 = mpyi(r9,r5)
		r6 = lsl(#1,r6)
		r20 = sub(#-1,r8)
	}
	{
		r1 = mpyi(r1,r0)
		r0 = add(r10,#-128)
		memw(r30+##-22448) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 += mpyi(r27,r4)
		r4 = asr(r6,#1)
		r0 = memw(r30+##-23640)
	}                                       // 4-byte Folded Reload
	{
		r6 = memw(r30+##-23624)
		memw(r30+##-21176) = r4
	}                                       // 4-byte Folded Reload
	{
		r4 = sub(r20,r8)
		memw(r30+##-22232) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r1 += mpyi(r0,r6)
		r6 = sub(#1,r3)
		r3 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r0 = or(r3,#134)
		r3 = sub(r16,r26)
		memw(r30+##-22960) = r1
	}                                       // 4-byte Folded Spill
	{
		r4 = mpyi(r23,r6)
		r6 = asl(r2,#7)
		memw(r30+##-23088) = r3
	}                                       // 4-byte Folded Spill
	{
		r3 = asl(r9,#7)
		r2 = sub(r11,r26)
		memw(r30+##-23216) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r5,r4)
		r3 = add(r17,#1)
		memw(r30+##-21184) = r3
	}                                       // 4-byte Folded Spill
	{
		r4 += add(r21,r5)
		r1 = sub(r2,r13)
		r5 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r2 = mpyi(r25,r9)
		r0 = sub(r5,r15)
		memw(r30+##-22456) = r0
	}                                       // 4-byte Folded Spill
	{
		r1 = or(#96,asl(r1,#7))
		r3 = memw(r30+#-560)
		memw(r30+##-4656) = r3
	}                                       // 4-byte Folded Reload
	{
		r0 = sub(r3,r7)
		r5 = and(r24,#3)
		memw(r30+##-21192) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = mpyi(r22,r9)
		memw(r30+##-21096) = r0
	}                                       // 4-byte Folded Spill
	{
		r1 = sub(r4,r13)
		r4 = and(r5,#1)
		memw(r30+##-21816) = r1
	}                                       // 4-byte Folded Spill
	{
		r2 = asl(r2,#7)
		r0 = asl(r0,#7)
		memw(r30+##-21200) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r4 = sub(r22,r15)
		r2 = ##16744702
		memw(r30+#-2608) = r4
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r2,#-32767)
		memw(r30+#-2352) = r0
		memw(r30+##-21216) = r4
	}                                       // 4-byte Folded Spill
	{
		r3 = asl(r1,#7)
		r4 = or(r19,r18)
		r5 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r5,#31)
		r2 = memw(r30+#-1840)
		memw(r30+##-21208) = r0
	}                                       // 4-byte Folded Reload
	{
		r19 = #0
		memw(r30+##-22248) = r4
	}                                       // 4-byte Folded Spill
	{
		r5 = and(r24,#-4)
		r4 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		r3 = sub(r21,r7)
		memw(r30+##-22064) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = extractu(r2,#2,#8)
		r2 = setbit(r6,#6)
		memw(r30+##-21048) = r0
	}                                       // 4-byte Folded Spill
	{
		r6 = asl(r21,#8)
		r21:20 = combine(#68,#-1)
		memw(r30+##-22240) = r6
	}                                       // 4-byte Folded Spill
	{
		p2 = r4
		memw(r30+##-21104) = r3
	}                                       // 4-byte Folded Spill
	{
		r7 = clrbit(r24,#0)
		r3 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r3,#2)
		r3 = add(r30,#-22192)
		memw(r30+##-23224) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+#-2864) = r7
		memw(r30+##-23232) = r5
	}                                       // 4-byte Folded Spill
	{
		r7 = asl(r23,#7)
		r5 = #0
		memw(r30+##-22264) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = p3
		memw(r30+##-22256) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-21056) = r7
		memw(r30+#-1072) = r6
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-22280) = r5
	}                                       // 4-byte Folded Spill
	{
		vmemu(r3+#0) = v8
	}                                       // 128-byte Folded Spill
	{
		memw(r30+##-22216) = r2
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_88:                             // %"for output.s0.c.co30"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_96 Depth 2
                                        //       Child Loop BB131_98 Depth 3
                                        //       Child Loop BB131_102 Depth 3
                                        //     Child Loop BB131_108 Depth 2
                                        //       Child Loop BB131_111 Depth 3
                                        //       Child Loop BB131_115 Depth 3
                                        //     Child Loop BB131_121 Depth 2
                                        //       Child Loop BB131_139 Depth 3
                                        //         Child Loop BB131_140 Depth 4
                                        //       Child Loop BB131_131 Depth 3
                                        //         Child Loop BB131_132 Depth 4
                                        //       Child Loop BB131_144 Depth 3
                                        //         Child Loop BB131_148 Depth 4
                                        //           Child Loop BB131_153 Depth 5
                                        //             Child Loop BB131_155 Depth 6
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-28044)
		r17 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r0+#4)
	}
	{
		p0 = cmp.gtu(r17,r1); if (p0.new) jump:nt .LBB131_167
		r1 = memw(r0+#0)
	}
// %bb.89:                              //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = r1
	}
.LBB131_90:                             // %pseudostack_alloc.exit1974
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		p0 = cmp.eq(r0,#0)
		r7 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r17 = memw(r30+##-23232)
		r16 = memw(r30+##-23224)
	}                                       // 4-byte Folded Reload
	{
		if (p0) jump:nt .LBB131_174
		memw(r30+#-3120) = r0
	}                                       // 4-byte Folded Spill
.LBB131_91:                             // %"produce filter_zeroed37"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = memw(r30+##-22272)
		r1 = memw(r30+##-22448)
	}                                       // 4-byte Folded Reload
	{
		r0 = min(r0,r1)
		if (!p2) jump:nt .LBB131_173
		memw(r30+##-21232) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.92:                              // %"for filter_zeroed.s0.y38.preheader"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		if (!p1) jump:nt .LBB131_173
		r0 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
// %bb.93:                              // %"for filter_zeroed.s0.y38.us.preheader"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r14 = add(r0,#512)
		r13 = add(r0,#128)
		r6 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r7,r6)
		r4 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r12 = #0
		r15 = #0
		r6 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		r28 = r14
	}
	{
		loop1(.LBB131_96,r6)
		jump .LBB131_96
	}
	.p2align	4
.LBB131_94:                             //   in Loop: Header=BB131_96 Depth=2
	{
		v0 = valign(v2,v0,r6)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v31:30.uh = vunpack(v0.ub)
		v1.h = vsub(v28.h,v8.h)
		vmem(r0+#0) = v1.new
	}
	{
		v0.h = vsub(v30.h,v8.h)
		vmem(r0+#-1) = v0.new
	}
.LBB131_95:                             // %"end for filter_zeroed.s0.x42.loopexit.us"
                                        //   in Loop: Header=BB131_96 Depth=2
	{
		r0 = memw(r30+##-23672)
		r7 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r3,r0)
		r28 = add(r28,r7)
		r6 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r15 = add(r15,r6)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_103
	}
.Ltmp23:                                // Block address taken
.LBB131_96:                             // %"for filter_zeroed.s0.y38.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_98 Depth 3
                                        //       Child Loop BB131_102 Depth 3
	{
		r10 = #0
		r0 = memw(r30+#-2096)
		memw(r30+#-304) = r3
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r0,#2); if (!p0.new) jump:t .LBB131_100
	}
// %bb.97:                              //   in Loop: Header=BB131_96 Depth=2
	{
		r8 = lsr(r17,#2)
		r0 = add(r3,r4)
		r1 = add(r3,#64)
		v0 = vmem(r3+#0)
	}
	{
		r6 = add(r0,r4)
		r2 = add(r0,#64)
		r11 = r4
		v1 = vmem(r1+#0)
	}
	{
		r7 = add(r6,r4)
		r4 = add(r30,#-22192)
		v3 = vmem(r0+#0)
	}
	{
		r5 = add(r7,#64)
		r18 = memw(r30+##-22256)
		v6 = vmem(r1+#1)
	}                                       // 4-byte Folded Reload
	{
		r10 = add(r12,#4)
		v8 = valign(v6,v1,r1)
		v1 = vmem(r0+#1)
	}
	{
		p1 = cmp.gtu(r8,#1)
		r9 = r28
		v7 = valign(v1,v3,r0)
		v4 = vmem(r6+#0)
	}
	{
		v5 = vmem(r2+#0)
	}
	{
		v1 = vmem(r2+#1)
	}
	{
		r2 = add(r3,r18)
		v6 = valign(v1,v5,r2)
		v9 = vmem(r5+#0)
	}
	{
		v1 = vmem(r6+#1)
	}
	{
		r6 = add(r6,#64)
		v1 = valign(v1,v4,r6)
		v2 = vmem(r3+#1)
	}
	{
		v0 = valign(v2,v0,r3)
		v2 = vmem(r5+#1)
	}
	{
		r5 = add(r8,#-1)
		r8 = add(r28,#1024)
		v5 = valign(v2,v9,r5)
		v4 = vmem(r7+#0)
	}
	{
		loop0(.LBB131_98,r5)
		v3 = vmem(r6+#0)
	}
	{
		v3 = valign(v2,v3,r6)
		v2.cur = vmem(r6+#1)
	}
	{
		r6 = r28
		v2 = vmem(r7+#1)
	}
	{
		v2 = valign(v2,v4,r7)
	}
	{
		v5:4.uh = vunpack(v5.ub)
	}
	{
		if (!p1) jump:nt .LBB131_99
		v22 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	.p2align	4
.LBB131_98:                             // %"for filter_zeroed.s0.x41.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_96 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		r1 = add(r2,r11)
		v15:14.uh = vunpack(v1.ub)
		v4.h = vsub(v4.h,v22.h)
		vmem(r9+#3) = v4.new
	}
	{
		r3 = add(r1,r11)
		r0 = add(r1,#64)
		v13:12.uh = vunpack(v3.ub)
		v5 = vmem(r2+#0)
	}
	{
		r4 = add(r3,r11)
		r7 = add(r3,#64)
		v17:16.uh = vunpack(v6.ub)
		v9 = vmem(r1+#0)
	}
	{
		r5 = add(r4,#64)
		r6 = r8
		v11:10.uh = vunpack(v2.ub)
		v1 = vmem(r4+#0)
	}
	{
		v19:18.uh = vunpack(v7.ub)
		v7.h = vsub(v14.h,v22.h)
		v3 = vmem(r7+#0)
	}
	{
		v21:20.uh = vunpack(v8.ub)
		v6 = vmem(r5+#0)
		vmem(r9+#0) = v7
	}
	{
		v8.h = vsub(v18.h,v22.h)
		v2 = vmem(r4+#1)
		vmem(r9+#-2) = v8.new
	}
	{
		r8 = add(r8,#1024)
		r10 = add(r10,#4)
		v2 = valign(v2,v1,r4)
		v1 = vmem(r5+#1)
	}
	{
		v4 = valign(v1,v6,r5)
		v6.h = vsub(v10.h,v22.h)
		v1 = vmem(r7+#1)
		vmem(r9+#2) = v6.new
	}
	{
		r7 = add(r2,#64)
		v3 = valign(v1,v3,r7)
	}
	{
		v11:10.uh = vunpack(v0.ub)
		v0.h = vsub(v12.h,v22.h)
		v1 = vmem(r3+#0)
		vmem(r9+#1) = v0.new
	}
	{
		v1 = valign(v0,v1,r3)
		v0.cur = vmem(r3+#1)
	}
	{
		v0 = vmem(r2+#1)
	}
	{
		r2 = add(r2,r18)
		v0 = valign(v0,v5,r2)
		v5 = vmem(r7+#0)
	}
	{
		v6 = vmem(r0+#0)
	}
	{
		v8 = vmem(r7+#1)
	}
	{
		v8 = valign(v8,v5,r7)
		v5.h = vsub(v20.h,v22.h)
		v7 = vmem(r0+#1)
		vmem(r9+#-3) = v5.new
	}
	{
		v6 = valign(v7,v6,r0)
		v5.h = vsub(v10.h,v22.h)
		v7.h = vsub(v16.h,v22.h)
		vmem(r9+#-4) = v5.new
	}
	{
		r9 = r6
		v5:4.uh = vunpack(v4.ub)
		v7 = vmem(r1+#1)
		vmem(r9+#-1) = v7
	}
	{
		nop
		v7 = valign(v7,v9,r1)
	} :endloop0
.LBB131_99:                             //   in Loop: Header=BB131_96 Depth=2
	{
		r4 = r11
		v11:10.uh = vunpack(v0.ub)
		r3 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v9:8.uh = vunpack(v8.ub)
		v0.h = vsub(v10.h,v22.h)
		vmem(r6+#-4) = v0.new
	}
	{
		v11:10.uh = vunpack(v7.ub)
		v0.h = vsub(v8.h,v22.h)
		vmem(r6+#-3) = v0.new
	}
	{
		v7:6.uh = vunpack(v6.ub)
		v8 = v22
		v0.h = vsub(v10.h,v22.h)
	}
	{
		v29:28.uh = vunpack(v1.ub)
		v1.h = vsub(v6.h,v22.h)
		vmem(r6+#-2) = v0
	}
	{
		v7:6.uh = vunpack(v3.ub)
		v0.h = vsub(v28.h,v22.h)
		vmem(r6+#-1) = v1
	}
	{
		v31:30.uh = vunpack(v2.ub)
		v1.h = vsub(v6.h,v22.h)
		vmem(r6+#0) = v0
	}
	{
		v0.h = vsub(v30.h,v22.h)
		vmem(r6+#1) = v1
	}
	{
		v31.h = vsub(v4.h,v22.h)
		vmem(r6+#2) = v0
	}
	{
		vmem(r6+#3) = v31
	}
.LBB131_100:                            // %"end for filter_zeroed.s0.x42.loopexit.us.unr-lcssa"
                                        //   in Loop: Header=BB131_96 Depth=2
	{
		p1 = cmp.eq(r16,#0); if (p1.new) jump:nt .LBB131_95
	}
// %bb.101:                             // %"for filter_zeroed.s0.x41.us.epil.preheader"
                                        //   in Loop: Header=BB131_96 Depth=2
	{
		r0 = mpyi(r4,r10)
		r1 = r13
		r2 = add(r10,r15)
		p2 = cmp.gtu(r16,#1)
	}
	{
		r1 += asl(r2,#8)
		r6 = add(r3,r0)
		r3 = add(r16,#-1)
		r2 = add(r0,r4)
	}
	{
		loop0(.LBB131_102,r3)
		r7 = add(r6,#64)
		r5 = add(r1,#256)
		v0 = vmem(r6+#0)
	}
	{
		r0 = r1
		r3 = memw(r30+#-304)
		v1 = vmem(r7+#0)
	}                                       // 4-byte Folded Reload
	{
		v1 = valign(v2,v1,r7)
		v2.cur = vmem(r7+#1)
	}
	{
		if (!p2) jump:nt .LBB131_94
		v2 = vmem(r6+#1)
	}
	.p2align	4
.LBB131_102:                            // %"for filter_zeroed.s0.x41.us.epil"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_96 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		r6 = add(r3,r2)
		r0 = r5
		r5 = add(r5,#256)
		v2 = valign(v2,v0,r6)
	}
	{
		r7 = add(r6,#64)
		r2 = add(r2,r4)
		v5:4.uh = vunpack(v1.ub)
		v0 = vmem(r6+#0)
	}
	{
		v7:6.uh = vunpack(v2.ub)
		v2.h = vsub(v4.h,v8.h)
		v1 = vmem(r7+#0)
	}
	{
		v3.h = vsub(v6.h,v8.h)
		vmem(r1+#0) = v2
	}
	{
		r1 = r0
		v2 = vmem(r6+#1)
		vmem(r1+#-1) = v3
	}
	{
		v1 = valign(v3,v1,r7)
		v3.cur = vmem(r7+#1)
	} :endloop0
	{
		jump .LBB131_94
	}
	.p2align	4
.LBB131_103:                            // %"for sum_filter.s1.r19$y48.preheader"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		v5:4.w = vsub(v5:4.w,v5:4.w)
		r1 = memw(r30+##-7232)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27392)
		v1:0 = vcombine(v5,v4)
		v3:2 = vcombine(v5,v4)
	}
	{
		r1 = memw(r30+##-10312)
		vmem(r1+#0) = v5
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-23584)
		vmem(r0+#0) = v4
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r1 = memw(r30+##-10320)
		vmem(r1+#0) = v4
	}                                       // 4-byte Folded Reload
	{
		if (!p2) jump:nt .LBB131_119
		r1 = memw(r30+##-23536)
		vmem(r1+#0) = v5
	}                                       // 4-byte Folded Reload
// %bb.104:                             // %"for sum_filter.s1.r19$y48.us.preheader"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r3:2 = combine(#0,#0)
		v3:2.w = vsub(v3:2.w,v3:2.w)
	}
	{
		jump .LBB131_108
		v1:0 = vcombine(v3,v2)
	}
	.p2align	4
.LBB131_105:                            //   in Loop: Header=BB131_108 Depth=2
	{
		r1 = memw(r30+##-23536)
	}                                       // 4-byte Folded Reload
.LBB131_106:                            //   in Loop: Header=BB131_108 Depth=2
	{
		v5:4.w = vunpack(v4.h)
	}
	{
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
.LBB131_107:                            // %"end for sum_filter.s1.r19$x52.loopexit.us"
                                        //   in Loop: Header=BB131_108 Depth=2
	{
		r3 = add(r3,#1)
		r0 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		p2 = cmp.eq(r3,r0)
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r14 = add(r14,r0)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r2,r0)
		if (p2) jump:nt .LBB131_117
	}
.LBB131_108:                            // %"for sum_filter.s1.r19$y48.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_111 Depth 3
                                        //       Child Loop BB131_115 Depth 3
	{
		if (p0) jump:nt .LBB131_110
	}
// %bb.109:                             //   in Loop: Header=BB131_108 Depth=2
	{
		r4 = #0 ; jump .LBB131_112
	}
	.p2align	4
.LBB131_110:                            //   in Loop: Header=BB131_108 Depth=2
	{
		r0 = lsr(r17,#2)
		r5:4 = combine(r14,#0)
	}
	{
		loop0(.LBB131_111,r0)
	}
	.p2align	4
.Ltmp24:                                // Block address taken
.LBB131_111:                            // %"for sum_filter.s1.r19$x51.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_108 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		v4 = vmem(r5+#-4)
	}
	{
		r4 = add(r4,#4)
		v5:4.w = vunpack(v4.h)
		v6 = vmem(r5+#-3)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#-2)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#-1)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#0)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#1)
	}
	{
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		v4 = vmem(r5+#2)
	}
	{
		v5:4.w = vunpack(v4.h)
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v6 = vmem(r5+#3)
	}
	{
		r5 = add(r5,#1024)
		v7:6.w = vunpack(v6.h)
		v3:2.w = vadd(v5:4.w,v3:2.w)
	}
	{
		nop
		v1:0.w = vadd(v7:6.w,v1:0.w)
	} :endloop0
.LBB131_112:                            // %"end for sum_filter.s1.r19$x52.loopexit.us.unr-lcssa"
                                        //   in Loop: Header=BB131_108 Depth=2
	{
		if (p1) jump:nt .LBB131_107
	}
// %bb.113:                             // %"for sum_filter.s1.r19$x51.us.epil.preheader"
                                        //   in Loop: Header=BB131_108 Depth=2
	{
		r0 = r13
		r1 = add(r4,r2)
		p2 = cmp.gtu(r16,#1)
	}
	{
		r0 += asl(r1,#8)
	}
	{
		r1 = add(r0,#256)
	}
	{
		v7:6.w = vunpack(v4.h)
		v4.cur = vmem(r0+#0)
	}
	{
		if (!p2) jump:nt .LBB131_105
		v1:0.w = vadd(v7:6.w,v1:0.w)
		v4 = vmem(r0+#-1)
	}
// %bb.114:                             // %"for sum_filter.s1.r19$x51.us.epil"
                                        //   in Loop: Header=BB131_108 Depth=2
	{
		r0 = add(r16,#-2)
		p2 = cmp.gtu(r16,#2)
		v9:8.w = vunpack(v5.h)
		v5.cur = vmem(r1+#0)
	}
	{
		loop0(.LBB131_115,r0)
		r0 = add(r1,#256)
		v7:6.w = vunpack(v4.h)
		v1:0.w = vadd(v9:8.w,v1:0.w)
	}
	{
		if (!p2) jump:nt .LBB131_116
		v4 = vmem(r1+#-1)
	}
	.p2align	4
.LBB131_115:                            // %"for sum_filter.s1.r19$x51.us.epil"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_108 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		v9:8.w = vunpack(v5.h)
		v5.cur = vmem(r0+#0)
	}
	{
		v7:6.w = vunpack(v4.h)
		v3:2.w = vadd(v7:6.w,v3:2.w)
	}
	{
		r0 = add(r0,#256)
		v1:0.w = vadd(v9:8.w,v1:0.w)
		v4 = vmem(r0+#-1)
	} :endloop0
.LBB131_116:                            //   in Loop: Header=BB131_108 Depth=2
	{
		r0 = add(r30,#-22192)
		v3:2.w = vadd(v7:6.w,v3:2.w)
		r1 = memw(r30+##-23536)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_106
		v8 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	.p2align	4
.LBB131_117:                            // %"consume sum_filter54.loopexit.split.us"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27392)
	}
	{
		vmem(r0+#0) = v2
	}
.LBB131_118:                            // %"consume sum_filter54.sink.split"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = memw(r30+##-7232)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-10312)
		vmem(r0+#0) = v3
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-10320)
		vmem(r0+#0) = v0
	}                                       // 4-byte Folded Reload
	{
		vmem(r0+#0) = v1
	}
.LBB131_119:                            // %"consume sum_filter54"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r0 = memw(r30+##-24744)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_166
	}
// %bb.120:                             // %"for output.s0.b.rebased57.preheader"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r2 = add(r30,#-22704)
		r0 = memw(r30+##-22280)
		r5 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r0 = asl(r0,#7)
		r2 = add(r5,#256)
		v9 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r4 = memw(r30+##-22448)
		memw(r30+##-21088) = r2
	}                                       // 4-byte Folded Reload
	{
		r2 = min(r0,r4)
		r3 = add(r5,#2048)
		r7 = memw(r30+##-24752)
	}                                       // 4-byte Folded Reload
	{
		r6 = addasl(r7,r2,#2)
		v5.w = vmpyieo(v9.h,v2.h)
		memw(r30+##-19776) = r3
	}                                       // 4-byte Folded Spill
	{
		v7.w = vmpyieo(v9.h,v0.h)
		r0 = add(r2,#64)
		r3 = memw(r30+##-22960)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r6,#256)
		r0 = sub(r2,r3)
		memw(r30+##-22208) = r0
	}                                       // 4-byte Folded Spill
	{
		v4.w = vmpyieo(v9.h,v3.h)
		r2 = and(r7,#-128)
		memw(r30+##-22200) = r2
	}                                       // 4-byte Folded Spill
	{
		v6.w = vmpyieo(v9.h,v1.h)
		v8 = vmem(r6+#1)
	}
	{
		v5.w += vmpyie(v9.w,v2.h)
		r3 = add(r30,#-20528)
		v2 = vmem(r6+#0)
	}
	{
		v7.w += vmpyie(v9.w,v0.h)
		r5 = memw(r30+##-23216)
		v0 = vmem(r2+#1)
	}                                       // 4-byte Folded Reload
	{
		v4.w += vmpyie(v9.w,v3.h)
		v3 = vmem(r2+#0)
		memw(r30+##-21248) = r0
	}                                       // 4-byte Folded Spill
	{
		v2 = valign(v8,v2,r6)
		r4 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		r4 += add(r1,r5)
		v6.w += vmpyie(v9.w,v1.h)
		v10 = valign(v0,v3,r7)
		v3 = vmem(r6+#2)
	}
	{
		r5 = add(r30,#-20272)
		v3 = valign(v3,v8,r6)
		v1 = vmem(r2+#2)
	}
	{
		v11 = valign(v1,v0,r7)
		v1:0.w = vsub(v3:2.w,v5:4.w)
		memw(r30+##-21232) = r4
	}                                       // 4-byte Folded Spill
	{
		r4 = add(r30,#-20144)
		r2 = add(r30,#-20400)
		r7 = #0
	}
	{
		r6 = memw(r30+##-23088)
		memw(r30+##-21224) = r7
	}                                       // 4-byte Folded Reload
	{
		vmemu(r5+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		v1:0.w = vsub(v11:10.w,v7:6.w)
		vmemu(r4+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		memw(r30+##-21240) = r6
	}                                       // 4-byte Folded Spill
	{
		vmemu(r3+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v1
	}                                       // 256-byte Folded Spill
	.p2align	4
.LBB131_121:                            // %"for output.s0.b.rebased57"
                                        //   Parent Loop BB131_88 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_139 Depth 3
                                        //         Child Loop BB131_140 Depth 4
                                        //       Child Loop BB131_131 Depth 3
                                        //         Child Loop BB131_132 Depth 4
                                        //       Child Loop BB131_144 Depth 3
                                        //         Child Loop BB131_148 Depth 4
                                        //           Child Loop BB131_153 Depth 5
                                        //             Child Loop BB131_155 Depth 6
	{
		r0 = memw(r30+#-3896)
		r17 = memw(r30+##-21256)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r0,#-27404)
	}
	{
		r1 = memw(r2+#4)
	}
	{
		p0 = cmp.gtu(r17,r1); if (!p0.new) jump:nt .LBB131_126
		r0 = memw(r2+#0)
	}
// %bb.122:                             // %if.then.i1979
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		p0 = cmp.eq(r0,#0); if (!p0.new) jump:nt .LBB131_133
		r3 = memw(r2+#8)
	}
.LBB131_123:                            // %if.end.i1987
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = #0
		r17 = memw(r30+##-21256)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r1,#-27404)
		r2 = add(r3,r17)
		r1 = #16384
	}
	{
		p0 = cmp.gtu(r2,r1); if (!p0.new) jump:t .LBB131_125
		memw(r16+#8) = r2
	}
// %bb.124:                             // %if.then8.i1989
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		call ##halide_malloc
		r1:0 = combine(r17,#0)
	}
	{
		r1 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
		r1 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		r1 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
	}
.LBB131_125:                            // %if.end11.i1991
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		memw(r16+#0) = r0
		memw(r16+#4) = r17
	}
.LBB131_126:                            // %pseudostack_alloc.exit1992
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB131_135
	}
// %bb.127:                             // %"produce resampled_input63"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r2 = memw(r30+##-23656)
		memw(r30+#-3376) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		if (!p0.new) jump:t .LBB131_136
	}
.LBB131_128:                            // %then_bb65
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = memw(r30+##-22248)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_142
	}                                       // 4-byte Folded Reload
// %bb.129:                             // %"for resampled_input.s0.y.rebased67.us.preheader"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r1 = memw(r30+#-3376)
		r0 = memw(r30+##-22240)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		r1 = #0
		r7 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_131
	}
	.p2align	4
.LBB131_130:                            //   in Loop: Header=BB131_131 Depth=3
	{
		v0 = valign(v1,v0,r4)
		r4 = memw(r30+##-23344)
		vmem(r2++#1) = v0.new
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r4)
		r2 = memw(r30+##-21152)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r2)
		r1 = add(r1,#1)
		r5 = memw(r30+##-21184)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r5)
		if (p0) jump:nt .LBB131_142
	}
.LBB131_131:                            // %"for resampled_input.s0.x.rebased70.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_132 Depth 4
	{
		r2 = memw(r30+##-21168)
		r4 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r2,#-1)
		p0 = cmp.gtu(r2,#1)
		r6 = add(r7,r4)
		v0 = vmem(r7+#0)
	}
	{
		loop0(.LBB131_132,r3)
		r3:2 = combine(r4,r0)
		r5:4 = combine(r7,r7)
		v1 = vmem(r7+#1)
	}
	{
		if (!p0) jump:nt .LBB131_130
	}
	.p2align	4
.LBB131_132:                            // %"for resampled_input.s0.x.rebased70.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        //       Parent Loop BB131_131 Depth=3
                                        // =>      This Inner Loop Header: Depth=4
	{
		r4 = r6
		v0 = valign(v1,v0,r5)
		v1 = vmem(r6+#0)
		vmem(r2++#1) = v0.new
	}
	{
		r6 = add(r6,r3)
		r5 = r4
		v0 = v1
		v1 = vmem(r4+#1)
	} :endloop0
	{
		jump .LBB131_130
	}
	.p2align	4
.LBB131_133:                            // %if.then.i1979
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r1 = #16384
		if (!cmp.gtu(r3,r1.new)) jump:nt .LBB131_123
	}
// %bb.134:                             // %if.then3.i1983
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r2 = r0
	}
	{
		call ##halide_free
		r1:0 = combine(r2,#0)
	}
	{
		r0 = memw(r30+##-22216)
		r4 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		p3 = r0
		r0 = add(r4,#-27404)
		r6 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r6
		r5 = memw(r30+##-23584)
		r3 = memw(r0+#8)
	}                                       // 4-byte Folded Reload
	{
		p1 = r5
		jump .LBB131_123
	}
.LBB131_135:                            // %then_bb61
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r17 = add(#7,asl(r17,#2))
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r17,#-8)
		r1 = add(r1,#-27404)
	}
	{
		r0 = sub(r29,r0)
	}
	{
		r0 = and(r0,#-128)
		memw(r1+#0) = r0.new
	}
	{
		r29 = r0
		r2 = memw(r30+##-23656)
		memw(r30+#-3376) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		if (p0.new) jump:t .LBB131_128
	}
	.p2align	4
.LBB131_136:                            // %next_bb66
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = memw(r30+##-21152)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_142
	}                                       // 4-byte Folded Reload
// %bb.137:                             // %next_bb66
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = memw(r30+##-22224)
		if (!cmp.gt(r0.new,#-1)) jump:nt ##.LBB131_142
	}                                       // 4-byte Folded Reload
// %bb.138:                             // %"for resampled_input.s0.y.rebased73.preheader.split.us"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		v7:6.w = vsub(v7:6.w,v7:6.w)
		r0 = memw(r30+##-24632)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-22200)
		v0 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v2 = vsplat(r0)
		r0 = add(r30,#-24624)
	}
	{
		v3 = v2
		v5 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v4 = v5
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r0,##-33536)
		r18 = add(r0,##-33280)
	}
	{
		r1 = setbit(r19,#7)
		r0 = memw(r30+##-24640)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-8240)
		v1 = vmem(r0+#0)
	}
	{
		v1:0.w = vsub(v7:6.w,v1:0.w)
	}
	{
		r0 = add(r30,#-8112)
		vmemu(r0+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-21808)
		v1:0.w = vsub(v3:2.w,v1:0.w)
		vmemu(r0+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-21680)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = setbit(r18,#7)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3:2.w = vsub(v5:4.w,v3:2.w)
		vmem(r0+#0) = v3.new
	}
	{
		vmem(r1+#0) = v1
	}
	{
		vmem(r18+#0) = v2
	}
	{
		r0 = memw(r19+#252)
		vmem(r19+#0) = v0
	}
	{
		memw(r30+#-3632) = r0
		r0 = memw(r18+#252)

	} :mem_noshuf
	{
		memw(r30+#-560) = r0
		r0 = memw(r18+#248)

	} :mem_noshuf
	{
		memw(r30+#-816) = r0
		r0 = memw(r19+#248)

	} :mem_noshuf
	{
		memw(r30+##-5168) = r0
		r0 = memw(r18+#244)

	} :mem_noshuf
	{
		memw(r30+#-1328) = r0
		r0 = memw(r19+#244)

	} :mem_noshuf
	{
		memw(r30+##-5424) = r0
		r0 = memw(r18+#240)

	} :mem_noshuf
	{
		memw(r30+#-1584) = r0
		r0 = memw(r19+#240)

	} :mem_noshuf
	{
		memw(r30+##-6192) = r0
		r0 = memw(r18+#236)

	} :mem_noshuf
	{
		memw(r30+##-4784) = r0
		r0 = memw(r19+#236)

	} :mem_noshuf
	{
		memw(r30+##-6960) = r0
		r0 = memw(r18+#232)

	} :mem_noshuf
	{
		memw(r30+##-5680) = r0
		r0 = memw(r19+#232)

	} :mem_noshuf
	{
		memw(r30+##-7216) = r0
		r0 = memw(r18+#228)

	} :mem_noshuf
	{
		memw(r30+##-5936) = r0
		r0 = memw(r19+#228)

	} :mem_noshuf
	{
		memw(r30+##-7472) = r0
		r0 = memw(r18+#224)

	} :mem_noshuf
	{
		memw(r30+##-6448) = r0
		r0 = memw(r19+#224)

	} :mem_noshuf
	{
		memw(r30+##-7728) = r0
		r17 = memw(r18+#220)

	} :mem_noshuf
	{
		memw(r30+##-12080) = r17
		r0 = memw(r19+#220)

	} :mem_noshuf
	{
		memw(r30+##-7984) = r0
		r22 = memw(r18+#216)

	} :mem_noshuf
	{
		memw(r30+##-12336) = r22
		r0 = memw(r19+#216)

	} :mem_noshuf
	{
		memw(r30+##-13744) = r0
		r25 = memw(r18+#212)

	} :mem_noshuf
	{
		memw(r30+##-12592) = r25
		r0 = memw(r19+#212)

	} :mem_noshuf
	{
		memw(r30+##-14128) = r0
		r26 = memw(r18+#208)

	} :mem_noshuf
	{
		memw(r30+##-12848) = r26
		r0 = memw(r19+#208)

	} :mem_noshuf
	{
		memw(r30+##-14256) = r0
		r16 = memw(r18+#204)

	} :mem_noshuf
	{
		memw(r30+##-12976) = r16
		r0 = memw(r19+#204)

	} :mem_noshuf
	{
		memw(r30+##-14384) = r0
		r0 = memw(r19+#132)

	} :mem_noshuf
	{
		r24 = memw(r18+#200)
		memw(r30+##-13104) = r24.new
	}
	{
		r1 = memw(r19+#200)
		memw(r30+##-14512) = r1.new
	}
	{
		r27 = memw(r18+#192)
		memw(r30+##-13616) = r27.new
	}
	{
		r1 = memw(r18+#132)
		memw(r30+##-14896) = r1.new
	}
	{
		r2 = memw(r19+#192)
		memw(r30+##-14640) = r2.new
	}
	{
		r23 = memw(r18+#196)
		memw(r30+##-13232) = r23.new
	}
	{
		r2 = memw(r19+#196)
		memw(r30+##-15024) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		memw(r30+##-8752) = r0
		r1 = memw(r18+#128)

	} :mem_noshuf
	{
		memw(r30+##-8496) = r1
		r0 = memw(r19+#128)

	} :mem_noshuf
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-9008)
		v0 = vxor(v0,v0)
	}
	{
		v0.w = vinsert(r0)
		vmemu(r1+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r30+##-8752)
		r1 = memw(r18+#136)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		memw(r30+##-8752) = r1
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r0)
		r0 = memw(r19+#136)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9008)
		r2 = add(r30,#-9264)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#140)
		memw(r30+##-9008) = r1.new
	}
	{
		r0 = memw(r19+#140)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9264)
		r2 = add(r30,#-9520)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#144)
		memw(r30+##-9264) = r1.new
	}
	{
		r0 = memw(r19+#144)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9776)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#148)
		memw(r30+##-9520) = r1.new
	}
	{
		r0 = memw(r19+#148)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9776)
		r2 = add(r30,#-10032)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#152)
		memw(r30+##-9776) = r1.new
	}
	{
		r0 = memw(r19+#152)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10032)
		r2 = add(r30,#-10288)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#156)
		memw(r30+##-10032) = r1.new
	}
	{
		r0 = memw(r19+#156)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10288)
		r2 = add(r30,#-10800)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#160)
		memw(r30+##-10288) = r1.new
	}
	{
		r0 = memw(r19+#160)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10800)
		r2 = add(r30,#-10928)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#164)
		memw(r30+##-10800) = r1.new
	}
	{
		r0 = memw(r19+#164)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10928)
		r2 = add(r30,#-11056)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#168)
		memw(r30+##-10928) = r1.new
	}
	{
		r0 = memw(r19+#168)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11056)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#172)
		memw(r30+##-11056) = r1.new
	}
	{
		r0 = memw(r19+#172)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11568)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#176)
		memw(r30+##-11184) = r1.new
	}
	{
		r0 = memw(r19+#176)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11568)
		r2 = add(r30,#-11824)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#180)
		memw(r30+##-11568) = r1.new
	}
	{
		r0 = memw(r19+#180)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11824)
		r2 = add(r30,#-13360)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#184)
		memw(r30+##-11824) = r1.new
	}
	{
		r0 = memw(r19+#184)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13360)
		r2 = add(r30,#-15152)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#188)
		memw(r30+##-13360) = r1.new
	}
	{
		r0 = memw(r19+#188)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15152)
		r2 = add(r30,#-15024)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r21)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r23 = r0
		r1 = r27
		r0 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-14512)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r23)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14512)
		r2 = add(r30,#-14384)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14384)
		r2 = add(r30,#-14256)
	}
	{
		r1 = r26
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14256)
		r2 = add(r30,#-14128)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14128)
		r2 = add(r30,#-13744)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-13744)
		r2 = add(r30,#-7984)
	}
	{
		r1 = r17
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7984)
		r2 = add(r30,#-7728)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7728)
		r2 = add(r30,#-7472)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7472)
		r2 = add(r30,#-7216)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7216)
		r2 = add(r30,#-6960)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-6960)
		r2 = add(r30,#-6192)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-6192)
		r1 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-6192)
		r2 = add(r30,#-5424)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5424)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5424)
		r2 = add(r30,#-5168)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5168)
		r1 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5168)
		r2 = add(r30,#-3632)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-3632)
		r1 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-3632)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#124)
		memw(r30+#-3632) = r0.new
	}
	{
		r0 = memw(r19+#124)
		memw(r30+##-16176) = r0.new
	}
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r18+#120)
		memw(r30+##-5168) = r0.new
	}
	{
		r0 = memw(r19+#120)
		memw(r30+##-16432) = r0.new
	}
	{
		r0 = memw(r18+#116)
		memw(r30+##-5424) = r0.new
	}
	{
		r0 = memw(r19+#116)
		memw(r30+##-16688) = r0.new
	}
	{
		r0 = memw(r18+#112)
		memw(r30+##-6192) = r0.new
	}
	{
		r0 = memw(r19+#112)
		memw(r30+##-16816) = r0.new
	}
	{
		r0 = memw(r18+#108)
		memw(r30+##-6960) = r0.new
	}
	{
		r0 = memw(r19+#108)
		memw(r30+##-16944) = r0.new
	}
	{
		r0 = memw(r18+#104)
		memw(r30+##-7216) = r0.new
	}
	{
		r0 = memw(r19+#104)
		memw(r30+##-17072) = r0.new
	}
	{
		r0 = memw(r18+#100)
		memw(r30+##-7472) = r0.new
	}
	{
		r0 = memw(r19+#100)
		memw(r30+##-17200) = r0.new
	}
	{
		r0 = memw(r18+#96)
		memw(r30+##-7728) = r0.new
	}
	{
		r0 = memw(r19+#96)
		memw(r30+##-17328) = r0.new
	}
	{
		r0 = memw(r18+#92)
		memw(r30+##-7984) = r0.new
	}
	{
		r0 = memw(r19+#92)
		memw(r30+##-17456) = r0.new
	}
	{
		r16 = memw(r18+#88)
		r0 = memw(r19+#88)
	}
	{
		memw(r30+##-17584) = r0
		r17 = memw(r18+#84)

	} :mem_noshuf
	{
		r0 = memw(r19+#84)
		memw(r30+##-17712) = r0.new
	}
	{
		r0 = memw(r18+#80)
		memw(r30+##-17840) = r0.new
	}
	{
		r0 = memw(r19+#80)
		memw(r30+##-17968) = r0.new
	}
	{
		r22 = memw(r18+#76)
		r0 = memw(r19+#76)
	}
	{
		memw(r30+##-18096) = r0
		r0 = memw(r19+#72)

	} :mem_noshuf
	{
		r0 = add(r30,#-15024)
		r25 = memw(r18+#72)
		memw(r30+##-18224) = r0

	} :mem_noshuf
	{
		r0 = add(r30,#-16560)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vor(v1,v0)
	}
	{
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r19+#4)
		r26 = memw(r18+#64)
	}
	{
		r1 = memw(r19+#64)
		memw(r30+##-18232) = r1.new
	}
	{
		r23 = memw(r18+#68)
		r1 = memw(r19+#68)
	}
	{
		memw(r30+##-18480) = r1
		r1 = memw(r18+#4)

	} :mem_noshuf
	{
		r27 = memw(r18+#60)
		memw(r30+##-16304) = r1

	} :mem_noshuf
	{
		r2 = memw(r19+#60)
		memw(r30+##-18608) = r2.new
	}
	{
		r24 = memw(r18+#56)
		r2 = memw(r19+#56)
	}
	{
		call ##__hexagon_divsi3
		memw(r30+##-18736) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-14128) = r0
		r1 = memw(r18+#0)

	} :mem_noshuf
	{
		memw(r30+##-13744) = r1
		r0 = memw(r19+#0)

	} :mem_noshuf
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-14256)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-14128)
		r1 = memw(r18+#8)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-14128) = r1
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r19+#8)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14256)
		r2 = add(r30,#-14384)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#12)
		memw(r30+##-14256) = r1.new
	}
	{
		r0 = memw(r19+#12)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14384)
		r2 = add(r30,#-14512)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#16)
		memw(r30+##-14384) = r1.new
	}
	{
		r0 = memw(r19+#16)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14512)
		r2 = add(r30,#-14640)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#20)
		memw(r30+##-14512) = r1.new
	}
	{
		r0 = memw(r19+#20)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-14640)
		r2 = add(r30,#-15024)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#24)
		memw(r30+##-14640) = r1.new
	}
	{
		r0 = memw(r19+#24)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15024)
		r2 = add(r30,#-15152)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#28)
		memw(r30+##-15024) = r1.new
	}
	{
		r0 = memw(r19+#28)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15152)
		r2 = add(r30,#-15280)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#32)
		memw(r30+##-15152) = r1.new
	}
	{
		r0 = memw(r19+#32)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15280)
		r2 = add(r30,#-15664)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#36)
		memw(r30+##-15280) = r1.new
	}
	{
		r0 = memw(r19+#36)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15664)
		r2 = add(r30,#-15792)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#40)
		memw(r30+##-15664) = r1.new
	}
	{
		r0 = memw(r19+#40)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15792)
		r2 = add(r30,#-15920)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#44)
		memw(r30+##-15792) = r1.new
	}
	{
		r0 = memw(r19+#44)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-15920)
		r2 = add(r30,#-16048)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#48)
		memw(r30+##-15920) = r1.new
	}
	{
		r0 = memw(r19+#48)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16048)
		r2 = add(r30,#-18864)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r1 = memw(r18+#52)
		memw(r30+##-16048) = r1.new
	}
	{
		r0 = memw(r19+#52)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-18864)
		r2 = add(r30,#-18736)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-18736)
		r2 = add(r30,#-18608)
	}
	{
		r1 = r27
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-18608)
		r2 = add(r30,#-18480)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r21)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r18 = r0
		r1 = r26
		r0 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-18224)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r18)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-18224)
		r2 = add(r30,#-18096)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-18096)
		r2 = add(r30,#-17968)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r19 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1 = r19
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17968)
		r2 = add(r30,#-17712)
	}
	{
		r1 = r17
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17712)
		r2 = add(r30,#-17584)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17584)
		r2 = add(r30,#-17456)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17456)
		r2 = add(r30,#-17328)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17328)
		r2 = add(r30,#-17200)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17200)
		r2 = add(r30,#-17072)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-17072)
		r2 = add(r30,#-16944)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16944)
		r2 = add(r30,#-16816)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6192)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16816)
		r2 = add(r30,#-16688)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16688)
		r2 = add(r30,#-16432)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16432)
		r2 = add(r30,#-16176)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16176)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-16176)
		r2 = add(r30,#-18480)
		v3:2.w = vsub(v3:2.w,v3:2.w)
	}
	{
		v1 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.w = vinsert(r0)
		r0 = memw(r30+##-22232)
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r0)
		r0 = add(r30,#-21808)
		r1 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-21680)
		v9 = vand(v2,v0)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-16432)
		v6 = vnot(v4)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v8 = vand(v3,v0)
		v7 = vnot(v5)
		v0 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = valign(v1,v1,#4)
		r2 = memw(r30+##-22208)
	}                                       // 4-byte Folded Reload
	{
		v2 = vsplat(r2)
		r0 = add(r30,#-16176)
		vmemu(r0+#0) = v6
	}                                       // 128-byte Folded Spill
	{
		r2 = add(r30,#-16560)
		v0 = vor(v0,v1)
		vmemu(r0+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		r2 = add(r30,#-8240)
		v3 = v2
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v1:0.w = vadd(v9:8.w,v1:0.w)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r18 = add(r0,##-33024)
		r0 = add(r30,#-14896)
	}
	{
		r2 = add(r30,#-8112)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-14768)
		vmemu(r0+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = setbit(r18,#7)
		v3:2.w = vsub(v3:2.w,v5:4.w)
		vmemu(r0+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v4 = vand(v1,v6)
		v5 = vand(v0,v7)
		vmem(r0+#0) = v3
	}
	{
		r0 = memw(r18+#252)
		vmem(r18+#0) = v2
	}
	{
		memw(r30+##-16560) = r0
		r0 = memw(r18+#248)

	} :mem_noshuf
	{
		memw(r30+##-16688) = r0
		r0 = memw(r18+#244)

	} :mem_noshuf
	{
		memw(r30+##-16816) = r0
		r0 = memw(r18+#240)

	} :mem_noshuf
	{
		memw(r30+##-16944) = r0
		r0 = memw(r18+#236)

	} :mem_noshuf
	{
		memw(r30+##-17072) = r0
		r0 = memw(r18+#232)

	} :mem_noshuf
	{
		memw(r30+##-17200) = r0
		r0 = memw(r18+#228)

	} :mem_noshuf
	{
		memw(r30+##-17328) = r0
		r0 = memw(r18+#224)

	} :mem_noshuf
	{
		memw(r30+##-17456) = r0
		r0 = memw(r18+#220)

	} :mem_noshuf
	{
		memw(r30+##-17584) = r0
		r0 = memw(r18+#216)

	} :mem_noshuf
	{
		memw(r30+##-17712) = r0
		r0 = memw(r18+#212)

	} :mem_noshuf
	{
		memw(r30+##-17968) = r0
		r0 = memw(r18+#208)

	} :mem_noshuf
	{
		memw(r30+##-18096) = r0
		r0 = memw(r18+#204)

	} :mem_noshuf
	{
		memw(r30+##-18224) = r0
		r2 = memw(r18+#200)

	} :mem_noshuf
	{
		r2 = add(r30,#-8240)
		r0 = memw(r18+#132)
		memw(r30+##-18232) = r2

	} :mem_noshuf
	{
		r2 = add(r30,#-8112)
		vmemu(r2+#0) = v4
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v5
	}                                       // 256-byte Folded Spill
	{
		r2 = memw(r18+#192)
		memw(r30+##-18480) = r2.new
	}
	{
		r2 = memw(r18+#196)
		memw(r30+##-18608) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = memw(r30+##-8496)
		memw(r30+##-18736) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r0 = memw(r18+#128)
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#136)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#140)
		r1 = memw(r30+##-9008)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#144)
		r1 = memw(r30+##-9264)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#148)
		r1 = memw(r30+##-9520)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#152)
		r1 = memw(r30+##-9776)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#156)
		r1 = memw(r30+##-10032)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#160)
		r1 = memw(r30+##-10288)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#164)
		r1 = memw(r30+##-10800)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#168)
		r1 = memw(r30+##-10928)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#172)
		r1 = memw(r30+##-11056)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#176)
		r1 = memw(r30+##-11184)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#180)
		r1 = memw(r30+##-11568)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#184)
		r1 = memw(r30+##-11824)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#188)
		r1 = memw(r30+##-13360)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-10800)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r21)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = memw(r30+##-18480)
		memw(r30+##-8496) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r1 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-8496)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8496)
		r2 = add(r30,#-6448)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-6448)
		r2 = add(r30,#-5936)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5936)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-4784)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-4784)
		r2 = add(r30,#-1584)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16944)
		r1 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1584)
		r2 = add(r30,#-1328)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16816)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1328)
		r2 = add(r30,#-816)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16688)
		r1 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-816)
		r2 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-16560)
		r1 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#124)
		memw(r30+#-560) = r0.new
	}
	{
		r0 = memw(r18+#120)
		memw(r30+#-816) = r0.new
	}
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r18+#116)
		memw(r30+#-1584) = r0.new
	}
	{
		r0 = memw(r18+#112)
		memw(r30+##-4784) = r0.new
	}
	{
		r0 = memw(r18+#108)
		memw(r30+##-5680) = r0.new
	}
	{
		r0 = memw(r18+#104)
		memw(r30+##-5936) = r0.new
	}
	{
		r0 = memw(r18+#100)
		memw(r30+##-6448) = r0.new
	}
	{
		r0 = memw(r18+#96)
		memw(r30+##-8496) = r0.new
	}
	{
		r0 = memw(r18+#92)
		memw(r30+##-8752) = r0.new
	}
	{
		r0 = memw(r18+#88)
		memw(r30+##-9008) = r0.new
	}
	{
		r0 = memw(r18+#84)
		memw(r30+##-9264) = r0.new
	}
	{
		r0 = memw(r18+#80)
		memw(r30+##-9520) = r0.new
	}
	{
		r0 = memw(r18+#76)
		memw(r30+##-9776) = r0.new
	}
	{
		r0 = memw(r18+#72)
		memw(r30+##-10032) = r0.new
	}
	{
		r0 = memw(r18+#64)
		memw(r30+##-10288) = r0.new
	}
	{
		r0 = memw(r18+#4)
		r1 = memw(r30+##-16304)
	}
	{
		r2 = memw(r18+#68)
		memw(r30+##-10928) = r2.new
	}
	{
		r2 = add(r30,#-10800)
	}
	{
		r2 = add(r30,#-1328)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vor(v1,v0)
	}
	{
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r2 = memw(r18+#60)
		memw(r30+##-10800) = r2.new
	}
	{
		r2 = memw(r18+#56)
		memw(r30+##-11056) = r2.new
	}
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = memw(r30+##-13744)
		memw(r30+##-11184) = r0
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
		r0 = memw(r18+#0)
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#8)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#12)
		r1 = memw(r30+##-14256)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#16)
		r1 = memw(r30+##-14384)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#20)
		r1 = memw(r30+##-14512)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#24)
		r1 = memw(r30+##-14640)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#28)
		r1 = memw(r30+##-15024)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#32)
		r1 = memw(r30+##-15152)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#36)
		r1 = memw(r30+##-15280)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#40)
		r1 = memw(r30+##-15664)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#44)
		r1 = memw(r30+##-15792)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#48)
		r1 = memw(r30+##-15920)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11184)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r18+#52)
		r1 = memw(r30+##-16048)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11184)
		r2 = add(r30,#-11056)
	}
	{
		r1 = r24
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-11056)
		r2 = add(r30,#-10800)
	}
	{
		r1 = r27
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10800)
		r2 = add(r30,#-10800)
	}
	{
		r1 = r23
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		v0 = vror(v0,r21)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r18 = r0
		r1 = r26
		r0 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		call ##__hexagon_divsi3
	}
	{
		r1 = add(r30,#-304)
		r2 = add(r30,#-10032)
	}
	{
		r1 = r25
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r18)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-10032)
		r2 = add(r30,#-9776)
	}
	{
		r1 = r22
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9776)
		r2 = add(r30,#-9520)
	}
	{
		r1 = r19
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9520)
		r2 = add(r30,#-9264)
	}
	{
		r1 = r17
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9264)
		r2 = add(r30,#-9008)
	}
	{
		r1 = r16
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-9008)
		r2 = add(r30,#-7984)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7984)
		r2 = add(r30,#-7728)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-7728)
		r2 = add(r30,#-6448)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-6448)
		r2 = add(r30,#-5936)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5936)
		r2 = add(r30,#-5680)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-5680)
		r2 = add(r30,#-4784)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-6192)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-4784)
		r2 = add(r30,#-1584)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-1584)
		r1 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-1584)
		r2 = add(r30,#-816)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-816)
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-816)
		r2 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = memw(r30+#-560)
		r1 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		call ##__hexagon_divsi3
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = add(r30,#-8240)
		r6 = #64
		r2 = #0
	}
	{
		r1 = add(r30,#-8112)
		v10 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-304)
		v11 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
		r1 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		r1 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
		r1 = add(r30,#-560)
	}
	{
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.w = vinsert(r0)
		r0 = add(r30,#-10800)
		r1 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-1328)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r0 = add(r30,#-14896)
		v0 = vor(v1,v0)
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-14768)
		v2 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-16176)
		v3 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-16432)
		v3:2.w = vadd(v3:2.w,v1:0.w)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = vand(v2,v0)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vand(v3,v0)
		r0 = memw(r30+##-22264)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		r1 = memw(r30+##-21240)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_139:                            // %"for resampled_input.s0.y.rebased73.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_140 Depth 4
	{
		r7 = r0
		r3 = memw(r30+##-21168)
		r11 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = memw(r30+##-23336)
		r17 = memw(r30+##-23536)
	}                                       // 4-byte Folded Reload
	{
		loop0(.LBB131_140,r3)
		r3 = r1
	}
	.p2align	4
.Ltmp25:                                // Block address taken
.LBB131_140:                            // %"for resampled_input.s0.x.rebased76.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        //       Parent Loop BB131_139 Depth=3
                                        // =>      This Inner Loop Header: Depth=4
	{
		v4 = vsplat(r3)
		r8 = add(r11,##-33792)
		v6 = v7
	}
	{
		r9 = setbit(r8,#7)
		r5 = add(r7,#-64)
		v2 = vror(v7,r6)
		v5 = v4
	}
	{
		r4 = add(r11,##-34048)
		q0 = vsetq(r6)
		v9:8.w = vadd(v5:4.w,v11:10.w)
	}
	{
		r6 = setbit(r4,#7)
		r3 = add(r3,r16)
		v5:4.w = vadd(v5:4.w,v1:0.w)
		vmem(r8+#0) = v8
	}
	{
		v3 = v7
		r12 = memw(r8+#0)
		vmem(r9+#0) = v9
	}
	{
		r14 = memw(r8+#8)
		r13 = memw(r8+#4)
	}
	{
		r25 = memw(r8+#12)
		r15 = memw(r8+#16)
	}
	{
		r27 = memw(r8+#24)
		r12 = memub(r17+r12<<#0)
	}
	{
		r13 = memub(r17+r13<<#0)
		r14 = memub(r17+r14<<#0)
	}
	{
		r12 |= asl(r13,#8)
		r9 = memub(r17+r25<<#0)
		r28 = memw(r8+#28)
	}
	{
		r14 |= asl(r9,#8)
		r26 = memw(r8+#20)
		r18 = memw(r8+#32)
	}
	{
		r12 = combine(r14.l,r12.l)
		r22 = memub(r17+r28<<#0)
		r28 = memw(r8+#40)
	}
	{
		v6.w = vinsert(r12)
		r9 = memub(r17+r27<<#0)
		r13 = memub(r17+r26<<#0)
	}
	{
		r9 |= asl(r22,#8)
		r15 = memub(r17+r15<<#0)
		r19 = memw(r8+#36)
	}
	{
		r15 |= asl(r13,#8)
		v6 = valign(v6,v6,#4)
		r23 = memw(r8+#44)
		r24 = memw(r8+#56)
	}
	{
		r9 = combine(r9.l,r15.l)
		r26 = memw(r8+#48)
		r14 = memub(r17+r18<<#0)
	}
	{
		v6.w = vinsert(r9)
		r12 = memub(r17+r19<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r14 |= asl(r12,#8)
		r13 = memub(r17+r23<<#0)
		r25 = memw(r8+#60)
	}
	{
		r28 |= asl(r13,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#52)
		r22 = memw(r8+#72)
	}
	{
		r14 = combine(r28.l,r14.l)
		r28 = memw(r8+#64)
		r15 = memub(r17+r24<<#0)
	}
	{
		v6.w = vinsert(r14)
		r9 = memub(r17+r25<<#0)
		r12 = memub(r17+r26<<#0)
	}
	{
		r15 |= asl(r9,#8)
		r19 = memub(r17+r27<<#0)
		r18 = memw(r8+#68)
	}
	{
		r12 |= asl(r19,#8)
		v6 = valign(v6,v6,#4)
		r23 = memw(r8+#76)
		r24 = memw(r8+#80)
	}
	{
		r12 = combine(r15.l,r12.l)
		r26 = memw(r8+#88)
		r28 = memub(r17+r28<<#0)
	}
	{
		v6.w = vinsert(r12)
		r14 = memub(r17+r18<<#0)
		r13 = memub(r17+r22<<#0)
	}
	{
		r28 |= asl(r14,#8)
		r9 = memub(r17+r23<<#0)
		r25 = memw(r8+#84)
	}
	{
		r13 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#92)
		r15 = memub(r17+r24<<#0)
	}
	{
		r13 = combine(r13.l,r28.l)
		r28 = memw(r8+#96)
		r12 = memub(r17+r25<<#0)
	}
	{
		r15 |= asl(r12,#8)
		v6.w = vinsert(r13)
		r19 = memw(r8+#104)
		r14 = memub(r17+r26<<#0)
	}
	{
		r9 = memub(r17+r27<<#0)
		r18 = memw(r8+#100)
	}
	{
		r14 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r22 = memw(r8+#108)
		r23 = memw(r8+#124)
	}
	{
		r14 = combine(r14.l,r15.l)
		r13 = memub(r17+r18<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r28 |= asl(r13,#8)
		v6.w = vinsert(r14)
		r24 = memw(r8+#112)
		r12 = memub(r17+r19<<#0)
	}
	{
		r9 = memub(r17+r22<<#0)
		r25 = memw(r8+#116)
	}
	{
		r12 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r26 = memw(r8+#120)
		r13 = memub(r17+r24<<#0)
	}
	{
		r12 = combine(r12.l,r28.l)
		r28 = memw(r8+#128)
		r14 = memub(r17+r25<<#0)
	}
	{
		r13 |= asl(r14,#8)
		v6.w = vinsert(r12)
		r18 = memw(r8+#136)
		r15 = memub(r17+r23<<#0)
	}
	{
		r9 = memub(r17+r26<<#0)
		r27 = memw(r8+#132)
	}
	{
		r9 |= asl(r15,#8)
		v6 = valign(v6,v6,#4)
		r19 = memw(r8+#140)
		r22 = memw(r8+#144)
	}
	{
		r9 = combine(r9.l,r13.l)
		r15 = memub(r17+r18<<#0)
		r24 = memw(r8+#152)
	}
	{
		v6.w = vinsert(r9)
		r14 = memub(r17+r19<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r15 |= asl(r14,#8)
		r12 = memub(r17+r27<<#0)
		r23 = memw(r8+#148)
	}
	{
		r28 |= asl(r12,#8)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#156)
		r18 = memw(r8+#168)
	}
	{
		r15 = combine(r15.l,r28.l)
		r28 = memw(r8+#160)
		r13 = memub(r17+r22<<#0)
	}
	{
		v6.w = vinsert(r15)
		r9 = memub(r17+r23<<#0)
		r12 = memub(r17+r24<<#0)
	}
	{
		r13 |= asl(r9,#8)
		r27 = memub(r17+r25<<#0)
		r26 = memw(r8+#164)
	}
	{
		r12 |= asl(r27,#8)
		v6 = valign(v6,v6,#4)
		r19 = memw(r8+#172)
		r22 = memw(r8+#176)
	}
	{
		r12 = combine(r12.l,r13.l)
		r24 = memw(r8+#184)
		r28 = memub(r17+r28<<#0)
	}
	{
		v6.w = vinsert(r12)
		r15 = memub(r17+r26<<#0)
		r14 = memub(r17+r18<<#0)
	}
	{
		r28 |= asl(r15,#8)
		r9 = memub(r17+r19<<#0)
		r23 = memw(r8+#180)
	}
	{
		r14 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r25 = memw(r8+#188)
		r27 = memw(r8+#200)
	}
	{
		r14 = combine(r14.l,r28.l)
		r28 = memw(r8+#192)
		r12 = memub(r17+r23<<#0)
	}
	{
		v6.w = vinsert(r14)
		r13 = memub(r17+r22<<#0)
		r15 = memub(r17+r24<<#0)
	}
	{
		r13 |= asl(r12,#8)
		r9 = memub(r17+r25<<#0)
		r26 = memw(r8+#196)
	}
	{
		r15 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r18 = memw(r8+#204)
		r19 = memw(r8+#208)
	}
	{
		r13 = combine(r15.l,r13.l)
		r14 = memub(r17+r26<<#0)
		r28 = memub(r17+r28<<#0)
	}
	{
		r28 |= asl(r14,#8)
		v6.w = vinsert(r13)
		r23 = memw(r8+#216)
		r12 = memub(r17+r27<<#0)
	}
	{
		r9 = memub(r17+r18<<#0)
		r22 = memw(r8+#212)
	}
	{
		r12 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r24 = memw(r8+#220)
		r26 = memw(r8+#232)
	}
	{
		r12 = combine(r12.l,r28.l)
		r28 = memw(r8+#224)
		r13 = memub(r17+r22<<#0)
	}
	{
		v6.w = vinsert(r12)
		r15 = memub(r17+r19<<#0)
		r14 = memub(r17+r23<<#0)
	}
	{
		r15 |= asl(r13,#8)
		r9 = memub(r17+r24<<#0)
		r25 = memw(r8+#228)
	}
	{
		r14 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r27 = memw(r8+#236)
		r18 = memw(r8+#240)
	}
	{
		r14 = combine(r14.l,r15.l)
		r22 = memw(r8+#248)
		r28 = memub(r17+r28<<#0)
	}
	{
		v6.w = vinsert(r14)
		r12 = memub(r17+r25<<#0)
		r13 = memub(r17+r26<<#0)
	}
	{
		r28 |= asl(r12,#8)
		r9 = memub(r17+r27<<#0)
		r19 = memw(r8+#244)
	}
	{
		r13 |= asl(r9,#8)
		v6 = valign(v6,v6,#4)
		r10 = memw(r8+#252)
		r23 = memub(r17+r18<<#0)
	}
	{
		r9 = combine(r13.l,r28.l)
		r12 = memub(r17+r19<<#0)
		r24 = memub(r17+r22<<#0)
	}
	{
		r23 |= asl(r12,#8)
		v6.w = vinsert(r9)
		r25 = memub(r17+r10<<#0)
	}
	{
		r24 |= asl(r25,#8)
	}
	{
		r8 = combine(r24.l,r23.l)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r8)
	}
	{
		v6 = vror(v6,r21)
	}
	{
		v6 = vor(v6,v2)
	}
	{
		if (q0) vmem(r5+#0) = v6
	}
	{
		v4 = vand(q0,r20)
		vmem(r4+#0) = v4
	}
	{
		r26 = memw(r4+#8)
		vmem(r6+#0) = v5
	}
	{
		r5 = memw(r4+#0)
		r27 = memw(r4+#12)
	}
	{
		r6 = memw(r4+#4)
		r18 = memw(r4+#16)
	}
	{
		r22 = memw(r4+#24)
		r5 = memub(r17+r5<<#0)
	}
	{
		r6 = memub(r17+r6<<#0)
		r8 = memub(r17+r26<<#0)
	}
	{
		r5 |= asl(r6,#8)
		r9 = memub(r17+r27<<#0)
		r19 = memw(r4+#20)
	}
	{
		r8 |= asl(r9,#8)
		r23 = memw(r4+#28)
		r6 = memw(r4+#32)
	}
	{
		r5 = combine(r8.l,r5.l)
		r24 = memw(r4+#40)
		r12 = memub(r17+r18<<#0)
	}
	{
		v3.w = vinsert(r5)
		r25 = memub(r17+r19<<#0)
		r5 = memub(r17+r22<<#0)
	}
	{
		r12 |= asl(r25,#8)
		r27 = memub(r17+r23<<#0)
		r28 = memw(r4+#36)
	}
	{
		r5 |= asl(r27,#8)
		v3 = valign(v3,v3,#4)
		r26 = memw(r4+#44)
		r22 = memw(r4+#48)
	}
	{
		r5 = combine(r5.l,r12.l)
		r18 = memw(r4+#56)
		r23 = memub(r17+r28<<#0)
	}
	{
		v3.w = vinsert(r5)
		r6 = memub(r17+r6<<#0)
		r5 = memub(r17+r24<<#0)
	}
	{
		r6 |= asl(r23,#8)
		r24 = memub(r17+r26<<#0)
		r28 = memw(r4+#52)
	}
	{
		r5 |= asl(r24,#8)
		v3 = valign(v3,v3,#4)
		r19 = memw(r4+#60)
		r8 = memub(r17+r22<<#0)
	}
	{
		r5 = combine(r5.l,r6.l)
		r6 = memub(r17+r28<<#0)
		r25 = memw(r4+#64)
	}
	{
		r8 |= asl(r6,#8)
		v3.w = vinsert(r5)
		r27 = memw(r4+#72)
		r5 = memub(r17+r18<<#0)
	}
	{
		r6 = memub(r17+r19<<#0)
		r26 = memw(r4+#68)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#76)
		r18 = memw(r4+#80)
	}
	{
		r5 = combine(r5.l,r8.l)
		r6 = memw(r4+#88)
		r12 = memub(r17+r25<<#0)
	}
	{
		v3.w = vinsert(r5)
		r22 = memub(r17+r26<<#0)
		r5 = memub(r17+r27<<#0)
	}
	{
		r12 |= asl(r22,#8)
		r24 = memub(r17+r28<<#0)
		r19 = memw(r4+#84)
	}
	{
		r5 |= asl(r24,#8)
		v3 = valign(v3,v3,#4)
		r23 = memw(r4+#92)
		r25 = memw(r4+#96)
	}
	{
		r5 = combine(r5.l,r12.l)
		r27 = memub(r17+r19<<#0)
		r14 = memub(r17+r18<<#0)
	}
	{
		r14 |= asl(r27,#8)
		v3.w = vinsert(r5)
		r26 = memw(r4+#104)
		r5 = memub(r17+r6<<#0)
	}
	{
		r6 = memub(r17+r23<<#0)
		r18 = memw(r4+#108)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#100)
		r22 = memw(r4+#112)
	}
	{
		r5 = combine(r5.l,r14.l)
		r6 = memw(r4+#116)
		r9 = memub(r17+r25<<#0)
	}
	{
		v3.w = vinsert(r5)
		r23 = memub(r17+r28<<#0)
		r19 = memw(r4+#124)
	}
	{
		r9 |= asl(r23,#8)
		r5 = memub(r17+r26<<#0)
		r24 = memub(r17+r18<<#0)
	}
	{
		r5 |= asl(r24,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#120)
		r13 = memub(r17+r22<<#0)
	}
	{
		r5 = combine(r5.l,r9.l)
		r6 = memub(r17+r6<<#0)
		r25 = memw(r4+#128)
	}
	{
		r13 |= asl(r6,#8)
		v3.w = vinsert(r5)
		r27 = memw(r4+#136)
		r6 = memub(r17+r19<<#0)
	}
	{
		r5 = memub(r17+r28<<#0)
		r26 = memw(r4+#132)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r18 = memw(r4+#140)
		r19 = memw(r4+#144)
	}
	{
		r5 = combine(r5.l,r13.l)
		r6 = memw(r4+#152)
		r14 = memub(r17+r25<<#0)
	}
	{
		v3.w = vinsert(r5)
		r22 = memub(r17+r26<<#0)
		r5 = memub(r17+r27<<#0)
	}
	{
		r14 |= asl(r22,#8)
		r24 = memub(r17+r18<<#0)
		r28 = memw(r4+#148)
	}
	{
		r5 |= asl(r24,#8)
		v3 = valign(v3,v3,#4)
		r23 = memw(r4+#156)
		r25 = memw(r4+#160)
	}
	{
		r5 = combine(r5.l,r14.l)
		r12 = memub(r17+r19<<#0)
		r18 = memub(r17+r28<<#0)
	}
	{
		r12 |= asl(r18,#8)
		v3.w = vinsert(r5)
		r27 = memw(r4+#168)
		r5 = memub(r17+r6<<#0)
	}
	{
		r6 = memub(r17+r23<<#0)
		r26 = memw(r4+#164)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#172)
		r19 = memw(r4+#176)
	}
	{
		r5 = combine(r5.l,r12.l)
		r6 = memw(r4+#184)
		r9 = memub(r17+r25<<#0)
	}
	{
		v3.w = vinsert(r5)
		r23 = memub(r17+r26<<#0)
		r5 = memub(r17+r27<<#0)
	}
	{
		r9 |= asl(r23,#8)
		r25 = memub(r17+r28<<#0)
		r22 = memw(r4+#180)
	}
	{
		r5 |= asl(r25,#8)
		v3 = valign(v3,v3,#4)
		r24 = memw(r4+#188)
		r26 = memw(r4+#192)
	}
	{
		r5 = combine(r5.l,r9.l)
		r18 = memub(r17+r22<<#0)
		r14 = memub(r17+r19<<#0)
	}
	{
		r14 |= asl(r18,#8)
		v3.w = vinsert(r5)
		r27 = memw(r4+#200)
		r5 = memub(r17+r6<<#0)
	}
	{
		r6 = memub(r17+r24<<#0)
		r19 = memw(r4+#204)
	}
	{
		r5 |= asl(r6,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#196)
		r22 = memw(r4+#208)
	}
	{
		r5 = combine(r5.l,r14.l)
		r6 = memw(r4+#216)
		r12 = memub(r17+r26<<#0)
	}
	{
		v3.w = vinsert(r5)
		r24 = memub(r17+r28<<#0)
		r5 = memub(r17+r27<<#0)
	}
	{
		r12 |= asl(r24,#8)
		r25 = memub(r17+r19<<#0)
		r23 = memw(r4+#212)
	}
	{
		r5 |= asl(r25,#8)
		v3 = valign(v3,v3,#4)
		r28 = memw(r4+#220)
		r26 = memw(r4+#224)
	}
	{
		r5 = combine(r5.l,r12.l)
		r18 = memw(r4+#232)
		r9 = memub(r17+r22<<#0)
	}
	{
		v3.w = vinsert(r5)
		r19 = memub(r17+r23<<#0)
		r5 = memub(r17+r28<<#0)
	}
	{
		r9 |= asl(r19,#8)
		r6 = memub(r17+r6<<#0)
		r27 = memw(r4+#228)
	}
	{
		r6 |= asl(r5,#8)
		v3 = valign(v3,v3,#4)
		r22 = memw(r4+#236)
		r5 = memw(r4+#248)
	}
	{
		r6 = combine(r6.l,r9.l)
		r28 = memw(r4+#240)
		r23 = memub(r17+r26<<#0)
	}
	{
		v3.w = vinsert(r6)
		r24 = memub(r17+r27<<#0)
		r25 = memub(r17+r18<<#0)
	}
	{
		r23 |= asl(r24,#8)
		r26 = memub(r17+r22<<#0)
		r10 = memw(r4+#244)
	}
	{
		r25 |= asl(r26,#8)
		v3 = valign(v3,v3,#4)
		r4 = memw(r4+#252)
		r6 = memub(r17+r28<<#0)
	}
	{
		r9 = combine(r25.l,r23.l)
		r27 = memub(r17+r10<<#0)
		r5 = memub(r17+r5<<#0)
	}
	{
		r6 |= asl(r27,#8)
		v3.w = vinsert(r9)
		r4 = memub(r17+r4<<#0)
	}
	{
		r5 |= asl(r4,#8)
	}
	{
		r4 = combine(r5.l,r6.l)
		r6 = #64
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r4)
	}
	{
		v3 = vror(v3,r21)
	}
	{
		v3 = vlalign(v7,v4,r7)
		v2 = vor(v3,v2)
	}
	{
		q2 = vand(v3,r20)
		v3 = vlalign(v4,v7,r7)
	}
	{
		q3 = vand(v3,r20)
		v5 = vlalign(v7,v2,r7)
	}
	{
		v2 = vlalign(v2,v7,r7)
		if (q2) vmem(r7+#1) = v5
	}
	{
		r7 = add(r7,#128)
		if (q3) vmem(r7+#0) = v2
	} :endloop0
// %bb.141:                             // %"end for resampled_input.s0.x.rebased77.loopexit.us"
                                        //   in Loop: Header=BB131_139 Depth=3
	{
		r3 = memw(r30+##-21152)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r2,r3)
		r2 = add(r2,#1)
		r3 = memw(r30+##-21184)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r3)
		r3 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,r3)
		if (!p0) jump:nt .LBB131_139
	}
.LBB131_142:                            // %"consume resampled_input91"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = memw(r30+##-23664)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_164
	}
// %bb.143:                             // %"for output.s0.y.yo92.preheader"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r0 = memw(r30+##-23624)
		r6 = memw(r30+##-21816)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-21224)
		r2 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,r0)
		r1 = add(r2,r6)
		r5 = memw(r30+##-22064)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r2,r5)
		r2 = #0
		memw(r30+##-21112) = r1
	}                                       // 4-byte Folded Spill
	{
		r4 = memw(r30+##-23640)
		memw(r30+##-21120) = r1
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-21248)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r0,r4)
		memw(r30+##-21136) = r3.new
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_144:                            // %"for output.s0.y.yo92"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_148 Depth 4
                                        //           Child Loop BB131_153 Depth 5
                                        //             Child Loop BB131_155 Depth 6
	{
		r0 = memw(r30+##-23376)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_163
	}
// %bb.145:                             // %"for output.s0.x.xo95.preheader"
                                        //   in Loop: Header=BB131_144 Depth=3
	{
		r1 = memw(r30+##-21176)
		memw(r30+##-21128) = r2
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r1)
		r16 = #0
		r1 = add(r30,#-19248)
	}
	{
		r0 = memw(r30+##-23360)
		r5 = memw(r30+##-21136)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r2,r0)
		r2 = memw(r30+##-21192)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-21216)
	}                                       // 4-byte Folded Reload
	{
		vmemu(r1+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = memw(r30+##-21208)
		r4 = memw(r30+##-21160)
	}                                       // 4-byte Folded Reload
	{
		v31 = vsplat(r1)
		r1 = add(r30,#-19504)
	}
	{
		vmemu(r1+#0) = v31
	}                                       // 128-byte Folded Spill
	{
		r1 = memw(r30+##-23352)
	}                                       // 4-byte Folded Reload
	{
		r1 = mpyi(r0,r1)
	}
	{
		r4 = sub(r1,r4)
		r2 = add(r2,r1)
		r3 = add(r3,r1)
	}
	{
		r1 = memw(r30+##-23368)
		memw(r30+##-4784) = r4
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r0,r1)
		r0 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r1 = mpyi(r2,r0)
		memw(r30+##-19760) = r5
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r30+##-21112)
		memw(r30+##-21064) = r1
	}                                       // 4-byte Folded Reload
	{
		r3 = mpyi(r3,r0)
		r0 = mpyi(r4,r0)
		r1 = memw(r30+##-21120)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-21072) = r3
		memw(r30+##-21080) = r0
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_148
	}
	.p2align	4
.LBB131_146:                            // %then_bb100
                                        //   in Loop: Header=BB131_148 Depth=4
	{
		v8 = vxor(v8,v8)
		r2 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r2,#-1792)
		r5 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r16,r5)
		r8 = memw(r30+##-10304)
		r3 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r3,#1024)
		r0 = memw(r30+##-21096)
		v2 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r8 = mpyi(r7,r8)
		r6 = add(r2,#-1664)
		r13 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r3,#1792)
		memw(r30+##-5936) = r7
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r8,r0)
		r13 = add(r8,r13)
		r7 = memw(r30+##-21064)
	}                                       // 4-byte Folded Reload
	{
		r14 = add(r8,r7)
		r17 = memw(r30+##-20664)
		r20 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r15 = add(r0,r7)
		r28 = add(r13,r7)
		r14 = sub(r14,r17)
	}
	{
		r9 = addasl(r20,r15,#7)
		r10 = asl(r15,#7)
		r7 = memw(r30+##-21072)
	}                                       // 4-byte Folded Reload
	{
		r14 = addasl(r20,r14,#7)
		r18 = asl(r14,#7)
		r15 = add(r0,r7)
	}
	{
		r16 = add(r13,r7)
		r19 = memw(r30+##-21080)
		memw(r30+#-560) = r14
	}                                       // 4-byte Folded Reload
	{
		r7 = addasl(r20,r15,#7)
		r14 = add(r8,r7)
		r0 = add(r0,r19)
		memw(r30+#-304) = r7.new
	}                                       // 4-byte Folded Spill
	{
		r7 = asl(r0,#7)
		v16 = v8
		v1 = vmem(r1+#0)
	}
	{
		r2 = add(r30,#-7472)
		v3 = vmem(r2+#0)
		memw(r30+##-12336) = r7
	}                                       // 4-byte Folded Spill
	{
		r7 = #64
		r5 = add(r3,#64)
		r25:24 = memd(r20+r10<<#0)
		v12 = vmem(r4+#0)
	}
	{
		v16.w = vinsert(r24)
		r4 = add(r30,#-1584)
		v6 = valign(v1,v1,r7)
		v29 = vmem(r5+#0)
	}
	{
		r23 = add(r3,#1280)
		r22 = add(r3,#1536)
		v19:18.w = vunpack(v3.h)
		v17 = v8
	}
	{
		r0 = addasl(r20,r0,#7)
		r26 = add(r3,#1152)
		vmemu(r2+#0) = v6
	}                                       // 128-byte Folded Spill
	{
		r6 = add(r30,#-10288)
		v14 = valign(v3,v3,r7)
		r2 = memw(r30+#-3912)
		v3 = vmem(r6+#0)
	}                                       // 4-byte Folded Reload
	{
		r12 = add(r3,#1408)
		r21 = add(r3,#1664)
		vmemu(r4+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		r13 = add(r13,r19)
		v16 = valign(v16,v16,#4)
		r2 = memw(r30+#-3904)
		v22 = vmem(r2+#0)
	}                                       // 4-byte Folded Reload
	{
		v16.w = vinsert(r25)
		r1 = add(r30,#-7984)
		r5:4 = memd(r9+#16)
		v0 = vmem(r3+#4)
	}
	{
		v17.w = vinsert(r4)
		r2 = add(r30,#-6960)
		v15 = vmem(r2+#0)
		memw(r30+#-3120) = r3
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r30,#-6832)
		r4 = add(r30,#-3632)
		vmemu(r2+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		r11 = asl(r28,#7)
		r8 = add(r8,r19)
		vmemu(r2+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		v19:18.w = vunpack(v14.h)
		v5 = vmem(r3+#6)
		memw(r30+##-12848) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = asl(r13,#7)
		v17 = valign(v17,v17,#4)
		v26 = vmem(r3+#0)
	}
	{
		r0 = addasl(r20,r13,#7)
		memw(r30+##-12592) = r0
	}                                       // 4-byte Folded Spill
	{
		v17.w = vinsert(r5)
		r6 = add(r30,#-10160)
		vmemu(r6+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		r13 = r9
		v14 = valign(v16,v16,#4)
		r3:2 = memd(r9+#8)
		v3 = vmem(r26+#0)
	}
	{
		v14.w = vinsert(r2)
		r6 = add(r30,#-1328)
		vmemu(r6+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		v13 = valign(v12,v12,r7)
		v16 = v8
		r25:24 = memd(r9+#24)
		v10 = vmem(r23+#0)
	}
	{
		r27 = addasl(r20,r28,#7)
		v18 = v8
		vmemu(r6+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		r28 = #116
		r2 = add(r30,#-6448)
		v21:20.w = vunpack(v12.h)
		v3 = vmem(r12+#0)
	}
	{
		r4 = add(r30,#-816)
		q0 = vcmp.eq(v15.b,v8.b)
		vmemu(r4+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v12 = valign(v17,v17,#4)
		v11 = vmem(r22+#0)
		memw(r30+##-12080) = r0
	}                                       // 4-byte Folded Spill
	{
		r19 = asl(r15,#7)
		v12.w = vinsert(r24)
		vmemu(r1+#0) = v1
	}                                       // 128-byte Folded Spill
	{
		r1 = sub(r8,r17)
		v14 = valign(v14,v14,#4)
		v3 = vmem(r21+#0)
	}
	{
		r0 = asl(r1,#7)
		r8 = asl(r16,#7)
		vmemu(r4+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v14.w = vinsert(r3)
		r21 = r27
		v4 = valign(v10,v10,r7)
		r5:4 = memd(r13+#48)
	}
	{
		r0 = addasl(r20,r1,#7)
		v9 = valign(v11,v11,r7)
		memw(r30+##-11056) = r0
	}                                       // 4-byte Folded Spill
	{
		v18.w = vinsert(r4)
		r1:0 = memd(r20+r11<<#0)
		memw(r30+##-11184) = r0
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v5,v5,r7)
		r11:10 = memd(r20+r8<<#0)
		memw(r30+##-10928) = r9
	}                                       // 4-byte Folded Spill
	{
		r16 = addasl(r20,r16,#7)
		r14 = sub(r14,r17)
		v7 = valign(v0,v0,r7)
		r13:12 = memd(r13+#56)
	}
	{
		r17 = addasl(r20,r14,#7)
		v27 = valign(v2,v2,r7)
		r23:22 = memd(r20+r18<<#0)
		r19:18 = memd(r20+r19<<#0)
	}
	{
		r15 = asl(r14,#7)
		v28 = vror(v8,r7)
		r7:6 = memd(r9+#32)
	}
	{
		v16.w = vinsert(r6)
		v12 = valign(v12,v12,#4)
		memw(r30+##-7728) = r17
	}                                       // 4-byte Folded Spill
	{
		v12.w = vinsert(r25)
		r9:8 = memd(r9+#40)
		memw(r30+##-7216) = r16
	}                                       // 4-byte Folded Spill
	{
		v14 = vror(v14,r28)
		r25:24 = memd(r27+#48)
		r15:14 = memd(r20+r15<<#0)
	}
	{
		v16 = valign(v16,v16,#4)
		v14 = vor(v14,v28)
	}
	{
		v16.w = vinsert(r7)
		v17 = valign(v18,v18,#4)
		r7:6 = memd(r27+#8)
	}
	{
		v17.w = vinsert(r5)
		v12 = vror(v12,r28)
		r5:4 = memd(r27+#32)
	}
	{
		r2 = add(r30,#-6320)
		v12 = vor(v12,v28)
		vmemu(r2+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		v16 = valign(v16,v16,#4)
		r3:2 = memd(r27+#16)
	}
	{
		v16.w = vinsert(r8)
		v21:20.uh = vunpack(v14.ub)
	}
	{
		v14 = valign(v17,v17,#4)
		v17 = v8
	}
	{
		v14.w = vinsert(r12)
		v31:30.uh = vunpack(v12.ub)
		v12 = v8
	}
	{
		v12.w = vinsert(r0)
		v17.w = vinsert(r2)
		r0 = add(r30,#-6192)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r9)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r13)
		v12 = valign(v12,v12,#4)
		r13:12 = memd(r27+#24)
	}
	{
		v12.w = vinsert(r1)
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r3)
		v19:18.uw = vunpack(v20.uh)
		r3:2 = memd(r27+#56)
	}
	{
		v14 = vror(v14,r28)
	}
	{
		v16 = vror(v16,r28)
		v19 = vor(v14,v28)
	}
	{
		v25:24.w = vunpack(v13.h)
		v13 = vor(v16,v28)
	}
	{
		v21:20.uw = vunpack(v30.uh)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r6)
		v16 = vdelta(v20,v22)
	}
	{
		v21:20.uh = vunpack(v13.ub)
		v6 = vmux(q0,v16,v18)
		v18 = v8
	}
	{
		v18.w = vinsert(r24)
		v13 = valign(v17,v17,#4)
		v17 = v8
	}
	{
		v13.w = vinsert(r12)
		v17.w = vinsert(r4)
		r4 = add(r30,#-10800)
		v31:30.uh = vunpack(v19.ub)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r7)
		v31:30.uw = vunpack(v30.uh)
	}
	{
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r13)
		v21:20.uw = vunpack(v20.uh)
		r13:12 = memd(r17+#16)
	}
	{
		v16 = vdelta(v30,v22)
	}
	{
		v21:20.w = vunpack(v11.h)
		v3 = vmux(q0,v16,v20)
	}
	{
		v11 = vror(v12,r28)
	}
	{
		v12 = valign(v17,v17,#4)
		v11 = vor(v11,v28)
		v17 = v8
	}
	{
		v12.w = vinsert(r5)
		v13 = vror(v13,r28)
	}
	{
		v16 = valign(v18,v18,#4)
		v13 = vor(v13,v28)
	}
	{
		v16.w = vinsert(r25)
		r0 = add(r30,#-6064)
		vmemu(r0+#0) = v24
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-9008)
		vmemu(r4+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v31:30.uh = vunpack(v11.ub)
	}
	{
		vmemu(r0+#0) = v25
	}                                       // 256-byte Folded Spill
	{
		v11 = valign(v12,v12,#4)
		v12 = v8
		r1:0 = memd(r27+#40)
		r27 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v11.w = vinsert(r0)
		r4 = add(r30,#-8880)
		vmemu(r4+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		v12.w = vinsert(r22)
		r0 = add(r30,#-8752)
		vmemu(r4+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		v21:20.uh = vunpack(v13.ub)
		r7:6 = memd(r27+#16)
		r5:4 = memd(r27+#8)
	}
	{
		v17.w = vinsert(r6)
		v13 = valign(v16,v16,#4)
		r25:24 = memd(r27+#24)
	}
	{
		v13.w = vinsert(r2)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r1)
		v19:18.uw = vunpack(v30.uh)
	}
	{
		v31:30.uw = vunpack(v20.uh)
	}
	{
		v21:20.w = vunpack(v9.h)
	}
	{
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r3)
		v9 = vdelta(v30,v22)
		r3:2 = memd(r27+#40)
	}
	{
		v9 = vror(v11,r28)
		v3 = vmux(q0,v9,v18)
	}
	{
		v11 = valign(v12,v12,#4)
		v9 = vor(v9,v28)
	}
	{
		v11.w = vinsert(r23)
		v12 = valign(v17,v17,#4)
	}
	{
		v12.w = vinsert(r7)
		v13 = vror(v13,r28)
		r7:6 = memd(r27+#48)
	}
	{
		v31:30.uh = vunpack(v9.ub)
		v9 = vor(v13,v28)
		v13 = v8
	}
	{
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r4)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r24)
		r0 = add(r30,#-8624)
		vmemu(r0+#0) = v20
	}                                       // 256-byte Folded Spill
	{
		v15:14.uh = vunpack(v9.ub)
		r24 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r30,#-8496)
		vmemu(r0+#0) = v21
	}                                       // 256-byte Folded Spill
	{
		v9 = valign(v11,v11,#4)
		r9:8 = memd(r24+#16)
		r23:22 = memd(r24+#8)
	}
	{
		v9.w = vinsert(r5)
		vmemu(r0+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v21:20.uw = vunpack(v14.uh)
		r1:0 = memd(r27+#32)
		r5:4 = memd(r27+#56)
	}
	{
		v13.w = vinsert(r0)
		r0 = add(r30,#-11824)
		v11 = valign(v12,v12,#4)
		r27:26 = memd(r24+#48)
	}
	{
		v11.w = vinsert(r25)
		v17:16.uw = vunpack(v30.uh)
	}
	{
		v31:30.w = vunpack(v10.h)
		v17 = v8
	}
	{
		v17.w = vinsert(r6)
		v10 = vdelta(v20,v22)
	}
	{
		v9 = vror(v9,r28)
		v3 = vmux(q0,v10,v16)
		v16 = v8
	}
	{
		v16.w = vinsert(r8)
		v10 = valign(v13,v13,#4)
		v9 = vor(v9,v28)
		v13 = v8
	}
	{
		v10.w = vinsert(r1)
		v13.w = vinsert(r18)
		r8 = add(r30,#-9264)
		v11 = vror(v11,r28)
	}
	{
		v15:14.uh = vunpack(v9.ub)
		v11 = vor(v11,v28)
	}
	{
		v9 = valign(v10,v10,#4)
	}
	{
		v9.w = vinsert(r2)
		v21:20.uh = vunpack(v11.ub)
	}
	{
		v12 = valign(v17,v17,#4)
	}
	{
		v12.w = vinsert(r7)
		r0 = add(r30,#-11696)
		vmemu(r0+#0) = v30
	}                                       // 256-byte Folded Spill
	{
		r0 = add(r30,#-8240)
		vmemu(r0+#0) = v31
	}                                       // 256-byte Folded Spill
	{
		v9 = valign(v9,v9,#4)
		r7:6 = memd(r24+#56)
	}
	{
		v9.w = vinsert(r3)
		v31:30.uw = vunpack(v14.uh)
		r3:2 = memd(r24+#32)
	}
	{
		v15:14.uw = vunpack(v20.uh)
	}
	{
		v11 = valign(v12,v12,#4)
	}
	{
		v11.w = vinsert(r4)
		r4 = add(r30,#-9776)
		v12 = vdelta(v14,v22)
	}
	{
		v3 = vmux(q0,v12,v30)
		vmemu(r0+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v19:18.w = vunpack(v4.h)
		r1:0 = memd(r24+#24)
	}
	{
		v4 = vror(v9,r28)
	}
	{
		r4 = add(r30,#-10032)
		v4 = vor(v4,v28)
		vmemu(r4+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v10 = valign(v11,v11,#4)
	}
	{
		v10.w = vinsert(r5)
		r4 = add(r30,#-9904)
		vmemu(r4+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		v21:20.uh = vunpack(v4.ub)
	}
	{
		v9 = valign(v13,v13,#4)
	}
	{
		v19:18 = vcombine(v8,v8)
		vmemu(r4+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		v9.w = vinsert(r19)
		v19.w = vinsert(r26)
		v11 = valign(v16,v16,#4)
		r19:18 = memd(r16+#16)
	}
	{
		v11.w = vinsert(r9)
		v18.w = vinsert(r2)
		v13:12.uw = vunpack(v20.uh)
		r5:4 = memd(r24+#40)
	}
	{
		v10 = vror(v10,r28)
	}
	{
		v4 = valign(v9,v9,#4)
		v10 = vor(v10,v28)
	}
	{
		v4.w = vinsert(r22)
		v9 = valign(v11,v11,#4)
	}
	{
		r8 = add(r30,#-9136)
		v12 = v8
		vmemu(r8+#0) = v12
	}                                       // 256-byte Folded Spill
	{
		v9.w = vinsert(r0)
		v12.w = vinsert(r10)
		v11 = valign(v19,v19,#4)
	}
	{
		v11.w = vinsert(r27)
		v13 = v8
		vmemu(r8+#0) = v13
	}                                       // 256-byte Folded Spill
	{
		v13.w = vinsert(r18)
		r8 = add(r30,#-9520)
		v31:30.uh = vunpack(v10.ub)
		r27:26 = memd(r16+#32)
	}
	{
		v10 = valign(v18,v18,#4)
	}
	{
		v10.w = vinsert(r3)
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r16+#24)
	}
	{
		v11.w = vinsert(r6)
		v9 = valign(v9,v9,#4)
		r25:24 = memd(r16+#56)
	}
	{
		v9.w = vinsert(r1)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r16+#48)
	}
	{
		v12.w = vinsert(r11)
		v13 = valign(v13,v13,#4)
		r11:10 = memd(r17+#8)
	}
	{
		v13.w = vinsert(r19)
		v17:16.uw = vunpack(v30.uh)
		r19:18 = memd(r17+#24)
	}
	{
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r4)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r7)
		r8 = add(r30,#-9392)
		vmemu(r8+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r17+#40)
	}
	{
		v4.w = vinsert(r23)
		v9 = vror(v9,r28)
	}
	{
		v13 = valign(v13,v13,#4)
		v9 = vor(v9,v28)
	}
	{
		v13.w = vinsert(r2)
		r2 = add(r30,#-11568)
		vmemu(r8+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v12 = valign(v12,v12,#4)
		r9:8 = memd(r16+#8)
	}
	{
		v12.w = vinsert(r8)
		v10 = valign(v10,v10,#4)
		r23:22 = memd(r16+#40)
	}
	{
		v10.w = vinsert(r5)
		v11 = vror(v11,r28)
		r16 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		v4 = vror(v4,r28)
		v11 = vor(v11,v28)
		r5:4 = memd(r17+#32)
	}
	{
		v12 = valign(v12,v12,#4)
		v4 = vor(v4,v28)
	}
	{
		v12.w = vinsert(r9)
		v13 = valign(v13,v13,#4)
		r9:8 = memd(r17+#48)
	}
	{
		v13.w = vinsert(r3)
		v21:20.uh = vunpack(v9.ub)
	}
	{
		v10 = vror(v10,r28)
	}
	{
		v21:20.uw = vunpack(v20.uh)
		v10 = vor(v10,v28)
	}
	{
		v19:18.uh = vunpack(v11.ub)
	}
	{
		v31:30.uh = vunpack(v4.ub)
	}
	{
		v4 = vror(v13,r28)
	}
	{
		v12 = vror(v12,r28)
		v4 = vor(v4,v28)
	}
	{
		v13:12.uh = vunpack(v10.ub)
		v9 = vor(v12,v28)
		v10 = v8
	}
	{
		v10.w = vinsert(r12)
		v11 = vdelta(v20,v22)
	}
	{
		v21:20.uw = vunpack(v18.uh)
	}
	{
		v31:30.uw = vunpack(v30.uh)
	}
	{
		v31:30.uh = vunpack(v4.ub)
		v25 = vmux(q0,v11,v30)
	}
	{
		v4 = vdelta(v20,v22)
	}
	{
		v13:12.uw = vunpack(v12.uh)
	}
	{
		v17:16.uw = vunpack(v30.uh)
		v24 = vmux(q0,v4,v12)
		v4 = v8
	}
	{
		v4.w = vinsert(r26)
		v21:20.uh = vunpack(v9.ub)
		v9 = v8
	}
	{
		v9.w = vinsert(r0)
		v31:30.w = vunpack(v5.h)
		r0 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r27)
		v5 = vdelta(v16,v22)
		r27:26 = memd(r17+#56)
	}
	{
		v17:16.uw = vunpack(v20.uh)
	}
	{
		v5 = valign(v9,v9,#4)
		v23 = vmux(q0,v5,v16)
		v9 = v8
	}
	{
		v5.w = vinsert(r1)
		r2 = add(r30,#-11440)
		vmemu(r2+#0) = v30
	}                                       // 256-byte Folded Spill
	{
		v9.w = vinsert(r14)
		vmemu(r2+#0) = v31
	}                                       // 256-byte Folded Spill
	{
		v31:30.w = vunpack(v1.h)
	}
	{
		v1 = valign(v4,v4,#4)
	}
	{
		v1.w = vinsert(r22)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r24)
		v5 = valign(v9,v9,#4)
	}
	{
		v5.w = vinsert(r15)
		v9 = valign(v10,v10,#4)
	}
	{
		v9.w = vinsert(r13)
		v1 = valign(v1,v1,#4)
		r13:12 = memd(r20+r0<<#0)
	}
	{
		v1.w = vinsert(r23)
		v4 = valign(v4,v4,#4)
		r0 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
		r17 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r10)
		v9 = valign(v9,v9,#4)
		r23:22 = memd(r20+r0<<#0)
	}
	{
		v9.w = vinsert(r18)
		v1 = vror(v1,r28)
		r3:2 = memd(r17+#16)
		r15:14 = memd(r17+#8)
	}
	{
		v4 = vror(v4,r28)
		v1 = vor(v1,v28)
		r25:24 = memd(r17+#24)
		r1:0 = memd(r17+#32)
	}
	{
		v5 = valign(v5,v5,#4)
		v4 = vor(v4,v28)
	}
	{
		v5.w = vinsert(r11)
		v9 = valign(v9,v9,#4)
		r11:10 = memd(r17+#40)
	}
	{
		v9.w = vinsert(r19)
		v21:20.uh = vunpack(v1.ub)
		r19:18 = memd(r17+#48)
	}
	{
		v17:16.uh = vunpack(v4.ub)
	}
	{
		v1 = vror(v5,r28)
	}
	{
		v5:4.uw = vunpack(v20.uh)
		v1 = vor(v1,v28)
	}
	{
		v5 = vror(v9,r28)
	}
	{
		v21:20.uw = vunpack(v16.uh)
		v17:16 = vcombine(v8,v8)
	}
	{
		v16.w = vinsert(r4)
		r4 = add(r30,#-12336)
		v13:12.uh = vunpack(v1.ub)
		v5 = vor(v5,v28)
	}
	{
		v17.w = vinsert(r8)
		v9 = vdelta(v20,v22)
	}
	{
		v21:20.w = vunpack(v0.h)
	}
	{
		v1 = valign(v16,v16,#4)
		v21 = vmux(q0,v9,v4)
	}
	{
		v1.w = vinsert(r5)
		v5:4.uh = vunpack(v5.ub)
	}
	{
		v5 = valign(v17,v17,#4)
	}
	{
		v5.w = vinsert(r9)
		v11:10.uw = vunpack(v4.uh)
		v4 = v8
		r9:8 = memd(r16+#8)
	}
	{
		v4.w = vinsert(r12)
		v1 = valign(v1,v1,#4)
		v11 = v8
	}
	{
		v1.w = vinsert(r6)
		v11.w = vinsert(r2)
		v17:16.w = vunpack(v7.h)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r26)
		v13:12.uw = vunpack(v12.uh)
	}
	{
		v7 = vdelta(v10,v22)
	}
	{
		v1 = valign(v1,v1,#4)
		v18 = vmux(q0,v7,v12)
	}
	{
		v1.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r17+#56)
	}
	{
		v4.w = vinsert(r13)
		v7 = valign(v11,v11,#4)
		r13:12 = memd(r16+#24)
	}
	{
		v7.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r16+#16)
	}
	{
		v5.w = vinsert(r27)
		v1 = vror(v1,r28)
		r27:26 = memd(r16+#48)
	}
	{
		v4 = valign(v4,v4,#4)
		v1 = vor(v1,v28)
	}
	{
		v4.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r24)
		v5 = vror(v5,r28)
	}
	{
		v15:14.uh = vunpack(v1.ub)
		v1 = vor(v5,v28)
	}
	{
		v5 = valign(v7,v7,#4)
		v7 = v8
	}
	{
		v5.w = vinsert(r25)
		v7.w = vinsert(r18)
		v4 = valign(v4,v4,#4)
		r25:24 = memd(r16+#56)
	}
	{
		v4.w = vinsert(r15)
		r4 = add(r30,#-12208)
		vmemu(r4+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		vmemu(r4+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v17:16.uh = vunpack(v1.ub)
		r5:4 = memd(r16+#32)
		r15:14 = memd(r16+#40)
	}
	{
		v1 = vror(v4,r28)
	}
	{
		v4 = vror(v5,r28)
		v5 = v8
		v1 = vor(v1,v28)
	}
	{
		v5.w = vinsert(r0)
		v11:10.uw = vunpack(v16.uh)
		v4 = vor(v4,v28)
	}
	{
		v1:0.uh = vunpack(v1.ub)
		r0 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r1)
		v13:12.uw = vunpack(v14.uh)
	}
	{
		v15:14.w = vunpack(v2.h)
	}
	{
		v1 = vdelta(v10,v22)
	}
	{
		v1:0.uw = vunpack(v0.uh)
		v15 = vmux(q0,v1,v12)
	}
	{
		v1 = valign(v7,v7,#4)
		v7 = v8
	}
	{
		v1.w = vinsert(r19)
		v7.w = vinsert(r2)
		v3:2.uh = vunpack(v4.ub)
	}
	{
		v4 = valign(v5,v5,#4)
		v5 = v8
	}
	{
		v4.w = vinsert(r10)
		v5.w = vinsert(r22)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r6)
		v17:16.uw = vunpack(v2.uh)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r23)
		v11 = vdelta(v16,v22)
		r23:22 = memd(r20+r0<<#0)
	}
	{
		v0 = valign(v1,v1,#4)
		v31 = vmux(q0,v11,v0)
		r20 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r7)
		v1 = vror(v4,r28)
	}
	{
		v4 = valign(v7,v7,#4)
		v1 = vor(v1,v28)
		r1:0 = memd(r20+#16)
		r7:6 = memd(r20+#48)
	}
	{
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r19:18 = memd(r20+#24)
		r11:10 = memd(r20+#8)
	}
	{
		v5.w = vinsert(r8)
		v0 = vror(v0,r28)
		r3:2 = memd(r20+#32)
	}
	{
		v11:10.uh = vunpack(v1.ub)
		v0 = vor(v0,v28)
	}
	{
		v1 = valign(v4,v4,#4)
	}
	{
		v1.w = vinsert(r12)
		v4 = valign(v5,v5,#4)
	}
	{
		v4.w = vinsert(r9)
		v17:16.uw = vunpack(v10.uh)
	}
	{
		v11:10.uh = vunpack(v0.ub)
	}
	{
		v0 = valign(v1,v1,#4)
	}
	{
		v0.w = vinsert(r13)
		v1 = vror(v4,r28)
		v4 = v8
	}
	{
		v4.w = vinsert(r26)
		v3:2.uw = vunpack(v10.uh)
		v1 = vor(v1,v28)
	}
	{
		v0 = vror(v0,r28)
		v3 = v8
	}
	{
		v3.w = vinsert(r4)
		v2 = vdelta(v2,v22)
		v0 = vor(v0,v28)
	}
	{
		v17:16.uh = vunpack(v1.ub)
		v9 = vmux(q0,v2,v16)
	}
	{
		v2 = valign(v4,v4,#4)
	}
	{
		v2.w = vinsert(r27)
		v5:4.uh = vunpack(v0.ub)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v11:10.uw = vunpack(v16.uh)
		v17:16 = vcombine(v8,v8)
	}
	{
		v3.w = vinsert(r5)
		v17.w = vinsert(r0)
		v13:12.w = vunpack(v27.h)
		r5:4 = memd(r20+#40)
	}
	{
		v16.w = vinsert(r22)
		v5:4.uw = vunpack(v4.uh)
		v13 = v8
	}
	{
		v1 = valign(v3,v3,#4)
		v5 = v8
		r0 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		v13.w = vinsert(r6)
		v5.w = vinsert(r2)
		v0 = valign(v2,v2,#4)
	}
	{
		v1.w = vinsert(r14)
		v0.w = vinsert(r24)
		v3 = valign(v17,v17,#4)
		r27:26 = memd(r0+#80)
	}
	{
		v3.w = vinsert(r1)
		r2 = add(r30,#-7984)
		r1 = add(r30,#-6960)
		v2 = vdelta(v4,v22)
	}
	{
		v2 = valign(v16,v16,#4)
		v19 = vmux(q0,v2,v10)
	}
	{
		v2.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r3)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r7)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r25)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r18)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r15)
		v2 = valign(v2,v2,#4)
		r15:14 = memd(r20+#56)
	}
	{
		v2.w = vinsert(r10)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r4)
		r4 = r0
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r14)
		v0 = vror(v0,r28)
		r7:6 = memd(r4+#88)
	}
	{
		v3 = valign(v3,v3,#4)
		v0 = vor(v0,v28)
	}
	{
		v3.w = vinsert(r19)
		v1 = vror(v1,r28)
	}
	{
		v2 = valign(v2,v2,#4)
		v1 = vor(v1,v28)
	}
	{
		v2.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r5)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r15)
		v17:16.uh = vunpack(v0.ub)
	}
	{
		v3 = vror(v3,r28)
	}
	{
		v1:0.uh = vunpack(v1.ub)
		v3 = vor(v3,v28)
	}
	{
		v2 = vror(v2,r28)
	}
	{
		v1 = vror(v13,r28)
		v2 = vor(v2,v28)
	}
	{
		v5 = vror(v5,r28)
		v13 = vor(v1,v28)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		v7 = vor(v5,v28)
	}
	{
		v1:0.uw = vunpack(v0.uh)
	}
	{
		v5:4.uh = vunpack(v3.ub)
	}
	{
		v1 = vdelta(v16,v22)
	}
	{
		v3:2.uh = vunpack(v2.ub)
	}
	{
		v11:10.w = vunpack(v29.h)
		v29 = vmux(q0,v1,v0)
	}
	{
		v1:0.uw = vunpack(v4.uh)
	}
	{
		r1 = add(r30,#-6832)
		v16 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-11824)
		v17 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v17.w = vmpyieo(v6.h,v16.h)
		v3:2.uw = vunpack(v2.uh)
	}
	{
		v17.w += vmpyie(v6.w,v16.h)
		v0 = vdelta(v0,v22)
	}
	{
		r1 = add(r30,#-11696)
		v11 = vmux(q0,v0,v2)
		v0 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v5:4.uh = vunpack(v13.ub)
	}
	{
		v13.w = vmpyieo(v25.h,v0.h)
		r1 = add(r30,#-10288)
		v1 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v13.w += vmpyie(v25.w,v0.h)
		r1 = add(r30,#-10160)
		v6 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v29.h,v12.h)
		v3:2.uh = vunpack(v7.ub)
	}
	{
		v0.w += vmpyie(v29.w,v12.h)
		v5:4.uw = vunpack(v4.uh)
	}
	{
		r1 = add(r30,#-10800)
		v7 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-6960)
		v5 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v27:26.w = vunpack(v26.h)
	}
	{
		v16.w = vmpyieo(v5.h,v6.h)
		v1 = vdelta(v4,v22)
	}
	{
		v16.w += vmpyie(v5.w,v6.h)
		v3:2.uw = vunpack(v2.uh)
	}
	{
		v27.w = vmpyieo(v11.h,v26.h)
		v1.w = vmpyieo(v19.h,v14.h)
		v2 = vmux(q0,v1,v2)
	}
	{
		v27.w += vmpyie(v11.w,v26.h)
		r1 = add(r30,#-6832)
		vmemu(r1+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v26.w = vmpyieo(v2.h,v10.h)
		r1 = add(r30,#-12336)
		vmemu(r1+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v26.w += vmpyie(v2.w,v10.h)
		r1 = add(r30,#-12208)
		v6 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-20272)
		v2 = v8
		v7 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vmpyieo(v9.h,v6.h)
		r1 = add(r30,#-20144)
		v10 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w += vmpyie(v9.w,v6.h)
		r1 = add(r30,#-11568)
		v11 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r26)
		r1 = add(r30,#-11440)
		v6 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vmpyieo(v31.h,v20.h)
		v9 = v8
		v7 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v19.w,v14.h)
		v7 = valign(v2,v2,#4)
		r1:0 = memd(r0+#64)
	}
	{
		v7.w = vinsert(r27)
		v9.w = vinsert(r0)
		r0 = add(r30,#-10032)
		v11:10.w = vadd(v27:26.w,v11:10.w)
	}
	{
		v5.w += vmpyie(v31.w,v20.h)
		v1:0.w = vadd(v11:10.w,v1:0.w)
	}
	{
		r2 = add(r30,#-9008)
		v5:4.w = vadd(v5:4.w,v1:0.w)
		v0 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v18.h,v6.h)
		v2.w = vmpyieo(v15.h,v30.h)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r6)
		r0 = add(r30,#-9904)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w += vmpyie(v18.w,v6.h)
		v1:0.w = vunpack(v0.h)
	}
	{
		v12.w = vmpyieo(v24.h,v10.h)
		v9 = valign(v9,v9,#4)
		v6 = v10
	}
	{
		v9.w = vinsert(r1)
		v1.w = vmpyieo(v23.h,v0.h)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w += vmpyie(v15.w,v30.h)
		r2 = add(r30,#-8880)
		v10 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v23.w,v0.h)
		r0 = add(r30,#-7472)
		v7 = valign(v7,v7,#4)
	}
	{
		r2 = add(r30,#-9776)
		v5:4.w = vadd(v5:4.w,v3:2.w)
		v11 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v7.w = vinsert(r7)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v12.w += vmpyie(v24.w,v6.h)
		v2 = valign(v9,v9,#4)
		r1:0 = memd(r4+#72)
		r7:6 = memd(r4+#112)
	}
	{
		v2.w = vinsert(r0)
		r0 = add(r30,#-9520)
		v9 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r2 = add(r30,#-6448)
		v3 = vror(v7,r28)
		v7 = v8
	}
	{
		v17.w = vmpyieo(v9.h,v10.h)
		v15:14.w = vunpack(v0.h)
		v3 = vor(v3,v28)
	}
	{
		v17.w += vmpyie(v9.w,v10.h)
		r0 = add(r30,#-9392)
		v10 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v21.h,v14.h)
		r0 = add(r30,#-9264)
		v11 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w += vmpyie(v21.w,v14.h)
		v2 = valign(v2,v2,#4)
	}
	{
		v7.w = vinsert(r6)
		r0 = add(r30,#-9136)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r1)
		v9 = vdelta(v10,v22)
	}
	{
		v21:20.uh = vunpack(v3.ub)
		v3 = vmux(q0,v9,v14)
		v9 = v8
	}
	{
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r4+#96)
	}
	{
		v9.w = vinsert(r0)
		v7.w = vinsert(r7)
		v11:10.uw = vunpack(v20.uh)
		r7:6 = memd(r21+#80)
	}
	{
		v14 = vror(v2,r28)
		v21:20.w = vadd(v5:4.w,v1:0.w)
		v11 = v8
	}
	{
		v11.w = vinsert(r6)
		v9 = valign(v9,v9,#4)
		v14 = vor(v14,v28)
	}
	{
		v9.w = vinsert(r1)
		v2 = vdelta(v10,v22)
		v10 = v8
		r1:0 = memd(r21+#64)
	}
	{
		v10.w = vinsert(r0)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r7)
		v7 = valign(v7,v7,#4)
		r7:6 = memd(r4+#120)
		r5:4 = memd(r4+#104)
	}
	{
		v7.w = vinsert(r6)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r1)
		v11 = valign(v11,v11,#4)
		r1:0 = memd(r21+#88)
	}
	{
		v11.w = vinsert(r0)
		r0 = add(r30,#-8752)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r7)
		r0 = add(r30,#-8624)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v9 = valign(v9,v9,#4)
		v13:12.w = vadd(v13:12.w,v21:20.w)
		v6 = v4
		r7:6 = memd(r21+#72)
	}
	{
		v16.w = vmpyieo(v3.h,v4.h)
		v9.w = vinsert(r4)
		v7 = vror(v7,r28)
	}
	{
		v16.w += vmpyie(v3.w,v6.h)
		v1:0.uh = vunpack(v14.ub)
	}
	{
		r2 = add(r30,#-6320)
		v1 = vor(v7,v28)
		v6 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = add(r30,#-8496)
		v7 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r5)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v31:30.uh = vunpack(v1.ub)
		v5 = v8
		r5:4 = memd(r21+#112)
	}
	{
		v5.w = vinsert(r4)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r21+#96)
	}
	{
		v19.w = vmpyieo(v1.h,v6.h)
		v11.w = vinsert(r1)
		v10 = valign(v10,v10,#4)
	}
	{
		v19.w += vmpyie(v1.w,v6.h)
		v1:0.uw = vunpack(v0.uh)
	}
	{
		v10.w = vinsert(r6)
		v3 = vror(v9,r28)
		v6 = vmux(q0,v2,v0)
		v0 = v8
	}
	{
		v3 = valign(v5,v5,#4)
		v1 = vor(v3,v28)
	}
	{
		v3.w = vinsert(r5)
		v7 = vror(v11,r28)
		r5:4 = memd(r21+#104)
	}
	{
		v4 = valign(v10,v10,#4)
		v5 = vor(v7,v28)
		v7 = v8
	}
	{
		v4.w = vinsert(r7)
		v7.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r21+#120)
	}
	{
		v3.w = vinsert(r6)
		v11:10.uw = vunpack(v30.uh)
		r6 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v31:30.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v7,v7,#4)
		v7 = v8
		r1:0 = memd(r6+#80)
	}
	{
		v1.w = vinsert(r3)
		v7.w = vinsert(r0)
		r0 = add(r30,#-6448)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v27:26.uh = vunpack(v5.ub)
		r3:2 = memd(r6+#120)
	}
	{
		v1 = valign(v1,v1,#4)
		r7 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		v1.w = vinsert(r4)
		v5 = vror(v4,r28)
	}
	{
		v3 = vror(v3,r28)
		v5 = vor(v5,v28)
	}
	{
		v7 = valign(v7,v7,#4)
		v3 = vor(v3,v28)
	}
	{
		v7.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r5)
		r0 = add(r30,#-6320)
		vmemu(r0+#0) = v12
	}                                       // 256-byte Folded Spill
	{
		v9 = vdelta(v10,v22)
		r5:4 = memd(r6+#64)
	}
	{
		v11:10.uh = vunpack(v5.ub)
		v5 = v8
	}
	{
		v5.w = vinsert(r4)
		v25:24.uw = vunpack(v30.uh)
	}
	{
		vmemu(r0+#0) = v13
	}                                       // 256-byte Folded Spill
	{
		v31:30.uh = vunpack(v3.ub)
		r1:0 = memd(r6+#88)
	}
	{
		v3 = valign(v7,v7,#4)
	}
	{
		v3.w = vinsert(r0)
		r0 = add(r30,#-6192)
		v7 = valign(v5,v5,#4)
	}
	{
		v7.w = vinsert(r5)
		r0 = add(r30,#-6064)
		v21 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r6+#72)
	}
	{
		v3.w = vinsert(r1)
		r0 = add(r30,#-8240)
		v23 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v11:10.uw = vunpack(v10.uh)
		v23 = v8
	}
	{
		v11 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v3 = vror(v3,r28)
		r1:0 = memd(r6+#112)
	}
	{
		v18.w = vmpyieo(v11.h,v21.h)
		v7 = valign(v7,v7,#4)
		v3 = vor(v3,v28)
	}
	{
		v18.w += vmpyie(v11.w,v21.h)
		v1 = vror(v1,r28)
		v11 = v8
	}
	{
		v7.w = vinsert(r4)
		v11.w = vinsert(r0)
		v1 = vor(v1,v28)
	}
	{
		v15:14.uw = vunpack(v26.uh)
		r4 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		v27:26.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v7,v7,#4)
		v7 = v8
	}
	{
		v3.w = vinsert(r5)
		v21:20.uh = vunpack(v1.ub)
		r5 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v1 = valign(v11,v11,#4)
	}
	{
		v1.w = vinsert(r1)
		v3 = vror(v3,r28)
		r1:0 = memd(r5+#80)
	}
	{
		v7.w = vinsert(r0)
		v4 = vdelta(v14,v22)
		v3 = vor(v3,v28)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r2)
		v11 = valign(v7,v7,#4)
	}
	{
		v11.w = vinsert(r1)
		v15:14.uw = vunpack(v30.uh)
		r1:0 = memd(r5+#64)
	}
	{
		v0.w = vinsert(r0)
		r0 = add(r30,#-6192)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r3)
		v31:30.uw = vunpack(v26.uh)
		r3:2 = memd(r5+#88)
	}
	{
		v5 = vdelta(v14,v22)
	}
	{
		v7 = vdelta(v30,v22)
	}
	{
		v31:30.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v11,v11,#4)
	}
	{
		v3.w = vinsert(r2)
		v11 = valign(v0,v0,#4)
		v0 = vmux(q0,v9,v24)
		v9 = v8
	}
	{
		v11.w = vinsert(r1)
		v1 = vror(v1,r28)
	}
	{
		v3 = valign(v3,v3,#4)
		v1 = vor(v1,v28)
	}
	{
		v3.w = vinsert(r3)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v27:26.uw = vunpack(v30.uh)
		r1:0 = memd(r5+#112)
		r3:2 = memd(r5+#72)
	}
	{
		v9.w = vinsert(r0)
		v31:30.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v11,v11,#4)
	}
	{
		v1.w = vinsert(r2)
		r2 = add(r30,#-7472)
		v3 = vror(v3,r28)
	}
	{
		v9 = valign(v9,v9,#4)
		v3 = vor(v3,v28)
	}
	{
		v9.w = vinsert(r1)
		v15:14.uw = vunpack(v30.uh)
		r1:0 = memd(r5+#120)
	}
	{
		v11 = valign(v1,v1,#4)
		v15 = v8
	}
	{
		v11.w = vinsert(r3)
		v0 = vdelta(v14,v22)
	}
	{
		v25:24.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v9,v9,#4)
	}
	{
		v3.w = vinsert(r0)
		v0 = vmux(q0,v4,v10)
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r30,#-7984)
		v9 = vror(v11,r28)
		v10 = v8
		r3:2 = memd(r4+#80)
	}
	{
		v15.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		v11 = vor(v9,v28)
	}
	{
		v3.w = vinsert(r1)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v14 = valign(v15,v15,#4)
		r1:0 = memd(r5+#96)
	}
	{
		v14.w = vinsert(r3)
		v10.w = vinsert(r0)
		v3 = vror(v3,r28)
		r3:2 = memd(r4+#88)
	}
	{
		r0 = add(r30,#-8240)
		v21:20.uw = vunpack(v20.uh)
		v3 = vor(v3,v28)
	}
	{
		v31:30.uw = vunpack(v24.uh)
		v0 = vmux(q0,v5,v20)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r2)
		v25:24.uh = vunpack(v11.ub)
	}
	{
		v15 = valign(v10,v10,#4)
	}
	{
		v15.w = vinsert(r1)
		v11:10.uw = vunpack(v24.uh)
	}
	{
		v0 = vmux(q0,v7,v26)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v25:24.uh = vunpack(v3.ub)
		v3 = v8
		r1:0 = memd(r4+#64)
	}
	{
		v3.w = vinsert(r0)
		v11 = valign(v14,v14,#4)
	}
	{
		v11.w = vinsert(r3)
		v14 = valign(v15,v15,#4)
		v15 = v8
		r3:2 = memd(r5+#104)
	}
	{
		v14.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v11 = vror(v11,r28)
		r1:0 = memd(r4+#112)
	}
	{
		v15.w = vinsert(r0)
		r0 = add(r30,#-304)
		v9 = vdelta(v30,v22)
		v11 = vor(v11,v28)
	}
	{
		v31:30.uw = vunpack(v24.uh)
		v10 = vmux(q0,v9,v10)
	}
	{
		v25:24.uh = vunpack(v11.ub)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r3)
		v11 = valign(v15,v15,#4)
		r3:2 = memd(r4+#72)
	}
	{
		v11.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v11 = valign(v11,v11,#4)
		r1:0 = memd(r4+#120)
	}
	{
		v11.w = vinsert(r0)
		v14 = vror(v14,r28)
	}
	{
		v3 = valign(v3,v3,#4)
		v14 = vor(v14,v28)
	}
	{
		v3.w = vinsert(r3)
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r7+#80)
	}
	{
		v11.w = vinsert(r1)
		v15:14.uh = vunpack(v14.ub)
		r1:0 = memd(r4+#96)
	}
	{
		v23.w = vinsert(r0)
		v3 = vror(v3,r28)
		v15 = v8
	}
	{
		v15.w = vinsert(r2)
		v25:24.uw = vunpack(v24.uh)
		v3 = vor(v3,v28)
	}
	{
		v9 = vror(v11,r28)
	}
	{
		v27:26.uh = vunpack(v3.ub)
		v9 = vor(v9,v28)
	}
	{
		v3 = valign(v23,v23,#4)
		v23 = v8
	}
	{
		v3.w = vinsert(r1)
		v21 = vdelta(v24,v22)
		r1:0 = memd(r7+#88)
	}
	{
		v25:24.uw = vunpack(v14.uh)
	}
	{
		v14 = valign(v15,v15,#4)
		v15 = v8
	}
	{
		v14.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r4+#104)
		r5:4 = memd(r7+#64)
	}
	{
		v3.w = vinsert(r2)
		v15.w = vinsert(r4)
		v20 = vdelta(v30,v22)
	}
	{
		v14 = valign(v14,v14,#4)
		v11 = vmux(q0,v20,v24)
	}
	{
		v14.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v25:24.uh = vunpack(v9.ub)
		r3:2 = memd(r17+#88)
	}
	{
		v9 = valign(v14,v14,#4)
	}
	{
		v9.w = vinsert(r1)
		v14 = valign(v15,v15,#4)
		v15 = v8
		r1:0 = memd(r7+#112)
	}
	{
		v14.w = vinsert(r5)
		v15.w = vinsert(r0)
		v3 = vror(v3,r28)
		r5:4 = memd(r7+#72)
	}
	{
		r0 = add(r30,#-7216)
		v27:26.uw = vunpack(v26.uh)
		v3 = vor(v3,v28)
	}
	{
		v14 = valign(v14,v14,#4)
		v0 = vmux(q0,v21,v26)
	}
	{
		v14.w = vinsert(r4)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r1)
		v27:26.uh = vunpack(v3.ub)
		v3 = v8
	}
	{
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v9 = vror(v9,r28)
		r1:0 = memd(r17+#80)
	}
	{
		v3.w = vinsert(r0)
		v14 = valign(v14,v14,#4)
		v9 = vor(v9,v28)
	}
	{
		v14.w = vinsert(r5)
		v25:24.uw = vunpack(v24.uh)
		r5:4 = memd(r7+#120)
	}
	{
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v29 = vdelta(v24,v22)
		r1:0 = memd(r17+#112)
	}
	{
		v25:24.uh = vunpack(v9.ub)
	}
	{
		v14 = vror(v14,r28)
	}
	{
		v15 = valign(v15,v15,#4)
		v14 = vor(v14,v28)
	}
	{
		v15.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r7+#96)
	}
	{
		v3.w = vinsert(r2)
		v23.w = vinsert(r4)
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v3 = valign(v3,v3,#4)
		v20 = vmux(q0,v29,v26)
	}
	{
		v3.w = vinsert(r3)
		v30 = vdelta(v24,v22)
		r3:2 = memd(r17+#64)
	}
	{
		v25:24.uh = vunpack(v14.ub)
	}
	{
		v14 = vror(v15,r28)
		v15 = v8
	}
	{
		v15.w = vinsert(r0)
		v25:24.uw = vunpack(v24.uh)
		v14 = vor(v14,v28)
	}
	{
		v3 = vror(v3,r28)
		v21 = vmux(q0,v30,v24)
	}
	{
		v25:24.uh = vunpack(v14.ub)
		v3 = vor(v3,v28)
	}
	{
		v14 = valign(v15,v15,#4)
	}
	{
		v14.w = vinsert(r1)
		v25:24.uw = vunpack(v24.uh)
		r1:0 = memd(r17+#120)
	}
	{
		v27:26.uh = vunpack(v3.ub)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r0)
		v3 = vdelta(v24,v22)
	}
	{
		v25:24.uw = vunpack(v26.uh)
	}
	{
		v15 = valign(v23,v23,#4)
		v25 = v8
	}
	{
		v15.w = vinsert(r5)
		v25.w = vinsert(r2)
		v14 = valign(v14,v14,#4)
		r5:4 = memd(r7+#104)
	}
	{
		v14.w = vinsert(r1)
		v23 = vdelta(v24,v22)
		v24 = v8
		r1:0 = memd(r17+#96)
	}
	{
		v24.w = vinsert(r0)
		r7 = add(r30,#-3632)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r4)
		v14 = vror(v14,r28)
	}
	{
		v24 = valign(v24,v24,#4)
		v14 = vor(v14,v28)
	}
	{
		v24.w = vinsert(r1)
		v25 = valign(v25,v25,#4)
		r1:0 = memd(r16+#80)
	}
	{
		v25.w = vinsert(r3)
		v15 = valign(v15,v15,#4)
		r3:2 = memd(r17+#72)
	}
	{
		v15.w = vinsert(r5)
		v27:26.uh = vunpack(v14.ub)
		r5:4 = memd(r17+#104)
	}
	{
		v14 = valign(v24,v24,#4)
	}
	{
		v14.w = vinsert(r4)
		v25 = valign(v25,v25,#4)
	}
	{
		v25.w = vinsert(r2)
		v15 = vror(v15,r28)
	}
	{
		v14 = valign(v14,v14,#4)
		v15 = vor(v15,v28)
	}
	{
		v14.w = vinsert(r5)
		v24 = valign(v25,v25,#4)
		v25 = v8
		r5:4 = memd(r16+#112)
	}
	{
		v25.w = vinsert(r0)
		v24.w = vinsert(r3)
		v27:26.uw = vunpack(v26.uh)
		r3:2 = memd(r16+#64)
	}
	{
		v1:0.uh = vunpack(v15.ub)
	}
	{
		v14 = vror(v14,r28)
	}
	{
		v25 = valign(v25,v25,#4)
		v14 = vor(v14,v28)
	}
	{
		v25.w = vinsert(r1)
		v15 = vdelta(v26,v22)
		r1:0 = memd(r16+#88)
	}
	{
		v27:26.uw = vunpack(v0.uh)
	}
	{
		v24 = vror(v24,r28)
		v27 = v8
	}
	{
		v27.w = vinsert(r4)
		v1:0.uh = vunpack(v14.ub)
		v24 = vor(v24,v28)
	}
	{
		v1 = valign(v25,v25,#4)
		v14 = v8
	}
	{
		v1.w = vinsert(r0)
		v14.w = vinsert(r2)
		v13:12.uh = vunpack(v24.ub)
	}
	{
		v27 = valign(v27,v27,#4)
		v13 = vmux(q0,v3,v26)
	}
	{
		v27.w = vinsert(r5)
		v25:24.uw = vunpack(v12.uh)
		r5:4 = memd(r16+#120)
	}
	{
		v1 = valign(v1,v1,#4)
		v24 = vmux(q0,v23,v24)
		v25 = v8
	}
	{
		v1.w = vinsert(r1)
		v31:30.uw = vunpack(v0.uh)
		r1:0 = memd(r16+#96)
	}
	{
		v0 = valign(v27,v27,#4)
		v23 = vmux(q0,v15,v30)
		v15 = v8
	}
	{
		v0.w = vinsert(r4)
		v15.w = vinsert(r0)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r3)
		v1 = vror(v1,r28)
		r3:2 = memd(r16+#72)
	}
	{
		v0 = valign(v0,v0,#4)
		v1 = vor(v1,v28)
	}
	{
		v0.w = vinsert(r5)
		v15 = valign(v15,v15,#4)
		r5:4 = memd(r16+#104)
	}
	{
		v15.w = vinsert(r1)
		v27:26.uh = vunpack(v1.ub)
		r1:0 = memd(r20+#80)
	}
	{
		v25.w = vinsert(r0)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r2)
		v0 = vror(v0,r28)
	}
	{
		v1 = valign(v15,v15,#4)
		v0 = vor(v0,v28)
	}
	{
		v1.w = vinsert(r4)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r3)
		v31:30.uh = vunpack(v0.ub)
		r3:2 = memd(r20+#112)
	}
	{
		v0 = valign(v1,v1,#4)
	}
	{
		v0.w = vinsert(r5)
		v1 = vror(v14,r28)
		r5:4 = memd(r20+#64)
	}
	{
		v27:26.uw = vunpack(v26.uh)
		v1 = vor(v1,v28)
	}
	{
		v0 = vror(v0,r28)
	}
	{
		v31:30.uw = vunpack(v30.uh)
		v0 = vor(v0,v28)
	}
	{
		v15 = vdelta(v26,v22)
	}
	{
		v27:26.uh = vunpack(v1.ub)
	}
	{
		v14 = vdelta(v30,v22)
		v27 = v8
	}
	{
		v31:30.uh = vunpack(v0.ub)
	}
	{
		v8.w = vinsert(r2)
		v5 = valign(v25,v25,#4)
		v25 = v8
		v31 = v8
	}
	{
		v5.w = vinsert(r1)
		v25.w = vinsert(r4)
		v1:0.uw = vunpack(v30.uh)
		r1:0 = memd(r20+#96)
	}
	{
		v27.w = vinsert(r0)
		v1 = valign(v8,v8,#4)
		v30 = vmux(q0,v14,v0)
	}
	{
		v1.w = vinsert(r3)
		v3:2.uw = vunpack(v26.uh)
		r3:2 = memd(r20+#88)
	}
	{
		v3 = valign(v5,v5,#4)
		v29 = vmux(q0,v15,v2)
	}
	{
		v3.w = vinsert(r2)
		v5 = valign(v25,v25,#4)
	}
	{
		v5.w = vinsert(r5)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r20+#120)
	}
	{
		v1.w = vinsert(r4)
		r4 = #64
		v8 = valign(v27,v27,#4)
	}
	{
		v8.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r20+#72)
	}
	{
		v3.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r20+#104)
	}
	{
		v5.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r5)
		v3 = vror(v3,r28)
	}
	{
		v5 = valign(v5,v5,#4)
		v3 = vor(v3,v28)
	}
	{
		v5.w = vinsert(r1)
		v8 = valign(v8,v8,#4)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		v8.w = vinsert(r2)
		r0 = add(r1,#2176)
		r2 = r1
		v1 = vror(v1,r28)
	}
	{
		v15:14.uh = vunpack(v3.ub)
		v1 = vor(v1,v28)
		v12 = vmem(r0+#0)
	}
	{
		v3 = vror(v5,r28)
		r0 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-1920)
		v27:26.uh = vunpack(v1.ub)
		v1 = vor(v3,v28)
	}
	{
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r3)
		r3 = add(r30,#-1328)
		v15:14.uw = vunpack(v14.uh)
	}
	{
		v1:0.uh = vunpack(v1.ub)
	}
	{
		v3 = vror(v8,r28)
	}
	{
		v8 = vdelta(v14,v22)
		v3 = vor(v3,v28)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v15:14.uw = vunpack(v0.uh)
	}
	{
		v0 = vmemu(r7+#0)
	}                                       // 128-byte Folded Reload
	{
		v22 = vdelta(v26,v22)
	}
	{
		v27:26.uh = vunpack(v3.ub)
	}
	{
		v3 = valign(v0,v0,r4)
	}
	{
		v1:0.w = vunpack(v0.h)
	}
	{
		v5:4.w = vunpack(v3.h)
	}
	{
		v15.w = vmpyieo(v10.h,v0.h)
		v3:2.w = vunpack(v12.h)
	}
	{
		v15.w += vmpyie(v10.w,v0.h)
		v3 = vmux(q0,v8,v14)
		v0 = vmem(r1+#5)
	}
	{
		v14.w = vmpyieo(v11.h,v4.h)
		v25.w = vmpyieo(v6.h,v2.h)
		v8 = valign(v0,v0,r4)
	}
	{
		v25.w += vmpyie(v6.w,v2.h)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v14.w += vmpyie(v11.w,v4.h)
		v22 = vmux(q0,v22,v26)
		r1:0 = memd(r6+#96)
		v2 = vmem(r0+#0)
	}
	{
		v31.w = vinsert(r0)
		r0 = add(r30,#-1584)
		v5:4.w = vunpack(v8.h)
		r7:6 = memd(r6+#104)
	}
	{
		v27:26.w = vunpack(v0.h)
	}
	{
		v10.w = vmpyieo(v23.h,v4.h)
		v1:0.w = vunpack(v2.h)
	}
	{
		v10.w += vmpyie(v23.w,v4.h)
		v4 = valign(v31,v31,#4)
	}
	{
		v1.w = vmpyieo(v3.h,v0.h)
		v4.w = vinsert(r1)
		v2 = valign(v2,v2,r4)
	}
	{
		v1.w += vmpyie(v3.w,v0.h)
		v3 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v11.w = vmpyieo(v24.h,v26.h)
		v7:6.w = vunpack(v2.h)
		v2 = vmem(r2+#7)
	}
	{
		v11.w += vmpyie(v24.w,v26.h)
		v0 = valign(v3,v3,r4)
	}
	{
		v27:26.w = vunpack(v3.h)
	}
	{
		v3 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r6)
		v0.w = vmpyieo(v22.h,v6.h)
		r6 = add(r30,#-20400)
		v9:8.w = vunpack(v0.h)
	}
	{
		v5.w = vmpyieo(v29.h,v26.h)
		r6 = add(r30,#-816)
		v9 = vmemu(r6+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vmpyieo(v30.h,v8.h)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		r7 = add(r30,#-20528)
	}
	{
		v0.w += vmpyie(v22.w,v6.h)
		v7:6.w = vunpack(v2.h)
	}
	{
		v4.w += vmpyie(v30.w,v8.h)
		r7 = add(r30,#-7216)
		v8 = vmemu(r7+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w += vmpyie(v29.w,v26.h)
		v2 = valign(v2,v2,r4)
	}
	{
		v3 = vror(v3,r28)
		v1:0.w = vadd(v1:0.w,v9:8.w)
	}
	{
		v5:4.w = vunpack(v2.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v11:10.w,v1:0.w)
		v3 = vor(v3,v28)
		v5 = vmemu(r3+#0)
	}                                       // 128-byte Folded Reload
	{
		v7.w = vmpyieo(v21.h,v6.h)
		r3 = add(r2,#1920)
		r2 = add(r30,#-304)
	}
	{
		v7.w += vmpyie(v21.w,v6.h)
		v11:10.uh = vunpack(v3.ub)
	}
	{
		v6.w = vmpyieo(v13.h,v4.h)
		v2 = valign(v5,v5,r4)
	}
	{
		v6.w += vmpyie(v13.w,v4.h)
		r7 = add(r30,#-7472)
		v3 = vmemu(r7+#0)
	}                                       // 128-byte Folded Reload
	{
		v9:8.w = vunpack(v5.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
	}
	{
		v29:28.w = vunpack(v2.h)
	}
	{
		v5.w = vmpyieo(v3.h,v8.h)
		r6 = add(r30,#-7984)
		v11 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		v5.w += vmpyie(v3.w,v8.h)
		v9:8.uw = vunpack(v10.uh)
	}
	{
		v4.w = vmpyieo(v20.h,v28.h)
		r7 = add(r30,#-6320)
		v2 = vmemu(r7+#0)
	}                                       // 128-byte Folded Reload
	{
		v4.w += vmpyie(v20.w,v28.h)
		v8 = valign(v11,v11,r4)
		v2 = vmux(q0,v2,v8)
	}
	{
		v31:30.w = vunpack(v11.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		r3 = add(r30,#-8240)
		v5:4.w = vunpack(v8.h)
		v3 = vmem(r3+#0)
	}
	{
		r2 = add(r30,#-6448)
		v1:0.w = vadd(v15:14.w,v1:0.w)
		v10 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v8.w = vmpyieo(v2.h,v4.h)
		v7:6.w = vunpack(v3.h)
	}
	{
		v9.w = vmpyieo(v10.h,v30.h)
		r6 = add(r30,#-6192)
		v31 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v10.w,v30.h)
		v5 = valign(v12,v12,r4)
	}
	{
		v7.w = vmpyieo(v31.h,v6.h)
		v3 = valign(v3,v3,r4)
	}
	{
		v7.w += vmpyie(v31.w,v6.h)
		v31:30.w = vunpack(v5.h)
	}
	{
		v8.w += vmpyie(v2.w,v4.h)
		r3 = add(r30,#-6960)
		v31 = vmemu(r3+#0)
	}                                       // 128-byte Folded Reload
	{
		v29:28.w = vunpack(v3.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
	}
	{
		r2 = add(r30,#-6832)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w = vmpyieo(v31.h,v28.h)
		v5 = vmemu(r7+#0)
	}                                       // 256-byte Folded Reload
	{
		v6.w += vmpyie(v31.w,v28.h)
		v31 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		v5:4.w = vadd(v5:4.w,v17:16.w)
		v1:0.w = vadd(v1:0.w,v7:6.w)
	}
	{
		v24.w = vmpyieo(v31.h,v30.h)
		v5:4.w = vadd(v5:4.w,v19:18.w)
	}
	{
		v24.w += vmpyie(v31.w,v30.h)
		v30 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v25:24.w = vadd(v25:24.w,v1:0.w)
		v31 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v5:4.w = vadd(v31:30.w,v5:4.w)
	}
.LBB131_147:                            // %"consume convolved108"
                                        //   in Loop: Header=BB131_148 Depth=4
	{
		r1 = #32767
		v6 = vxor(v6,v6)
		r0 = memw(r30+##-21432)
	}                                       // 4-byte Folded Reload
	{
		v14 = vsplat(r1)
		r8 = ##-1073741825
		r1 = #-32768
	}
	{
		r9 = ##2147483647
		r26 = ##-2147483648
	}
	{
		v13 = vsplat(r1)
		v8 = vror(v6,r4)
		r0 = memw(r30+#-3896)
		v7 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r0,##-40320)
		r1 = memw(r30+##-21448)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r0,##-40448)
		memw(r30+##-7472) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r0,##-35072)
		vmem(r2+#0) = v25
	}
	{
		r28 = add(r0,##-40576)
		memw(r30+##-9008) = r3
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-34432)
		v0 = vmem(r1+#0)
	}
	{
		r12 = add(r0,##-40704)
		memw(r30+##-6448) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = setbit(r2,#7)
		memw(r30+##-7216) = r2
		vmem(r3+#0) = v24
	}                                       // 4-byte Folded Spill
	{
		r27 = #-1
		memw(r30+##-6960) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-42752)
		memw(r30+##-6192) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-42880)
		q0 = vcmp.eq(v0.b,v6.b)
		memw(r30+#-1584) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-34688)
		v1:0 = vcombine(v6,v6)
		vmem(r28+#0) = v5
	}
	{
		v19:18 = vcombine(v6,v6)
		v22 = v6
		memw(r30+#-3632) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-34816)
		memw(r30+#-1328) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-43008)
		v17:16 = vcombine(v6,v6)
		vmem(r12+#0) = v4
	}
	{
		v15 = v6
		v12 = v6
		memw(r30+#-816) = r1
	}                                       // 4-byte Folded Spill
	{
		r1 = add(r0,##-34560)
		memw(r30+#-560) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r0,##-43136)
		v11:10 = vcombine(v6,v6)
		r1 = memw(r12+#120)
	}
	{
		v9 = v6
		r2 = memw(r12+#124)
		memw(r30+#-304) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-12592) = r2
	}                                       // 4-byte Folded Spill
	{
		r2 = asr(r1,#31)
		memw(r30+##-9776) = r0
		r4 = memw(r12+#112)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-12336) = r4
		r0 = memw(r12+#116)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r3 = asr(r0,#31)
		r0 = asr(r4,#31)
		memw(r30+##-12080) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-10800) = r3
		memw(r30+##-10928) = r0
	}                                       // 4-byte Folded Spill
	{
		r6 = memw(r12+#104)
		memw(r30+##-12976) = r6.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#108)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r3 = asr(r0,#31)
		r24 = memw(r30+##-6456)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r6,#31)
		memw(r30+##-11056) = r3
		r3 = memw(r12+#96)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-13616) = r3
		memw(r30+##-11184) = r0
	}                                       // 4-byte Folded Spill
	{
		r5:4 = mpyu(r24,r1)
		r0 = memw(r12+#100)
	}
	{
		r5 += mpyi(r24,r2)
		r2 = asr(r3,#31)
		r14 = memw(r30+##-21048)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+##-13232) = r0
		memw(r30+##-11824) = r2
	}                                       // 4-byte Folded Spill
	{
		r5 += mpyi(r1,r14)
		r1 = asr(r0,#31)
		memw(r30+##-11568) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = ##1073741824
		r6 = memw(r12+#88)
	}
	{
		r3:2 = min(r5:4,r9:8)
		r1 = #0
		memw(r30+##-14384) = r6
	}                                       // 4-byte Folded Spill
	{
		r3:2 = add(r3:2,r1:0)
		r4 = memw(r12+#92)
		memw(r30+##-14256) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r4,#31)
		r1 = #0
		r5 = memw(r12+#80)
	}
	{
		r3:2 = asr(r3:2,#31)
		r0 = asr(r6,#31)
		memw(r30+##-13104) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-13360) = r0
		memw(r30+##-15152) = r5
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r5,#31)
		r4 = memw(r12+#84)
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = ##2147483647
		memw(r30+##-15024) = r4
	}                                       // 4-byte Folded Spill
	{
		r3:2 = min(r3:2,r1:0)
		r0 = asr(r4,#31)
		memw(r30+##-13744) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r3:2 = max(r3:2,r27:26)
	}
	{
		memd(r30+#-7728) = r3:2
		r2 = memw(r12+#72)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		memw(r30+##-15792) = r2
		r0 = memw(r12+#76)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-15280) = r0
		r3 = memw(r12+#64)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-14640) = r0.new
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15920) = r3
		memw(r30+##-14512) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		r1 = memw(r12+#68)
		memw(r30+##-16048) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r2 = memw(r12+#56)
		memw(r30+##-14896) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-16304) = r2
		memw(r30+##-8240) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r12+#60)
		memw(r30+##-16176) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r12+#48)
	}
	{
		memw(r30+##-16560) = r3
		memw(r30+##-15664) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-7984) = r0
		r0 = memw(r12+#52)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r0,#31)
		memw(r30+##-16432) = r0
		r25 = memw(r12+#40)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-8496) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-8752) = r0
		r0 = memw(r12+#44)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r0,#31)
		memw(r30+##-16688) = r0
		r17 = memw(r12+#32)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r25,#31)
		memw(r30+##-9264) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-9520) = r0
		r18 = memw(r12+#36)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r18,#31)
		r10 = memw(r12+#24)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r17,#31)
		memw(r30+##-10288) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r11 = memw(r12+#28)
		r13 = memw(r12+#16)
	}
	{
		r0 = asr(r11,#31)
		r2 = asr(r13,#31)
		memw(r30+##-16816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r10,#31)
		memw(r30+##-16944) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = mpyu(r24,r13)
		r15 = memw(r12+#20)
		r21 = memw(r12+#8)
	}
	{
		r1 += mpyi(r24,r2)
		r22 = asr(r15,#31)
		r7 = memw(r12+#12)
		r6 = memw(r12+#0)
	}
	{
		r1 += mpyi(r13,r14)
		r19 = asr(r6,#31)
		r13 = #0
		r12 = memw(r12+#4)
	}
	{
		r5:4 = mpyu(r24,r6)
		r20 = asr(r12,#31)
	}
	{
		r3:2 = mpyu(r24,r12)
		r5 += mpyi(r24,r19)
	}
	{
		r3 += mpyi(r24,r20)
		r5 += mpyi(r6,r14)
		r20 = ##-1073741825
	}
	{
		r3 += mpyi(r12,r14)
		r1:0 = min(r1:0,r9:8)
		r12 = ##1073741824
	}
	{
		r5:4 = min(r5:4,r9:8)
		r1:0 = add(r1:0,r13:12)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r5:4 = add(r5:4,r13:12)
		r8 = ##2147483647
	}
	{
		r1:0 = asr(r1:0,#31)
		r5:4 = asr(r5:4,#31)
		r9 = #0
	}
	{
		r1:0 = min(r1:0,r9:8)
		r5:4 = min(r5:4,r9:8)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r23 = asr(r21,#31)
	}
	{
		r1:0 = max(r5:4,r27:26)
		r5:4 = mpyu(r24,r21)
		memd(r30+##-17072) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r24,r15)
		v1.w = vinsert(r0)
	}
	{
		r5 += mpyi(r24,r23)
		r1 += mpyi(r24,r22)
		r22 = ##1073741824
	}
	{
		r5 += mpyi(r21,r14)
		r21 = ##2147483647
		r23 = #0
	}
	{
		r1 += mpyi(r15,r14)
		r3:2 = add(r3:2,r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = min(r1:0,r21:20)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = add(r1:0,r23:22)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r1:0 = asr(r1:0,#31)
	}
	{
		r13:12 = max(r3:2,r27:26)
		r5:4 = min(r5:4,r21:20)
	}
	{
		r1:0 = min(r1:0,r9:8)
		r16 = asr(r7,#31)
	}
	{
		r3:2 = mpyu(r24,r7)
		r5:4 = add(r5:4,r23:22)
	}
	{
		r3 += mpyi(r24,r16)
		r1:0 = max(r1:0,r27:26)
	}
	{
		r3 += mpyi(r7,r14)
		v1.w = vinsert(r12)
		r1 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r21:20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r9:8)
		r7:6 = mpyu(r24,r10)
		v1 = valign(v1,v1,#4)
	}
	{
		r7 += mpyi(r24,r1)
		r3:2 = add(r3:2,r23:22)
		r1 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = mpyu(r24,r11)
		v1.w = vinsert(r4)
	}
	{
		r5 += mpyi(r24,r1)
		r3:2 = min(r3:2,r9:8)
		r1 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r27:26)
		r5 += mpyi(r11,r14)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = min(r5:4,r21:20)
		v1.w = vinsert(r2)
	}
	{
		r7 += mpyi(r10,r14)
		r13:12 = mpyu(r24,r17)
	}
	{
		r13 += mpyi(r24,r1)
		r3:2 = add(r3:2,r23:22)
		r1 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r21:20)
		r3:2 = asr(r3:2,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = mpyu(r24,r18)
		r5:4 = add(r7:6,r23:22)
		r11:10 = memd(r30+##-17072)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r9:8)
		r7 += mpyi(r24,r1)
		r1 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r27:26)
		v1.w = vinsert(r10)
	}
	{
		r7 += mpyi(r18,r14)
		r5:4 = asr(r5:4,#31)
		r3 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r9:8)
		r19:18 = combine(r9,r8)
		v1 = valign(v1,v1,#4)
	}
	{
		r13 += mpyi(r17,r14)
		v1.w = vinsert(r0)
		r8 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r24,r3)
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r25)
		r5:4 = max(r5:4,r27:26)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r1)
		r17 += mpyi(r24,r0)
		memd(r30+##-10288) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r3,r14)
		v1 = valign(v1,v1,#4)
		r5 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r25,r14)
		r3 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r8)
		r1:0 = mpyu(r24,r5)
		memd(r30+##-9520) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r3)
		memd(r30+##-9264) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r8,r14)
		v1.w = vinsert(r4)
		r3 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r24,r3)
		r3 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r5,r14)
		v20 = valign(v1,v1,#4)
		memd(r30+##-8752) = r7:6
	}                                       // 8-byte Folded Spill
	{
		v20.w = vinsert(r2)
		r12 = memw(r30+##-16176)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-7984)
		memd(r30+##-8496) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r3)
		r5:4 = mpyu(r24,r12)
		r8 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r24,r0)
		v2 = valign(v20,v20,#4)
	}
	{
		r7 += mpyi(r3,r14)
		r1:0 = mpyu(r24,r8)
		r3 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		memd(r30+#-7984) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r24,r3)
		r3 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r12,r14)
	}
	{
		r1 += mpyi(r24,r3)
		memd(r30+##-8240) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r8,r14)
		r8 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r21:20)
		r12 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r24,r8)
		r1:0 = add(r1:0,r23:22)
		r6 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r24,r12)
		r1:0 = asr(r1:0,#31)
	}
	{
		r3 += mpyi(r24,r6)
		r1:0 = min(r1:0,r19:18)
	}
	{
		r3 += mpyi(r8,r14)
		r1:0 = max(r1:0,r27:26)
		r8 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r21:20)
		r15 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r24,r8)
		r3:2 = add(r3:2,r23:22)
		r8 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r12,r14)
		r3:2 = asr(r3:2,#31)
		r1 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r21:20)
		r3:2 = min(r3:2,r19:18)
		r13:12 = combine(r21,r20)
	}
	{
		r3:2 = max(r3:2,r27:26)
		r5:4 = add(r5:4,r23:22)
	}
	{
		r5:4 = asr(r5:4,#31)
		v0.w = vinsert(r2)
	}
	{
		r5:4 = min(r5:4,r19:18)
		r7:6 = mpyu(r24,r8)
	}
	{
		r3:2 = max(r5:4,r27:26)
		r5:4 = mpyu(r24,r15)
		v0 = valign(v0,v0,#4)
	}
	{
		r5 += mpyi(r24,r1)
		v0.w = vinsert(r0)
		r3 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r15,r14)
		r1 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r24,r3)
		r5:4 = min(r5:4,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r24,r1)
		v0.w = vinsert(r2)
		r1 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r8,r14)
	}
	{
		r11 += mpyi(r24,r1)
		r1:0 = min(r7:6,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r11 += mpyi(r3,r14)
		r1:0 = add(r1:0,r23:22)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r5:4,r23:22)
	}
	{
		r1:0 = min(r1:0,r19:18)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r3:2 = min(r3:2,r19:18)
	}
	{
		r7:6 = min(r11:10,r21:20)
		r3:2 = max(r3:2,r27:26)
		r1 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = add(r7:6,r23:22)
		v0.w = vinsert(r2)
		r8 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r1)
		r5:4 = asr(r7:6,#31)
		r3 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r19:18)
		r17:16 = mpyu(r24,r8)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r24,r3)
		r5:4 = max(r5:4,r27:26)
	}
	{
		r7 += mpyi(r1,r14)
		v0.w = vinsert(r0)
		r1 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r7:6,r21:20)
		r5 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r24,r1)
		r3:2 = add(r3:2,r23:22)
		r1 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r5)
		r17 += mpyi(r8,r14)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r24,r1)
		r3:2 = asr(r3:2,#31)
		r8 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r5,r14)
		r3:2 = min(r3:2,r19:18)
		r5 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r8)
		v0.w = vinsert(r4)
		memd(r30+##-11824) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r5)
		r3:2 = max(r3:2,r27:26)
	}
	{
		r7 += mpyi(r8,r14)
		v0 = valign(v0,v0,#4)
		r8 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r17:16,r21:20)
		v0.w = vinsert(r2)
		r15 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r1:0,r23:22)
		memd(r30+##-11568) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r24,r8)
		r1:0 = asr(r1:0,#31)
		r3 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r15)
		r1:0 = min(r1:0,r19:18)
		r2 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r24,r3)
		r1:0 = max(r1:0,r27:26)
		v21 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r24,r2)
		r5 += mpyi(r8,r14)
		r8 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r15,r14)
		v21.w = vinsert(r0)
		memd(r30+##-11184) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7 = memw(r30+##-12080)
		memd(r30+##-11056) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r24,r8)
		v3 = valign(v21,v21,#4)
		r1 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r24,r7)
		r0 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r24,r1)
		r6 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r8,r14)
		r3 += mpyi(r24,r0)
	}
	{
		r3 += mpyi(r7,r14)
		r1:0 = mpyu(r24,r6)
		memd(r30+##-10928) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5 = memw(r28+#120)
		memw(r30+##-13744) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r4 = memw(r28+#124)
		memd(r30+##-10800) = r3:2

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r2 = memw(r30+##-9776)
		memw(r30+##-12336) = r4
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r24,r2)
		r2 = asr(r4,#31)
		memw(r30+##-12080) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r1 += mpyi(r6,r14)
	}
	{
		r0 = asr(r5,#31)
		memd(r30+##-9776) = r1:0
	}                                       // 8-byte Folded Spill
	{
		memw(r30+##-13360) = r0
		r2 = memw(r28+#112)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-14640) = r2
		r0 = memw(r28+#116)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-14256) = r0
		r3 = memw(r28+#104)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-15664) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-14128) = r1
		memw(r30+##-14512) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r28+#108)
		memw(r30+##-15024) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r2 = memw(r28+#96)
	}
	{
		memw(r30+##-16304) = r2
		memw(r30+##-14896) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15152) = r0
		r0 = memw(r28+#100)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-15920) = r0
		r3 = memw(r28+#88)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-16816) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15792) = r1
		memw(r30+##-16048) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r28+#92)
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r2 = memw(r28+#80)
	}
	{
		memw(r30+##-17328) = r2
		memw(r30+##-16432) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-16688) = r0
		r0 = memw(r28+#84)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-17200) = r0
		r3 = memw(r28+#72)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-17712) = r3
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-16944) = r1
		memw(r30+##-17072) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r28+#76)
		memw(r30+##-17584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r2 = memw(r28+#64)
	}
	{
		memw(r30+##-16176) = r0
		memw(r30+##-17968) = r2
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r2,#31)
		memw(r30+##-17456) = r1
		r1 = memw(r28+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-18224) = r1
		r3 = memw(r28+#56)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+##-17840) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-18480) = r3
		memw(r30+##-12592) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r28+#60)
		memw(r30+##-18232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r23 = memw(r28+#48)
	}
	{
		memw(r30+##-18096) = r1
		memw(r30+##-12976) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r28+#52)
		memw(r30+##-18608) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r23,#31)
		r17 = memw(r28+#40)
	}
	{
		memw(r30+##-12848) = r1
		memw(r30+##-13104) = r0
	}                                       // 4-byte Folded Spill
	{
		r22 = memw(r28+#44)
		r10 = memw(r28+#32)
	}
	{
		r0 = asr(r22,#31)
		memw(r30+##-13232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r17,#31)
		memw(r30+##-13616) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r11 = memw(r28+#36)
		r15 = memw(r28+#24)
	}
	{
		r0 = asr(r11,#31)
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r10,#31)
		memw(r30+##-15280) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25 = memw(r28+#28)
		r8 = memw(r28+#16)
	}
	{
		r0 = asr(r25,#31)
		r2 = asr(r8,#31)
		memw(r30+##-18736) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r15,#31)
		memw(r30+##-18864) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r20 = memw(r28+#20)
		r18 = memw(r28+#8)
	}
	{
		r0 = asr(r20,#31)
		r21 = asr(r18,#31)
		memw(r30+##-19000) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = mpyu(r24,r8)
		r7 = memw(r28+#12)
		r6 = memw(r28+#0)
	}
	{
		r1 += mpyi(r24,r2)
		r19 = asr(r6,#31)
		r9 = memw(r28+#4)
	}
	{
		r5:4 = mpyu(r24,r6)
		r16 = asr(r9,#31)
	}
	{
		r3:2 = mpyu(r24,r9)
		r1 += mpyi(r8,r14)
		r8 = ##1073741824
	}
	{
		r3 += mpyi(r24,r16)
		r5 += mpyi(r24,r19)
	}
	{
		r3 += mpyi(r9,r14)
		r5 += mpyi(r6,r14)
		r9 = #0
	}
	{
		r1:0 = min(r1:0,r13:12)
		r3:2 = min(r3:2,r13:12)
		r6 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r1:0 = add(r1:0,r9:8)
	}
	{
		r3:2 = add(r3:2,r9:8)
		r5:4 = add(r5:4,r9:8)
		r8 = ##2147483647
	}
	{
		r1:0 = asr(r1:0,#31)
		r5:4 = asr(r5:4,#31)
		r9 = #0
	}
	{
		r1:0 = min(r1:0,r9:8)
		r5:4 = min(r5:4,r9:8)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = max(r5:4,r27:26)
		r3:2 = min(r3:2,r9:8)
		memd(r30+##-18992) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r24,r20)
		v22.w = vinsert(r0)
	}
	{
		r1 += mpyi(r24,r6)
		r5:4 = mpyu(r24,r18)
		r6 = ##1073741824
	}
	{
		r9:8 = max(r3:2,r27:26)
		r28 = asr(r7,#31)
		v0 = valign(v22,v22,#4)
	}
	{
		r3:2 = mpyu(r24,r7)
		r1 += mpyi(r20,r14)
	}
	{
		r5 += mpyi(r24,r21)
		r3 += mpyi(r24,r28)
		r21:20 = combine(r13,r12)
	}
	{
		r5 += mpyi(r18,r14)
		r3 += mpyi(r7,r14)
		r7 = #0
	}
	{
		r1:0 = min(r1:0,r13:12)
		r5:4 = min(r5:4,r13:12)
	}
	{
		r3:2 = min(r3:2,r13:12)
		r1:0 = add(r1:0,r7:6)
		r13:12 = combine(r7,r6)
	}
	{
		r5:4 = add(r5:4,r7:6)
		r3:2 = add(r3:2,r7:6)
		r6 = ##2147483647
	}
	{
		r1:0 = asr(r1:0,#31)
		v0.w = vinsert(r8)
		r7 = #0
	}
	{
		r1:0 = min(r1:0,r7:6)
		r3:2 = asr(r3:2,#31)
		r9:8 = combine(r7,r6)
	}
	{
		r1:0 = max(r1:0,r27:26)
		r5:4 = asr(r5:4,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = min(r3:2,r7:6)
		r5:4 = min(r5:4,r7:6)
		r1 = memw(r30+##-18864)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r15)
		r5:4 = max(r5:4,r27:26)
	}
	{
		r7 += mpyi(r24,r1)
		v0.w = vinsert(r4)
		r1 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r24,r25)
		r3:2 = max(r3:2,r27:26)
	}
	{
		r5 += mpyi(r24,r1)
		r19:18 = mpyu(r24,r10)
		r1 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r15,r14)
		v0 = valign(v0,v0,#4)
	}
	{
		r19 += mpyi(r24,r1)
		v0.w = vinsert(r2)
	}
	{
		r5 += mpyi(r25,r14)
		r19 += mpyi(r10,r14)
		r1 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r21:20)
		r3:2 = min(r5:4,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = add(r7:6,r13:12)
		memd(r30+##-15280) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r24,r11)
		r3:2 = add(r3:2,r13:12)
		r19:18 = memd(r30+##-18992)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r24,r1)
		v0.w = vinsert(r18)
		r1 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r11,r14)
		r3:2 = asr(r3:2,#31)
	}
	{
		r7:6 = mpyu(r24,r17)
		v0 = valign(v0,v0,#4)
		memd(r30+##-14384) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3:2 = min(r3:2,r9:8)
		r7 += mpyi(r24,r1)
	}
	{
		r1:0 = max(r3:2,r27:26)
		v0.w = vinsert(r0)
	}
	{
		r5:4 = asr(r5:4,#31)
		r1 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r9:8)
		r21:20 = mpyu(r24,r22)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = max(r5:4,r27:26)
		r7 += mpyi(r17,r14)
		r26 = ##2147483647
	}
	{
		r21 += mpyi(r24,r1)
		r27 = #0
		r1 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r23)
		memd(r30+##-13616) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r22,r14)
		v0.w = vinsert(r4)
		r5 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r24,r1)
		memd(r30+##-13232) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r5)
		v0 = valign(v0,v0,#4)
		r5 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r23,r14)
		v0.w = vinsert(r0)
	}
	{
		r3 += mpyi(r24,r5)
		memd(r30+##-13104) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r1,r14)
		r7 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r24,r7)
		r6 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r24,r1)
		memd(r30+##-12848) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r24,r6)
		r5 += mpyi(r7,r14)
		r1 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r7 = ##2147483647
		memd(r30+##-12976) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r13:12 = mpyu(r24,r1)
		r4 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r24,r4)
		r4 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r6,r14)
		r5 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r24,r4)
		r6 = ##-1073741825
	}
	{
		r2 = memw(r30+##-17840)
		memd(r30+##-12592) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r1,r14)
		r1:0 = mpyu(r24,r5)
		r4 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r24,r2)
	}
	{
		r1 += mpyi(r5,r14)
		r3:2 = mpyu(r24,r4)
		r5 = memw(r30+##-16176)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r7:6)
		r7:6 = min(r13:12,r7:6)
		r12 = ##-2147483648
	}
	{
		r3 += mpyi(r24,r5)
		r13 = #-1
		r5 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r4,r14)
	}
	{
		r2 = memw(r30+##-17456)
		memd(r30+##-16176) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r24,r5)
		r3 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r24,r2)
		r4 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = mpyu(r24,r3)
		r21 += mpyi(r5,r14)
		r2 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r24,r4)
		r5 = #0
	}
	{
		r19 += mpyi(r24,r2)
		r2 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r3,r14)
	}
	{
		r17 += mpyi(r24,r2)
		r2 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r4,r14)
		r3 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r23:22 = mpyu(r24,r2)
		r9 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r23 += mpyi(r24,r3)
		r8 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r24,r9)
		r23 += mpyi(r2,r14)
		r4 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r24,r8)
		r15 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r24,r4)
		r4 = ##1073741824
	}
	{
		r3 += mpyi(r24,r15)
		r1:0 = add(r1:0,r5:4)
		r15 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r8,r14)
		r11 += mpyi(r9,r14)
	}
	{
		r3:2 = asr(r1:0,#31)
		memd(r30+##-16048) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r9:8 = min(r3:2,r27:26)
		r0 = ##1073741824
		r1 = #0
	}
	{
		r9:8 = max(r9:8,r13:12)
		r1:0 = add(r7:6,r1:0)
		r2 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		v19.w = vinsert(r8)
		r9 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r24,r15)
		r1:0 = min(r1:0,r27:26)
		r8 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r24,r2)
		r7:6 = mpyu(r24,r9)
		r3:2 = combine(r13,r12)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r12 = ##-1073741825
		v1 = valign(v19,v19,#4)
	}
	{
		r7 += mpyi(r24,r8)
		v1.w = vinsert(r0)
		r13 = ##2147483647
	}
	{
		r7 += mpyi(r9,r14)
		r1:0 = min(r21:20,r13:12)
		r20 = ##1073741824
	}
	{
		r21 = #0
		memd(r30+##-17328) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r15,r14)
		r1:0 = add(r1:0,r21:20)
		r7:6 = memd(r30+##-16176)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r13:12)
		r1:0 = asr(r1:0,#31)
		r15 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = add(r7:6,r21:20)
		v1 = valign(v1,v1,#4)
		r28 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r27:26)
		r9:8 = asr(r9:8,#31)
		memd(r30+##-17840) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r24,r15)
		r9:8 = min(r9:8,r27:26)
	}
	{
		r7 += mpyi(r24,r28)
		r9:8 = max(r9:8,r3:2)
	}
	{
		r1:0 = max(r1:0,r3:2)
		v1.w = vinsert(r8)
		r8 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r19:18,r13:12)
		r7 += mpyi(r15,r14)
		r1 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = add(r5:4,r21:20)
		v1 = valign(v1,v1,#4)
		memd(r30+##-17200) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r24,r8)
	}
	{
		r7 += mpyi(r24,r1)
		v1.w = vinsert(r0)
	}
	{
		r5:4 = min(r17:16,r13:12)
		r1:0 = asr(r5:4,#31)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r7 += mpyi(r8,r14)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r1:0,r3:2)
		memd(r30+##-16560) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = add(r5:4,r21:20)
		v1.w = vinsert(r0)
		r6 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r5:4,#31)
		r4 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r24,r6)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r17 += mpyi(r24,r4)
		r5:4 = min(r23:22,r13:12)
		r23:22 = combine(r13,r12)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r1:0,r3:2)
		r17 += mpyi(r6,r14)
	}
	{
		r1:0 = add(r5:4,r21:20)
		v1.w = vinsert(r0)
		r4 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		memd(r30+##-16176) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r11:10,r13:12)
		r17:16 = combine(r21,r20)
		r5 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = mpyu(r24,r4)
		r1:0 = min(r1:0,r27:26)
		r11:10 = combine(r27,r26)
		v1 = valign(v1,v1,#4)
	}
	{
		r19 += mpyi(r24,r5)
		r1:0 = max(r1:0,r3:2)
	}
	{
		r19 += mpyi(r4,r14)
		v1.w = vinsert(r0)
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = add(r7:6,r21:20)
		memd(r30+##-15792) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r27:26 = asr(r7:6,#31)
		r1 = memw(r0+#120)
		memw(r30+##-13744) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r1 = memw(r0+#124)
		memw(r30+##-13360) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r27:26 = min(r27:26,r11:10)
		r1 = memw(r0+#112)
		memw(r30+##-14256) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r27:26 = max(r27:26,r3:2)
		r1 = memw(r0+#116)
		memw(r30+##-14128) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r26)
		r1 = memw(r0+#104)
		memw(r30+##-14640) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#108)
		memw(r30+##-14512) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r1 = memw(r0+#96)
		memw(r30+##-15024) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#100)
		memw(r30+##-14896) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#88)
		memw(r30+##-15920) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#92)
		memw(r30+##-15152) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#80)
		memw(r30+##-16432) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#84)
		memw(r30+##-16304) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#72)
		memw(r30+##-16944) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#76)
		memw(r30+##-16816) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#64)
		memw(r30+##-17072) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#68)
		memw(r30+##-17456) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#56)
		memw(r30+##-17712) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#60)
		memw(r30+##-17584) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#48)
		memw(r30+##-18096) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = memw(r0+#52)
		memw(r30+##-18224) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r20 = memw(r0+#40)
		r1 = memw(r0+#44)
	}
	{
		memw(r30+##-17968) = r1
		r28 = memw(r0+#32)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r21 = memw(r0+#36)
		r6 = memw(r0+#24)
	}
	{
		r12 = memw(r0+#28)
		r7 = memw(r0+#16)
	}
	{
		r5 = memw(r0+#20)
		r15 = memw(r0+#8)
	}
	{
		r1 = memw(r30+##-12336)
		r4 = memw(r0+#12)
	}                                       // 4-byte Folded Reload
	{
		r13 = memw(r0+#0)
		r9 = memw(r0+#4)
	}
	{
		r19:18 = mpyu(r24,r1)
		r25 = asr(r9,#31)
		r0 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r24,r9)
	}
	{
		r19 += mpyi(r24,r0)
		r3 += mpyi(r24,r25)
	}
	{
		r19 += mpyi(r1,r14)
		r3 += mpyi(r9,r14)
	}
	{
		r19:18 = combine(r23,r22)
		memd(r30+##-16688) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-11824)
		memd(r30+##-15664) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r3:2,r23:22)
		r3:2 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r23:22)
		r27:26 = add(r9:8,r17:16)
		r3:2 = memd(r30+##-15280)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r23:22)
		r1:0 = add(r1:0,r17:16)
		r9:8 = combine(r11,r10)
	}
	{
		r3:2 = add(r3:2,r17:16)
		r27:26 = asr(r27:26,#31)
	}
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r27:26 = min(r27:26,r11:10)
		r10 = ##-2147483648
	}
	{
		r3:2 = min(r3:2,r9:8)
		r11 = #-1
	}
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = max(r3:2,r11:10)
	}
	{
		r27:26 = max(r27:26,r11:10)
		v0.w = vinsert(r2)
		r3:2 = memd(r30+##-16048)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = mpyu(r24,r15)
		r1 = asr(r15,#31)
	}
	{
		r3:2 = min(r3:2,r19:18)
		v2.w = vinsert(r0)
	}
	{
		r23 += mpyi(r24,r1)
		v3.w = vinsert(r26)
		v0 = valign(v0,v0,#4)
	}
	{
		r27:26 = mpyu(r24,r13)
		r0 = asr(r13,#31)
	}
	{
		r27 += mpyi(r24,r0)
		r3:2 = add(r3:2,r17:16)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = asr(r3:2,#31)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r15,r14)
		r1:0 = min(r1:0,r9:8)
	}
	{
		r23:22 = memd(r30+##-10032)
		memd(r30+##-10288) = r23:22
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = min(r3:2,r19:18)
	}
	{
		r23:22 = min(r23:22,r19:18)
		v1.w = vinsert(r0)
	}
	{
		r27 += mpyi(r13,r14)
		r1:0 = add(r23:22,r17:16)
		r23:22 = memd(r30+##-14384)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r3:2,r17:16)
		r13 = r14
		r15:14 = combine(r9,r8)
	}
	{
		r23:22 = min(r23:22,r19:18)
		r1:0 = min(r1:0,r15:14)
		r9:8 = combine(r15,r14)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = asr(r3:2,#31)
		r25 = r13
	}
	{
		r23:22 = add(r23:22,r17:16)
		v2.w = vinsert(r0)
	}
	{
		r3:2 = min(r3:2,r15:14)
		r1:0 = asr(r23:22,#31)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r3:2 = max(r3:2,r11:10)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r1:0,r11:10)
		v3.w = vinsert(r2)
	}
	{
		r23:22 = mpyu(r24,r4)
		r2 = asr(r4,#31)
	}
	{
		r15:14 = mpyu(r24,r7)
		r3 = asr(r7,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r23 += mpyi(r24,r2)
		v0.w = vinsert(r0)
	}
	{
		r15 += mpyi(r24,r3)
		r0 = asr(r5,#31)
	}
	{
		r3:2 = mpyu(r24,r5)
		r23 += mpyi(r4,r13)
		v0 = valign(v0,v0,#4)
	}
	{
		r3 += mpyi(r24,r0)
		r15 += mpyi(r7,r13)
		memd(r30+##-12080) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r5,r13)
		memd(r30+##-11824) = r15:14
	}                                       // 8-byte Folded Spill
	{
		r3:2 = memd(r30+##-11184)
		memd(r30+##-11568) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r19:18)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r19:18)
		r1:0 = add(r1:0,r17:16)
		r5:4 = memd(r30+##-17840)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = add(r3:2,r17:16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = min(r5:4,r19:18)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = min(r1:0,r9:8)
		r23:22 = add(r5:4,r17:16)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r23:22 = asr(r23:22,#31)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r23:22 = min(r23:22,r9:8)
	}
	{
		r3:2 = max(r3:2,r11:10)
		r1 = asr(r12,#31)
	}
	{
		r5:4 = mpyu(r24,r12)
		v2.w = vinsert(r2)
		r3:2 = memd(r30+##-13616)
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r24,r1)
		r23:22 = max(r23:22,r11:10)
	}
	{
		v1.w = vinsert(r22)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r3:2 = min(r3:2,r19:18)
		r5 += mpyi(r12,r13)
	}
	{
		r23:22 = mpyu(r24,r6)
		r0 = asr(r6,#31)
		memd(r30+##-11184) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r23 += mpyi(r24,r0)
		r3:2 = add(r3:2,r17:16)
		r5:4 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r6,r13)
		r1:0 = asr(r3:2,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = min(r5:4,r19:18)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r30+##-17328)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r9:8)
		r3:2 = add(r3:2,r17:16)
		memd(r30+##-12336) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r7:6,r19:18)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r5:4 = add(r7:6,r17:16)
		r7:6 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r5:4,#31)
		v0.w = vinsert(r0)
	}
	{
		r23:22 = min(r7:6,r19:18)
		r3:2 = min(r3:2,r9:8)
	}
	{
		r1:0 = min(r1:0,r9:8)
		r3:2 = max(r3:2,r11:10)
		v0 = valign(v0,v0,#4)
	}
	{
		r15:14 = add(r23:22,r17:16)
		v2.w = vinsert(r2)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r5:4 = asr(r15:14,#31)
	}
	{
		r7:6 = mpyu(r24,r28)
		r2 = asr(r28,#31)
	}
	{
		r1:0 = min(r5:4,r9:8)
		v1.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r7 += mpyi(r24,r2)
		r3 = asr(r21,#31)
	}
	{
		r5:4 = mpyu(r24,r21)
		v1 = valign(v1,v1,#4)
	}
	{
		r5 += mpyi(r24,r3)
		r1:0 = max(r1:0,r11:10)
	}
	{
		r7 += mpyi(r28,r13)
		v3.w = vinsert(r0)
	}
	{
		r5 += mpyi(r21,r13)
		r0 = asr(r20,#31)
		memd(r30+##-11056) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r24,r20)
		r7:6 = memd(r30+##-10928)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = memd(r30+##-13232)
		memd(r30+##-9520) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r3 += mpyi(r24,r0)
		r1:0 = min(r7:6,r19:18)
		v3 = valign(v3,v3,#4)
	}
	{
		r3 += mpyi(r20,r13)
		r1:0 = add(r1:0,r17:16)
		r7:6 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r19:18)
		r5:4 = min(r7:6,r19:18)
		memd(r30+##-9008) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = add(r3:2,r17:16)
		r7:6 = memd(r30+##-17200)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r19:18)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r7:6 = add(r7:6,r17:16)
	}
	{
		r3:2 = max(r3:2,r11:10)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r9:8)
		v0.w = vinsert(r2)
	}
	{
		r3:2 = max(r7:6,r11:10)
		r5:4 = add(r5:4,r17:16)
	}
	{
		r1:0 = asr(r1:0,#31)
		r5:4 = asr(r5:4,#31)
		r3 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r1:0,r9:8)
		r5:4 = min(r5:4,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r12 = r3
		r14 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r11:10)
		v1.w = vinsert(r2)
		r1 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r3)
		r2 = asr(r3,#31)
	}
	{
		r7 += mpyi(r24,r2)
		v2.w = vinsert(r4)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = mpyu(r24,r14)
		r3 = asr(r14,#31)
	}
	{
		r5 += mpyi(r24,r3)
		v3.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		r7 += mpyi(r12,r13)
		r0 = asr(r1,#31)
	}
	{
		r3:2 = mpyu(r24,r1)
		r5 += mpyi(r14,r13)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r24,r0)
		r7:6 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = memd(r30+##-8496)
		memd(r30+##-9264) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r3 += mpyi(r1,r13)
		r1:0 = min(r7:6,r19:18)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = memd(r30+##-16560)
		memd(r30+##-8752) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r19:18)
		r5:4 = min(r7:6,r19:18)
		r7:6 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r19:18)
		r3:2 = add(r3:2,r17:16)
	}
	{
		r3:2 = asr(r3:2,#31)
		r5:4 = add(r5:4,r17:16)
	}
	{
		r3:2 = min(r3:2,r9:8)
		r1:0 = add(r1:0,r17:16)
	}
	{
		r3:2 = max(r3:2,r11:10)
		r7:6 = add(r7:6,r17:16)
	}
	{
		r5:4 = asr(r5:4,#31)
		r7:6 = asr(r7:6,#31)
		r3 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r1:0 = asr(r1:0,#31)
		r12 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r9:8)
		r1:0 = min(r1:0,r9:8)
		r14 = r3
	}
	{
		r1:0 = max(r1:0,r11:10)
		r7:6 = max(r7:6,r11:10)
	}
	{
		r5:4 = max(r5:4,r11:10)
		v0.w = vinsert(r6)
		r1 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		v2.w = vinsert(r2)
		v1.w = vinsert(r4)
	}
	{
		r5:4 = mpyu(r24,r3)
		r2 = asr(r3,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = mpyu(r24,r12)
		r3 = asr(r12,#31)
		v19 = valign(v1,v1,#4)
	}
	{
		r7 += mpyi(r24,r3)
		r5 += mpyi(r24,r2)
		v2 = valign(v2,v2,#4)
	}
	{
		r7 += mpyi(r12,r13)
		v3.w = vinsert(r0)
	}
	{
		r3:2 = mpyu(r24,r1)
		r0 = asr(r1,#31)
		memd(r30+##-10800) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r24,r0)
		r5 += mpyi(r14,r13)
		r7:6 = memd(r30+##-16176)
	}                                       // 8-byte Folded Reload
	{
		r3 += mpyi(r1,r13)
		r1:0 = min(r7:6,r19:18)
		v1 = valign(v3,v3,#4)
	}
	{
		r5:4 = memd(r30+##-12848)
		memd(r30+##-8496) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r19:18)
		r1:0 = add(r1:0,r17:16)
		memd(r30+##-10928) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r1:0 = asr(r1:0,#31)
		r7:6 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r19:18)
		r1:0 = min(r1:0,r9:8)
		r7:6 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r3:2 = add(r3:2,r17:16)
	}
	{
		r3:2 = asr(r3:2,#31)
		r1 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r9:8)
		r5:4 = add(r5:4,r17:16)
		r12 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r11:10)
		r5:4 = asr(r5:4,#31)
		r15 = r1
	}
	{
		v0.w = vinsert(r2)
		v1.w = vinsert(r6)
		r14 = r12
	}
	{
		r0 = asr(r1,#31)
		v19.w = vinsert(r0)
	}
	{
		r3:2 = mpyu(r24,r1)
		r1 = asr(r12,#31)
		v31 = valign(v0,v0,#4)
	}
	{
		r7:6 = mpyu(r24,r12)
		r5:4 = min(r5:4,r9:8)
		v0 = valign(v19,v19,#4)
	}
	{
		r7 += mpyi(r24,r1)
		r5:4 = max(r5:4,r11:10)
		r1 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r14,r13)
		v31.w = vinsert(r4)
		r12 = r1
		v28 = valign(v1,v1,#4)
	}
	{
		r5:4 = mpyu(r24,r1)
		r3 += mpyi(r24,r0)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r0 = asr(r1,#31)
		r7:6 = combine(r5,r4)
		r5:4 = memd(r30+##-15792)
	}                                       // 8-byte Folded Reload
	{
		r3 += mpyi(r15,r13)
		r7 += mpyi(r24,r0)
	}
	{
		r1:0 = min(r5:4,r19:18)
		r3:2 = memd(r30+#-7984)
		memd(r30+##-12848) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r3:2,r19:18)
		r1:0 = add(r1:0,r17:16)
		r3:2 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r12,r13)
		r1:0 = asr(r1:0,#31)
		v20 = valign(v31,v31,#4)
	}
	{
		r1:0 = min(r1:0,r9:8)
		r5:4 = add(r5:4,r17:16)
		memd(r30+#-7984) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = min(r3:2,r19:18)
		r1:0 = max(r1:0,r11:10)
	}
	{
		r5:4 = asr(r5:4,#31)
		r7:6 = add(r7:6,r17:16)
		r1 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r9:8)
		r7:6 = asr(r7:6,#31)
		r12 = r1
	}
	{
		r3:2 = max(r5:4,r11:10)
		r7:6 = min(r7:6,r9:8)
		r4 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r7:6,r11:10)
		v2.w = vinsert(r2)
		r7:6 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r0 = asr(r1,#31)
		v0.w = vinsert(r0)
		r22 = r4
	}
	{
		r15:14 = mpyu(r24,r1)
		r1 = asr(r4,#31)
		r28 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r24,r4)
		r15 += mpyi(r24,r0)
		v19 = valign(v2,v2,#4)
	}
	{
		r5:4 = min(r7:6,r19:18)
		r21 += mpyi(r24,r1)
		v29 = valign(v0,v0,#4)
	}
	{
		r15 += mpyi(r12,r13)
		r13:12 = add(r5:4,r17:16)
		r5:4 = memd(r30+##-12592)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = mpyu(r24,r28)
		r1 = asr(r28,#31)
	}
	{
		r7 += mpyi(r24,r1)
		r21 += mpyi(r22,r25)
	}
	{
		r1:0 = min(r5:4,r19:18)
		r5:4 = memd(r30+##-16688)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r5:4,r19:18)
		r21:20 = combine(r17,r16)
		memd(r30+##-8240) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r1:0,r21:20)
		r17:16 = add(r17:16,r21:20)
		r5:4 = combine(r19,r18)
	}
	{
		r1:0 = asr(r1:0,#31)
		r17:16 = asr(r17:16,#31)
	}
	{
		r13:12 = asr(r13:12,#31)
		v19.w = vinsert(r2)
	}
	{
		r17:16 = min(r17:16,r9:8)
		r3:2 = min(r1:0,r9:8)
	}
	{
		r1:0 = max(r17:16,r11:10)
		r17:16 = max(r3:2,r11:10)
	}
	{
		r13:12 = min(r13:12,r9:8)
		r19:18 = min(r27:26,r19:18)
		r3:2 = memd(r30+##-15664)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = max(r13:12,r11:10)
		r13:12 = min(r3:2,r5:4)
		r27:26 = combine(r5,r4)
	}
	{
		r3:2 = add(r19:18,r21:20)
	}
	{
		r19:18 = asr(r3:2,#31)
		v29.w = vinsert(r0)
		r0 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r19:18,r9:8)
		r13:12 = add(r13:12,r21:20)
	}
	{
		r3:2 = max(r5:4,r11:10)
		r7 += mpyi(r28,r25)
	}
	{
		v28.w = vinsert(r22)
		v18.w = vinsert(r2)
		r3:2 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = asr(r13:12,#31)
		r28 = asr(r0,#31)
	}
	{
		r23:22 = mpyu(r24,r0)
		r13:12 = min(r13:12,r9:8)
		v0 = valign(v18,v18,#4)
	}
	{
		r23 += mpyi(r24,r28)
		r5:4 = min(r3:2,r27:26)
		v3 = valign(v28,v28,#4)
	}
	{
		r13:12 = max(r13:12,r11:10)
		v20.w = vinsert(r16)
	}
	{
		r23 += mpyi(r0,r25)
		r17:16 = add(r5:4,r21:20)
		r5:4 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = asr(r17:16,#31)
		v0.w = vinsert(r12)
		memd(r30+##-9776) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r13:12 = min(r23:22,r9:8)
		r3:2 = min(r5:4,r27:26)
		r0 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = max(r13:12,r11:10)
		r17:16 = add(r3:2,r21:20)
		r1 = r0
		v0 = valign(v0,v0,#4)
	}
	{
		r17:16 = asr(r17:16,#31)
		v0.w = vinsert(r12)
	}
	{
		r23:22 = mpyu(r24,r0)
		r28 = asr(r0,#31)
		r0 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r17:16,r9:8)
		v0 = valign(v0,v0,#4)
		r17:16 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r23 += mpyi(r24,r28)
		r13:12 = max(r5:4,r11:10)
	}
	{
		r19:18 = min(r17:16,r27:26)
		v0.w = vinsert(r12)
		r5:4 = combine(r11,r10)
		r11:10 = combine(r9,r8)
	}
	{
		r13:12 = add(r19:18,r21:20)
		r28 = asr(r0,#31)
		r19:18 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = mpyu(r24,r0)
	}
	{
		r3 += mpyi(r24,r28)
		r13:12 = asr(r13:12,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r3 += mpyi(r0,r25)
		r13:12 = min(r13:12,r9:8)
		r0 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = min(r19:18,r27:26)
		r13:12 = max(r13:12,r5:4)
		memd(r30+##-11568) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = add(r19:18,r21:20)
		v0.w = vinsert(r12)
	}
	{
		r19:18 = asr(r3:2,#31)
		r13 = asr(r0,#31)
		r3:2 = memd(r30+##-12336)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = mpyu(r24,r0)
		r3:2 = min(r3:2,r27:26)
		v0 = valign(v0,v0,#4)
	}
	{
		r17 += mpyi(r24,r13)
		r13:12 = min(r19:18,r9:8)
		r9 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = max(r13:12,r5:4)
		r19:18 = add(r3:2,r21:20)
		r8 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = asr(r19:18,#31)
		v0.w = vinsert(r12)
	}
	{
		r23 += mpyi(r1,r25)
		r17 += mpyi(r0,r25)
	}
	{
		r1:0 = mpyu(r24,r9)
		r28 = asr(r9,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r13:12 = mpyu(r24,r8)
		r2 = asr(r8,#31)
	}
	{
		r19:18 = min(r19:18,r11:10)
		r13 += mpyi(r24,r2)
	}
	{
		r1 += mpyi(r24,r28)
		r3:2 = max(r19:18,r5:4)
	}
	{
		r1 += mpyi(r9,r25)
		v0.w = vinsert(r2)
		r3:2 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r27:26)
		memd(r30+##-10288) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r8,r25)
		r1:0 = add(r1:0,r21:20)
		r3:2 = memd(r30+##-10928)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		v0 = valign(v0,v0,#4)
		r19:18 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r19:18,r27:26)
		r1:0 = asr(r1:0,#31)
		r19:18 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r11:10)
		r3:2 = add(r3:2,r21:20)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r9:8 = add(r9:8,r21:20)
	}
	{
		r3:2 = asr(r3:2,#31)
		v17.w = vinsert(r0)
	}
	{
		r3:2 = min(r3:2,r11:10)
		r9:8 = asr(r9:8,#31)
	}
	{
		r1:0 = min(r9:8,r11:10)
		r9:8 = min(r19:18,r27:26)
		v1 = valign(v17,v17,#4)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r3:2 = max(r3:2,r5:4)
	}
	{
		v0.w = vinsert(r0)
		v1.w = vinsert(r2)
		r3:2 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r1:0 = add(r9:8,r21:20)
		r19:18 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r3:2 = add(r3:2,r21:20)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r9:8 = min(r19:18,r27:26)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r3:2 = asr(r3:2,#31)
		r18 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r3:2,r11:10)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r9:8 = add(r9:8,r21:20)
	}
	{
		r9:8 = asr(r9:8,#31)
		v0.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = min(r15:14,r27:26)
		r9:8 = min(r9:8,r11:10)
		r15:14 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = max(r9:8,r5:4)
		r1:0 = add(r1:0,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r15:14 = min(r15:14,r27:26)
		v1.w = vinsert(r8)
		r9:8 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r9:8,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r15:14 = add(r15:14,r21:20)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r1:0,r5:4)
		r9:8 = add(r9:8,r21:20)
	}
	{
		r15:14 = asr(r15:14,#31)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = min(r15:14,r11:10)
		r9:8 = asr(r9:8,#31)
	}
	{
		r3:2 = mpyu(r24,r18)
		r28 = asr(r18,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r3 += mpyi(r24,r28)
		r1:0 = max(r1:0,r5:4)
		r28 = r25
	}
	{
		r9:8 = min(r9:8,r11:10)
		r3 += mpyi(r18,r25)
		r19:18 = combine(r5,r4)
	}
	{
		r1:0 = max(r9:8,r5:4)
		v0.w = vinsert(r0)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r5:4,r27:26)
		r7:6 = min(r7:6,r27:26)
		r9 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r7:6,r21:20)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = add(r15:14,r21:20)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r7:6 = asr(r5:4,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = min(r7:6,r11:10)
		r1:0 = max(r1:0,r19:18)
		r5:4 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r19:18)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = min(r5:4,r27:26)
		v0.w = vinsert(r6)
		r7:6 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = add(r1:0,r21:20)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r7:6 = add(r7:6,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = min(r23:22,r27:26)
		r1:0 = min(r1:0,r11:10)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = add(r5:4,r21:20)
	}
	{
		r1:0 = max(r1:0,r19:18)
		r7:6 = min(r7:6,r11:10)
	}
	{
		r5:4 = asr(r5:4,#31)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = max(r7:6,r19:18)
		r5:4 = min(r5:4,r11:10)
		r7:6 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = max(r5:4,r19:18)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = add(r7:6,r21:20)
		v0.w = vinsert(r0)
	}
	{
		r1:0 = asr(r7:6,#31)
		v1.w = vinsert(r4)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r27:26)
		r1:0 = min(r1:0,r11:10)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = min(r17:16,r27:26)
		r1:0 = max(r1:0,r19:18)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = add(r5:4,r21:20)
		r7:6 = add(r7:6,r21:20)
	}
	{
		r1:0 = asr(r5:4,#31)
		v1.w = vinsert(r0)
	}
	{
		r5:4 = asr(r7:6,#31)
		r8 = asr(r9,#31)
		r7 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r15:14 = mpyu(r24,r9)
		r6 = asr(r7,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r23:22 = mpyu(r24,r7)
		r5:4 = min(r5:4,r11:10)
	}
	{
		r23 += mpyi(r24,r6)
		r15 += mpyi(r24,r8)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r23 += mpyi(r7,r25)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r19:18)
		r1:0 = max(r1:0,r19:18)
		memd(r30+#-7728) = r23:22
	}                                       // 8-byte Folded Spill
	{
		r5:4 = min(r13:12,r27:26)
		v1.w = vinsert(r4)
		r13:12 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r15 += mpyi(r9,r25)
		v0.w = vinsert(r0)
	}
	{
		r1:0 = min(r15:14,r27:26)
		r5:4 = add(r5:4,r21:20)
		r15:14 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r7:6 = min(r7:6,r27:26)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = asr(r5:4,#31)
		v0 = valign(v0,v0,#4)
		r17:16 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r21:20)
		r3:2 = add(r3:2,r21:20)
		r25 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r11:10)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = max(r5:4,r19:18)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r11:10)
		r3:2 = min(r3:2,r11:10)
		r5 = memw(r25+#120)
	}
	{
		r7:6 = max(r7:6,r19:18)
		memw(r30+##-8240) = r5
		r5 = memw(r25+#124)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r3:2 = max(r3:2,r19:18)
		v1.w = vinsert(r6)
		memw(r30+##-7984) = r5
	}                                       // 4-byte Folded Spill
	{
		r1:0 = add(r1:0,r21:20)
		r3 = memw(r25+#112)
		memw(r30+##-8752) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = min(r13:12,r27:26)
		r3 = memw(r25+#116)
		memw(r30+##-8496) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = asr(r1:0,#31)
		v1 = valign(v1,v1,#4)
		r3 = memw(r25+#104)
	}
	{
		r1:0 = min(r1:0,r11:10)
		v1.w = vinsert(r4)
		memw(r30+##-10800) = r3
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r1:0,r19:18)
		r3 = memw(r25+#108)
		memw(r30+##-9264) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = add(r9:8,r21:20)
		r3 = memw(r25+#96)
		memw(r30+##-14128) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r13:12 = min(r15:14,r27:26)
		v1 = valign(v1,v1,#4)
		r3 = memw(r25+#100)
	}
	{
		r15:14 = min(r17:16,r27:26)
		v1.w = vinsert(r2)
		memw(r30+##-13616) = r3
	}                                       // 4-byte Folded Spill
	{
		r9:8 = asr(r9:8,#31)
		r2 = memw(r25+#88)
		memw(r30+##-14256) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r15:14 = add(r15:14,r21:20)
		r2 = memw(r25+#92)
		memw(r30+##-13232) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = min(r9:8,r11:10)
		r13:12 = add(r13:12,r21:20)
		r0 = memw(r25+#80)
	}
	{
		r15:14 = asr(r15:14,#31)
		r13:12 = asr(r13:12,#31)
		memw(r30+##-13360) = r0
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r9:8,r19:18)
		r0 = memw(r25+#84)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r15:14 = min(r15:14,r11:10)
		v0.w = vinsert(r6)
		r22 = memw(r25+#72)
	}
	{
		r17:16 = min(r13:12,r11:10)
		r7:6 = max(r15:14,r19:18)
		v18 = valign(v1,v1,#4)
		r23 = memw(r25+#76)
	}
	{
		v18.w = vinsert(r4)
		r0 = memw(r25+#64)
		memw(r30+##-13104) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r9 = memw(r25+#68)
	}
	{
		v0.w = vinsert(r6)
		r7:6 = combine(r19,r18)
		r5 = memw(r25+#56)
		r8 = memw(r25+#60)
	}
	{
		r3:2 = max(r17:16,r7:6)
		r4 = asr(r9,#31)
		r13 = memw(r25+#48)
		r12 = memw(r25+#52)
	}
	{
		v17 = valign(v0,v0,#4)
		r15 = memw(r25+#40)
		r14 = memw(r25+#44)
	}
	{
		v17.w = vinsert(r2)
		r11 = memw(r25+#32)
		r10 = memw(r25+#36)
	}
	{
		r18 = memw(r25+#24)
		r19 = memw(r25+#28)
	}
	{
		r20 = memw(r25+#16)
		r21 = memw(r25+#20)
	}
	{
		r0 = memw(r25+#8)
		r26 = memw(r25+#12)
	}
	{
		r27 = memw(r25+#0)
		r1 = memw(r25+#4)
	}
	{
		r2 = asr(r27,#31)
		r3 = asr(r1,#31)
		r25 = r28
	}
	{
		r17:16 = mpyu(r24,r27)
		r7:6 = mpyu(r24,r1)
	}
	{
		r17 += mpyi(r24,r2)
		r7 += mpyi(r24,r3)
	}
	{
		r7 += mpyi(r1,r28)
		r3 = asr(r0,#31)
	}
	{
		r7:6 = mpyu(r24,r0)
		r1 = asr(r26,#31)
		memd(r30+##-13744) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r17 += mpyi(r27,r28)
		r7 += mpyi(r24,r3)
	}
	{
		r3:2 = mpyu(r24,r26)
	}
	{
		r3 += mpyi(r24,r1)
		r7 += mpyi(r0,r28)
	}
	{
		r3 += mpyi(r26,r28)
		r7:6 = mpyu(r24,r20)
		memd(r30+##-11824) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r24,r18)
		r2 = asr(r20,#31)
		memd(r30+##-12592) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r2)
		r2 = asr(r18,#31)
	}
	{
		r1 += mpyi(r24,r2)
		r7 += mpyi(r20,r28)
	}
	{
		r1 += mpyi(r18,r28)
		r2 = asr(r19,#31)
		memd(r30+##-12080) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r24,r19)
		r3 = asr(r21,#31)
		memd(r30+##-11184) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r24,r2)
		r27:26 = mpyu(r24,r21)
	}
	{
		r1:0 = mpyu(r24,r10)
		r2 = asr(r10,#31)
	}
	{
		r27 += mpyi(r24,r3)
		r1 += mpyi(r24,r2)
	}
	{
		r7 += mpyi(r19,r28)
		r2 = asr(r15,#31)
	}
	{
		r19:18 = mpyu(r24,r15)
		r27 += mpyi(r21,r28)
		memd(r30+##-11568) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r24,r2)
		r3 = asr(r11,#31)
		memd(r30+##-12336) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r24,r11)
		r2 = asr(r13,#31)
	}
	{
		r7:6 = mpyu(r24,r13)
		r21 += mpyi(r24,r3)
	}
	{
		r7 += mpyi(r24,r2)
		r1 += mpyi(r10,r28)
	}
	{
		r21 += mpyi(r11,r28)
		r7 += mpyi(r13,r28)
		memd(r30+##-9520) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r24,r12)
		r2 = asr(r12,#31)
		memd(r30+##-11056) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r24,r2)
		r3 = asr(r14,#31)
		memd(r30+#-7472) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r24,r14)
		r2 = asr(r8,#31)
	}
	{
		r7:6 = mpyu(r24,r8)
		r21 += mpyi(r24,r3)
	}
	{
		r7 += mpyi(r24,r2)
		r3 = asr(r5,#31)
		r2 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = mpyu(r24,r5)
		r1 += mpyi(r12,r28)
	}
	{
		r27 += mpyi(r24,r3)
		r7 += mpyi(r8,r28)
		memd(r30+##-10032) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r27 += mpyi(r5,r28)
		r5 = asr(r2,#31)
		memd(r30+##-10928) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r13:12 = mpyu(r24,r2)
		r7:6 = mpyu(r24,r9)
		memd(r30+##-10288) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r24,r5)
		r7 += mpyi(r24,r4)
	}
	{
		r13 += mpyi(r2,r28)
		r7 += mpyi(r9,r28)
		r2 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r5 = asr(r2,#31)
		r9 = #0
		memd(r30+##-12976) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r19 += mpyi(r15,r28)
		r7:6 = mpyu(r24,r2)
		r8 = r2
	}
	{
		r21 += mpyi(r14,r28)
		r7 += mpyi(r24,r5)
		memd(r30+##-9776) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r4 = asr(r22,#31)
		r5 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r15:14 = mpyu(r24,r22)
		r1:0 = mpyu(r24,r23)
		memd(r30+##-9008) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r15 += mpyi(r24,r4)
		r4 = asr(r23,#31)
	}
	{
		r1 += mpyi(r24,r4)
		r4 = asr(r5,#31)
	}
	{
		r3:2 = mpyu(r24,r5)
		r1 += mpyi(r23,r28)
	}
	{
		r3 += mpyi(r24,r4)
		memd(r30+##-13104) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r8,r28)
		r3 += mpyi(r5,r28)
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r4 = asr(r0,#31)
		memd(r30+##-13360) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r2 = memw(r30+##-13232)
		memd(r30+##-12848) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r24,r0)
		r3 = r0
		r8 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r24,r4)
		r5 = asr(r2,#31)
	}
	{
		r1:0 = mpyu(r24,r2)
		r15 += mpyi(r22,r28)
		r18 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r24,r5)
		r28 = asr(r8,#31)
	}
	{
		r5:4 = mpyu(r24,r8)
		r1 += mpyi(r2,r25)
		r2 = ##-1073741825
	}
	{
		r5 += mpyi(r24,r28)
		r7 += mpyi(r3,r25)
		r3 = ##2147483647
	}
	{
		r5 += mpyi(r8,r25)
		r1:0 = min(r17:16,r3:2)
		memd(r30+##-13232) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r11:10 = mpyu(r24,r18)
		r28 = asr(r18,#31)
		r8 = ##1073741824
	}
	{
		r5:4 = memd(r30+##-13744)
		memd(r30+##-14128) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r11 += mpyi(r24,r28)
		r21:20 = add(r1:0,r9:8)
	}
	{
		r3:2 = min(r5:4,r3:2)
		r21:20 = asr(r21:20,#31)
		r5:4 = combine(r11,r10)
	}
	{
		r17:16 = add(r3:2,r9:8)
		r10 = ##2147483647
		r11 = #0
	}
	{
		r5 += mpyi(r18,r25)
		r23:22 = min(r21:20,r11:10)
		r20 = ##-2147483648
	}
	{
		r17:16 = asr(r17:16,#31)
		r21 = #-1
	}
	{
		r3:2 = max(r23:22,r21:20)
		r17:16 = min(r17:16,r11:10)
		memd(r30+##-13616) = r5:4
	}                                       // 8-byte Folded Spill
	{
		v16.w = vinsert(r2)
		r3 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = max(r17:16,r21:20)
		r28 = asr(r3,#31)
		r2 = ##-1073741825
	}
	{
		r27:26 = mpyu(r24,r3)
		v0 = valign(v16,v16,#4)
		r5:4 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r27 += mpyi(r24,r28)
		v0.w = vinsert(r16)
		v16 = valign(v18,v18,#4)
	}
	{
		r27 += mpyi(r3,r25)
		r3 = ##2147483647
		v18 = valign(v29,v29,#4)
	}
	{
		r5:4 = min(r5:4,r3:2)
		r13:12 = min(r13:12,r3:2)
		v0 = valign(v0,v0,#4)
	}
	{
		r17:16 = add(r5:4,r9:8)
		r13:12 = add(r13:12,r9:8)
		r4 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = asr(r17:16,#31)
		r28 = asr(r4,#31)
		r1:0 = memd(r30+##-12592)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r1:0,r3:2)
		r23:22 = mpyu(r24,r4)
		r5 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = min(r17:16,r11:10)
		r23 += mpyi(r24,r28)
	}
	{
		r17:16 = max(r17:16,r21:20)
		r19:18 = add(r19:18,r9:8)
	}
	{
		r1:0 = mpyu(r24,r5)
		r28 = asr(r5,#31)
	}
	{
		r1 += mpyi(r24,r28)
		v0.w = vinsert(r16)
	}
	{
		r1 += mpyi(r5,r25)
		r19:18 = asr(r19:18,#31)
	}
	{
		r17:16 = min(r19:18,r11:10)
		r23 += mpyi(r4,r25)
		r5:4 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = max(r17:16,r21:20)
		r5:4 = min(r5:4,r3:2)
		v0 = valign(v0,v0,#4)
	}
	{
		r17:16 = add(r5:4,r9:8)
		v0.w = vinsert(r16)
		memd(r30+##-8752) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r17:16 = asr(r17:16,#31)
		r5:4 = memd(r30+##-12336)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r3:2)
		r17:16 = min(r17:16,r11:10)
	}
	{
		r19:18 = add(r1:0,r9:8)
		v0 = valign(v0,v0,#4)
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = max(r17:16,r21:20)
		r19:18 = asr(r19:18,#31)
	}
	{
		r5:4 = min(r19:18,r11:10)
		v0.w = vinsert(r16)
	}
	{
		r17:16 = max(r5:4,r21:20)
		r13:12 = asr(r13:12,#31)
	}
	{
		r13:12 = min(r13:12,r11:10)
		r15:14 = min(r15:14,r3:2)
		v0 = valign(v0,v0,#4)
	}
	{
		r13:12 = max(r13:12,r21:20)
		v0.w = vinsert(r16)
		r17:16 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r3:2)
		v15.w = vinsert(r12)
	}
	{
		r17:16 = add(r5:4,r9:8)
		r15:14 = add(r15:14,r9:8)
		r5:4 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r5:4,r3:2)
		r17:16 = asr(r17:16,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r17:16 = min(r17:16,r11:10)
		r13:12 = add(r13:12,r9:8)
		v1 = valign(v15,v15,#4)
	}
	{
		r17:16 = max(r17:16,r21:20)
		r13:12 = asr(r13:12,#31)
	}
	{
		r15:14 = asr(r15:14,#31)
		v1.w = vinsert(r16)
	}
	{
		r5:4 = min(r13:12,r11:10)
		r15:14 = min(r15:14,r11:10)
	}
	{
		r17:16 = max(r5:4,r21:20)
		r15:14 = max(r15:14,r21:20)
		r5:4 = memd(r30+##-13360)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r5:4,r3:2)
		v1 = valign(v1,v1,#4)
		r5:4 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r5:4,r3:2)
		v1.w = vinsert(r14)
	}
	{
		r19:18 = mpyu(r24,r0)
		r28 = asr(r0,#31)
		r5:4 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r19 += mpyi(r24,r28)
		r15:14 = add(r15:14,r9:8)
		v1 = valign(v1,v1,#4)
	}
	{
		r19 += mpyi(r0,r25)
		r15:14 = asr(r15:14,#31)
	}
	{
		r15:14 = min(r15:14,r11:10)
		r1:0 = min(r5:4,r3:2)
	}
	{
		r17:16 = add(r1:0,r9:8)
		v0.w = vinsert(r16)
	}
	{
		r15:14 = max(r15:14,r21:20)
		r13:12 = add(r13:12,r9:8)
		r0 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = asr(r13:12,#31)
		v1.w = vinsert(r14)
		v0 = valign(v0,v0,#4)
	}
	{
		r13:12 = min(r13:12,r11:10)
		r17:16 = asr(r17:16,#31)
	}
	{
		r5:4 = min(r17:16,r11:10)
	}
	{
		r15:14 = max(r5:4,r21:20)
		r13:12 = max(r13:12,r21:20)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r12)
		r5:4 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r5:4,r3:2)
		v0.w = vinsert(r14)
		r5:4 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = add(r13:12,r9:8)
	}
	{
		r7:6 = min(r7:6,r3:2)
		r15:14 = asr(r15:14,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r15:14 = min(r15:14,r11:10)
		r7:6 = add(r7:6,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r15:14 = max(r15:14,r21:20)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r11:10)
		v1.w = vinsert(r14)
	}
	{
		r13:12 = min(r5:4,r3:2)
		r7:6 = max(r7:6,r21:20)
		r5:4 = memd(r30+##-14128)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = add(r13:12,r9:8)
		r28 = asr(r0,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r17:16 = mpyu(r24,r0)
		v1.w = vinsert(r6)
		r7:6 = memd(r30+##-13232)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r17 += mpyi(r24,r28)
		r28 = #68
	}
	{
		r7:6 = add(r7:6,r9:8)
		r13:12 = asr(r13:12,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = min(r5:4,r3:2)
		r7:6 = asr(r7:6,#31)
		v2 = vror(v19,r28)
	}
	{
		r7:6 = min(r7:6,r11:10)
		r5:4 = add(r5:4,r9:8)
		v30 = vror(v20,r28)
		v2 = vor(v2,v3)
	}
	{
		r13:12 = min(r13:12,r11:10)
		r17 += mpyi(r0,r25)
		r1:0 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = max(r13:12,r21:20)
		r5:4 = asr(r5:4,#31)
		v17 = vror(v17,r28)
		v15 = vor(v30,v18)
	}
	{
		r1:0 = min(r1:0,r3:2)
		r7:6 = max(r7:6,r21:20)
	}
	{
		r13:12 = add(r1:0,r9:8)
		v0.w = vinsert(r14)
		r14 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r11:10)
		v1.w = vinsert(r6)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r13:12 = asr(r13:12,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r13:12,r11:10)
		r5 = asr(r14,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r13:12 = mpyu(r24,r14)
		v1.w = vinsert(r4)
	}
	{
		r13 += mpyi(r24,r5)
		r7:6 = max(r1:0,r21:20)
		r5:4 = memd(r30+##-13616)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r3:2)
		r5:4 = min(r27:26,r3:2)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = add(r1:0,r9:8)
		v0.w = vinsert(r6)
		r7:6 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r5:4 = add(r5:4,r9:8)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r7:6 = min(r7:6,r3:2)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7:6 = add(r7:6,r9:8)
		v1.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r11:10)
		r7:6 = asr(r7:6,#31)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r7:6 = min(r7:6,r11:10)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r7:6,r21:20)
		v1.w = vinsert(r4)
		r5:4 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r3:2)
		v0.w = vinsert(r0)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r3:2)
		r5:4 = min(r23:22,r3:2)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = add(r7:6,r9:8)
		r5:4 = add(r5:4,r9:8)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7:6 = min(r7:6,r11:10)
		r13 += mpyi(r14,r25)
		r15:14 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = max(r7:6,r21:20)
		r5:4 = min(r5:4,r11:10)
	}
	{
		r7:6 = min(r17:16,r3:2)
		v0.w = vinsert(r6)
		r17:16 = memd(r30+##-10928)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r23:22 = min(r15:14,r3:2)
		r25:24 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r13:12,r3:2)
		v1.w = vinsert(r4)
		v0 = valign(v0,v0,#4)
	}
	{
		r13:12 = min(r17:16,r3:2)
		r15:14 = min(r25:24,r3:2)
		r17:16 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r17:16,r3:2)
		r23:22 = add(r23:22,r9:8)
		r17:16 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r17:16,r3:2)
		r25:24 = add(r27:26,r9:8)
		v1 = valign(v1,v1,#4)
	}
	{
		r19:18 = min(r19:18,r3:2)
		r27:26 = asr(r23:22,#31)
	}
	{
		r23:22 = asr(r25:24,#31)
		r3:2 = add(r19:18,r9:8)
		r19:18 = combine(r9,r8)
	}
	{
		r1:0 = add(r1:0,r9:8)
		r15:14 = add(r15:14,r9:8)
	}
	{
		r17:16 = add(r17:16,r9:8)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r23:22 = min(r23:22,r11:10)
		r13:12 = add(r13:12,r9:8)
	}
	{
		r9:8 = min(r27:26,r11:10)
		r23:22 = max(r23:22,r21:20)
	}
	{
		r9:8 = max(r9:8,r21:20)
		r3:2 = asr(r3:2,#31)
	}
	{
		r15:14 = asr(r15:14,#31)
		v0.w = vinsert(r22)
	}
	{
		r15:14 = min(r15:14,r11:10)
		v1.w = vinsert(r8)
	}
	{
		r3:2 = min(r3:2,r11:10)
		r15:14 = max(r15:14,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r7:6 = asr(r7:6,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r2)
		v0.w = vinsert(r14)
	}
	{
		r17:16 = asr(r17:16,#31)
		r1:0 = asr(r1:0,#31)
	}
	{
		r7:6 = min(r7:6,r11:10)
		r27:26 = min(r17:16,r11:10)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r1:0,r11:10)
		r3:2 = add(r5:4,r19:18)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = max(r27:26,r21:20)
		r7:6 = max(r7:6,r21:20)
	}
	{
		v0.w = vinsert(r4)
		v1.w = vinsert(r6)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r13:12,#31)
	}
	{
		r7:6 = asr(r3:2,#31)
		v16.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r7:6,r11:10)
		r5:4 = min(r5:4,r11:10)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = max(r5:4,r21:20)
		r1:0 = max(r1:0,r21:20)
		v3 = valign(v16,v16,#4)
	}
	{
		v1.w = vinsert(r0)
		v0.w = vinsert(r2)
		r0 = add(r30,#-19248)
		v3 = vor(v17,v3)
	}
	{
		r0 = add(r30,#-21424)
		v18 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vadd(v2.w,v18.w):sat
		v15.w = vadd(v15.w,v18.w):sat
		v16 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = add(r30,#-20656)
		v0 = vror(v0,r28)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vasr(v2.w,v16.w)
		v0 = vor(v0,v1)
		v2.w = vadd(v3.w,v18.w):sat
	}
	{
		v3.w = vasr(v15.w,v16.w)
		v0.w = vadd(v0.w,v18.w):sat
		v1.w = vmin(v14.w,v1.w)
	}
	{
		v2.w = vasr(v2.w,v16.w)
		v3.w = vmin(v3.w,v14.w)
		v1.w = vmax(v13.w,v1.w)
	}
	{
		v0.w = vasr(v0.w,v16.w)
		v2.w = vmin(v14.w,v2.w)
		v3.w = vmax(v13.w,v3.w)
	}
	{
		v0.w = vmin(v0.w,v14.w)
		v2.w = vmax(v13.w,v2.w)
	}
	{
		v1.h = vpacke(v1.w,v3.w)
		v0.w = vmax(v13.w,v0.w)
	}
	{
		v0.h = vpacke(v2.w,v0.w)
	}
	{
		r0 = add(r30,#-19504)
		v2 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.h = vadd(v2.h,v1.h):sat
		v0.h = vadd(v2.h,v0.h):sat
		v31 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v3:2.w = vsub(v3:2.w,v3:2.w)
		v1.h = vmin(v1.h,v31.h)
		r0 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		v0.h = vmin(v31.h,v0.h)
		v1.h = vmax(v2.h,v1.h)
	}
	{
		v0.h = vmax(v3.h,v0.h)
		vmem(r0+#0) = v1
	}
	{
		r2 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = memd(r2+#192)
		vmem(r2+#0) = v0
	}
	{
		r28 = memw(r30+##-6448)
		memd(r30+##-10032) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r1:0 = memd(r2+#200)
	}
	{
		memd(r30+##-10288) = r1:0
		r1:0 = memd(r2+#208)

	} :mem_noshuf
	{
		memd(r30+##-10800) = r1:0
		r1:0 = memd(r2+#216)

	} :mem_noshuf
	{
		memd(r30+##-10928) = r1:0
		r1:0 = memd(r2+#224)

	} :mem_noshuf
	{
		memd(r30+##-11056) = r1:0
		r1:0 = memd(r2+#232)

	} :mem_noshuf
	{
		memd(r30+##-11184) = r1:0
		r1:0 = memd(r2+#240)

	} :mem_noshuf
	{
		memd(r30+##-11568) = r1:0
		r1 = memw(r2+#124)

	} :mem_noshuf
	{
		r7:6 = memd(r2+#248)
	}
	{
		memd(r30+##-11824) = r7:6
		r0 = memw(r2+#120)

	} :mem_noshuf
	{
		memd(r30+##-9008) = r1:0
		r1:0 = memd(r2+#128)

	} :mem_noshuf
	{
		memd(r30+##-9776) = r1:0
		r1:0 = memd(r2+#136)

	} :mem_noshuf
	{
		memd(r30+##-12080) = r1:0
		r11:10 = memd(r2+#160)

	} :mem_noshuf
	{
		r9:8 = memd(r2+#144)
		r13:12 = memd(r2+#152)
	}
	{
		r1:0 = memd(r2+#64)
		r17:16 = memd(r2+#168)
	}
	{
		r23:22 = memd(r2+#176)
		r25:24 = memd(r2+#184)
	}
	{
		memd(r30+#-6960) = r1:0
		r1:0 = memd(r2+#72)

	} :mem_noshuf
	{
		memd(r30+#-7472) = r1:0
		r1:0 = memd(r2+#80)

	} :mem_noshuf
	{
		memd(r30+#-7728) = r1:0
		r1:0 = memd(r2+#88)

	} :mem_noshuf
	{
		memd(r30+#-7984) = r1:0
		r1:0 = memd(r2+#96)

	} :mem_noshuf
	{
		memd(r30+##-8240) = r1:0
		r1:0 = memd(r2+#104)

	} :mem_noshuf
	{
		memd(r30+##-8496) = r1:0
		r1:0 = memd(r2+#112)

	} :mem_noshuf
	{
		memd(r30+##-8752) = r1:0
		r7:6 = memd(r2+#0)

	} :mem_noshuf
	{
		r19:18 = memd(r2+#16)
		r21:20 = memd(r2+#24)
	}
	{
		r15:14 = memd(r2+#8)
		r27:26 = memd(r2+#32)
	}
	{
		r5:4 = memd(r2+#48)
		r1:0 = memd(r2+#56)
	}
	{
		r3:2 = memd(r2+#40)
		memd(r28+#56) = r1:0

	} :mem_noshuf
	{
		memd(r28+#48) = r5:4
		memd(r28+#40) = r3:2
	}
	{
		memd(r28+#32) = r27:26
		memd(r28+#24) = r21:20
	}
	{
		memd(r28+#16) = r19:18
		memd(r28+#8) = r15:14
	}
	{
		memd(r28+#0) = r7:6
		r0 = memw(r30+##-6192)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		v0.cur = vmem(r28+#0)
		vmem(r0+#0) = v0
	}
	{
		r18 = memw(r0+#56)
		r19 = memw(r0+#60)
	}
	{
		r26 = memw(r0+#48)
		r27 = memw(r0+#52)
	}
	{
		r6 = memw(r0+#40)
		r7 = memw(r0+#44)
	}
	{
		r28 = vtrunehb(r7:6)
		r2 = memw(r0+#32)
		r3 = memw(r0+#36)
	}
	{
		memd(r30+##-9264) = r3:2
		r20 = memw(r0+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r21 = memw(r0+#28)
		r2 = memw(r0+#16)
	}
	{
		r3 = memw(r0+#20)
	}
	{
		memd(r30+##-9520) = r3:2
		r4 = memw(r0+#0)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r5 = memw(r0+#4)
		r14 = memw(r0+#8)
	}
	{
		r15 = memw(r0+#12)
		r0 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r14 = vtrunehb(r15:14)
	}
	{
		memd(r0+#56) = r25:24
		memd(r0+#48) = r23:22
	}
	{
		memd(r0+#40) = r17:16
		memd(r0+#32) = r11:10
	}
	{
		memd(r0+#24) = r13:12
		memd(r0+#16) = r9:8
	}
	{
		r3:2 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#8) = r3:2
		r3:2 = memd(r30+##-9776)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r3:2
	}
	{
		r0 = memw(r30+#-1584)
		v0 = vmem(r0+#0)
	}
	{
		vmem(r0+#0) = v0
	}
	{
		r2 = memw(r0+#56)
		r3 = memw(r0+#60)
	}
	{
		memd(r30+#-3632) = r3:2
		r2 = memw(r0+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#52)
	}
	{
		memd(r30+#-6192) = r3:2
		r2 = memw(r0+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#44)
	}
	{
		memd(r30+#-6448) = r3:2
		r2 = memw(r0+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#36)
	}
	{
		memd(r30+#-7216) = r3:2
		r2 = memw(r0+#24)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#28)
	}
	{
		memd(r30+##-9776) = r3:2
		r2 = memw(r0+#16)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r0+#20)
		r8 = memw(r0+#0)
	}
	{
		r25:24 = combine(r3,r2)
		r9 = memw(r0+#4)
		r12 = memw(r0+#8)
	}
	{
		r8 = vtrunehb(r9:8)
		r13 = memw(r0+#12)
		r0 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r15 = vtrunehb(r13:12)
		v11.w = vinsert(r8)
		r3:2 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#56) = r3:2
		r3:2 = memd(r30+##-11568)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#48) = r3:2
		r3:2 = memd(r30+##-11184)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#40) = r3:2
		r3:2 = memd(r30+##-11056)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#32) = r3:2
		r3:2 = memd(r30+##-10928)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#24) = r3:2
		r3:2 = memd(r30+##-10800)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#16) = r3:2
		r3:2 = memd(r30+##-10288)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r3:2
		r3:2 = memd(r30+##-10032)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r3:2
		r1 = memw(r30+#-816)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		r0 = vtrunehb(r5:4)
		v0 = vmem(r0+#0)
	}
	{
		r0 = vtrunehb(r19:18)
		v12.w = vinsert(r0)
		vmem(r1+#0) = v0
	}
	{
		r2 = memw(r1+#56)
	}
	{
		v0 = valign(v12,v12,#4)
		r3 = memw(r1+#60)
	}
	{
		v0.w = vinsert(r14)
		memd(r30+#-1328) = r3:2
		r2 = memw(r1+#48)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r3 = memw(r1+#52)
	}
	{
		memd(r30+#-1584) = r3:2
		r2 = memw(r1+#40)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r3 = memw(r1+#44)
	}
	{
		memd(r30+##-10288) = r3:2
		r22 = memw(r1+#32)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		r23 = memw(r1+#36)
		r10 = memw(r1+#24)
	}
	{
		r11 = memw(r1+#28)
		r4 = memw(r1+#16)
	}
	{
		r0 = vtrunehb(r27:26)
		memw(r30+##-10032) = r0
		r5 = memw(r1+#20)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r4 = vtrunehb(r5:4)
		r18 = memw(r1+#0)
		r19 = memw(r1+#4)
	}
	{
		r16 = memw(r1+#8)
		memw(r30+##-10800) = r0

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r17 = memw(r1+#12)
		r0 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		memd(r0+#48) = r3:2
		r3:2 = memd(r30+##-8496)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#40) = r3:2
		r3:2 = memd(r30+##-8240)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#32) = r3:2
		r3:2 = memd(r30+#-7984)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#24) = r3:2
		r3:2 = memd(r30+#-7728)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#16) = r3:2
		r3:2 = memd(r30+#-7472)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#8) = r3:2
		r3:2 = memd(r30+#-6960)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#0) = r3:2
		r3:2 = memd(r30+##-9008)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r0+#56) = r3:2
		r7 = memw(r30+#-304)

	} :mem_noshuf                           // 4-byte Folded Reload
	{
		v1.cur = vmem(r0+#0)
		vmem(r7+#0) = v1
	}
	{
		v1 = valign(v11,v11,#4)
		r3:2 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r9 = vtrunehb(r3:2)
		v1.w = vinsert(r15)
		r8 = memw(r7+#56)
	}
	{
		r2 = vtrunehb(r25:24)
		v0.w = vinsert(r9)
		r1:0 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r6 = vtrunehb(r1:0)
		r9 = memw(r7+#60)
		r26 = memw(r7+#48)
	}
	{
		r1 = vtrunehb(r19:18)
		r0 = vtrunehb(r21:20)
		r27 = memw(r7+#52)
		r12 = memw(r7+#40)
	}
	{
		v10.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
		r13 = memw(r7+#44)
		r14 = memw(r7+#32)
	}
	{
		v0.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
		r15 = memw(r7+#36)
		r18 = memw(r7+#24)
	}
	{
		r2 = vtrunehb(r17:16)
		v1.w = vinsert(r2)
		r19 = memw(r7+#28)
		r0 = memw(r7+#16)
	}
	{
		v2 = valign(v10,v10,#4)
		r1 = memw(r7+#20)
		r20 = memw(r7+#0)
	}
	{
		v2.w = vinsert(r2)
		r25:24 = memd(r30+##-9776)
		r21 = memw(r7+#4)
	}                                       // 8-byte Folded Reload
	{
		r16 = vtrunehb(r21:20)
		r3 = vtrunehb(r25:24)
		v1 = valign(v1,v1,#4)
	}
	{
		v9.w = vinsert(r16)
		v0 = valign(v0,v0,#4)
		r20 = #-1
		r16 = memw(r7+#8)
	}
	{
		v1.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r17 = memw(r7+#12)
	}
	{
		r0 = vtrunehb(r1:0)
		v2.w = vinsert(r4)
	}
	{
		r2 = vtrunehb(r17:16)
		v0.w = vinsert(r6)
		v3 = valign(v9,v9,#4)
		r7:6 = memd(r30+#-7216)
	}                                       // 8-byte Folded Reload
	{
		r2 = vtrunehb(r7:6)
		v3.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-6448)
	}                                       // 8-byte Folded Reload
	{
		r4 = vtrunehb(r11:10)
		v1.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		r2 = vtrunehb(r19:18)
		v2.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		r1 = vtrunehb(r7:6)
		v3.w = vinsert(r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r0 = vtrunehb(r23:22)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		v1.w = vinsert(r1)
		v2.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r15:14)
		v3.w = vinsert(r2)
	}
	{
		r4 = vtrunehb(r7:6)
		v1 = valign(v1,v1,#4)
		r7:6 = memd(r30+#-6192)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r7:6)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r4)
		v1.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r30+#-3632)
	}                                       // 8-byte Folded Reload
	{
		r1 = vtrunehb(r13:12)
		v3.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r3:2 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r2 = vtrunehb(r3:2)
		v0.w = vinsert(r28)
		v2 = valign(v2,v2,#4)
	}
	{
		r2 = vtrunehb(r27:26)
		v2.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		r0 = vtrunehb(r5:4)
		v3.w = vinsert(r1)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r30+#-1328)
	}                                       // 8-byte Folded Reload
	{
		r0 = vtrunehb(r1:0)
		v3.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		r1 = vtrunehb(r9:8)
		v2.w = vinsert(r0)
		r0 = #100
		v0 = valign(v0,v0,#4)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v1 = vror(v1,r0)
		r1 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r1)
		r1 = add(r30,#-20912)
		v2 = vror(v2,r0)
		v1 = vor(v1,v8)
	}
	{
		r1 = add(r30,#-21040)
		v2 = vor(v2,v8)
		v9 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.ub = vmin(v1.ub,v9.ub)
		v2.ub = vmin(v2.ub,v9.ub)
		v10 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v3 = vror(v3,r0)
		v1.ub = vmax(v1.ub,v10.ub)
		v2.ub = vmax(v2.ub,v10.ub)
	}
	{
		v0 = vror(v0,r0)
		v3 = vor(v3,v8)
		r0 = memw(r30+##-21440)
	}                                       // 4-byte Folded Reload
	{
		v1 = vdelta(v1,v7)
		v3.ub = vmin(v3.ub,v9.ub)
		v0 = vor(v0,v8)
	}
	{
		v2 = vdelta(v2,v7)
		r0 = memw(r30+##-21464)
		v7.cur = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v3.ub = vmax(v3.ub,v10.ub)
		v1 = vmux(q0,v2,v1)
		v0.ub = vmin(v0.ub,v9.ub)
	}
	{
		q2 = vcmp.eq(v2.b,v6.b)
		r0 = memw(r30+##-21456)
		v2.cur = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		v0.ub = vmax(v0.ub,v10.ub)
		r1 = memw(r30+##-19760)
	}                                       // 4-byte Folded Reload
	{
		v2 = vmem(r0+#0)
	}
	{
		v2 = vdelta(v3,v2)
		r0 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		v0 = vmux(q2,v2,v0)
	}
	{
		q3 = vcmp.eq(v2.b,v6.b)
		r0 = memw(r30+##-7224)
		v2.cur = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r1)
		v0 = vmux(q3,v1,v0)
		r1 = memw(r30+##-21264)
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-5936)
		r16 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r0 += mpyi(r2,r1)
		r16 = add(r16,#1)
		r1 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r16,r1)
		r2 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5424)
	}                                       // 4-byte Folded Reload
	{
		vmemu(r0+#0) = v0
	}
	{
		r0 = memw(r30+##-21056)
	}                                       // 4-byte Folded Reload
	{
		if (p0) jump:nt .LBB131_162
		r2 = add(r2,r0)
		r1 = add(r1,r0)
	}
.LBB131_148:                            // %"for output.s0.x.xo95"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        //       Parent Loop BB131_144 Depth=3
                                        // =>      This Loop Header: Depth=4
                                        //           Child Loop BB131_153 Depth 5
                                        //             Child Loop BB131_155 Depth 6
	{
		memw(r30+##-5168) = r16
		memw(r30+##-5424) = r1
	}                                       // 4-byte Folded Spill
	{
		if (p3) jump:nt .LBB131_146
		memw(r30+##-5680) = r2
	}                                       // 4-byte Folded Spill
// %bb.149:                             // %next_bb101
                                        //   in Loop: Header=BB131_148 Depth=4
	{
		r0 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r16,r0)
		if (!p2) jump:nt .LBB131_159
		memw(r30+##-5936) = r3.new
	}                                       // 4-byte Folded Spill
// %bb.150:                             // %"for convolved.s1.r19$y102.preheader"
                                        //   in Loop: Header=BB131_148 Depth=4
	{
		r4 = #64
		if (!p1) jump:nt .LBB131_160
	}
// %bb.151:                             //   in Loop: Header=BB131_148 Depth=4
	{
		r20 = r1
		r16 = #0
		r0 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-20664)
		r6 = memw(r30+##-21088)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r3,r0)
		r3 = add(r30,#-20528)
	}
	{
		r3 = add(r30,#-20400)
		r0 = sub(r0,r1)
		v0 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r3 = add(r30,#-20272)
		v1 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		r3 = add(r30,#-20144)
		v25:24 = vcombine(v1,v0)
		v4 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v5 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		jump .LBB131_153
		memw(r30+#-3632) = r0
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_152:                            // %"end for convolved.s1.r19$x106.loopexit.us"
                                        //   in Loop: Header=BB131_153 Depth=5
	{
		r16 = add(r16,#1)
		r1 = memw(r30+#-2352)
		r0 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r16,r0)
		r2 = add(r2,r1)
		r20 = add(r20,r1)
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r6 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r6,r0)
		if (p0) jump:nt .LBB131_147
	}
.LBB131_153:                            // %"for convolved.s1.r19$y102.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        //       Parent Loop BB131_144 Depth=3
                                        //         Parent Loop BB131_148 Depth=4
                                        // =>        This Loop Header: Depth=5
                                        //             Child Loop BB131_155 Depth 6
	{
		r0 = memw(r30+#-2096)
		memw(r30+#-1328) = r16
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:nt .LBB131_157
		memw(r30+#-1584) = r6
	}                                       // 4-byte Folded Spill
// %bb.154:                             //   in Loop: Header=BB131_153 Depth=5
	{
		r0 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r5 = lsr(r0,#1)
		r0 = #0
	}
	{
		loop0(.LBB131_155,r5)
		r5 = #0
	}
	.p2align	4
.Ltmp26:                                // Block address taken
.LBB131_155:                            // %"for convolved.s1.r19$x105.us"
                                        //   Parent Loop BB131_88 Depth=1
                                        //     Parent Loop BB131_121 Depth=2
                                        //       Parent Loop BB131_144 Depth=3
                                        //         Parent Loop BB131_148 Depth=4
                                        //           Parent Loop BB131_153 Depth=5
                                        // =>          This Inner Loop Header: Depth=6
	{
		r1 = add(r30,#-816)
		r12 = add(r2,r0)
		v12 = vxor(v12,v12)
		v28 = vmem(r6+#0)
	}
	{
		r1 = add(r30,#-688)
		v9 = v12
		vmemu(r1+#0) = v24
	}                                       // 256-byte Folded Spill
	{
		v17:16 = vcombine(v12,v12)
		v20 = v12
		vmemu(r1+#0) = v25
	}                                       // 256-byte Folded Spill
	{
		r28 = add(r20,r0)
		v19:18 = vcombine(v12,v12)
		r1 = memw(r30+#-3912)
		r9:8 = memd(r12+#-96)
	}                                       // 4-byte Folded Reload
	{
		v9.w = vinsert(r8)
		v8 = valign(v1,v1,r4)
		r15:14 = memd(r12+#-88)
		v1.cur = vmem(r6+#-2)
	}
	{
		v23:22 = vcombine(v12,v12)
		v13 = v12
		v0 = vmem(r1+#0)
		memw(r30+#-304) = r0
	}                                       // 4-byte Folded Spill
	{
		v15:14.w = vunpack(v8.h)
		r1 = memw(r30+#-3904)
		v10 = vmem(r6+#-1)
	}                                       // 4-byte Folded Reload
	{
		v21 = valign(v10,v10,r4)
		v27:26 = vcombine(v12,v12)
		r11:10 = memd(r12+#-80)
		memw(r30+#-560) = r5
	}                                       // 4-byte Folded Spill
	{
		v17.w = vinsert(r10)
		q0 = vcmp.eq(v6.b,v12.b)
		r19:18 = memd(r12+#-64)
		v6.cur = vmem(r1+#0)
	}
	{
		v19.w = vinsert(r18)
		v7:6.w = vunpack(v28.h)
		v28 = v12
		r25:24 = memd(r12+#-48)
	}
	{
		v20.w = vinsert(r24)
		v7 = valign(v9,v9,#4)
		r0 = memw(r30+#-3896)
		r23:22 = memd(r12+#-56)
	}                                       // 4-byte Folded Reload
	{
		v7.w = vinsert(r9)
		r7 = add(r0,##-34176)
		v9:8.w = vunpack(v21.h)
	}
	{
		v11:10.w = vunpack(v10.h)
		v15 = v12
		r0 = memw(r30+#-304)
		r17:16 = memd(r12+#-72)
	}                                       // 4-byte Folded Reload
	{
		v29 = valign(v7,v7,#4)
		r9:8 = memd(r12+#-32)
	}
	{
		v22.w = vinsert(r8)
		v29.w = vinsert(r14)
		v7 = v12
		r27:26 = memd(r12+#-40)
	}
	{
		v11 = valign(v17,v17,#4)
	}
	{
		v11.w = vinsert(r11)
		v17 = valign(v19,v19,#4)
		r11:10 = memd(r12+#-24)
	}
	{
		v17.w = vinsert(r19)
		v9 = valign(v29,v29,#4)
		r19:18 = memd(r12+#-8)
	}
	{
		v9.w = vinsert(r15)
		v25:24.w = vunpack(v1.h)
	}
	{
		r5:4 = combine(#116,r20)
		v1 = vror(v12,r4)
		v25 = v12
		r21:20 = memd(r12+#-16)
	}
	{
		v23.w = vinsert(r20)
		r20 = r4
		v9 = vror(v9,r5)
		r5:4 = memd(r12+#24)
	}
	{
		v19 = valign(v20,v20,#4)
		v21 = vor(v9,v1)
	}
	{
		v19.w = vinsert(r25)
		v9 = valign(v11,v11,#4)
		r25:24 = memd(r2+r0<<#0)
		r1:0 = memd(r12+#8)
	}
	{
		v9.w = vinsert(r16)
		v25.w = vinsert(r24)
		r24 = r2
		v11 = valign(v17,v17,#4)
	}
	{
		v11.w = vinsert(r22)
		v17 = valign(v22,v22,#4)
		r3:2 = memd(r12+#16)
		r13:12 = memd(r28+#8)
	}
	{
		v26.w = vinsert(r2)
		v17.w = vinsert(r9)
		v9 = valign(v9,v9,#4)
		r2 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v9.w = vinsert(r17)
		v11 = valign(v11,v11,#4)
		r9:8 = memd(r28+#16)
		r17:16 = memd(r28+#24)
	}
	{
		v11.w = vinsert(r23)
		v19 = valign(v19,v19,#4)
		r23:22 = memd(r6+#184)
		r15:14 = memd(r20+r2<<#0)
	}
	{
		v19.w = vinsert(r26)
		r2 = #116
		v17 = valign(v17,v17,#4)
		memd(r7+#120) = r23:22
	}
	{
		v17.w = vinsert(r10)
		v27.w = vinsert(r14)
		v9 = vror(v9,r2)
		r23:22 = memd(r6+#176)
	}
	{
		v28.w = vinsert(r8)
		v20 = valign(v19,v19,#4)
		v22 = vor(v9,v1)
		memd(r7+#112) = r23:22
	}
	{
		v20.w = vinsert(r27)
		r8 = #116
		v9 = valign(v17,v17,#4)
		r23:22 = memd(r6+#168)
	}
	{
		v9.w = vinsert(r11)
		v17 = valign(v23,v23,#4)
		memd(r7+#104) = r23:22
		r27:26 = memd(r6+#160)

	} :mem_noshuf
	{
		v17.w = vinsert(r21)
		v23 = valign(v26,v26,#4)
		memd(r7+#96) = r27:26
	}
	{
		v23.w = vinsert(r3)
		v11 = vror(v11,r2)
		r11:10 = memd(r6+#152)
	}
	{
		v11 = vror(v20,r2)
		v19 = vor(v11,v1)
		memd(r7+#88) = r11:10
	}
	{
		v9 = vror(v9,r2)
		v29 = vor(v11,v1)
		r3:2 = memd(r6+#144)
	}
	{
		v11 = valign(v25,v25,#4)
		v20 = vor(v9,v1)
		memd(r7+#80) = r3:2
	}
	{
		v11.w = vinsert(r25)
		v9 = valign(v17,v17,#4)
		r3:2 = memd(r6+#136)
	}
	{
		v9.w = vinsert(r18)
		v17 = valign(v27,v27,#4)
		memd(r7+#72) = r3:2
	}
	{
		v17.w = vinsert(r15)
		v11 = valign(v11,v11,#4)
		r3:2 = memd(r6+#128)
	}
	{
		v11.w = vinsert(r0)
		v23 = valign(v23,v23,#4)
		memd(r7+#64) = r3:2
	}
	{
		v23.w = vinsert(r4)
		v9 = valign(v9,v9,#4)
		r3:2 = memd(r6+#120)
	}
	{
		v9.w = vinsert(r19)
		v11 = valign(v11,v11,#4)
		memd(r7+#56) = r3:2
	}
	{
		v11.w = vinsert(r1)
		v31:30.uh = vunpack(v22.ub)
		r1:0 = memd(r6+#112)
	}
	{
		v3:2.uh = vunpack(v21.ub)
		memd(r7+#48) = r1:0
	}
	{
		v31:30.uw = vunpack(v30.uh)
		r1:0 = memd(r6+#104)
	}
	{
		v26 = valign(v28,v28,#4)
		memd(r7+#40) = r1:0
	}
	{
		v26.w = vinsert(r9)
		v17 = valign(v17,v17,#4)
		r11:10 = memd(r6+#96)
	}
	{
		v17.w = vinsert(r12)
		v23 = valign(v23,v23,#4)
	}
	{
		v23.w = vinsert(r5)
		v9 = vror(v9,r8)
		memd(r7+#32) = r11:10
	}
	{
		v3:2.uw = vunpack(v2.uh)
		v25 = vor(v9,v1)
		r1:0 = memd(r6+#88)
	}
	{
		v3 = vdelta(v30,v0)
		memd(r7+#24) = r1:0
	}
	{
		v9 = valign(v17,v17,#4)
		v2 = vmux(q0,v3,v2)
		r1:0 = memd(r6+#80)
	}
	{
		v9.w = vinsert(r13)
		v22 = valign(v26,v26,#4)
		memd(r7+#16) = r1:0
	}
	{
		v22.w = vinsert(r16)
		v17 = vror(v23,r8)
		r1:0 = memd(r6+#72)
	}
	{
		v17.w = vmpyieo(v2.h,v24.h)
		v27:26.uh = vunpack(v25.ub)
		v23 = vor(v17,v1)
		memd(r7+#8) = r1:0
	}
	{
		v17.w += vmpyie(v2.w,v24.h)
		v29:28.uh = vunpack(v29.ub)
		r1:0 = memd(r6+#64)
	}
	{
		v3:2.uw = vunpack(v26.uh)
		memd(r7+#0) = r1:0
	}
	{
		v3 = valign(v22,v22,#4)
		r1:0 = memd(r28+#48)
		r19:18 = memd(r28+#32)
	}
	{
		v3.w = vinsert(r17)
		v18.w = vinsert(r0)
		v27:26.uh = vunpack(v19.ub)
		r15:14 = memd(r28+#56)
	}
	{
		v16.w = vinsert(r18)
		v29:28.uw = vunpack(v28.uh)
		r3:2 = memd(r28+#80)
		r23:22 = memd(r28+#40)
	}
	{
		v15.w = vinsert(r2)
		v22 = vdelta(v2,v0)
		r5:4 = memd(r28+#112)
		r27:26 = memd(r28+#64)
	}
	{
		v12.w = vinsert(r4)
		v13.w = vinsert(r26)
		v2 = vror(v3,r8)
	}
	{
		v21 = vdelta(v28,v0)
		v2 = vor(v2,v1)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v27:26.uh = vunpack(v2.ub)
		v3 = vmux(q0,v21,v26)
	}
	{
		v2 = valign(v18,v18,#4)
	}
	{
		v2.w = vinsert(r1)
		v11 = vror(v11,r8)
		r1:0 = memd(r28+#96)
	}
	{
		v16.w = vmpyieo(v3.h,v14.h)
		v7.w = vinsert(r0)
		v18 = valign(v16,v16,#4)
		v11 = vor(v11,v1)
	}
	{
		v18.w = vinsert(r19)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r14)
		v9 = vror(v9,r8)
	}
	{
		v31:30.uh = vunpack(v11.ub)
		v11 = vor(v9,v1)
	}
	{
		v16.w += vmpyie(v3.w,v14.h)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v2 = valign(v2,v2,#4)
		v5:4.w = vadd(v17:16.w,v5:4.w)
		v3 = vmem(r7+#0)
	}
	{
		v2.w = vinsert(r15)
		v9 = vdelta(v26,v0)
	}
	{
		v27:26.uh = vunpack(v11.ub)
	}
	{
		v11 = valign(v18,v18,#4)
	}
	{
		v11.w = vinsert(r22)
		v18 = valign(v15,v15,#4)
	}
	{
		v18.w = vinsert(r3)
		v25:24.uh = vunpack(v23.ub)
		r3:2 = memd(r28+#88)
	}
	{
		v2 = vror(v2,r8)
	}
	{
		v15:14.w = vunpack(v3.h)
		v2 = vor(v2,v1)
	}
	{
		v3 = valign(v11,v11,#4)
	}
	{
		v3.w = vinsert(r23)
		v11 = valign(v18,v18,#4)
	}
	{
		v11.w = vinsert(r2)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r5)
		v25:24.uw = vunpack(v24.uh)
		r5:4 = memd(r28+#72)
	}
	{
		v29:28.uh = vunpack(v2.ub)
	}
	{
		v2 = valign(v13,v13,#4)
	}
	{
		v2.w = vinsert(r27)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r3)
		v19 = vdelta(v24,v0)
		r3:2 = memd(r28+#120)
	}
	{
		v25:24.uh = vunpack(v20.ub)
	}
	{
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r2)
		r2 = r24
		v3 = vror(v3,r8)
	}
	{
		v2 = valign(v2,v2,#4)
		v3 = vor(v3,v1)
	}
	{
		v2.w = vinsert(r4)
		r4 = #64
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r1)
		v25:24.uw = vunpack(v24.uh)
		r1:0 = memd(r28+#104)
	}
	{
		v11 = vror(v11,r8)
		v15 = vmux(q0,v22,v24)
	}
	{
		v12 = valign(v12,v12,#4)
		v11 = vor(v11,v1)
	}
	{
		v3.w = vmpyieo(v15.h,v10.h)
		v12.w = vinsert(r3)
		v23:22.uh = vunpack(v3.ub)
	}
	{
		v3.w += vmpyie(v15.w,v10.h)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r5)
		v7 = valign(v7,v7,#4)
		r5 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v7.w = vinsert(r0)
		r5 = add(r5,#2)
		v11:10.uh = vunpack(v11.ub)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v11 = vror(v12,r8)
	}
	{
		v2 = vror(v2,r8)
		v11 = vor(v11,v1)
	}
	{
		v7 = valign(v7,v7,#4)
		v2 = vor(v2,v1)
	}
	{
		v7.w = vinsert(r1)
		r1 = add(r30,#-816)
		v25:24.uw = vunpack(v10.uh)
	}
	{
		v29:28.uw = vunpack(v28.uh)
	}
	{
		v11:10.uh = vunpack(v11.ub)
	}
	{
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v11 = vdelta(v24,v0)
	}
	{
		v25:24.uh = vunpack(v2.ub)
	}
	{
		r6 = add(r6,#512)
		v2 = vror(v7,r8)
		v7 = vmux(q0,v9,v26)
		v9 = vmem(r6+#1)
	}
	{
		v13 = vdelta(v28,v0)
		v1 = vor(v2,v1)
	}
	{
		v29:28.uw = vunpack(v10.uh)
	}
	{
		v21:20.uw = vunpack(v30.uh)
	}
	{
		v19.w = vmpyieo(v7.h,v6.h)
		v0 = vdelta(v28,v0)
		v10 = vmux(q0,v19,v20)
	}
	{
		v19.w += vmpyie(v7.w,v6.h)
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v2.w = vmpyieo(v10.h,v8.h)
		v7:6.w = vunpack(v9.h)
	}
	{
		v2.w += vmpyie(v10.w,v8.h)
		v25:24.uw = vunpack(v24.uh)
	}
	{
		v9 = valign(v9,v9,r4)
		v7 = vmux(q0,v11,v24)
	}
	{
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v11.w = vmpyieo(v7.h,v6.h)
		v31:30.uw = vunpack(v28.uh)
		v29 = vmux(q0,v13,v22)
	}
	{
		v9:8.w = vunpack(v9.h)
		v0 = vmux(q0,v0,v30)
	}
	{
		v11.w += vmpyie(v7.w,v6.h)
		r1 = add(r30,#-688)
		v6 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w = vmpyieo(v29.h,v14.h)
		v10.w = vmpyieo(v0.h,v8.h)
		v7 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v18.w += vmpyie(v29.w,v14.h)
		v3:2.w = vadd(v3:2.w,v7:6.w)
		r1 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v10.w += vmpyie(v0.w,v8.h)
		r0 = add(r0,r1)
		v5:4.w = vadd(v19:18.w,v5:4.w)
	}
	{
		nop
		v25:24.w = vadd(v11:10.w,v3:2.w)
	} :endloop0
// %bb.156:                             // %"end for convolved.s1.r19$x106.loopexit.us.unr-lcssa"
                                        //   in Loop: Header=BB131_153 Depth=5
	{
		r0 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:t .LBB131_152
		r16 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_158
	}
	.p2align	4
.LBB131_157:                            //   in Loop: Header=BB131_153 Depth=5
	{
		r5 = #0
		r0 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:t .LBB131_152
		r16 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
.LBB131_158:                            // %"for convolved.s1.r19$x105.us.epil"
                                        //   in Loop: Header=BB131_153 Depth=5
	{
		r17 = r2
		r0 = memw(r30+#-3912)
		r2 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		v6 = vxor(v6,v6)
		r3 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		r28 = #116
		v3:2 = vcombine(v6,v6)
		v10 = v6
		v0 = vmem(r0+#0)
	}
	{
		r2 = mpyi(r16,r2)
		v28 = vror(v6,r4)
		r0 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r5,r2)
		q0 = vcmp.eq(v1.b,v6.b)
		r1 = memw(r30+#-3632)
		v1.cur = vmem(r3+#0)
	}                                       // 4-byte Folded Reload
	{
		v9:8 = vcombine(v6,v6)
		v14 = v6
		v7 = v6
		r6 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r6 += asl(r2,#8)
		r3 = memw(r30+##-4400)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r2,##-34304)
		r2 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r0 += mpyi(r16,r3)
		v13:12.w = vunpack(v23.h)
		v23.cur = vmem(r6+#0)
	}
	{
		r1 += mpyi(r0,r2)
		r0 = memw(r30+#-3888)
		r3:2 = memd(r6+#184)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r5,r0)
	}
	{
		r5 = addasl(r4,r1,#7)
		r0 = asl(r1,#7)
	}
	{
		r1:0 = memd(r4+r0<<#0)
	}
	{
		v2.w = vinsert(r0)
		r9:8 = memd(r5+#16)
		r13:12 = memd(r5+#8)
	}
	{
		v3.w = vinsert(r8)
		r15:14 = memd(r5+#24)
		memd(r7+#120) = r3:2

	} :mem_noshuf
	{
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r6+#176)
	}
	{
		v2.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		memd(r7+#112) = r3:2
		r3:2 = memd(r6+#168)

	} :mem_noshuf
	{
		v3.w = vinsert(r9)
		memd(r7+#104) = r3:2
	}
	{
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r6+#160)
	}
	{
		v2.w = vinsert(r12)
		v3 = valign(v3,v3,#4)
		memd(r7+#96) = r3:2
	}
	{
		v3.w = vinsert(r14)
		r1:0 = memd(r6+#152)
	}
	{
		v2 = valign(v2,v2,#4)
		memd(r7+#88) = r1:0
	}
	{
		v2.w = vinsert(r13)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r6+#144)
	}
	{
		v3.w = vinsert(r15)
		memd(r7+#80) = r1:0
	}
	{
		v2 = vror(v2,r28)
		r1:0 = memd(r6+#136)
	}
	{
		v3 = vror(v3,r28)
		v2 = vor(v2,v28)
		memd(r7+#72) = r1:0
	}
	{
		v3 = vor(v3,v28)
		r1:0 = memd(r6+#128)
	}
	{
		v17:16.uh = vunpack(v2.ub)
		memd(r7+#64) = r1:0
	}
	{
		v31:30.uh = vunpack(v3.ub)
		r1:0 = memd(r6+#120)
	}
	{
		v17:16.uw = vunpack(v16.uh)
		memd(r7+#56) = r1:0
	}
	{
		v3:2.uw = vunpack(v30.uh)
		r1:0 = memd(r6+#112)
	}
	{
		memd(r7+#48) = r1:0
	}
	{
		v2 = vdelta(v2,v0)
		r1:0 = memd(r6+#104)
	}
	{
		v2 = vmux(q0,v2,v16)
		memd(r7+#40) = r1:0
		r1:0 = memd(r6+#96)

	} :mem_noshuf
	{
		v11.w = vmpyieo(v2.h,v12.h)
		memd(r7+#32) = r1:0
	}
	{
		v11.w += vmpyie(v2.w,v12.h)
		r1:0 = memd(r6+#88)
	}
	{
		memd(r7+#24) = r1:0
		r1:0 = memd(r6+#80)

	} :mem_noshuf
	{
		memd(r7+#16) = r1:0
		r1:0 = memd(r6+#72)

	} :mem_noshuf
	{
		memd(r7+#8) = r1:0
		r1:0 = memd(r6+#64)

	} :mem_noshuf
	{
		memd(r7+#0) = r1:0
		r25:24 = memd(r5+#80)

	} :mem_noshuf
	{
		r27:26 = memd(r5+#32)
		r19:18 = memd(r5+#48)
	}
	{
		v14.w = vinsert(r26)
		v7.w = vinsert(r24)
		r11:10 = memd(r5+#64)
	}
	{
		v10.w = vinsert(r18)
		v9.w = vinsert(r10)
		r3:2 = memd(r5+#112)
		r1:0 = memd(r5+#96)
	}
	{
		v6.w = vinsert(r2)
		v8.w = vinsert(r0)
		v7 = valign(v7,v7,#4)
		v2 = vmem(r7+#0)
	}
	{
		v7.w = vinsert(r25)
		v3 = valign(v10,v10,#4)
		v10 = vmem(r6+#1)
	}
	{
		v3.w = vinsert(r19)
		v13:12.w = vunpack(v2.h)
	}
	{
		v2 = valign(v14,v14,#4)
	}
	{
		v2.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v8 = valign(v8,v8,#4)
		r3:2 = memd(r5+#56)
	}
	{
		v8.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r5+#40)
		r25:24 = memd(r5+#88)
	}
	{
		v3.w = vinsert(r2)
		r2 = r17
		v9 = valign(v9,v9,#4)
		r23:22 = memd(r5+#72)
	}
	{
		v9.w = vinsert(r11)
		v2 = valign(v2,v2,#4)
		r27:26 = memd(r5+#104)
		r5:4 = memd(r5+#120)
	}
	{
		v2.w = vinsert(r0)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r24)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		r4 = #64
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r22)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r25)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
		v3 = vror(v3,r28)
	}
	{
		v9 = valign(v9,v9,#4)
		v3 = vor(v3,v28)
	}
	{
		v9.w = vinsert(r23)
		v2 = vror(v2,r28)
	}
	{
		v7 = vror(v7,r28)
		v2 = vor(v2,v28)
	}
	{
		v8 = vror(v8,r28)
		v7 = vor(v7,v28)
	}
	{
		v6 = vror(v6,r28)
		v1 = vor(v8,v28)
	}
	{
		v21:20.uh = vunpack(v3.ub)
	}
	{
		v9 = vror(v9,r28)
	}
	{
		v17:16.uh = vunpack(v2.ub)
		v2 = vor(v6,v28)
		v3 = vor(v9,v28)
	}
	{
		v19:18.uh = vunpack(v7.ub)
	}
	{
		v9:8.uw = vunpack(v20.uh)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v23:22.uh = vunpack(v2.ub)
	}
	{
		v7:6.uh = vunpack(v3.ub)
	}
	{
		v29 = vdelta(v8,v0)
	}
	{
		v9:8.uw = vunpack(v16.uh)
	}
	{
		v17:16.uw = vunpack(v18.uh)
		v1 = vmux(q0,v29,v8)
	}
	{
		v3:2.uw = vunpack(v22.uh)
	}
	{
		v3 = vdelta(v16,v0)
	}
	{
		v7:6.uw = vunpack(v6.uh)
	}
	{
		v31 = valign(v10,v10,r4)
		v30 = vmux(q0,v3,v6)
	}
	{
		v0 = vdelta(v2,v0)
	}
	{
		v7:6.uw = vunpack(v28.uh)
	}
	{
		v10.w = vmpyieo(v1.h,v12.h)
		v17:16.w = vunpack(v10.h)
		v0 = vmux(q0,v0,v6)
	}
	{
		v10.w += vmpyie(v1.w,v12.h)
		v9:8.w = vunpack(v31.h)
	}
	{
		v7.w = vmpyieo(v30.h,v16.h)
		v5:4.w = vadd(v11:10.w,v5:4.w)
	}
	{
		v6.w = vmpyieo(v0.h,v8.h)
	}
	{
		v7.w += vmpyie(v30.w,v16.h)
	}
	{
		v6.w += vmpyie(v0.w,v8.h)
	}
	{
		jump .LBB131_152
		v25:24.w = vadd(v7:6.w,v25:24.w)
	}
.LBB131_159:                            //   in Loop: Header=BB131_148 Depth=4
	{
		r0 = add(r30,#-20528)
		r6 = add(r30,#-20400)
		r3 = add(r30,#-20272)
		r2 = add(r30,#-20144)
	}
	{
		r4 = #64
		jump .LBB131_161
		v0 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
.LBB131_160:                            //   in Loop: Header=BB131_148 Depth=4
	{
		r0 = add(r30,#-20528)
		r6 = add(r30,#-20400)
		r3 = add(r30,#-20272)
		r2 = add(r30,#-20144)
	}
	{
		v0 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
.LBB131_161:                            // %"consume convolved108"
                                        //   in Loop: Header=BB131_148 Depth=4
	{
		v1 = vmemu(r6+#0)
	}                                       // 256-byte Folded Reload
	{
		v25:24 = vcombine(v1,v0)
		v4 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		jump .LBB131_147
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	.p2align	4
.LBB131_162:                            // %"end for output.s0.x.xo96.loopexit"
                                        //   in Loop: Header=BB131_144 Depth=3
	{
		r1 = memw(r30+##-23560)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-28032)
		r2 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-23544)
		r4 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		vmem(r1+#0) = v5
	}
	{
		vmem(r5+#0) = v24
	}
	{
		vmem(r4+#0) = v25
	}
	{
		vmem(r0+#0) = v4
	}
.LBB131_163:                            // %"end for output.s0.x.xo96"
                                        //   in Loop: Header=BB131_144 Depth=3
	{
		r2 = add(r2,#1)
		r0 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r21 = #68
		r5 = memw(r30+##-21200)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r2,r0)
		r1 = memw(r30+##-21112)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,r5)
		r4 = memw(r30+##-21120)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r4,r5)
		memw(r30+##-21112) = r1
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_144
		memw(r30+##-21120) = r1
	}                                       // 4-byte Folded Spill
.LBB131_164:                            // %"end for output.s0.y.yo93"
                                        //   in Loop: Header=BB131_121 Depth=2
	{
		r1 = memw(r30+##-21224)
		r4 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r0 = memw(r30+##-23648)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-21240)
		memw(r30+##-21224) = r1
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		r1 = add(r3,r4)
		r2 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r2,r4)
		memw(r30+##-21240) = r1
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_121
		memw(r30+##-21232) = r1
	}                                       // 4-byte Folded Spill
// %bb.165:                             //   in Loop: Header=BB131_88 Depth=1
	{
		r2 = add(r30,#-22192)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r19 = add(r0,#-27404)
		v8 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
.LBB131_166:                            // %"end for output.s0.b.rebased58"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r1 = memw(r30+##-22280)
		r0 = memw(r30+##-22832)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r2 = memw(r30+##-22272)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		r0 = add(r2,#128)
		memw(r30+##-22280) = r1
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_88
		memw(r30+##-22272) = r0
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_82
	}
.LBB131_167:                            // %if.then.i1961
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		p0 = cmp.eq(r1,#0); if (p0.new) jump:nt .LBB131_170
		r16 = memw(r0+#8)
	}
// %bb.168:                             // %if.then.i1961
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = #16384
		if (!cmp.gtu(r16,r0.new)) jump:nt .LBB131_170
	}
// %bb.169:                             // %if.then3.i1965
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		call ##halide_free
		r0 = #0
	}
	{
		r3 = add(r30,#-22192)
		r0 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p3 = r0
		p1 = r2
		v8 = vmemu(r3+#0)
	}                                       // 128-byte Folded Reload
	{
		r17 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
.LBB131_170:                            // %if.end.i1969
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = add(r16,r17)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r1,#-28044)
		r1 = #16384
	}
	{
		p0 = cmp.gtu(r0,r1); if (!p0.new) jump:t .LBB131_172
		r0 = #0
		memw(r16+#8) = r0
	}
// %bb.171:                             // %if.then8.i1971
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		call ##halide_malloc
		r1:0 = combine(r17,#0)
	}
	{
		r1 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		p3 = r1
		r1 = add(r30,#-22192)
	}
	{
		v8 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r1
	}
.LBB131_172:                            // %if.end11.i1973
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r1 = memw(r30+##-23568)
		memw(r16+#0) = r0
	}                                       // 4-byte Folded Reload
	{
		p2 = r1
		jump .LBB131_90
		memw(r16+#4) = r17
	}
.LBB131_173:                            // %"consume sum_filter54.critedge"
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		v5:4.w = vsub(v5:4.w,v5:4.w)
		r1 = memw(r30+##-23536)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27392)
		v3:2 = vcombine(v5,v4)
		v1:0 = vcombine(v5,v4)
	}
	{
		jump .LBB131_118
		vmem(r0+#0) = v4
	}
.LBB131_174:                            // %then_bb35
                                        //   in Loop: Header=BB131_88 Depth=1
	{
		r0 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(#7,asl(r0,#2))
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r0,#-8)
		r1 = add(r1,#-28044)
		r7 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r0 = sub(r29,r0)
	}
	{
		r0 = and(r0,#-128)
		memw(r30+#-3120) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r29 = r0
		jump .LBB131_91
		memw(r1+#0) = r0
	}
.LBB131_175:                            // %next_bb29
	{
		p0 = cmp.gt(r21,#0)
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		if (!p0) jump:nt .LBB131_726
	}
// %bb.176:                             // %if.end.i2005
	{
		r0 = #0
		r1 = memw(r30+##-5696)
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r2 = asl(r1,#8)
		r18 = add(r3,#-28044)
		memw(r30+#-2096) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r1 = mux(p1,r2,#0)
		memw(r30+##-4656) = r24
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r30+##-5688)
		memw(r30+##-5680) = r20
	}                                       // 4-byte Folded Reload
	{
		r0 = max(r2,r0)
		r2 = #16384
		memw(r30+##-5936) = r11
	}                                       // 4-byte Folded Spill
	{
		r16 = mpyi(r1,r0)
	}
	{
		r17 = or(r16,#134)
		memw(r18+#8) = r17.new
	}
	{
		p0 = cmp.gtu(r17,r2); if (!p0.new) jump:t .LBB131_178
	}
// %bb.177:                             // %pseudostack_alloc.exit2010
	{
		call ##halide_malloc
		r1:0 = combine(r17,#0)
	}
	{
		p0 = cmp.eq(r0,#0)
		memw(r30+#-2864) = r0
		memw(r18+#0) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-23568)
		memw(r18+#4) = r17
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r7 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-23560)
		r3 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		if (!p0) jump:nt .LBB131_180
	}
	{
		jump .LBB131_179
	}
.LBB131_178:                            // %pseudostack_alloc.exit2010.thread
	{
		r0 = memw(r30+##-23568)
		memw(r18+#4) = r17
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
.LBB131_179:                            // %then_bb112
	{
		r16 = insert(r17,#30,#2)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r2 = sub(r29,r16)
		r29 = sub(r29,r16)
		r0 = add(r0,#-28044)
	}
	{
		r29 = and(r29,#-128)
		r2 = and(r2,#-128)
		memw(r30+#-2864) = r2.new
	}                                       // 4-byte Folded Spill
	{
		memw(r0+#0) = r2
	}
.LBB131_180:                            // %"produce filter_zeroed114"
	{
		r21 = add(pc,##.LCPI131_0@PCREL)
		v1 = vsplat(r21)
		r2 = r21
	}
	{
		r22 = add(pc,##.LCPI131_1@PCREL)
		p0 = cmp.gt(r2,#63)
	}
	{
		r0 = p0
		memw(r30+##-20272) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r20 = add(pc,##.LCPI131_2@PCREL)
		p0 = cmp.gt(r2,#64)
	}
	{
		r18 = add(pc,##.LCPI131_3@PCREL)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-1)
		memw(r30+##-5168) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p2) jump:nt .LBB131_221
		memw(r30+##-23232) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.181:                             // %"for filter_zeroed.s0.y115.preheader"
	{
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_221
	}
// %bb.182:                             // %"for filter_zeroed.s0.y115.preheader.split.us"
	{
		r0 = and(r19,#255)
		r2 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		v0.h = vsplat(r0)
		r24 = add(r2,#1024)
		r0 = memw(r30+##-20272)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_194
	}
// %bb.183:                             // %"for filter_zeroed.s0.y115.preheader.split.us.split.us"
	{
		r0 = memw(r30+##-23232)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_206
	}
// %bb.184:                             // %"for filter_zeroed.s0.y115.us.us.us.preheader"
	{
		r1:0 = combine(#64,#-1)
		q0 = vcmp.gt(v1.w,v2.w)
		v2.cur = vmem(r18+#0)
	}
	{
		v2 = vxor(v2,v2)
		q1 = vcmp.gt(v1.w,v3.w)
		v3.cur = vmem(r20+#0)
	}
	{
		v3 = vand(q0,r0)
		v4 = vand(q1,r0)
		r4 = memw(r30+#-2864)
		r2 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r8 = asl(r7,#2)
		r24 = add(r4,#512)
	}
	{
		r23 = extractu(r2,#2,#8)
		v3.b = vpacke(v2.h,v3.h)
		r2 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r9 = add(r4,#128)
		r19 = and(r2,#-4)
		r13:12 = combine(#0,#0)
		v2.b = vpacke(v2.h,v4.h)
	}
	{
		p1 = cmp.eq(r23,#0)
		v3 = vror(v3,r1)
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r1,#2)
		v2 = vor(v3,v2)
		r1 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		q0 = vand(v2,r0)
	}
	{
		loop1(.LBB131_187,r1)
		jump .LBB131_187
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_185:                            //   in Loop: Header=BB131_187 Depth=1
	{
		r7 = r5
		v2 = valign(v4,v2,r4)
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		v5:4.uh = vunpack(v3.ub)
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		r0 = r6
		v3:2.uh = vunpack(v2.ub)
	}
	{
		v2.h = vsub(v2.h,v0.h)
		v3.h = vsub(v4.h,v0.h)
		vmem(r3+#-1) = v2.new
	}
	{
		r3 = memw(r30+##-23544)
		if (q0) vmem(r3+#0) = v3
	}                                       // 4-byte Folded Reload
.LBB131_186:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us.split.us.us.us.us"
                                        //   in Loop: Header=BB131_187 Depth=1
	{
		r2 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r24 = add(r24,r2)
		r2 = memw(r30+##-23672)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r2)
		memw(r30+##-24728) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r13 = add(r13,r0)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_221
	}
.Ltmp27:                                // Block address taken
.LBB131_187:                            // %"for filter_zeroed.s0.y115.us.us.us"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_189 Depth 2
                                        //     Child Loop BB131_193 Depth 2
	{
		r14 = #0
		if (!p0) jump:nt .LBB131_191
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
// %bb.188:                             //   in Loop: Header=BB131_187 Depth=1
	{
		r5 = add(r0,r7)
		r4 = add(r0,#64)
		r26 = r7
		v3 = vmem(r0+#0)
	}
	{
		r1 = lsr(r19,#2)
		r6 = add(r5,r7)
		r3 = add(r5,#64)
		v2 = vmem(r4+#0)
	}
	{
		r7 = add(r6,r7)
		r25 = add(r24,#-384)
		r15 = add(r24,#-128)
		v4 = vmem(r5+#0)
	}
	{
		r2 = add(r7,#64)
		r28 = add(r24,#128)
		r16 = add(r24,#384)
		v5 = vmem(r3+#0)
	}
	{
		r14 = add(r12,#4)
		p2 = cmp.gtu(r1,#1)
		r11 = add(r24,#1024)
		v7 = vmem(r6+#0)
	}
	{
		r17 = r24
		r10 = r24
		v8 = valign(v6,v2,r4)
		v6.cur = vmem(r4+#1)
	}
	{
		v2 = vmem(r5+#1)
	}
	{
		r5 = add(r6,#64)
		v2 = valign(v2,v4,r5)
		v4 = vmem(r3+#1)
	}
	{
		v6 = valign(v4,v5,r3)
		v10 = vmem(r2+#0)
	}
	{
		v4 = vmem(r6+#1)
	}
	{
		r6 = add(r1,#-1)
		v4 = valign(v4,v7,r6)
		v7 = vmem(r0+#1)
	}
	{
		loop0(.LBB131_189,r6)
		v3 = valign(v7,v3,r0)
		v7 = vmem(r2+#1)
	}
	{
		r2 = add(r0,r8)
		v10 = valign(v7,v10,r2)
		v9 = vmem(r7+#0)
	}
	{
		v5 = vmem(r5+#0)
	}
	{
		v11:10.uh = vunpack(v10.ub)
		v7 = vmem(r5+#1)
	}
	{
		v7 = valign(v7,v5,r5)
		v5 = vmem(r7+#1)
	}
	{
		if (!p2) jump:nt .LBB131_190
		v5 = valign(v5,v9,r7)
	}
	.p2align	4
.LBB131_189:                            // %"for filter_zeroed.s0.x118.us.us.us.us.us"
                                        //   Parent Loop BB131_187 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r3 = add(r2,r26)
		r6 = add(r2,#64)
		v15:14.uh = vunpack(v7.ub)
		v9 = vmem(r2+#0)
	}
	{
		r1 = add(r3,r26)
		r4 = add(r3,#64)
		v7:6.uh = vunpack(v6.ub)
		v13.h = vsub(v14.h,v0.h)
	}
	{
		r7 = add(r1,r26)
		r5 = add(r1,#64)
		v17:16.uh = vunpack(v8.ub)
		v12 = vmem(r4+#0)
	}
	{
		r0 = add(r7,#64)
		v15:14.uh = vunpack(v4.ub)
		v6.h = vsub(v6.h,v0.h)
		v7 = vmem(r5+#0)
	}
	{
		r10 = r11
		v19:18.uh = vunpack(v5.ub)
		v15.h = vsub(v16.h,v0.h)
		v8 = vmem(r0+#0)
	}
	{
		r11 = add(r11,#1024)
		v23:22.uh = vunpack(v2.ub)
		v5 = vmem(r6+#0)
		if (q0) vmem(r15+#0) = v6
	}
	{
		v16 = vmem(r0+#1)
	}
	{
		v16 = valign(v16,v8,r0)
		v10.h = vsub(v10.h,v0.h)
		v8 = vmem(r5+#1)
		if (q0) vmem(r28+#0) = v13
	}
	{
		r14 = add(r14,#4)
		v21:20.uh = vunpack(v3.ub)
		v2 = vmem(r4+#1)
		if (q0) vmem(r16+#0) = v10
	}
	{
		r16 = add(r10,#384)
		v6 = valign(v2,v12,r4)
		v4 = vmem(r7+#0)
		if (q0) vmem(r25+#0) = v15
	}
	{
		r28 = add(r10,#128)
		r15 = add(r10,#-128)
		v7 = valign(v8,v7,r5)
		v3 = vmem(r2+#1)
	}
	{
		r2 = add(r2,r8)
		r25 = add(r10,#-384)
		v3 = valign(v3,v9,r2)
		v2 = vmem(r6+#1)
	}
	{
		v8 = valign(v2,v5,r6)
		v5.h = vsub(v18.h,v0.h)
		v9 = vmem(r1+#0)
		vmem(r17+#2) = v5.new
	}
	{
		v5 = valign(v2,v4,r7)
		v4.h = vsub(v14.h,v0.h)
		v2.cur = vmem(r7+#1)
		vmem(r17+#0) = v4.new
	}
	{
		v11 = vmem(r3+#0)
	}
	{
		v4 = valign(v2,v9,r1)
		v9.h = vsub(v22.h,v0.h)
		v2.cur = vmem(r1+#1)
		vmem(r17+#-2) = v9.new
	}
	{
		v2 = vmem(r3+#1)
	}
	{
		v2 = valign(v2,v11,r3)
		v9.h = vsub(v20.h,v0.h)
		vmem(r17+#-4) = v9.new
	}
	{
		r17 = r10
		v11:10.uh = vunpack(v16.ub)
	} :endloop0
.LBB131_190:                            //   in Loop: Header=BB131_187 Depth=1
	{
		r7 = r26
		v13:12.uh = vunpack(v3.ub)
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		v3.h = vsub(v12.h,v0.h)
		r3 = memw(r30+##-23544)
		vmem(r10+#-4) = v3.new
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		v9:8.uh = vunpack(v8.ub)
	}
	{
		v3:2.uh = vunpack(v2.ub)
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		v9:8.uh = vunpack(v6.ub)
		v3.h = vsub(v8.h,v0.h)
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		v13:12.uh = vunpack(v4.ub)
		v2.h = vsub(v2.h,v0.h)
		if (q0) vmem(r25+#0) = v3
	}
	{
		v7:6.uh = vunpack(v7.ub)
		v3.h = vsub(v8.h,v0.h)
		vmem(r10+#-2) = v2
	}
	{
		v3.h = vsub(v12.h,v0.h)
		v2.h = vsub(v6.h,v0.h)
		if (q0) vmem(r15+#0) = v3
	}
	{
		v5:4.uh = vunpack(v5.ub)
		vmem(r10+#0) = v3
	}
	{
		v3.h = vsub(v4.h,v0.h)
		v2.h = vsub(v10.h,v0.h)
		if (q0) vmem(r28+#0) = v2
	}
	{
		vmem(r10+#2) = v3
	}
	{
		if (q0) vmem(r16+#0) = v2
	}
.LBB131_191:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us.split.us.us.us.us.unr-lcssa"
                                        //   in Loop: Header=BB131_187 Depth=1
	{
		if (p1) jump:nt .LBB131_186
	}
// %bb.192:                             // %"for filter_zeroed.s0.x118.us.us.us.us.us.epil.preheader"
                                        //   in Loop: Header=BB131_187 Depth=1
	{
		r1 = mpyi(r7,r14)
		r5 = r7
		r6 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r0 = r9
		r4 = add(r6,r1)
		r1 = add(r1,r7)
		r2 = add(r14,r13)
	}
	{
		r0 += asl(r2,#8)
		r3 = add(r23,#-1)
		r7 = add(r4,#64)
	}
	{
		loop0(.LBB131_193,r3)
		p2 = cmp.gtu(r23,#1)
		r2 = add(r0,#256)
		v3 = vmem(r7+#0)
	}
	{
		r3 = r0
		v3 = valign(v4,v3,r7)
		v4.cur = vmem(r7+#1)
	}
	{
		v2 = vmem(r4+#0)
	}
	{
		if (!p2) jump:nt .LBB131_185
		v4 = vmem(r4+#1)
	}
	.p2align	4
.LBB131_193:                            // %"for filter_zeroed.s0.x118.us.us.us.us.us.epil"
                                        //   Parent Loop BB131_187 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r4 = add(r6,r1)
		r3 = r2
		r2 = add(r2,#256)
		v4 = valign(v4,v2,r4)
	}
	{
		r7 = add(r4,#64)
		r1 = add(r1,r5)
		v7:6.uh = vunpack(v3.ub)
		v2 = vmem(r4+#0)
	}
	{
		v7:6.uh = vunpack(v4.ub)
		v3.h = vsub(v6.h,v0.h)
		v5 = vmem(r7+#0)
	}
	{
		v3.h = vsub(v6.h,v0.h)
		v4 = vmem(r4+#1)
		if (q0) vmem(r0+#0) = v3
	}
	{
		r0 = r3
		v3 = vmem(r7+#1)
		vmem(r0+#-1) = v3
	}
	{
		nop
		v3 = valign(v3,v5,r7)
	} :endloop0
	{
		jump .LBB131_185
	}
.LBB131_194:                            // %"for filter_zeroed.s0.y115.us.preheader"
	{
		r1:0 = combine(#64,#-1)
		q0 = vcmp.gt(v1.w,v2.w)
		v2.cur = vmem(r22+#0)
	}
	{
		v2 = vxor(v2,v2)
		q1 = vcmp.gt(v1.w,v3.w)
		v3.cur = vmem(r21+#0)
	}
	{
		v3 = vand(q0,r0)
		v4 = vand(q1,r0)
		r2 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r23 = extractu(r2,#3,#8)
		r8 = #0
		r2 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r17 = and(r2,#-8)
		v3.b = vpacke(v2.h,v3.h)
		r27 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		r9 = asl(r7,#3)
		p1 = cmp.eq(r23,#0)
		r12 = #0
		v2.b = vpacke(v2.h,v4.h)
	}
	{
		v3 = vror(v3,r1)
		r1 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r1,#6)
		v2 = vor(v3,v2)
		r1 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		q0 = vand(v2,r0)
	}
	{
		loop1(.LBB131_198,r1)
		jump .LBB131_198
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_195:                            //   in Loop: Header=BB131_198 Depth=1
	{
		r0 = r1
		r3 = memw(r30+##-23544)
		if (q0) vmem(r0+#0) = v3
	}                                       // 4-byte Folded Reload
.LBB131_196:                            //   in Loop: Header=BB131_198 Depth=1
	{
		v3:2.uh = vunpack(v2.ub)
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		v2.h = vsub(v2.h,v0.h)
		r7 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-23568)
		if (q0) vmem(r0+#0) = v2
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
	}
.LBB131_197:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us2155"
                                        //   in Loop: Header=BB131_198 Depth=1
	{
		r0 = memw(r30+##-23672)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r0)
		r0 = memw(r30+#-2096)
		memw(r30+##-24728) = r7.new
	}                                       // 4-byte Folded Reload
	{
		r24 = add(r24,r0)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r12 = add(r12,r0)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_221
	}
.Ltmp28:                                // Block address taken
.LBB131_198:                            // %"for filter_zeroed.s0.y115.us"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_200 Depth 2
                                        //     Child Loop BB131_205 Depth 2
	{
		r14 = #0
		if (!p0) jump:nt .LBB131_202
	}
// %bb.199:                             //   in Loop: Header=BB131_198 Depth=1
	{
		r1 = lsr(r17,#3)
		r2 = add(r24,#768)
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		r25 = add(r24,#-1024)
		r5 = add(r0,r27)
		p2 = cmp.gtu(r1,#1)
		r1 = add(r1,#-1)
	}
	{
		r13 = add(r24,#-768)
		r6 = add(r5,r27)
		r15 = add(r24,#-512)
		v3 = vmem(r5+#0)
	}
	{
		r28 = add(r24,#-256)
		r7 = add(r6,r27)
		r10 = add(r24,#256)
		v5 = vmem(r6+#0)
	}
	{
		r11 = add(r24,#512)
		r4 = add(r7,r27)
		v6 = vmem(r5+#1)
	}
	{
		r14 = add(r8,#8)
		r3 = add(r4,r27)
		v6 = valign(v6,v3,r5)
		v3 = vmem(r6+#1)
	}
	{
		r19 = add(r24,#2048)
		r5 = add(r3,r27)
		v5 = valign(v3,v5,r6)
		v7 = vmem(r7+#0)
	}
	{
		loop0(.LBB131_200,r1)
		r26 = r24
		r6 = add(r5,r27)
		v10 = vmem(r3+#0)
	}
	{
		r16 = r24
		v11 = vmem(r6+#0)
	}
	{
		v7 = valign(v3,v7,r7)
		v3.cur = vmem(r7+#1)
	}
	{
		v8 = vmem(r4+#0)
	}
	{
		v3 = vmem(r3+#1)
	}
	{
		v3 = valign(v3,v10,r3)
		v10 = vmem(r6+#1)
	}
	{
		v10 = valign(v10,v11,r6)
		v9 = vmem(r4+#1)
	}
	{
		v8 = valign(v9,v8,r4)
		v2 = vmem(r0+#0)
	}
	{
		v11:10.uh = vunpack(v10.ub)
		v9 = vmem(r5+#0)
	}
	{
		v4 = vmem(r0+#1)
	}
	{
		v4 = valign(v4,v2,r0)
		v2 = vmem(r5+#1)
	}
	{
		v2 = valign(v2,v9,r5)
		v9.h = vsub(v10.h,v0.h)
	}
	{
		r2 = add(r0,r9)
		if (!p2) jump:nt .LBB131_201
		if (q0) vmem(r2+#0) = v9
	}
	.p2align	4
.LBB131_200:                            // %"for filter_zeroed.s0.x118.us2152"
                                        //   Parent Loop BB131_198 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r1 = add(r2,r27)
		r16 = r19
		v13:12.uh = vunpack(v2.ub)
		v9 = vmem(r2+#0)
	}
	{
		r4 = add(r1,r27)
		r14 = add(r14,#8)
		v3:2.uh = vunpack(v3.ub)
		v10 = vmem(r1+#0)
	}
	{
		r7 = add(r4,r27)
		v19:18.uh = vunpack(v6.ub)
		v3.h = vsub(v12.h,v0.h)
		v11 = vmem(r4+#0)
	}
	{
		r6 = add(r7,r27)
		r11 = add(r16,#512)
		v13:12.uh = vunpack(v8.ub)
		if (q0) vmem(r11+#0) = v3
	}
	{
		r5 = add(r6,r27)
		v15:14.uh = vunpack(v7.ub)
		v12.h = vsub(v12.h,v0.h)
		v7 = vmem(r7+#0)
	}
	{
		r0 = add(r5,r27)
		v17:16.uh = vunpack(v5.ub)
		v5.h = vsub(v2.h,v0.h)
		v13.h = vsub(v14.h,v0.h)
	}
	{
		r3 = add(r0,r27)
		v15:14.uh = vunpack(v4.ub)
		v8 = vmem(r0+#0)
		if (q0) vmem(r10+#0) = v5
	}
	{
		r10 = add(r16,#256)
		v12.h = vsub(v14.h,v0.h)
		v6 = vmem(r3+#0)
		if (q0) vmem(r26+#0) = v12
	}
	{
		r28 = add(r16,#-256)
		r26 = r16
		v4 = vmem(r3+#1)
		if (q0) vmem(r28+#0) = v13
	}
	{
		v5 = valign(v4,v6,r3)
		v6.h = vsub(v18.h,v0.h)
		v2 = vmem(r0+#1)
		if (q0) vmem(r25+#0) = v12
	}
	{
		r0 = add(r19,#768)
		v2 = valign(v2,v8,r0)
		v8.h = vsub(v16.h,v0.h)
		v3 = vmem(r5+#0)
	}
	{
		r19 = add(r19,#2048)
		v15:14.uh = vunpack(v5.ub)
		v5 = vmem(r6+#0)
		if (q0) vmem(r15+#0) = v8
	}
	{
		v3 = valign(v4,v3,r5)
		v8.h = vsub(v14.h,v0.h)
		v4.cur = vmem(r5+#1)
		if (q0) vmem(r13+#0) = v6
	}
	{
		v4 = vmem(r2+#1)
	}
	{
		r2 = add(r2,r9)
		v4 = valign(v4,v9,r2)
		v9 = vmem(r6+#1)
		if (q0) vmem(r0+#0) = v8
	}
	{
		r15 = add(r16,#-512)
		r13 = add(r16,#-768)
		v8 = valign(v9,v5,r6)
		v5 = vmem(r7+#1)
	}
	{
		r25 = add(r16,#-1024)
		v7 = valign(v5,v7,r7)
		v5 = vmem(r4+#1)
	}
	{
		v5 = valign(v5,v11,r4)
		v6 = vmem(r1+#1)
	}
	{
		nop
		v6 = valign(v6,v10,r1)
	} :endloop0
.LBB131_201:                            //   in Loop: Header=BB131_198 Depth=1
	{
		v11:10.uh = vunpack(v4.ub)
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		v11:10.uh = vunpack(v6.ub)
		v4.h = vsub(v10.h,v0.h)
		r1 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		v6.h = vsub(v10.h,v0.h)
		r3 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v11:10.uh = vunpack(v5.ub)
		if (q0) vmem(r25+#0) = v4
	}
	{
		v5:4.uh = vunpack(v7.ub)
		if (q0) vmem(r13+#0) = v6
	}
	{
		v7:6.uh = vunpack(v8.ub)
		v5.h = vsub(v10.h,v0.h)
		v4.h = vsub(v4.h,v0.h)
	}
	{
		v9:8.uh = vunpack(v3.ub)
		v3.h = vsub(v6.h,v0.h)
		if (q0) vmem(r15+#0) = v5
	}
	{
		v5:4.uh = vunpack(v2.ub)
		v2.h = vsub(v8.h,v0.h)
		if (q0) vmem(r28+#0) = v4
	}
	{
		v3.h = vsub(v4.h,v0.h)
		if (q0) vmem(r16+#0) = v3
	}
	{
		if (q0) vmem(r10+#0) = v2
	}
	{
		if (q0) vmem(r11+#0) = v3
	}
.LBB131_202:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us2155.unr-lcssa"
                                        //   in Loop: Header=BB131_198 Depth=1
	{
		if (p1) jump:nt .LBB131_197
		r7 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
// %bb.203:                             // %"for filter_zeroed.s0.x118.us2152.epil.preheader"
                                        //   in Loop: Header=BB131_198 Depth=1
	{
		r1 = add(r14,r12)
		r4 = memw(r30+##-24736)
		r0 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r0 += asl(r1,#8)
		p2 = cmp.gtu(r23,#1)
	}
	{
		r7 += mpyi(r4,r14)
		r1 = add(r0,#256)
	}
	{
		v2 = vmem(r7+#0)
	}
	{
		r7 = add(r7,r4)
		if (!p2) jump:nt .LBB131_196
		v2 = valign(v3,v2,r7)
		v3.cur = vmem(r7+#1)
	}
// %bb.204:                             // %"for filter_zeroed.s0.x118.us2152.epil"
                                        //   in Loop: Header=BB131_198 Depth=1
	{
		r3 = add(r23,#-2)
		p2 = cmp.gtu(r23,#2)
		v5:4.uh = vunpack(v2.ub)
		v3 = vmem(r7+#0)
	}
	{
		loop0(.LBB131_205,r3)
		r2 = add(r1,#256)
		v2 = vmem(r7+#1)
	}
	{
		r7 = add(r7,r4)
		if (!p2) jump:nt .LBB131_195
		v2 = valign(v2,v3,r7)
		v3.h = vsub(v4.h,v0.h)
	}
	.p2align	4
.LBB131_205:                            // %"for filter_zeroed.s0.x118.us2152.epil"
                                        //   Parent Loop BB131_198 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r0 = r1
		v7:6.uh = vunpack(v2.ub)
		v4 = vmem(r7+#0)
		if (q0) vmem(r0+#0) = v3
	}
	{
		r1 = r2
		r2 = add(r2,#256)
		v3.h = vsub(v6.h,v0.h)
		v2 = vmem(r7+#1)
	}
	{
		r7 = add(r7,r4)
		v2 = valign(v2,v4,r7)
	} :endloop0
	{
		jump .LBB131_195
	}
.LBB131_206:                            // %"for filter_zeroed.s0.y115.us.us.preheader"
	{
		r8 = asl(r7,#3)
		r9 = #0
		r12 = #0
		r0 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r28 = extractu(r0,#3,#8)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r15 = and(r0,#-8)
		p1 = cmp.eq(r28,#0)
		r0 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r0,#6)
		r0 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		loop1(.LBB131_210,r0)
		jump .LBB131_210
	}
	.p2align	4
.LBB131_207:                            //   in Loop: Header=BB131_210 Depth=1
	{
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
.LBB131_208:                            //   in Loop: Header=BB131_210 Depth=1
	{
		v2 = valign(v3,v2,r7)
		r7 = memw(r30+##-24736)
	}                                       // 4-byte Folded Reload
	{
		v3:2.uh = vunpack(v2.ub)
	}
	{
		v2.h = vsub(v2.h,v0.h)
		vmem(r13++#2) = v2.new
	}
.LBB131_209:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us.split.us2160.us"
                                        //   in Loop: Header=BB131_210 Depth=1
	{
		r2 = memw(r30+##-23672)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r2)
		memw(r30+##-24728) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r24 = add(r24,r0)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r12 = add(r12,r0)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_221
	}
.Ltmp29:                                // Block address taken
.LBB131_210:                            // %"for filter_zeroed.s0.y115.us.us"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_212 Depth 2
                                        //     Child Loop BB131_218 Depth 2
	{
		r10 = #0
		if (!p0) jump:nt .LBB131_214
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
// %bb.211:                             //   in Loop: Header=BB131_210 Depth=1
	{
		r1 = lsr(r15,#3)
		r5 = add(r0,r7)
		r16 = r7
		v3 = vmem(r0+#0)
	}
	{
		r3 = add(r5,r7)
		r10 = add(r9,#8)
		p2 = cmp.gtu(r1,#1)
		v2 = vmem(r5+#0)
	}
	{
		r4 = add(r3,r7)
		r13 = add(r24,#2048)
		r14 = r24
		v5 = vmem(r3+#0)
	}
	{
		r7 = add(r4,r7)
		r11 = r24
		v7 = vmem(r4+#0)
	}
	{
		r6 = add(r7,r16)
		v3 = valign(v4,v3,r0)
		v4.cur = vmem(r0+#1)
	}
	{
		r2 = add(r6,r16)
		v2 = valign(v6,v2,r5)
		v6.cur = vmem(r5+#1)
	}
	{
		r5 = add(r2,r16)
		v8 = vmem(r7+#0)
	}
	{
		v9 = vmem(r6+#0)
	}
	{
		v6 = vmem(r3+#1)
	}
	{
		r3 = add(r0,r8)
		v6 = valign(v6,v5,r3)
		v5 = vmem(r4+#1)
	}
	{
		v7 = valign(v5,v7,r4)
		v4 = vmem(r7+#1)
	}
	{
		r7 = add(r1,#-1)
		v8 = valign(v4,v8,r7)
		v5 = vmem(r2+#0)
	}
	{
		loop0(.LBB131_212,r7)
		v10 = vmem(r5+#0)
	}
	{
		v4 = vmem(r6+#1)
	}
	{
		v4 = valign(v4,v9,r6)
		v9 = vmem(r2+#1)
	}
	{
		v5 = valign(v9,v5,r2)
		v9 = vmem(r5+#1)
	}
	{
		if (!p2) jump:nt .LBB131_213
		v9 = valign(v9,v10,r5)
	}
	.p2align	4
.LBB131_212:                            // %"for filter_zeroed.s0.x118.us.us2157.us"
                                        //   Parent Loop BB131_210 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r1 = add(r3,r16)
		r11 = r13
		v13:12.uh = vunpack(v9.ub)
		v10 = vmem(r3+#0)
	}
	{
		r4 = add(r1,r16)
		r13 = add(r13,#2048)
		v19:18.uh = vunpack(v7.ub)
		v11 = vmem(r1+#0)
	}
	{
		r7 = add(r4,r16)
		r10 = add(r10,#8)
		v7:6.uh = vunpack(v6.ub)
		v13 = vmem(r4+#0)
	}
	{
		r6 = add(r7,r16)
		v15:14.uh = vunpack(v5.ub)
		v7.h = vsub(v12.h,v0.h)
		vmem(r14+#6) = v7.new
	}
	{
		r5 = add(r6,r16)
		v17:16.uh = vunpack(v4.ub)
	}
	{
		r0 = add(r5,r16)
		v9:8.uh = vunpack(v8.ub)
		v4 = vmem(r5+#0)
	}
	{
		r2 = add(r0,r16)
		v21:20.uh = vunpack(v3.ub)
		v9.h = vsub(v14.h,v0.h)
		v3 = vmem(r0+#0)
	}
	{
		v15:14.uh = vunpack(v2.ub)
		v2.h = vsub(v8.h,v0.h)
		v7 = vmem(r2+#0)
	}
	{
		v4 = valign(v5,v4,r5)
		v5.cur = vmem(r5+#1)
		vmem(r14+#0) = v2
	}
	{
		v5 = vmem(r0+#1)
		vmem(r14+#4) = v9
	}
	{
		v5 = valign(v5,v3,r0)
		v3 = vmem(r2+#1)
	}
	{
		v9 = valign(v3,v7,r2)
		v7 = vmem(r6+#0)
	}
	{
		v3 = vmem(r3+#1)
	}
	{
		v3 = valign(v3,v10,r3)
		v10.h = vsub(v16.h,v0.h)
		v2 = vmem(r6+#1)
		vmem(r14+#2) = v10.new
	}
	{
		r3 = add(r3,r8)
		v8 = valign(v2,v7,r6)
		v2.h = vsub(v18.h,v0.h)
		vmem(r14+#-2) = v2.new
	}
	{
		v10 = vmem(r7+#0)
	}
	{
		v7 = valign(v2,v10,r7)
		v2.cur = vmem(r7+#1)
	}
	{
		v2.h = vsub(v6.h,v0.h)
		v10.h = vsub(v20.h,v0.h)
		vmem(r14+#-4) = v2.new
	}
	{
		v6 = valign(v2,v13,r4)
		v2.cur = vmem(r4+#1)
		vmem(r14+#-8) = v10
	}
	{
		r14 = r11
		v2.h = vsub(v14.h,v0.h)
		vmem(r14+#-6) = v2.new
	}
	{
		v2 = vmem(r1+#1)
	}
	{
		nop
		v2 = valign(v2,v11,r1)
	} :endloop0
.LBB131_213:                            //   in Loop: Header=BB131_210 Depth=1
	{
		r7 = r16
		v11:10.uh = vunpack(v3.ub)
		r0 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		p2 = r0
		v3:2.uh = vunpack(v2.ub)
		r3 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-23560)
		r0 = memw(r30+##-24728)
	}                                       // 4-byte Folded Reload
	{
		v3.h = vsub(v10.h,v0.h)
		v2.h = vsub(v2.h,v0.h)
		vmem(r11+#-8) = v3.new
	}
	{
		v3:2.uh = vunpack(v7.ub)
		vmem(r11+#-6) = v2
	}
	{
		v11:10.uh = vunpack(v6.ub)
		v2.h = vsub(v2.h,v0.h)
		vmem(r11+#-2) = v2.new
	}
	{
		v7:6.uh = vunpack(v8.ub)
		v3.h = vsub(v10.h,v0.h)
		vmem(r11+#-4) = v3.new
	}
	{
		v3:2.uh = vunpack(v4.ub)
	}
	{
		v3.h = vsub(v6.h,v0.h)
		v2.h = vsub(v2.h,v0.h)
		vmem(r11+#0) = v3.new
	}
	{
		v3:2.uh = vunpack(v9.ub)
		vmem(r11+#2) = v2
	}
	{
		v5:4.uh = vunpack(v5.ub)
		v2.h = vsub(v2.h,v0.h)
		vmem(r11+#6) = v2.new
	}
	{
		v3.h = vsub(v4.h,v0.h)
		vmem(r11+#4) = v3.new
	}
.LBB131_214:                            // %"end for filter_zeroed.s0.x119.loopexit.split.us.split.us2160.us.unr-lcssa"
                                        //   in Loop: Header=BB131_210 Depth=1
	{
		if (p1) jump:nt .LBB131_209
	}
// %bb.215:                             // %"for filter_zeroed.s0.x118.us.us2157.us.epil.preheader"
                                        //   in Loop: Header=BB131_210 Depth=1
	{
		r2 = r7
		r7 = r0
		r0 = add(r10,r12)
		r13 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r2,r10)
		r13 += asl(r0,#8)
		p2 = cmp.gtu(r28,#1)
	}
	{
		r4 = add(r7,r2)
	}
	{
		v2 = vmem(r7+#0)
	}
	{
		if (!p2) jump:nt .LBB131_207
		v3 = vmem(r7+#1)
	}
// %bb.216:                             // %"for filter_zeroed.s0.x118.us.us2157.us.epil"
                                        //   in Loop: Header=BB131_210 Depth=1
	{
		p2 = cmp.gtu(r28,#2)
		r7 = add(r4,r2)
		v4 = valign(v3,v2,r7)
		v2 = vmem(r4+#0)
	}
	{
		if (!p2) jump:nt .LBB131_220
		v3 = vmem(r4+#1)
	}
// %bb.217:                             // %"for filter_zeroed.s0.x118.us.us2157.us.epil"
                                        //   in Loop: Header=BB131_210 Depth=1
	{
		r0 = add(r28,#-3)
		p2 = cmp.gtu(r28,#3)
		r6 = add(r7,r2)
		v7:6.uh = vunpack(v4.ub)
	}
	{
		loop0(.LBB131_218,r0)
		r4 = r7
		v4 = valign(v3,v2,r4)
		v2 = vmem(r7+#0)
	}
	{
		if (!p2) jump:nt .LBB131_219
		v3 = vmem(r7+#1)
	}
	.p2align	4
.LBB131_218:                            // %"for filter_zeroed.s0.x118.us.us2157.us.epil"
                                        //   Parent Loop BB131_210 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r4 = r6
		v7:6.uh = vunpack(v4.ub)
		v5.h = vsub(v6.h,v0.h)
		vmem(r13++#2) = v5.new
	}
	{
		r6 = add(r6,r2)
		r7 = r4
		v4 = valign(v3,v2,r7)
		v2 = vmem(r6+#0)
	}
	{
		nop
		v3 = vmem(r4+#1)
	} :endloop0
.LBB131_219:                            //   in Loop: Header=BB131_210 Depth=1
	{
		v5.h = vsub(v6.h,v0.h)
		vmem(r13++#2) = v5.new
	}
.LBB131_220:                            //   in Loop: Header=BB131_210 Depth=1
	{
		r7 = r4
		v5:4.uh = vunpack(v4.ub)
		r2 = memw(r30+##-23568)
	}                                       // 4-byte Folded Reload
	{
		v4.h = vsub(v4.h,v0.h)
		r0 = memw(r30+##-24728)
		vmem(r13++#2) = v4.new
	}                                       // 4-byte Folded Reload
	{
		p2 = r2
		jump .LBB131_208
	}
.LBB131_221:                            // %"produce sum_filter132"
	{
		v5:4.w = vsub(v5:4.w,v5:4.w)
		q2 = vcmp.gt(v1.w,v2.w)
		r0 = memw(r30+#-3896)
		v2.cur = vmem(r20+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-28032)
		q3 = vcmp.gt(v1.w,v2.w)
		v2.cur = vmem(r21+#0)
		if (q2) vmem(r3+#0) = v4
	}
	{
		r0 = memw(r30+##-23552)
		if (q3) vmem(r0+#0) = v4
	}                                       // 4-byte Folded Reload
	{
		q1 = vcmp.gt(v1.w,v2.w)
		v2.cur = vmem(r18+#0)
	}
	{
		r0 = memw(r30+#-2864)
		if (q1) vmem(r0+#0) = v5
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#256)
		v0 = vmem(r22+#0)
		memw(r30+##-23240) = r0.new
	}                                       // 4-byte Folded Spill
	{
		q0 = vcmp.gt(v1.w,v0.w)
		v0 = vxor(v0,v0)
		r0 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r0 = extractu(r0,#1,#8)
		if (q0) vmem(r1+#0) = v5
	}
	{
		r0 = memw(r30+##-5696)
		memw(r30+#-2352) = r0
	}                                       // 4-byte Folded Reload
	{
		r1 = clrbit(r0,#0)
		p0 = cmp.eq(r0,#1)
		memw(r30+##-4784) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r2 = p0
		memw(r30+##-23224) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = ##16843009
	}
	{
		v1 = vand(q0,r2)
		r2 = add(r30,#-5424)
	}
	{
		if (!p2) jump:nt .LBB131_239
		vmemu(r2+#0) = v1
	}                                       // 256-byte Folded Spill
// %bb.222:                             // %"produce sum_filter132"
	{
		r2 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		if (!p0.new) jump:nt .LBB131_239
	}
// %bb.223:                             // %"for sum_filter.s1.r19$y133.preheader.split.us"
	{
		r1:0 = combine(#1,#-1)
		r7 = #64
		v1 = vxor(v1,v1)
	}
	{
		v2 = vand(q1,r0)
		v3 = vand(q0,r0)
	}
	{
		v4 = vand(q2,r0)
		v5 = vand(q3,r0)
	}
	{
		v6.b = vsplat(r1)
		r1 = #16
		v2.b = vpacke(v1.h,v2.h)
	}
	{
		v3.b = vpacke(v1.h,v3.h)
	}
	{
		v4.b = vpacke(v1.h,v4.h)
	}
	{
		v2 = vror(v2,r7)
	}
	{
		v5.b = vpacke(v1.h,v5.h)
		v2 = vor(v2,v4)
	}
	{
		q0 = vand(v2,r0)
		v3 = vror(v3,r7)
	}
	{
		v3 = vand(q0,r0)
		v2 = vor(v3,v5)
	}
	{
		q0 = vand(v2,r0)
	}
	{
		v2 = vand(q0,r0)
		v3.b = vpacke(v1.h,v3.h)
	}
	{
		v2.b = vpacke(v1.h,v2.h)
	}
	{
		v3 = vror(v3,r7)
	}
	{
		v2 = vor(v3,v2)
	}
	{
		q0 = vand(v2,r0)
		r0 = #32
	}
	{
		v2 = vmux(q0,v6,v1)
	}
	{
		v3 = valign(v2,v2,r7)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		r0 = #8
		v3 = valign(v2,v2,r0)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		v3 = valign(v2,v2,r1)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		r1:0 = combine(#0,#0)
		v3 = valign(v2,v2,r0)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
		memw(r30+#-1328) = r0
	}                                       // 4-byte Folded Spill
	{
		v3 = valign(v2,v2,#4)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		v3 = valign(v2,v2,#2)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		v3 = valign(v2,v2,#1)
	}
	{
		v2.ub = vmax(v2.ub,v3.ub)
	}
	{
		r0 = vextract(v2,r1)
	}
	{
		p0 = tstbit(r0,#0); if (p0.new) jump:t .LBB131_232
	}
// %bb.224:                             // %"for sum_filter.s1.r19$y133.preheader.split.us.split.us"
	{
		r4 = add(r30,#-5424)
		v7:6.w = vsub(v7:6.w,v7:6.w)
		r0 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-5696)
		r2 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r0 = extractu(r0,#3,#8)
		r1 = and(r1,#-8)
		r5 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r2,#6)
		v3:2.w = vunpack(v6.h)
		r2 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		r1 = sub(#0,r1)
		p1 = cmp.eq(r0,#0)
		v5:4.w = vunpack(v7.h)
		v3:2.w = vadd(v3:2.w,v7:6.w)
	}
	{
		loop1(.LBB131_226,r2)
		v1 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		r2 = ##16843009
		v5:4.w = vadd(v5:4.w,v7:6.w)
	}
	{
		q0 = vand(v1,r2)
		jump .LBB131_226
		r4 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_225:                            // %"end for sum_filter.s1.r19$x137.loopexit.split.us.us.us"
                                        //   in Loop: Header=BB131_226 Depth=1
	{
		r3 = r6
		nop
		nop
	} :endloop1
	{
		jump .LBB131_239
	}
.Ltmp30:                                // Block address taken
.LBB131_226:                            // %"for sum_filter.s1.r19$y133.us.us"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_228 Depth 2
                                        //     Child Loop BB131_231 Depth 2
	{
		r6 = r3
		if (!p0) jump:nt .LBB131_229
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
// %bb.227:                             //   in Loop: Header=BB131_226 Depth=1
	{
		r2 = sub(#0,r1)
	}
	{
		r2 = lsr(r2,#3)
	}
	{
		loop0(.LBB131_228,r2)
	}
	.p2align	4
.Ltmp31:                                // Block address taken
.LBB131_228:                            // %"for sum_filter.s1.r19$x136.us.us.us"
                                        //   Parent Loop BB131_226 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r2 = add(r3,#-28032)
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		if (q3) vmem(r2+#0) = v2
	}
	{
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		nop
		if (q3) vmem(r2+#0) = v2
	} :endloop0
.LBB131_229:                            // %"end for sum_filter.s1.r19$x137.loopexit.split.us.us.us.unr-lcssa"
                                        //   in Loop: Header=BB131_226 Depth=1
	{
		if (p1) jump:nt .LBB131_225
	}
// %bb.230:                             //   in Loop: Header=BB131_226 Depth=1
	{
		loop0(.LBB131_231,r0)
	}
	.p2align	4
.Ltmp32:                                // Block address taken
.LBB131_231:                            // %"for sum_filter.s1.r19$x136.us.us.us.epil"
                                        //   Parent Loop BB131_226 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		r2 = add(r3,#-28032)
		if (q0) vmem(r4+#0) = v3
	}
	{
		if (q2) vmem(r6+#0) = v4
	}
	{
		if (q1) vmem(r5+#0) = v5
	}
	{
		nop
		if (q3) vmem(r2+#0) = v2
	} :endloop0
	{
		jump .LBB131_225
	}
.LBB131_232:                            // %"for sum_filter.s1.r19$y133.us.preheader"
	{
		r2 = add(r30,#-5424)
		r0 = memw(r30+#-2352)
		r7 = memw(r30+##-23240)
	}                                       // 4-byte Folded Reload
	{
		r12 = #68
		p0 = cmp.eq(r0,#0)
		r0 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		v2 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		loop1(.LBB131_234,r0)
		r0 = ##16843009
	}
	{
		q0 = vand(v2,r0)
		r0 = memw(r30+##-23224)
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		jump .LBB131_234
	}
	.p2align	4
.LBB131_233:                            // %"end for sum_filter.s1.r19$x137.loopexit.split.us2165"
                                        //   in Loop: Header=BB131_234 Depth=1
	{
		r0 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#1)
		memw(r30+#-1328) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+#-2096)
		r7 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r0)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_239
	}
.Ltmp33:                                // Block address taken
.LBB131_234:                            // %"for sum_filter.s1.r19$y133.us"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_236 Depth 2
	{
		r3 = #0
		if (p1) jump:nt .LBB131_237
		memw(r30+#-1584) = r7
	}                                       // 4-byte Folded Spill
// %bb.235:                             //   in Loop: Header=BB131_234 Depth=1
	{
		r2 = r7
		r1 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-4784)
	}                                       // 4-byte Folded Reload
	{
		r0 = lsr(r0,#1)
	}
	{
		loop0(.LBB131_236,r0)
	}
	.p2align	4
.Ltmp34:                                // Block address taken
.LBB131_236:                            // %"for sum_filter.s1.r19$x136.us2162"
                                        //   Parent Loop BB131_234 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	{
		v3 = v1
		v6 = v1
		r7 = memw(r30+#-3896)
		memw(r30+#-304) = r2
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r7,#-29184)
		r28 = add(r7,#-28032)
		r0 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		r13 = add(r7,#-28928)
		v13:12 = vcombine(v1,v1)
		v5 = vmem(r28+#0)
		memw(r30+#-560) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = setbit(r6,#7)
		r3 = setbit(r13,#7)
		v2 = vmem(r0+#0)
		vmem(r6+#0) = v5
	}
	{
		v5:4 = vcombine(v1,v1)
		v15:14 = vcombine(v1,v1)
	}
	{
		r0 = memw(r6+#124)
		vmem(r0+#0) = v2
	}
	{
		r9:8 = memd(r6+#128)
		r17:16 = memd(r6+#136)
	}
	{
		v3.w = vinsert(r8)
		r11:10 = memd(r6+#192)
		memw(r30+#-1072) = r3

	} :mem_noshuf
	{
		v4.w = vinsert(r10)
		v17:16.w = vunpack(v2.h)
		r19:18 = memd(r6+#0)
		v2.cur = vmem(r2+#-2)
	}
	{
		v5.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
		r21:20 = memd(r6+#64)
		v2 = vmem(r2+#-1)
	}
	{
		v3.w = vinsert(r9)
		v6.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
		r3:2 = memd(r6+#200)
	}
	{
		v4.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r6+#8)
		r15:14 = memd(r6+#144)
	}
	{
		v5.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
		r23:22 = memd(r6+#72)
		r11:10 = memd(r6+#152)
	}
	{
		v6.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
		r19:18 = memd(r6+#160)
		r21:20 = memd(r6+#224)
	}
	{
		v3.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
		r9:8 = memd(r6+#168)
		v18 = vmem(r1+#0)
	}
	{
		v4.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r4)
		v7 = valign(v6,v6,#4)
	}
	{
		v7.w = vinsert(r22)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r6+#208)
	}
	{
		v5.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r25:24 = memd(r6+#16)
	}
	{
		v3.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
		r27:26 = memd(r6+#80)
	}
	{
		v7.w = vinsert(r23)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r6+#184)
	}
	{
		v4.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r15)
		v5 = valign(v5,v5,#4)
		r15:14 = memd(r6+#216)
	}
	{
		v5.w = vinsert(r24)
		v7 = valign(v7,v7,#4)
		r17:16 = memd(r6+#24)
	}
	{
		v7.w = vinsert(r26)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r6+#88)
	}
	{
		v4.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r10)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r25)
		v7 = valign(v7,v7,#4)
		r25:24 = memd(r6+#176)
	}
	{
		v7.w = vinsert(r27)
		v4 = valign(v4,v4,#4)
		r27:26 = memd(r6+#32)
	}
	{
		v4.w = vinsert(r14)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r6+#96)
	}
	{
		v3.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
		r11:10 = memd(r6+#232)
	}
	{
		v5.w = vinsert(r16)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r15)
		v3 = valign(v3,v3,#4)
		r15:14 = memd(r6+#40)
	}
	{
		v3.w = vinsert(r18)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r17)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r5)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r6+#104)
	}
	{
		v4.w = vinsert(r20)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r19)
		v5 = valign(v5,v5,#4)
		r19:18 = memd(r6+#240)
	}
	{
		v5.w = vinsert(r26)
		v7 = valign(v7,v7,#4)
		r17:16 = memd(r6+#48)
	}
	{
		v7.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
		r21:20 = memd(r6+#112)
	}
	{
		v3.w = vinsert(r8)
		r8 = #68
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r27)
		v7 = valign(v7,v7,#4)
		r27:26 = memd(r6+#248)
	}
	{
		v7.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
		r3 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r9)
		r9 = add(r7,#-28672)
		v5 = valign(v5,v5,#4)
		v19 = vmem(r3+#0)
	}
	{
		v5.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r15)
		v7 = valign(v7,v7,#4)
		r15:14 = memd(r6+#56)
	}
	{
		v7.w = vinsert(r5)
		r6 = add(r7,#-28416)
		v4 = valign(v4,v4,#4)
		r2 = memw(r6+#120)
	}
	{
		v4.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
		r7 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r16)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r19)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r17)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r15)
		v8 = vror(v3,r12)
	}
	{
		v3 = valign(v7,v7,#4)
		v7:6 = vcombine(v1,v1)
	}
	{
		r0 = setbit(r6,#7)
		v3.w = vinsert(r0)
		v4 = valign(v4,v4,#4)
		memw(r30+#-816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r12 = setbit(r9,#7)
		v4 = vror(v5,r12)
		v9:8 = vcombine(v1,v1)
		v21 = vor(v8,v4)
	}
	{
		v5 = valign(v3,v3,#4)
	}
	{
		v11:10.w = vunpack(v2.h)
		v20 = vor(v4,v5)
	}
	{
		v17:16.w = vadd(v17:16.w,v21:20.w)
		v3:2 = vcombine(v1,v1)
	}
	{
		v5:4 = vcombine(v1,v1)
		if (q3) vmem(r28+#0) = v16
	}
	{
		if (q0) vmem(r7+#0) = v17
	}
	{
		vmem(r13+#0) = v19
	}
	{
		r0 = memw(r30+#-1072)
		r3:2 = memd(r13+#0)
	}                                       // 4-byte Folded Reload
	{
		v14.w = vinsert(r2)
		r5:4 = memd(r13+#64)
	}
	{
		v15.w = vinsert(r4)
		vmem(r0+#0) = v18
	}
	{
		v14 = valign(v14,v14,#4)
		r27:26 = memd(r13+#128)
		r17:16 = memd(r13+#136)
	}
	{
		v12.w = vinsert(r26)
		v14.w = vinsert(r3)
		v15 = valign(v15,v15,#4)
		r25:24 = memd(r13+#192)
	}
	{
		v13.w = vinsert(r24)
		v15.w = vinsert(r5)
		r19:18 = memd(r13+#200)
		r3:2 = memd(r13+#144)
	}
	{
		v12 = valign(v12,v12,#4)
		r21:20 = memd(r13+#8)
		r15:14 = memd(r13+#176)
	}
	{
		v12.w = vinsert(r27)
		v13 = valign(v13,v13,#4)
		r1:0 = memd(r13+#72)
		r27:26 = memd(r13+#208)
	}
	{
		v13.w = vinsert(r25)
		v14 = valign(v14,v14,#4)
		r5:4 = memd(r13+#16)
	}
	{
		v14.w = vinsert(r20)
		v12 = valign(v12,v12,#4)
		r25:24 = memd(r13+#80)
	}
	{
		v12.w = vinsert(r16)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r0)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r18)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r21)
		v12 = valign(v12,v12,#4)
		r21:20 = memd(r13+#152)
	}
	{
		v12.w = vinsert(r17)
		v15 = valign(v15,v15,#4)
		r17:16 = memd(r13+#216)
	}
	{
		v15.w = vinsert(r1)
		v13 = valign(v13,v13,#4)
		r1:0 = memd(r13+#24)
	}
	{
		v13.w = vinsert(r19)
		v14 = valign(v14,v14,#4)
		r19:18 = memd(r13+#88)
	}
	{
		v14.w = vinsert(r4)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r2)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r24)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r26)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r5)
		v12 = valign(v12,v12,#4)
		r5:4 = memd(r13+#160)
	}
	{
		v12.w = vinsert(r3)
		v15 = valign(v15,v15,#4)
		r3:2 = memd(r13+#224)
	}
	{
		v15.w = vinsert(r25)
		v13 = valign(v13,v13,#4)
		r11:10 = memd(r13+#32)
	}
	{
		v13.w = vinsert(r27)
		v14 = valign(v14,v14,#4)
		r25:24 = memd(r13+#96)
		r27:26 = memd(r13+#168)
	}
	{
		v14.w = vinsert(r0)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r20)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r18)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r16)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r1)
		v12 = valign(v12,v12,#4)
		r1:0 = memd(r13+#232)
	}
	{
		v12.w = vinsert(r21)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r19)
		v13 = valign(v13,v13,#4)
		r19:18 = memd(r13+#40)
	}
	{
		v13.w = vinsert(r17)
		v14 = valign(v14,v14,#4)
		r21:20 = memd(r13+#104)
		r17:16 = memd(r13+#184)
	}
	{
		v14.w = vinsert(r10)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r4)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r24)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r2)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r11)
		v12 = valign(v12,v12,#4)
		r11:10 = memd(r13+#240)
	}
	{
		v12.w = vinsert(r5)
		v15 = valign(v15,v15,#4)
		r5:4 = memd(r13+#248)
	}
	{
		v15.w = vinsert(r25)
		v13 = valign(v13,v13,#4)
		r25:24 = memd(r13+#48)
	}
	{
		v13.w = vinsert(r3)
		v14 = valign(v14,v14,#4)
		r23:22 = memd(r13+#112)
		r3:2 = memd(r13+#56)
	}
	{
		v14.w = vinsert(r18)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r26)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r20)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r0)
		v14 = valign(v14,v14,#4)
		r0 = memw(r13+#120)
	}
	{
		v14.w = vinsert(r19)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r27)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r21)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r1)
		v16 = valign(v12,v12,#4)
		r1 = memw(r13+#124)
		v12 = vmem(r7+#0)
	}
	{
		v16.w = vinsert(r14)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r24)
		v17 = valign(v13,v13,#4)
		v13 = vmem(r28+#0)
	}
	{
		v17.w = vinsert(r10)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r22)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r25)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r15)
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r11)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r23)
		v16 = valign(v16,v16,#4)
	}
	{
		v16.w = vinsert(r16)
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r4)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r2)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r0)
		v18 = valign(v16,v16,#4)
		r0 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v18.w = vinsert(r17)
		v16 = valign(v14,v14,#4)
	}
	{
		v16.w = vinsert(r3)
		v14 = valign(v15,v15,#4)
	}
	{
		v14.w = vinsert(r1)
		v17 = valign(v17,v17,#4)
	}
	{
		v17.w = vinsert(r5)
		v15 = vror(v18,r8)
	}
	{
		v16 = vror(v16,r8)
	}
	{
		v14 = valign(v14,v14,#4)
	}
	{
		v17 = valign(v17,v17,#4)
		v14 = vor(v16,v14)
	}
	{
		v15 = vor(v15,v17)
	}
	{
		v11:10.w = vadd(v11:10.w,v15:14.w)
	}
	{
		r0 = memw(r30+##-23552)
		if (q2) vmem(r0+#0) = v10
	}                                       // 4-byte Folded Reload
	{
		if (q1) vmem(r0+#0) = v11
	}
	{
		vmem(r9+#0) = v13
	}
	{
		r1:0 = memd(r9+#0)
		vmem(r12+#0) = v12
	}
	{
		v9.w = vinsert(r0)
		r3:2 = memd(r9+#64)
		r27:26 = memd(r9+#136)
	}
	{
		v8.w = vinsert(r2)
		r5:4 = memd(r9+#128)
		r25:24 = memd(r9+#200)
	}
	{
		v6.w = vinsert(r4)
		v9 = valign(v9,v9,#4)
		r13:12 = memd(r9+#192)
		r17:16 = memd(r9+#8)
	}
	{
		v7.w = vinsert(r12)
		v9.w = vinsert(r1)
		v8 = valign(v8,v8,#4)
		r23:22 = memd(r9+#72)
	}
	{
		v8.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r9+#144)
	}
	{
		v6.w = vinsert(r5)
		v7 = valign(v7,v7,#4)
		r5:4 = memd(r9+#208)
	}
	{
		v7.w = vinsert(r13)
		v9 = valign(v9,v9,#4)
		r21:20 = memd(r9+#16)
		r13:12 = memd(r9+#152)
	}
	{
		v9.w = vinsert(r16)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r9+#80)
	}
	{
		v6.w = vinsert(r26)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r22)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r24)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r17)
		v6 = valign(v6,v6,#4)
		r17:16 = memd(r9+#216)
	}
	{
		v6.w = vinsert(r27)
		v8 = valign(v8,v8,#4)
		r27:26 = memd(r9+#24)
	}
	{
		v8.w = vinsert(r23)
		v7 = valign(v7,v7,#4)
		r19:18 = memd(r9+#88)
		r23:22 = memd(r9+#160)
	}
	{
		v7.w = vinsert(r25)
		v9 = valign(v9,v9,#4)
		r25:24 = memd(r9+#224)
	}
	{
		v9.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r0)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r4)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r21)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r1)
		v8 = valign(v8,v8,#4)
		r1:0 = memd(r9+#32)
	}
	{
		v8.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r9+#96)
	}
	{
		v7.w = vinsert(r5)
		v9 = valign(v9,v9,#4)
		r5:4 = memd(r9+#168)
	}
	{
		v9.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r12)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r18)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r16)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r9+#232)
	}
	{
		v6.w = vinsert(r13)
		v8 = valign(v8,v8,#4)
		r15:14 = memd(r9+#40)
		r13:12 = memd(r9+#176)
	}
	{
		v8.w = vinsert(r19)
		v7 = valign(v7,v7,#4)
		r19:18 = memd(r9+#104)
		r11:10 = memd(r9+#240)
	}
	{
		v7.w = vinsert(r17)
		v9 = valign(v9,v9,#4)
		r21:20 = memd(r9+#48)
	}
	{
		v9.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r22)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r24)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r9+#184)
	}
	{
		v6.w = vinsert(r23)
		v8 = valign(v8,v8,#4)
		r17:16 = memd(r9+#248)
	}
	{
		v8.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r9+#112)
		r23:22 = memd(r9+#56)
	}
	{
		v7.w = vinsert(r25)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		v8 = valign(v8,v8,#4)
		r4 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v8.w = vinsert(r18)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r26)
		v9 = valign(v9,v9,#4)
		r4 = memw(r9+#120)
		v12 = vmem(r4+#0)
	}
	{
		v9.w = vinsert(r15)
		v10 = valign(v6,v6,#4)
	}
	{
		v10.w = vinsert(r5)
		v8 = valign(v8,v8,#4)
		r5 = memw(r9+#124)
		r9 = memw(r30+#-304)
	}
	{
		v8.w = vinsert(r19)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r27)
		v9 = valign(v9,v9,#4)
		v6 = vmem(r9+#1)
	}
	{
		v9.w = vinsert(r20)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r12)
		v11 = valign(v8,v8,#4)
	}
	{
		v11.w = vinsert(r2)
		v13 = valign(v7,v7,#4)
		r2 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v13.w = vinsert(r10)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r21)
		v10 = valign(v10,v10,#4)
		v8 = vmem(r2+#0)
	}
	{
		v10.w = vinsert(r13)
		v11 = valign(v11,v11,#4)
		r2 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v11.w = vinsert(r3)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r11)
		v9 = valign(v9,v9,#4)
		v7 = vmem(r2+#0)
	}
	{
		v9.w = vinsert(r22)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r0)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r4)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r16)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r23)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r1)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r5)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r17)
		v14 = vror(v10,r8)
	}
	{
		v10 = valign(v11,v11,#4)
	}
	{
		v11 = valign(v13,v13,#4)
	}
	{
		v9 = vror(v9,r8)
		v11 = vor(v14,v11)
	}
	{
		v13:12.w = vunpack(v12.h)
		v10 = vor(v9,v10)
	}
	{
		v11:10.w = vadd(v13:12.w,v11:10.w)
	}
	{
		if (q3) vmem(r28+#0) = v10
	}
	{
		if (q0) vmem(r7+#0) = v11
	}
	{
		vmem(r6+#0) = v8
	}
	{
		r0 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = memd(r6+#0)
		vmem(r0+#0) = v7
	}
	{
		v4.w = vinsert(r0)
		r3:2 = memd(r6+#64)
		r13:12 = memd(r6+#136)
	}
	{
		v5.w = vinsert(r2)
		r5:4 = memd(r6+#128)
		r25:24 = memd(r6+#200)
	}
	{
		v3.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r6+#192)
		r27:26 = memd(r6+#8)
	}
	{
		v2.w = vinsert(r22)
		v4.w = vinsert(r1)
		v5 = valign(v5,v5,#4)
		r17:16 = memd(r6+#72)
	}
	{
		v5.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r6+#144)
	}
	{
		v3.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
		r5:4 = memd(r6+#208)
	}
	{
		v2.w = vinsert(r23)
		v4 = valign(v4,v4,#4)
		r19:18 = memd(r6+#16)
		r23:22 = memd(r6+#152)
	}
	{
		v4.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r6+#80)
		r21:20 = memd(r6+#216)
	}
	{
		v3.w = vinsert(r12)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r16)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r27)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r6+#24)
	}
	{
		v3.w = vinsert(r13)
		v5 = valign(v5,v5,#4)
		r13:12 = memd(r6+#88)
	}
	{
		v5.w = vinsert(r17)
		v2 = valign(v2,v2,#4)
		r17:16 = memd(r6+#160)
	}
	{
		v2.w = vinsert(r25)
		v4 = valign(v4,v4,#4)
		r15:14 = memd(r6+#224)
		r25:24 = memd(r6+#168)
	}
	{
		v4.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r0)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r4)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r19)
		v3 = valign(v3,v3,#4)
		r19:18 = memd(r6+#32)
	}
	{
		v3.w = vinsert(r1)
		v5 = valign(v5,v5,#4)
		r1:0 = memd(r6+#96)
	}
	{
		v5.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r6+#232)
	}
	{
		v2.w = vinsert(r5)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r12)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r27)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r6+#40)
	}
	{
		v3.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
		r23:22 = memd(r6+#104)
	}
	{
		v5.w = vinsert(r13)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
		r21:20 = memd(r6+#176)
	}
	{
		v4.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r6+#240)
	}
	{
		v3.w = vinsert(r16)
		v5 = valign(v5,v5,#4)
		r11:10 = memd(r6+#48)
	}
	{
		v5.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r14)
		v7 = valign(v3,v3,#4)
	}
	{
		v7.w = vinsert(r17)
		v3 = valign(v5,v5,#4)
		r17:16 = memd(r6+#112)
	}
	{
		v3.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v4.w = vinsert(r19)
		v5 = valign(v2,v2,#4)
		r19:18 = memd(r6+#184)
	}
	{
		v5.w = vinsert(r15)
		v3 = valign(v3,v3,#4)
		r13:12 = memd(r6+#248)
	}
	{
		v3.w = vinsert(r22)
		v2 = valign(v4,v4,#4)
		r0 = memw(r6+#120)
	}
	{
		v2.w = vinsert(r26)
		v4 = valign(v7,v7,#4)
	}
	{
		v4.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r27)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r6+#56)
	}
	{
		v3.w = vinsert(r23)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r20)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r21)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r5)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r11)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r18)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r12)
		r12 = #68
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		r2 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r0)
		r2 = add(r2,#512)
		v4 = valign(v4,v4,#4)
		r0 = memw(r6+#124)
	}
	{
		v4.w = vinsert(r19)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r13)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r0)
		v4 = vror(v4,r12)
		r0 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r3,#2)
		v5 = valign(v5,v5,#4)
	}
	{
		v2 = vror(v2,r12)
		v5 = vor(v4,v5)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v7:6.w = vunpack(v6.h)
		v4 = vor(v2,v3)
	}
	{
		v3:2.w = vadd(v7:6.w,v5:4.w)
	}
	{
		if (q2) vmem(r0+#0) = v2
	}
	{
		nop
		if (q1) vmem(r1+#0) = v3
	} :endloop0
.LBB131_237:                            // %"end for sum_filter.s1.r19$x137.loopexit.split.us2165.unr-lcssa"
                                        //   in Loop: Header=BB131_234 Depth=1
	{
		if (p0) jump:nt .LBB131_233
	}
// %bb.238:                             // %"for sum_filter.s1.r19$x136.us2162.epil"
                                        //   in Loop: Header=BB131_234 Depth=1
	{
		v9:8 = vcombine(v1,v1)
		r1 = memw(r30+#-1328)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		v7:6 = vcombine(v1,v1)
		v5:4 = vcombine(v1,v1)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r2,#-28032)
		v3:2 = vcombine(v1,v1)
		r9 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r1,r0)
		r8 = add(r2,#-29696)
		v11 = vmem(r4+#0)
		memw(r30+#-304) = r4
	}                                       // 4-byte Folded Spill
	{
		r1 = setbit(r8,#7)
		r0 = add(r3,r0)
		r28 = memw(r30+#-2864)
		v10 = vmem(r9+#0)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r2,#-29440)
		memw(r30+#-816) = r0
		vmem(r8+#0) = v11
	}
	{
		r0 = memw(r8+#124)
		vmem(r1+#0) = v10
	}
	{
		memw(r30+#-560) = r0
		r13:12 = memd(r8+#0)

	} :mem_noshuf
	{
		v9.w = vinsert(r12)
		r3:2 = memd(r8+#8)
		r15:14 = memd(r8+#64)
	}
	{
		v8.w = vinsert(r14)
		r7:6 = memd(r8+#72)
		r11:10 = memd(r8+#128)
	}
	{
		v7.w = vinsert(r10)
		v9 = valign(v9,v9,#4)
		r27:26 = memd(r8+#136)
		r1:0 = memd(r8+#192)
	}
	{
		v6.w = vinsert(r0)
		v9.w = vinsert(r13)
		v8 = valign(v8,v8,#4)
		r21:20 = memd(r8+#200)
	}
	{
		v8.w = vinsert(r15)
		v7 = valign(v7,v7,#4)
		r19:18 = memd(r8+#16)
		r5 = memw(r8+#120)
	}
	{
		v7.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
		r25:24 = memd(r8+#32)
		r23:22 = memd(r8+#24)
	}
	{
		v6.w = vinsert(r1)
		v9 = valign(v9,v9,#4)
		r17:16 = memd(r8+#96)
		r1:0 = memd(r8+#160)
	}
	{
		v9.w = vinsert(r2)
		v8 = valign(v8,v8,#4)
		r15:14 = memd(r8+#40)
	}
	{
		v8.w = vinsert(r6)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r20)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r3)
		v8 = valign(v8,v8,#4)
		r3:2 = memd(r8+#80)
	}
	{
		v8.w = vinsert(r7)
		v7 = valign(v7,v7,#4)
		r13:12 = memd(r8+#144)
		r7:6 = memd(r8+#88)
	}
	{
		v7.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r11:10 = memd(r8+#208)
		r27:26 = memd(r8+#152)
	}
	{
		v6.w = vinsert(r21)
		v9 = valign(v9,v9,#4)
		r21:20 = memd(r8+#168)
	}
	{
		v9.w = vinsert(r18)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r12)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r10)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r19)
		v8 = valign(v8,v8,#4)
		r19:18 = memd(r8+#224)
	}
	{
		v8.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r8+#176)
	}
	{
		v7.w = vinsert(r13)
		v6 = valign(v6,v6,#4)
		r13:12 = memd(r8+#184)
	}
	{
		v6.w = vinsert(r11)
		v9 = valign(v9,v9,#4)
		r11:10 = memd(r8+#216)
	}
	{
		v9.w = vinsert(r22)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r6)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r10)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r23)
		v8 = valign(v8,v8,#4)
		r23:22 = memd(r8+#240)
	}
	{
		v8.w = vinsert(r7)
		v7 = valign(v7,v7,#4)
		r7:6 = memd(r8+#248)
	}
	{
		v7.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		r27:26 = memd(r8+#232)
	}
	{
		v6.w = vinsert(r11)
		v9 = valign(v9,v9,#4)
		r11:10 = memd(r8+#48)
	}
	{
		v9.w = vinsert(r24)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r0)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r18)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r16)
		v9 = valign(v9,v9,#4)
		r16 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r28 += asl(r16,#8)
		v9.w = vinsert(r25)
		v7 = valign(v7,v7,#4)
		r25:24 = memd(r8+#56)
	}
	{
		v7.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r8+#104)
	}
	{
		v6.w = vinsert(r19)
		v8 = valign(v8,v8,#4)
		r19:18 = memd(r8+#112)
	}
	{
		v8.w = vinsert(r17)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r20)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r0)
		r0 = #68
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r15)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r21)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r1)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r10)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r22)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r18)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r11)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r23)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r19)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r24)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r12)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r5)
		v9 = valign(v9,v9,#4)
		r5 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v9.w = vinsert(r25)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r13)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		v9 = vror(v9,r0)
		r0 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v8 = valign(v8,v8,#4)
		r1 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v8.w = vinsert(r0)
		r0 = #68
		v6 = valign(v6,v6,#4)
	}
	{
		r0 = setbit(r4,#7)
		v7 = vror(v7,r0)
	}
	{
		v8 = valign(v8,v8,#4)
		v7 = vor(v7,v6)
		v6 = vmem(r28+#0)
	}
	{
		v11:10.w = vunpack(v6.h)
		v6 = vor(v9,v8)
	}
	{
		v9:8.w = vadd(v11:10.w,v7:6.w)
		v6 = vmem(r28+#1)
	}
	{
		r1 = memw(r30+##-23544)
		if (q3) vmem(r1+#0) = v8
	}                                       // 4-byte Folded Reload
	{
		v8 = vmem(r5+#0)
		if (q0) vmem(r9+#0) = v9
	}
	{
		v7.cur = vmem(r1+#0)
		vmem(r4+#0) = v7
	}
	{
		v7:6.w = vunpack(v6.h)
		vmem(r0+#0) = v8
	}
	{
		r1:0 = memd(r4+#128)
		r3:2 = memd(r4+#144)
	}
	{
		v5.w = vinsert(r0)
		r9:8 = memd(r4+#192)
		r21:20 = memd(r4+#0)
	}
	{
		v4.w = vinsert(r8)
		r23:22 = memd(r4+#64)
	}
	{
		v3.w = vinsert(r20)
		v2.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r4+#136)
	}
	{
		v4.w = vinsert(r9)
		v3 = valign(v3,v3,#4)
		r17:16 = memd(r4+#160)
		r9:8 = memd(r4+#176)
	}
	{
		v3.w = vinsert(r21)
		v2 = valign(v2,v2,#4)
		r21:20 = memd(r4+#152)
	}
	{
		v2.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
		r23:22 = memd(r4+#200)
		r7:6 = memd(r4+#184)
	}
	{
		v5.w = vinsert(r0)
		v4 = valign(v4,v4,#4)
		r15:14 = memd(r4+#168)
		r25:24 = memd(r4+#8)
	}
	{
		v4.w = vinsert(r22)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r4+#72)
	}
	{
		v3.w = vinsert(r24)
		v2 = valign(v2,v2,#4)
		memd(r30+#-304) = r7:6
	}                                       // 8-byte Folded Spill
	{
		v2.w = vinsert(r26)
		v5 = valign(v5,v5,#4)
		r13:12 = memd(r4+#208)
		r7:6 = memd(r4+#216)
	}
	{
		v5.w = vinsert(r1)
		v4 = valign(v4,v4,#4)
		r1:0 = memd(r4+#240)
		r19:18 = memd(r4+#224)
	}
	{
		v4.w = vinsert(r23)
		v3 = valign(v3,v3,#4)
		r23:22 = memd(r4+#16)
		r11:10 = memd(r4+#232)
	}
	{
		v3.w = vinsert(r25)
		v2 = valign(v2,v2,#4)
		r25:24 = memd(r4+#80)
	}
	{
		v2.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r12)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r22)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
		r3:2 = memd(r4+#24)
	}
	{
		v4.w = vinsert(r13)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r4+#88)
		r13:12 = memd(r4+#48)
	}
	{
		v3.w = vinsert(r23)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
		r25:24 = memd(r4+#32)
	}
	{
		v5.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r4+#96)
	}
	{
		v4.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r26)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r7)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r4+#40)
	}
	{
		v2.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r4+#104)
		r27:26 = memd(r30+#-304)
	}
	{
		v5.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r24)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r22)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r19)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r25)
		v2 = valign(v2,v2,#4)
		r25:24 = memd(r4+#112)
	}
	{
		v2.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r14)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r6)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r15)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r4+#248)
	}
	{
		v2.w = vinsert(r7)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r8)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
		r0 = memw(r4+#120)
	}
	{
		v3.w = vinsert(r12)
		r12 = #68
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r4+#56)
	}
	{
		v2.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r9)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r13)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r26)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r0)
		v5 = valign(v5,v5,#4)
		r0 = memw(r4+#124)
	}
	{
		v5.w = vinsert(r27)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r0)
		v5 = vror(v5,r12)
		r0 = memw(r30+##-23544)
	}                                       // 4-byte Folded Reload
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v3 = vror(v3,r12)
		v5 = vor(v5,v4)
	}
	{
		v2 = valign(v2,v2,#4)
	}
	{
		v4 = vor(v3,v2)
	}
	{
		v3:2.w = vadd(v7:6.w,v5:4.w)
	}
	{
		if (q2) vmem(r0+#0) = v2
	}
	{
		jump .LBB131_233
		if (q1) vmem(r5+#0) = v3
	}
.LBB131_239:                            // %"consume sum_filter163"
	{
		r1:0 = combine(#1,#-1)
		r7 = #64
		r2 = #0
	}
	{
		v3 = vand(q1,r0)
		v2 = vand(q0,r0)
	}
	{
		v4 = vand(q2,r0)
		v1 = vand(q3,r0)
	}
	{
		v3.b = vpacke(v0.h,v3.h)
	}
	{
		v2.b = vpacke(v0.h,v2.h)
	}
	{
		v4.b = vpacke(v0.h,v4.h)
	}
	{
		v3 = vror(v3,r7)
	}
	{
		v1.b = vpacke(v0.h,v1.h)
		v3 = vor(v3,v4)
	}
	{
		q0 = vand(v3,r0)
		v2 = vror(v2,r7)
	}
	{
		v2 = vand(q0,r0)
		v1 = vor(v2,v1)
	}
	{
		q0 = vand(v1,r0)
	}
	{
		v1 = vand(q0,r0)
		v2.b = vpacke(v0.h,v2.h)
	}
	{
		v1.b = vpacke(v0.h,v1.h)
	}
	{
		v2 = vror(v2,r7)
	}
	{
		v2.b = vsplat(r1)
		v1 = vor(v2,v1)
	}
	{
		q0 = vand(v1,r0)
	}
	{
		v1 = vmux(q0,v2,v0)
	}
	{
		r7 = #32
		v2 = valign(v1,v1,r7)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		r7 = #16
		v2 = valign(v1,v1,r7)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		r7 = #8
		v2 = valign(v1,v1,r7)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		v2 = valign(v1,v1,r7)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		v2 = valign(v1,v1,#4)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		v2 = valign(v1,v1,#2)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		v2 = valign(v1,v1,#1)
	}
	{
		v1.ub = vmax(v1.ub,v2.ub)
	}
	{
		r0 = vextract(v1,r2)
	}
	{
		p0 = tstbit(r0,#0)
		r28 = and(r0,#1)
		r0 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r0,#255)
		if (p0) jump:nt .LBB131_241
	}
// %bb.240:                             // %after_bb175.thread
	{
		v1 = vsplat(r0)
		r5:4 = combine(#0,#0)
		r3 = #0
		r7:6 = combine(#0,#0)
	}
	{
		r9:8 = combine(#0,#0)
		r13:12 = combine(#0,#0)
		memd(r30+#-304) = r7:6
		memd(r30+#-560) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r15:14 = combine(#0,#0)
		r11:10 = combine(#0,#0)
		r21:20 = combine(#0,#0)
		r19:18 = combine(#0,#0)
	}
	{
		r27:26 = combine(#0,#0)
		r25:24 = combine(#0,#0)
		r23:22 = combine(#0,#0)
		r17:16 = combine(#0,#0)
	}
	{
		r5:4 = combine(#0,#0)
		v5:4.w = vsub(v5:4.w,v5:4.w)
		r1:0 = combine(#0,#0)
		r7:6 = combine(#0,#0)
	}
	{
		jump .LBB131_242
		v2 = vxor(v2,v2)
	}
.LBB131_241:                            // %true_bb176
	{
		v1 = vsplat(r0)
		r7 = memw(r30+##-24752)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r0,#-29952)
		r2 = add(r0,#-28032)
		v5:4 = vcombine(v0,v0)
		v7:6 = vcombine(v0,v0)
	}
	{
		r1 = setbit(r5,#7)
		v2 = vmem(r7+#0)
	}
	{
		v3 = vmem(r7+#1)
	}
	{
		v8 = vmem(r7+#2)
	}
	{
		v2 = valign(v3,v2,r7)
		vmem(r5+#0) = v2.new
	}
	{
		r28 = add(r0,#-30208)
		v2 = valign(v8,v3,r7)
		memw(r30+#-816) = r28
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r0,#-30464)
		r1 = memw(r5+#124)
		vmem(r1+#0) = v2
	}
	{
		r6 = memw(r30+##-23560)
		v2 = vmem(r2+#0)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = memd(r5+#128)
		r15:14 = memd(r5+#136)
	}
	{
		v4.w = vinsert(r2)
		r7:6 = memd(r5+#192)
		v3 = vmem(r6+#0)
	}
	{
		v5.w = vinsert(r6)
		r9:8 = memd(r5+#0)
		r11:10 = memd(r5+#144)
	}
	{
		v6.w = vinsert(r8)
		v4 = valign(v4,v4,#4)
		r13:12 = memd(r5+#64)
		r23:22 = memd(r5+#208)
	}
	{
		v4.w = vinsert(r3)
		v7.w = vinsert(r12)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r5+#200)
	}
	{
		v5.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r5+#8)
		r25:24 = memd(r5+#16)
	}
	{
		v6.w = vinsert(r9)
		v7 = valign(v7,v7,#4)
		r21:20 = memd(r5+#72)
		r4 = memw(r5+#120)
	}
	{
		v7.w = vinsert(r13)
		v4 = valign(v4,v4,#4)
		r17:16 = memd(r5+#152)
		r19:18 = memd(r5+#160)
	}
	{
		v4.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
		r13:12 = memd(r5+#224)
	}
	{
		v6.w = vinsert(r6)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r2)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r15)
		v6 = valign(v6,v6,#4)
		r15:14 = memd(r5+#32)
	}
	{
		v6.w = vinsert(r7)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r5+#216)
	}
	{
		v5.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r5+#80)
		r27:26 = memd(r5+#24)
	}
	{
		v7.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
		r21:20 = memd(r5+#96)
	}
	{
		v4.w = vinsert(r10)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r22)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r2)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
		r11:10 = memd(r5+#88)
	}
	{
		v6.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r23)
		v7 = valign(v7,v7,#4)
		r23:22 = memd(r5+#168)
	}
	{
		v7.w = vinsert(r3)
		v4 = valign(v4,v4,#4)
		r25:24 = memd(r5+#232)
		r3:2 = memd(r5+#176)
	}
	{
		v4.w = vinsert(r16)
		v6 = valign(v6,v6,#4)
		r9:8 = memd(r5+#240)
	}
	{
		v6.w = vinsert(r26)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r6)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r10)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r17)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
		r27:26 = memd(r5+#40)
	}
	{
		v5.w = vinsert(r7)
		v7 = valign(v7,v7,#4)
		r17:16 = memd(r5+#104)
		r7:6 = memd(r5+#184)
	}
	{
		v7.w = vinsert(r11)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r14)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r12)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r20)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r15)
		v5 = valign(v5,v5,#4)
		r15:14 = memd(r5+#48)
	}
	{
		v5.w = vinsert(r13)
		v7 = valign(v7,v7,#4)
		r13:12 = memd(r5+#112)
	}
	{
		v7.w = vinsert(r21)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r24)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r25)
		v7 = valign(v7,v7,#4)
		r25:24 = memd(r5+#248)
	}
	{
		v7.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
		r21:20 = memd(r5+#56)
		memw(r30+#-1328) = r0

	} :mem_noshuf
	{
		r2 = setbit(r28,#7)
		v4.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
		vmem(r28+#0) = v2
	}
	{
		r2 = setbit(r0,#7)
		v6.w = vinsert(r14)
		v5 = valign(v5,v5,#4)
		vmem(r2+#0) = v3
	}
	{
		v7 = valign(v7,v7,#4)
		r2 = memw(r30+##-23544)
		memw(r30+#-1072) = r2
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r8)
		v7.w = vinsert(r12)
		v8 = valign(v4,v4,#4)
		r27:26 = memd(r28+#240)
	}
	{
		v8.w = vinsert(r3)
		r0 = #68
		v6 = valign(v6,v6,#4)
		v3 = vmem(r2+#0)
	}
	{
		v6.w = vinsert(r15)
		v5 = valign(v5,v5,#4)
		r2 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r9)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r13)
		r3:2 = memd(r28+#248)
		v4 = vmem(r2+#0)
	}
	{
		v8 = valign(v8,v8,#4)
		memd(r30+#-304) = r27:26
		r9:8 = memd(r28+#232)

	} :mem_noshuf
	{
		v8.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r20)
		v5 = valign(v5,v5,#4)
		memd(r30+#-560) = r9:8
	}                                       // 8-byte Folded Spill
	{
		v5.w = vinsert(r24)
		v7 = valign(v7,v7,#4)
		r13:12 = memd(r28+#224)
	}
	{
		v7.w = vinsert(r4)
		v9 = valign(v6,v6,#4)
		r5:4 = memd(r28+#216)
	}
	{
		v9.w = vinsert(r21)
		v8 = valign(v8,v8,#4)
		memd(r30+#-1584) = r5:4
	}                                       // 8-byte Folded Spill
	{
		v8.w = vinsert(r7)
		v5 = valign(v5,v5,#4)
		r15:14 = memd(r28+#208)
		r21:20 = memd(r28+#192)
	}
	{
		v5.w = vinsert(r25)
		v6 = valign(v7,v7,#4)
		r11:10 = memd(r28+#200)
		r19:18 = memd(r28+#184)
	}
	{
		v6.w = vinsert(r1)
		v8 = vror(v8,r0)
		r25:24 = memd(r28+#168)
		r23:22 = memd(r28+#160)
	}
	{
		v7 = vror(v9,r0)
		r27:26 = memd(r28+#176)
		r17:16 = memd(r28+#152)
	}
	{
		v5 = valign(v5,v5,#4)
		r1:0 = memd(r28+#144)
		r5:4 = memd(r28+#136)
	}
	{
		v6 = valign(v6,v6,#4)
		v5 = vor(v8,v5)
		r7:6 = memd(r28+#128)
		r28 = memw(r30+#-816)
	}
	{
		r8 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r9:8 = combine(r13,r12)
		v4 = vor(v7,v6)
		vmem(r8+#0) = v4
	}
	{
		r12 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		vmem(r12+#0) = v3
	}
	{
		r13:12 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
.LBB131_242:                            // %after_bb181
	{
		p0 = cmp.eq(r28,#0)
		r28 = #68
		v3 = v0
		v6 = v0
	}
	{
		v3.w = vinsert(r20)
		v6.w = vinsert(r6)
	}
	{
		v7.w = vmpyieo(v1.h,v2.h)
	}
	{
		v7.w += vmpyie(v1.w,v2.h)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r21)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r7)
		r7:6 = memd(r30+#-560)
	}                                       // 8-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r10)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r11)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r14)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r0)
		r0 = memw(r30+##-7232)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r15)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r1)
		r1 = ##16843009
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r12)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r16)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r13)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r17)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r8)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r22)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r9)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r23)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r24)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r30+#-304)
	}                                       // 8-byte Folded Reload
	{
		v6.w = vinsert(r25)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r27)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r18)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r19)
		r6 = add(r3,#-27392)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v6 = vror(v6,r28)
	}
	{
		v3 = vor(v6,v3)
	}
	{
		v6.w = vmpyieo(v1.h,v3.h)
	}
	{
		v6.w += vmpyie(v1.w,v3.h)
	}
	{
		v3:2.w = vsub(v5:4.w,v7:6.w)
	}
	{
		r6 = add(r30,#-5424)
		if (q3) vmem(r6+#0) = v2
	}
	{
		v2 = vmemu(r6+#0)
	}                                       // 256-byte Folded Reload
	{
		q0 = vand(v2,r1)
	}
	{
		if (!p0) jump:nt .LBB131_244
		if (q0) vmem(r0+#0) = v3
	}
// %bb.243:
	{
		r5:4 = combine(#0,#0)
		r9:8 = combine(#0,#0)
		r3:2 = combine(#0,#0)
		r7:6 = combine(#0,#0)
	}
	{
		r13:12 = combine(#0,#0)
		r15:14 = combine(#0,#0)
		memd(r30+#-304) = r3:2
		memd(r30+#-560) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r11:10 = combine(#0,#0)
		r19:18 = combine(#0,#0)
		r27:26 = combine(#0,#0)
		r25:24 = combine(#0,#0)
	}
	{
		r1:0 = combine(#0,#0)
		r17:16 = combine(#0,#0)
		r23:22 = combine(#0,#0)
		r3:2 = combine(#0,#0)
	}
	{
		v5:4.w = vsub(v5:4.w,v5:4.w)
		v2 = v0
		r5:4 = combine(#0,#0)
		r21:20 = combine(#0,#0)
	}
	{
		jump .LBB131_245
	}
.LBB131_244:                            // %true_bb194
	{
		r8 = add(r3,#-30720)
		r20 = add(r3,#-31232)
		r7 = memw(r30+##-24752)
	}                                       // 4-byte Folded Reload
	{
		r2 = setbit(r8,#7)
		r0 = add(r3,#-28032)
		r7 = add(r7,#256)
		r3 = add(r3,#-30976)
	}
	{
		r9 = setbit(r20,#7)
		r1 = and(r7,#-128)
		v3 = vxor(v3,v3)
		memw(r30+#-1072) = r3
	}                                       // 4-byte Folded Spill
	{
		r21 = #68
		v5:4 = vcombine(v3,v3)
		v6 = v3
		v2 = vmem(r1+#1)
	}
	{
		v7 = vmem(r1+#2)
	}
	{
		r1 = setbit(r3,#7)
		v8 = vmem(r1+#0)
		memw(r30+#-816) = r1.new
	}                                       // 4-byte Folded Spill
	{
		v7 = valign(v7,v2,r7)
		vmem(r2+#0) = v7.new
	}
	{
		v2 = valign(v2,v8,r7)
		r3:2 = memd(r8+#128)
		vmem(r8+#0) = v2.new
	}
	{
		v4.w = vinsert(r2)
		r5:4 = memd(r8+#192)
		r15:14 = memd(r8+#136)
	}
	{
		v5.w = vinsert(r4)
		r7:6 = memd(r8+#0)
		r1 = memw(r30+##-23544)
	}
	{
		v6.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
		r13:12 = memd(r8+#64)
	}
	{
		v3.w = vinsert(r12)
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r8+#144)
	}
	{
		v5.w = vinsert(r5)
		v6 = valign(v6,v6,#4)
		r5:4 = memd(r8+#200)
		r19:18 = memd(r8+#208)
	}
	{
		v6.w = vinsert(r7)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r8+#8)
		v2 = vmem(r1+#0)
	}
	{
		v3.w = vinsert(r13)
		v4 = valign(v4,v4,#4)
		r17:16 = memd(r8+#72)
	}
	{
		v4.w = vinsert(r14)
		v5 = valign(v5,v5,#4)
		r1 = memw(r30+##-23552)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r4)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
		r1 = memw(r8+#124)
		v7 = vmem(r1+#0)
	}
	{
		v3.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
		r11:10 = memd(r8+#152)
		r23:22 = memd(r8+#216)
	}
	{
		v4.w = vinsert(r15)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r5)
		v6 = valign(v6,v6,#4)
		r5:4 = memd(r8+#16)
	}
	{
		v6.w = vinsert(r7)
		v3 = valign(v3,v3,#4)
		r7:6 = memd(r8+#80)
	}
	{
		v3.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r18)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r8+#24)
	}
	{
		v5.w = vinsert(r19)
		v6 = valign(v6,v6,#4)
		r25:24 = memd(r8+#88)
	}
	{
		v6.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r8+#160)
	}
	{
		v3.w = vinsert(r7)
		v4 = valign(v4,v4,#4)
		r7:6 = memd(r8+#224)
	}
	{
		v4.w = vinsert(r10)
		v5 = valign(v5,v5,#4)
		r27:26 = memd(r8+#32)
	}
	{
		v5.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
		r19:18 = memd(r8+#96)
	}
	{
		v6.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r24)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r11)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r3:2 = memd(r8+#168)
	}
	{
		v3.w = vinsert(r25)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r8+#232)
		r25:24 = memd(r8+#176)
	}
	{
		v4.w = vinsert(r4)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r6)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r18)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r5)
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r8+#40)
	}
	{
		v5.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r17:16 = memd(r8+#104)
	}
	{
		v6.w = vinsert(r27)
		v3 = valign(v3,v3,#4)
		r27:26 = memd(r8+#240)
	}
	{
		v3.w = vinsert(r19)
		v4 = valign(v4,v4,#4)
		r19:18 = memd(r8+#48)
	}
	{
		v4.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
		r7:6 = memd(r8+#112)
	}
	{
		v5.w = vinsert(r22)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r16)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r8+#184)
	}
	{
		v5.w = vinsert(r23)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r8+#248)
	}
	{
		v3.w = vinsert(r17)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r8+#56)
	}
	{
		v4.w = vinsert(r24)
		v5 = valign(v5,v5,#4)
		r12 = memw(r8+#120)
		vmem(r9+#0) = v7
	}
	{
		v5.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
		r9:8 = memd(r20+#248)
		vmem(r20+#0) = v2
	}
	{
		v6.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
		r28 = memw(r30+#-816)
		memd(r30+#-304) = r9:8
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r6)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r25)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r27)
		v7 = valign(v3,v3,#4)
		v3 = vmem(r0+#0)
	}
	{
		v7.w = vinsert(r7)
		v6 = valign(v6,v6,#4)
		r0 = memw(r30+##-23560)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r19)
		v8 = valign(v4,v4,#4)
		r7:6 = memd(r20+#240)
	}
	{
		v8.w = vinsert(r2)
		v5 = valign(v5,v5,#4)
		v4 = vmem(r0+#0)
		memd(r30+#-560) = r7:6
	}                                       // 8-byte Folded Spill
	{
		v5.w = vinsert(r4)
		v6 = valign(v6,v6,#4)
		r7:6 = memd(r20+#232)
		r9:8 = memd(r20+#224)
	}
	{
		v6.w = vinsert(r22)
		v7 = valign(v7,v7,#4)
		r15:14 = memd(r20+#208)
		r11:10 = memd(r20+#200)
	}
	{
		v7.w = vinsert(r12)
		v8 = valign(v8,v8,#4)
		r19:18 = memd(r20+#192)
		r25:24 = memd(r20+#176)
	}
	{
		v8.w = vinsert(r3)
		v9 = valign(v6,v6,#4)
		r13:12 = memd(r20+#216)
		r27:26 = memd(r20+#184)
	}
	{
		v9.w = vinsert(r23)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r20+#144)
		r23:22 = memd(r20+#168)
	}
	{
		v5.w = vinsert(r5)
		v6 = valign(v7,v7,#4)
		r17:16 = memd(r20+#152)
		r5:4 = memd(r20+#136)
	}
	{
		v6.w = vinsert(r1)
		v8 = vror(v8,r21)
		r1:0 = memd(r20+#160)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v7 = vror(v9,r21)
		v5 = vor(v8,v5)
		r21:20 = memd(r20+#128)
		vmem(r28+#0) = v4
	}
	{
		v6 = valign(v6,v6,#4)
		r28 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v4 = vor(v7,v6)
	}
	{
		r28 = #68
		vmem(r28+#0) = v3
	}
.LBB131_245:                            // %after_bb199
	{
		v0.w = vinsert(r18)
		v7.w = vmpyieo(v1.h,v2.h)
		v3 = v0
	}
	{
		v3.w = vinsert(r20)
	}
	{
		v7.w += vmpyie(v1.w,v2.h)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r19)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r21)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r10)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r4)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r11)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r5)
		r5:4 = memd(r30+#-560)
	}                                       // 8-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r14)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r15)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		r3:2 = memd(r30+#-304)
	}                                       // 8-byte Folded Reload
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r12)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r16)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r13)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r17)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r8)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r0)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r9)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r22)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r7)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r23)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
		r4 = memw(r30+##-10312)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r24)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r25)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		r2 = memw(r30+##-24744)
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		v3.w = vinsert(r26)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r3)
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-10320)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r27)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v3 = vror(v3,r28)
	}
	{
		v0 = vor(v3,v0)
	}
	{
		v6.w = vmpyieo(v1.h,v0.h)
	}
	{
		v6.w += vmpyie(v1.w,v0.h)
	}
	{
		v1:0.w = vsub(v5:4.w,v7:6.w)
	}
	{
		if (q2) vmem(r4+#0) = v0
	}
	{
		if (!p0) jump:nt .LBB131_727
		if (q1) vmem(r3+#0) = v1
	}
// %bb.246:                             // %"for output.s0.b.rebased202.preheader"
	{
		r3 = #0
		r1 = memw(r30+#-3888)
		r14 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		r8 = max(r1,r3)
		r4 = add(r14,#-1)
		r0 = memw(r30+##-5168)
	}                                       // 4-byte Folded Reload
	{
		r9 = asr(r1,#31)
		p2 = cmp.eq(r14,#3)
		r21 = memw(r30+##-4400)
	}                                       // 4-byte Folded Reload
	{
		r28 = asl(r1,#1)
		r25 = memw(r30+##-23352)
		r5 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r7 = mpyi(r0,r1)
		r6 = max(r21,r3)
		r3 = r1
	}
	{
		r16 = mpyi(r4,r21)
		r5 = and(r5,#255)
		r0 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		v0.h = vsplat(r5)
		p3 = cmp.gt(r3,#-1)
		r4 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r12 = asr(r21,#31)
		r15 = asl(r21,#1)
		r2 = and(r9,r7)
		if (!p3) r7 = #0
	}
	{
		r26 = mpyi(r4,r0)
		p3 = cmp.gt(r25,#-1)
		r13 = and(r12,r16)
	}
	{
		r18 = asl(r6,#1)
		r0 = memw(r30+##-5680)
		memw(r30+#-560) = r15
	}                                       // 4-byte Folded Reload
	{
		r8 = asl(r8,#1)
		r4 = memw(r30+##-23336)
		memw(r30+#-304) = r28
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+#-3376)
		memw(r30+#-816) = r2
	}                                       // 4-byte Folded Reload
	{
		r26 += mpyi(r4,r0)
		r4 = add(r30,#-20528)
		r0 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		memw(r30+#-1072) = r13
	}                                       // 4-byte Folded Spill
	{
		v0.b = vsplat(r5)
		r4 = add(r30,#-20656)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		p0 = cmp.eq(r0,#3)
		r5 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-19768)
	}                                       // 4-byte Folded Reload
	{
		v0.b = vsplat(r5)
		r4 = add(r30,#-5168)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		p0 = and(p0,p2)
		p2 = cmp.gt(r21,#-1)
		r3 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r27 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		if (p0) r15 = and(r12,r15)
		if (p0) r12 = and(r9,r28)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		if (p3) r10 = add(r3,#-1)
		if (!p2) r16 = #0
		r4 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		p1 = cmp.gt(r4,#-1)
		r9 = mux(p3,#1,r3)
		r3 = memw(r30+##-23360)
	}                                       // 4-byte Folded Reload
	{
		if (!p3) r10 = #0
		if (p1) r17 = add(r0,#-1)
		r1 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r28 = add(r3,add(r9,#-1))
		r5 = mux(p1,#1,r0)
		r19 = add(r10,r3)
		if (!p0) r18 = add(r16,#0)
	}
	{
		r14 = add(r1,add(r5,#-1))
		if (!p1) r17 = #0
		if (!p0) r15 = add(r13,#0)
	}
	{
		r19 = mpyi(r19,r25)
		r10 = mpyi(r28,r25)
		r20 = add(r17,r1)
		p2 = cmp.gt(r18,r16)
	}
	{
		r11 = mpyi(r14,r4)
		r22 = mpyi(r20,r4)
		r1 = add(r19,r16)
		if (!p0) r12 = add(r2,#0)
	}
	{
		if (p2) r20 = add(r19,r18)
		p3 = cmp.gt(r15,r13)
		r16 = add(r10,r13)
		if (!p0) r8 = add(r7,#0)
	}
	{
		r0 = p0
		if (!p2) r20 = add(r1,#0)
		p1 = cmp.gt(r12,r2)
		r6 = add(r11,r2)
	}
	{
		p0 = or(p3,p3)
		p2 = cmp.gt(r8,r7)
		if (p3) r18 = add(r16,#0)
		p3 = cmp.eq(r27,#0)
	}
	{
		if (p1) r1 = add(r6,#0)
		memw(r30+#-1328) = r1
		memw(r30+##-20664) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = add(r22,r7)
		if (p2) r22 = add(r22,r8)
		if (!p1) r1 = add(r11,r12)
	}
	{
		if (!p2) r22 = add(r0,#0)
		r0 = sub(r0,r6)
		r24 = mux(p3,#-1,#0)
		if (!p0) r18 = add(r10,r15)
	}
	{
		r23 = min(r13,r15)
		r15 = sub(r22,r1)
		r22 = sub(r27,r24)
		memw(r30+#-1584) = r0
	}                                       // 4-byte Folded Spill
	{
		r17 = add(r17,sub(#1,r5))
		r27 = #-1
		r10 = sub(r20,r18)
		r0 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r20 = max(r15,r27)
		r0 += mpyi(r4,r14)
		r6 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r10 = max(r10,r27)
		r14 = mpyi(r4,r17)
		r17 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		r20 = add(#128,asl(r20,#7))
		r6 += mpyi(r25,r28)
		r19 = r4
	}
	{
		r10 = add(r10,#1)
		r27 = #131
		r28 = memw(r30+##-23624)
	}                                       // 4-byte Folded Reload
	{
		r8 = max(r7,r8)
		r20 = add(r27,mpyi(r20,r10))
		r7 = add(r7,r14)
		r4 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r10 = sub(r4,r16)
		r8 = add(r8,r14)
		r4 = memw(r30+##-23368)
	}                                       // 4-byte Folded Reload
	{
		r14 = memw(r30+##-6456)
		memw(r30+##-23448) = r20
	}                                       // 4-byte Folded Reload
	{
		r20 = add(r7,sub(#1,r2))
		r16 = mpyi(r4,r3)
		r7 = sub(#1,r9)
	}
	{
		r3 = asr(r14,#31)
		r11 = add(r22,#1)
		memw(r30+##-21056) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r12 = min(r2,r12)
		r26 += mpyi(r17,r28)
		p0 = cmp.gtu(r11,#2)
		r3 = memw(r30+#-2608)
	}                                       // 4-byte Folded Reload
	{
		r9 = mpyi(r28,r17)
		r14 = lsl(#1,r3)
		r3 = add(r15,#1)
		memw(r30+#-3632) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r8 = add(r8,sub(#1,r12))
		r3 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r7 = mpyi(r25,r7)
		r13 = sub(r13,r23)
		r17 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r6 = mpyi(r3,r6)
		r24 = sub(#-1,r24)
		memw(r30+##-23280) = r18
	}                                       // 4-byte Folded Spill
	{
		r3 = asr(r14,#1)
		r11 = mux(p0,#0,r22)
		memw(r30+##-23296) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r4 = mpyi(r17,r0)
		r2 += mpyi(r8,r13)
		r3 = memw(r30+##-21264)
	}                                       // 4-byte Folded Reload
	{
		r2 = sub(r2,r12)
		r7 = sub(r7,r23)
		r0 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r14 = and(r11,r24)
		r13 = r6
	}
	{
		r16 += mpyi(r3,r0)
		r15 = asl(r2,#7)
		r2 = sub(#1,r5)
		r0 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		r3 = sub(r0,r18)
		r0 = sub(r21,r18)
		memw(r30+##-23304) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r3 = mpyi(r8,r7)
		r2 = mpyi(r19,r2)
		r5 = add(r14,r9)
	}
	{
		r0 = memw(r30+##-23640)
		memw(r30+##-23312) = r0
	}                                       // 4-byte Folded Reload
	{
		r13 += add(r9,r4)
		r6 += add(r5,r4)
		r4 = add(r3,r2)
		r9 = memw(r30+#-3888)
	}                                       // 4-byte Folded Reload
	{
		r16 += mpyi(r0,r28)
		r2 += add(r9,r3)
		r4 = sub(r4,r12)
		r0 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r4 = or(#96,asl(r4,#7))
		r5 = setbit(r15,#6)
		r0 = add(r0,#2048)
	}
	{
		r3 = mpyi(r25,r8)
		r4 = memw(r30+#-304)
		memw(r30+##-23456) = r4
	}                                       // 4-byte Folded Reload
	{
		r5 = asl(r8,#7)
		memw(r30+##-23488) = r5
	}                                       // 4-byte Folded Spill
	{
		r4 = asl(r3,#7)
		r0 = sub(r4,r1)
		memw(r30+##-21064) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23320) = r5
		memw(r30+##-23328) = r4
	}                                       // 4-byte Folded Spill
	{
		r7 = sub(r13,r26)
		r5 = sub(r6,r26)
		r4 = memw(r30+##-23536)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r21,r8)
		r5 = add(r4,r5)
		memw(r30+##-23256) = r0
	}                                       // 4-byte Folded Spill
	{
		r4 = add(r4,r7)
		r2 = sub(r2,r12)
		memw(r30+##-23440) = r5
	}                                       // 4-byte Folded Spill
	{
		r0 = asl(r0,#7)
		r5 = asl(r2,#7)
		memw(r30+##-23424) = r4
	}                                       // 4-byte Folded Spill
	{
		r3 = r17
		r4 = memw(r30+#-1584)
		memw(r30+#-2608) = r0
	}                                       // 4-byte Folded Reload
	{
		r0 = or(r10,r4)
		memw(r30+##-23464) = r5
	}                                       // 4-byte Folded Spill
	{
		r5 = ##16744702
		memw(r30+##-23472) = r0
	}                                       // 4-byte Folded Spill
	{
		r5 = asl(r9,#8)
		r4 = asl(r19,#7)
		r0 = add(r5,#-32767)
		memw(r30+#-1328) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r5 = ##16843009
		memw(r30+##-21072) = r4
	}                                       // 4-byte Folded Spill
	{
		v4 = vand(q2,r5)
		r4 = #0
		memw(r30+##-23432) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r4 = add(r30,#-23088)
		r5 = ##16843009
	}
	{
		r21 = r22
		r18 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r1 = sub(r9,r1)
		memw(r30+##-20912) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-21048) = r16
	}                                       // 4-byte Folded Spill
	{
		v4 = vand(q3,r5)
		r4 = add(r30,#-19248)
		vmemu(r4+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		r5 = ##16843009
		memw(r30+##-23480) = r15
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-23264) = r1
	}                                       // 4-byte Folded Spill
	{
		v4 = vand(q1,r5)
		r4 = add(r30,#-23216)
		vmemu(r4+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		memw(r30+##-23288) = r10
		memw(r30+##-23384) = r0
	}                                       // 4-byte Folded Spill
	{
		vmemu(r4+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		memw(r30+##-21040) = r22
		memw(r30+##-23272) = r20
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_248
		memw(r30+##-23248) = r24
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_247:                            // %"end for output.s0.y.yo1762"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r1 = memw(r30+##-23432)
		r0 = memw(r30+##-23648)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r5 = memw(r30+##-23424)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		memw(r30+##-23432) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-23632)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r5,r0)
		r4 = memw(r30+##-23440)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r4,r0)
		memw(r30+##-23424) = r1
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_720
		memw(r30+##-23440) = r1
	}                                       // 4-byte Folded Spill
.LBB131_248:                            // %"for output.s0.b.rebased202"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB131_266 Depth 2
                                        //       Child Loop BB131_268 Depth 3
                                        //     Child Loop BB131_259 Depth 2
                                        //       Child Loop BB131_261 Depth 3
                                        //     Child Loop BB131_645 Depth 2
                                        //       Child Loop BB131_648 Depth 3
                                        //         Child Loop BB131_710 Depth 4
                                        //           Child Loop BB131_713 Depth 5
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27404)
		r17 = memw(r30+##-23448)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r0+#4)
	}
	{
		p0 = cmp.gtu(r17,r1); if (!p0.new) jump:nt .LBB131_253
		r2 = memw(r0+#0)
	}
// %bb.249:                             // %if.then.i2015
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		p0 = cmp.eq(r2,#0); if (!p0.new) jump:nt .LBB131_717
		r0 = memw(r0+#8)
	}
.LBB131_250:                            // %if.end.i2023
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r4 = #16384
		r2 = #0
		r17 = memw(r30+##-23448)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r17)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r1,#-27404)
		p0 = cmp.gtu(r0,r4)
	}
	{
		if (!p0) jump:nt .LBB131_252
		memw(r16+#8) = r0
	}
// %bb.251:                             // %if.then8.i2025
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		call ##halide_malloc
		r1:0 = combine(r17,#0)
	}
	{
		r2 = add(r30,#-23216)
		r1 = ##16843009
	}
	{
		r4 = ##16843009
		r5 = ##16843009
	}
	{
		r2 = add(r30,#-19248)
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q1 = vand(v4,r1)
		r2 = add(r30,#-23088)
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q3 = vand(v4,r4)
		r2 = add(r30,#-5424)
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q2 = vand(v4,r5)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		r4 = ##16843009
	}
	{
		q0 = vand(v4,r4)
		r2 = r0
		r20 = memw(r30+##-23272)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
.LBB131_252:                            // %if.end11.i2027
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		memw(r16+#0) = r2
		memw(r16+#4) = r17
	}
.LBB131_253:                            // %pseudostack_alloc.exit2028
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		p0 = cmp.eq(r2,#0); if (p0.new) jump:nt .LBB131_719
	}
.LBB131_254:                            // %"produce resampled_input208"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r0 = memw(r30+##-23472)
		memw(r30+#-3120) = r2
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gt(r0,#-1)
		r4 = memw(r30+##-23656)
	}                                       // 4-byte Folded Reload
	{
		p0 = not(p0)
		p1 = r4
		if (!p1.new) jump:t .LBB131_263
	}
// %bb.255:                             // %then_bb210
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		if (p0) jump:nt .LBB131_642
	}
// %bb.256:                             // %"for resampled_input.s0.y.rebased212.us.preheader"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r0 = memw(r30+##-23480)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r8 = add(r1,r0)
		r1 = #0
		r7 = memw(r30+##-23424)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_259
	}
	.p2align	4
.LBB131_257:                            //   in Loop: Header=BB131_259 Depth=2
	{
		r6 = r7
		r2 = r8
	}
.LBB131_258:                            //   in Loop: Header=BB131_259 Depth=2
	{
		r5 = #64
		r0 = #-1
	}
	{
		v1 = vror(v5,r5)
	}
	{
		v3 = vror(v3,r5)
		v1 = vor(v1,v4)
	}
	{
		q0 = vand(v1,r0)
		v1 = vor(v3,v2)
	}
	{
		v2 = vand(q0,r0)
		q2 = vand(v1,r0)
	}
	{
		v1 = vand(q2,r0)
	}
	{
		v2.b = vpacke(v0.h,v2.h)
	}
	{
		v0.b = vpacke(v0.h,v1.h)
		v1 = vmem(r6+#0)
	}
	{
		v2 = vror(v2,r5)
	}
	{
		v0 = vor(v2,v0)
		v2 = vmem(r6+#1)
	}
	{
		q3 = vand(v0,r0)
		r0 = ##16843009
		v1 = valign(v2,v1,r6)
	}
	{
		r2 = add(r30,#-23216)
		if (q3) vmem(r2+#0) = v1
	}
	{
		r2 = add(r30,#-19248)
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q1 = vand(v4,r0)
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = ##16843009
		r2 = add(r30,#-23088)
	}
	{
		q3 = vand(v4,r0)
		r0 = ##16843009
	}
	{
		v4 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q2 = vand(v4,r0)
		r2 = memw(r30+##-23288)
	}                                       // 4-byte Folded Reload
	{
		r0 = ##16843009
		p0 = cmp.eq(r1,r2)
	}
	{
		r1 = add(r1,#1)
		r2 = memw(r30+##-23320)
	}                                       // 4-byte Folded Reload
	{
		r8 = add(r8,r2)
		r2 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r2)
		r2 = add(r30,#-5424)
	}
	{
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		q0 = vand(v4,r0)
		if (p0) jump:nt .LBB131_642
	}
.LBB131_259:                            // %"for resampled_input.s0.x.rebased215.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_261 Depth 3
	{
		r0 = #-1
		p0 = cmp.gtu(r20,#1)
		v0 = vxor(v0,v0)
	}
	{
		v1 = vand(q3,r0)
		v3 = vand(q0,r0)
	}
	{
		v4 = vand(q2,r0)
	}
	{
		v1 = vand(q1,r0)
		v2.b = vpacke(v0.h,v1.h)
	}
	{
		v3.b = vpacke(v0.h,v3.h)
	}
	{
		v4.b = vpacke(v0.h,v4.h)
	}
	{
		if (!p0) jump:nt .LBB131_257
		v5.b = vpacke(v0.h,v1.h)
	}
// %bb.260:                             // %"for resampled_input.s0.x.rebased215.us"
                                        //   in Loop: Header=BB131_259 Depth=2
	{
		r0 = #64
		r9 = #-1
		q2 = and(q3,q3)
		q3 = and(q1,q1)
	}
	{
		r5 = add(r30,#-23088)
		v1 = vror(v5,r0)
		q1 = and(q0,q0)
		v5 = vmem(r7+#0)
	}
	{
		v6 = vand(q2,r9)
		v3 = vror(v3,r0)
		v1 = vor(v1,v4)
		v4 = vmem(r7+#1)
	}
	{
		q0 = vand(v1,r9)
		r4 = ##16843009
		v2 = vor(v3,v2)
	}
	{
		v3 = vand(q1,r9)
		v4 = vand(q0,r9)
		v1 = valign(v4,v5,r7)
	}
	{
		q0 = vand(v2,r9)
		r6 = add(r7,r3)
	}
	{
		v2 = vand(q0,r9)
		r2 = add(r8,#128)
		p0 = cmp.gtu(r20,#2)
		v4.b = vpacke(v0.h,v4.h)
	}
	{
		v5.b = vpacke(v0.h,v2.h)
		v0 = vxor(v0,v0)
	}
	{
		v2 = vmemu(r5+#0)
	}                                       // 128-byte Folded Reload
	{
		q0 = vand(v2,r4)
		r5:4 = combine(r3,r8)
		r3 = add(r20,#-2)
		v4 = vror(v4,r0)
	}
	{
		v7 = vand(q0,r9)
		v6 = vand(q3,r9)
		v2.b = vpacke(v0.h,v6.h)
		v5 = vor(v4,v5)
	}
	{
		loop0(.LBB131_261,r3)
		q0 = vand(v5,r9)
		r3 = r8
		v3.b = vpacke(v0.h,v3.h)
	}
	{
		v4.b = vpacke(v0.h,v7.h)
	}
	{
		if (!p0) jump:nt .LBB131_262
		v5.b = vpacke(v0.h,v6.h)
	}
	.p2align	4
.LBB131_261:                            // %"for resampled_input.s0.x.rebased215.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        //     Parent Loop BB131_259 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		r9 = #-1
		v5 = vror(v5,r0)
		v6 = vmem(r6+#0)
		if (q0) vmem(r3+#0) = v1
	}
	{
		v7 = vand(q2,r9)
		r4 = add(r30,#-23088)
		v1 = vror(v3,r0)
		v3 = vor(v5,v4)
	}
	{
		q0 = vand(v3,r9)
		v1 = valign(v4,v6,r6)
		v2 = vor(v1,v2)
		v4.cur = vmem(r6+#1)
	}
	{
		v3 = vand(q0,r9)
		q0 = vand(v2,r9)
		v4 = vxor(v4,v4)
	}
	{
		v2 = vand(q0,r9)
		v5 = vand(q1,r9)
		r3 = ##16843009
	}
	{
		v8 = vand(q3,r9)
		r6 = add(r6,r5)
		v3.b = vpacke(v0.h,v3.h)
	}
	{
		v0.b = vpacke(v0.h,v2.h)
	}
	{
		v6 = vror(v3,r0)
	}
	{
		r4 = r2
		v6 = vor(v6,v0)
		v3 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		q0 = vand(v3,r3)
		r2 = add(r2,#128)
		r3 = r4
		v2.b = vpacke(v4.h,v7.h)
	}
	{
		v7 = vand(q0,r9)
		q0 = vand(v6,r9)
		v3.b = vpacke(v4.h,v5.h)
		v0 = v4
	}
	{
		v5.b = vpacke(v4.h,v8.h)
	}
	{
		nop
		v4.b = vpacke(v4.h,v7.h)
	} :endloop0
.LBB131_262:                            //   in Loop: Header=BB131_259 Depth=2
	{
		r3 = r5
		r20 = memw(r30+##-23272)
		if (q0) vmem(r4+#0) = v1
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_258
	}
	.p2align	4
.LBB131_263:                            // %next_bb211
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		if (p0) jump:nt .LBB131_642
	}
// %bb.264:                             // %"for resampled_input.s0.y.rebased218.us.preheader"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r0 = memw(r30+##-23488)
		r1 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r1,r0)
		r0 = #0
		r1 = memw(r30+##-23440)
	}                                       // 4-byte Folded Reload
	{
		jump .LBB131_266
		r22 = memw(r30+##-23424)
	}                                       // 4-byte Folded Reload
	.p2align	4
.LBB131_265:                            // %"end for resampled_input.s0.x.rebased222.loopexit.us"
                                        //   in Loop: Header=BB131_266 Depth=2
	{
		r0 = memw(r30+##-23288)
		r3 = memw(r30+##-23392)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-23416)
		r2 = memw(r30+##-23320)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		r0 = add(r1,#1)
		r1 = memw(r30+##-23344)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r3,r2)
		r22 = memw(r30+##-23408)
	}                                       // 4-byte Folded Reload
	{
		r22 = add(r22,r1)
		r3 = memw(r30+##-23400)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r3,r1)
		r3 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		if (p0) jump:nt .LBB131_642
		r20 = memw(r30+##-23272)
	}                                       // 4-byte Folded Reload
.LBB131_266:                            // %"for resampled_input.s0.y.rebased218.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_268 Depth 3
	{
		memw(r30+##-23416) = r0
		memw(r30+##-23400) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+#-304) = r1
		memw(r30+##-23392) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+#-560) = r2
		memw(r30+##-23408) = r22
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_268
		memw(r30+##-16816) = r20
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_267:                            // %after_bb1759.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r27 = memw(r30+##-16816)
		memw(r30+##-22960) = r0
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r30,#-23216)
		r5:4 = combine(#124,#4)
		r1 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = combine(#24,#20)
		r9:8 = combine(#108,#96)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r1)
		r22 = add(r22,r1)
		r1 = add(r30,#-23088)
		memw(r30+#-304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = ##16843009
		v0 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = ##16843009
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q2 = vand(v0,r0)
		q1 = vand(v1,r1)
		r0 = #-1
	}
	{
		v0 = vand(q2,r0)
		v1 = vand(q1,r0)
		r23 = #104
		r15:14 = combine(#32,#28)
	}
	{
		r6 = #36
		r28 = #40
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r0,##-40832)
		r11:10 = combine(#48,#44)
		r25:24 = combine(#60,#76)
	}
	{
		r26 = #72
		r27 = add(r27,#-1)
		r1 = memw(r1+#0)
		r0 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
		r0 = add(r30,#-17200)
		p0 = cmp.eq(r27,#0)
	}
	{
		r0 = #0
		v2 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vinsert(r1)
	}
	{
		r1 = vextract(v2,r0)
	}
	{
		r2 = memw(r30+##-21096)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#16)
		r2 = #16
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r1 = vextract(v2,r0)
	}
	{
		r0 = memw(r30+##-21464)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r1 = vextract(v2,r4)
	}
	{
		r0 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r5:4 = combine(#120,#8)
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r5:4 = combine(#116,#12)
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r5:4 = combine(#92,#88)
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r1 = insert(r16,#8,#0)
		r16 = #112
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r16)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r0 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r16)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r0 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r16)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r2 = #100
		r0 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r16)
	}
	{
		r1 = vextract(v3,r12)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r9)
	}
	{
		r1 = vextract(v3,r12)
	}
	{
		r0 = memw(r30+##-6192)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r9)
	}
	{
		r1 = vextract(v3,r12)
	}
	{
		r0 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r9)
	}
	{
		r1 = vextract(v3,r12)
	}
	{
		r0 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r9)
	}
	{
		r1 = vextract(v3,r13)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r23)
	}
	{
		r1 = vextract(v3,r13)
	}
	{
		r0 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r23)
	}
	{
		r1 = vextract(v3,r13)
	}
	{
		r0 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r23)
	}
	{
		r1 = vextract(v3,r13)
	}
	{
		r0 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r23)
	}
	{
		r1 = vextract(v3,r14)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r14)
	}
	{
		r0 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r14)
	}
	{
		r0 = memw(r30+##-18864)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r14)
	}
	{
		r0 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r2 = #84
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r15)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-18992)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r8)
	}
	{
		r1 = vextract(v3,r15)
	}
	{
		r0 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r8)
	}
	{
		r1 = vextract(v3,r15)
	}
	{
		r0 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r8)
	}
	{
		r1 = vextract(v3,r15)
	}
	{
		r0 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r8)
	}
	{
		r1 = vextract(v3,r6)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-19504)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r6)
	}
	{
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r6)
	}
	{
		r0 = memw(r30+##-19760)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r6)
	}
	{
		r0 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r5)
	}
	{
		r1 = vextract(v3,r28)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21080)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r28)
	}
	{
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r28)
	}
	{
		r0 = memw(r30+##-21088)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r28)
	}
	{
		r0 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r10)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21112)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r10)
	}
	{
		r0 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r10)
	}
	{
		r0 = memw(r30+##-21120)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r10)
	}
	{
		r0 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r11)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
		r0 = #80
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r0)
	}
	{
		r1 = vextract(v3,r11)
	}
	{
		r3 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r0)
	}
	{
		r1 = vextract(v3,r11)
	}
	{
		r1 = insert(r19,#8,#16)
		r19 = #52
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r0)
	}
	{
		r1 = vextract(v3,r11)
	}
	{
		r3 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r0)
	}
	{
		r1 = vextract(v3,r19)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21136)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r24)
	}
	{
		r1 = vextract(v3,r19)
	}
	{
		r3 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r24)
	}
	{
		r1 = vextract(v3,r19)
	}
	{
		r1 = insert(r20,#8,#16)
		r20 = #56
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r24)
	}
	{
		r1 = vextract(v3,r19)
	}
	{
		r3 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r24)
	}
	{
		r1 = vextract(v3,r20)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21152)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r26)
	}
	{
		r1 = vextract(v3,r20)
	}
	{
		r3 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r26)
	}
	{
		r1 = vextract(v3,r20)
	}
	{
		r3 = memw(r30+##-21160)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r26)
	}
	{
		r1 = vextract(v3,r20)
	}
	{
		r3 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r26)
	}
	{
		r1 = vextract(v3,r25)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21168)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #68
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r3)
	}
	{
		r1 = vextract(v3,r25)
	}
	{
		r7 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r7,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r3)
	}
	{
		r1 = vextract(v3,r25)
	}
	{
		r7 = memw(r30+##-21176)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r7,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r3)
	}
	{
		r1 = vextract(v3,r25)
	}
	{
		r7 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r7,#8,#24)
		r7 = #64
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r3)
	}
	{
		r1 = vextract(v3,r7)
	}
	{
		r1 = insert(r17,#8,#0)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r7)
	}
	{
		r1 = vextract(v3,r7)
	}
	{
		r17 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r7)
	}
	{
		r1 = vextract(v3,r7)
	}
	{
		r17 = memw(r30+##-21184)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r7)
	}
	{
		r1 = vextract(v3,r7)
	}
	{
		r17 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r7)
	}
	{
		r1 = vextract(v3,r3)
	}
	{
		v2 = valign(v2,v2,#4)
		r17 = memw(r30+##-21192)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r25)
	}
	{
		r1 = vextract(v3,r3)
	}
	{
		r17 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r25)
	}
	{
		r1 = vextract(v3,r3)
	}
	{
		r17 = memw(r30+##-21200)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r17,#8,#16)
		r17 = #100
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r25)
	}
	{
		r1 = vextract(v3,r3)
	}
	{
		r3 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r25)
	}
	{
		r1 = vextract(v3,r26)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21208)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r20)
	}
	{
		r1 = vextract(v3,r26)
	}
	{
		r3 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r20)
	}
	{
		r1 = vextract(v3,r26)
	}
	{
		r3 = memw(r30+##-21216)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r20)
	}
	{
		r1 = vextract(v3,r26)
	}
	{
		r26 = #-1
		r3 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r20)
	}
	{
		r1 = vextract(v3,r24)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21224)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r19)
	}
	{
		r1 = vextract(v3,r24)
	}
	{
		r3 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r19)
	}
	{
		r1 = vextract(v3,r24)
	}
	{
		r3 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r19)
	}
	{
		r1 = vextract(v3,r24)
	}
	{
		r3 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r19)
	}
	{
		r1 = vextract(v3,r0)
	}
	{
		v2 = valign(v2,v2,#4)
		r3 = memw(r30+##-21240)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r11)
	}
	{
		r1 = vextract(v3,r0)
	}
	{
		r3 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r11)
	}
	{
		r1 = vextract(v3,r0)
	}
	{
		r3 = memw(r30+##-21248)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r11)
	}
	{
		r1 = vextract(v3,r0)
	}
	{
		r0 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r11)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21256)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r10)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r0 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r10)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r0 = memw(r30+##-21432)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r10)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r2 = #16
		r0 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r10)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21440)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r28)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r28)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-21448)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r28)
	}
	{
		r1 = vextract(v3,r4)
	}
	{
		r0 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r28)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21456)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r6)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r6)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-21808)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r6)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r5:4 = combine(#116,#16)
		r0 = memw(r30+##-14896)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r6)
	}
	{
		r1 = vextract(v3,r8)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-21816)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r15)
	}
	{
		r1 = vextract(v3,r8)
	}
	{
		r0 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r15)
	}
	{
		r1 = vextract(v3,r8)
	}
	{
		r0 = memw(r30+##-22064)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r15)
	}
	{
		r1 = vextract(v3,r8)
	}
	{
		r0 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r15)
	}
	{
		r1 = vextract(v3,r17)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22192)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r14)
	}
	{
		r1 = vextract(v3,r17)
	}
	{
		r0 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r14)
	}
	{
		r1 = vextract(v3,r17)
	}
	{
		r0 = memw(r30+##-22200)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r14)
	}
	{
		r1 = vextract(v3,r17)
	}
	{
		r0 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r14)
	}
	{
		r1 = vextract(v3,r23)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r13)
	}
	{
		r1 = vextract(v3,r23)
	}
	{
		r0 = memw(r30+##-22208)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r13)
	}
	{
		r1 = vextract(v3,r23)
	}
	{
		r0 = memw(r30+##-22232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r13)
	}
	{
		r1 = vextract(v3,r23)
	}
	{
		r0 = memw(r30+##-22224)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r13)
	}
	{
		r1 = vextract(v3,r9)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22248)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r12)
	}
	{
		r1 = vextract(v3,r9)
	}
	{
		r0 = memw(r30+##-22240)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r12)
	}
	{
		r1 = vextract(v3,r9)
	}
	{
		r0 = memw(r30+##-22256)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r12)
	}
	{
		r1 = vextract(v3,r9)
	}
	{
		r0 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r12)
	}
	{
		r1 = vextract(v3,r16)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22264)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r16)
	}
	{
		r0 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r16)
	}
	{
		r0 = memw(r30+##-22272)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r2 = #12
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r16)
	}
	{
		r16 = add(r30,#-17072)
		r0 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22280)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-16176)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r5:4 = combine(#116,#12)
		r0 = memw(r30+##-22448)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r3:2 = combine(#120,#8)
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r5:4 = combine(#120,#8)
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22456)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r5)
	}
	{
		r0 = memw(r30+##-22704)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = vror(v2,r4)
	}
	{
		r1 = vextract(v3,r3)
	}
	{
		r0 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#24)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		r2 = #124
		v3 = vror(v2,r2)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		v2 = valign(v2,v2,#4)
		r0 = memw(r30+##-22832)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#0)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = valign(v2,v2,#4)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r2 = #124
		r0 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#8)
	}
	{
		v2.w = vinsert(r1)
	}
	{
		v3 = valign(v2,v2,#4)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r0 = memw(r30+##-22960)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r0,#8,#16)
		r0 = add(r30,#-16944)
		r5 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v2.w = vinsert(r1)
		v4 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r4 = add(r5,##-41600)
		r5 = add(r5,#-31488)
	}
	{
		v1.b = vpacke(v4.h,v1.h)
	}
	{
		v2 = valign(v2,v2,#4)
		vmem(r4+#0) = v2.new
	}
	{
		v0.b = vpacke(v4.h,v0.h)
		r0 = memw(r4+#64)
	}
	{
		v1 = vror(v1,r7)
		r1 = memw(r4+#68)
		r2 = memw(r4+#72)
	}
	{
		v0 = vor(v1,v0)
		r3 = memw(r4+#76)
		r6 = memw(r4+#80)
	}
	{
		q0 = vand(v0,r26)
		r7 = memw(r4+#84)
		r8 = memw(r4+#88)
	}
	{
		v0 = vand(q0,r26)
		r9 = memw(r4+#92)
		r12 = memw(r4+#96)
	}
	{
		r13 = memw(r4+#100)
		r14 = memw(r4+#104)
	}
	{
		v0.b = vpacke(v4.h,v0.h)
		r15 = memw(r4+#108)
		r10 = memw(r4+#112)
	}
	{
		v1 = vmemu(r16+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vor(v1,v0)
		r11 = memw(r4+#116)
		r16 = memw(r4+#120)
	}
	{
		q3 = vand(v0,r26)
		r4 = add(r30,#-5424)
		r17 = memw(r4+#124)
		memd(r5+#48) = r11:10

	} :mem_noshuf
	{
		v0 = vand(q3,r26)
		memd(r5+#40) = r15:14
		memd(r5+#32) = r13:12
	}
	{
		memd(r5+#24) = r9:8
		memd(r5+#16) = r7:6
	}
	{
		memd(r5+#8) = r3:2
		memd(r5+#0) = r1:0
	}
	{
		memd(r5+#56) = r17:16
	}
	{
		r5 = ##16843009
		r7 = memw(r30+#-560)
		v1 = vmem(r5+#0)
	}
	{
		v3 = vlalign(v4,v0,r7)
		memw(r30+##-16816) = r27
	}                                       // 4-byte Folded Spill
	{
		q3 = vand(v3,r26)
		v2 = vlalign(v4,v1,r7)
	}
	{
		v0 = vlalign(v0,v4,r7)
		if (q3) vmem(r7+#1) = v2
	}
	{
		q3 = vand(v0,r26)
		v1 = vlalign(v1,v4,r7)
	}
	{
		r4 = add(r30,#-19248)
		v4 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		q0 = vand(v4,r5)
		r5 = ##16843009
	}
	{
		v4 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		q3 = vand(v4,r5)
		r7 = add(r7,#128)
		if (q3) vmem(r7+#0) = v1
	}
	{
		if (p0) jump:nt .LBB131_265
		memw(r30+#-560) = r7
	}                                       // 4-byte Folded Spill
.LBB131_268:                            // %"for resampled_input.s0.x.rebased221.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        //     Parent Loop BB131_266 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	{
		p0 = cmp.gt(r18,#1)
		r1:0 = combine(#0,#0)
	}
	{
		r4 = p0
		if (p0) r0 = memw(r30+##-304)
	}                                       // 4-byte Folded Reload
	{
		r27 = memw(r30+##-23248)
		memw(r30+##-10288) = r4
	}                                       // 4-byte Folded Reload
	{
		if (p0) r0 = memub(r0+#0)
	}
	{
		p0 = cmp.gt(r18,#2)
		r0 = memub(r22+#0)
		memw(r30+#-816) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		memw(r30+##-17200) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_270
		memw(r30+##-10800) = r0
	}                                       // 4-byte Folded Spill
// %bb.269:                             // %true_bb614.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#2)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r1 = memub(r22+r0<<#0)
	}
.LBB131_270:                            // %after_bb616.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#3)
		r16 = #0
		r0 = #0
	}
	{
		r4 = p0
		memw(r30+##-16944) = r1
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_272
		memw(r30+##-10928) = r4
	}                                       // 4-byte Folded Spill
// %bb.271:                             // %true_bb617.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#3)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_272:                            // %after_bb619.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#4)
		memw(r30+##-17072) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_274
		memw(r30+##-11056) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.273:                             // %true_bb620.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#4)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r16 = memub(r22+r0<<#0)
	}
.LBB131_274:                            // %after_bb622.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#5)
		r17 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_276
		memw(r30+##-11184) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.275:                             // %true_bb623.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#5)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_276:                            // %after_bb625.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#6)
		memw(r30+##-17328) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_278
		memw(r30+##-11568) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.277:                             // %true_bb626.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#6)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r17 = memub(r22+r0<<#0)
	}
.LBB131_278:                            // %after_bb628.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#7)
		r19 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_280
		memw(r30+##-11824) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.279:                             // %true_bb629.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#7)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_280:                            // %after_bb631.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#8)
		memw(r30+##-17456) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_282
		memw(r30+##-12080) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.281:                             // %true_bb632.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#8)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r19 = memub(r22+r0<<#0)
	}
.LBB131_282:                            // %after_bb634.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#9)
		r20 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_284
		memw(r30+##-12336) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.283:                             // %true_bb635.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#9)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_284:                            // %after_bb637.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#10)
		memw(r30+##-17584) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_286
		memw(r30+##-12592) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.285:                             // %true_bb638.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#10)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r20 = memub(r22+r0<<#0)
	}
.LBB131_286:                            // %after_bb640.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#11)
		r23 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_288
		memw(r30+##-12848) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.287:                             // %true_bb641.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#11)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_288:                            // %after_bb643.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#12)
		memw(r30+##-17712) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_290
		memw(r30+##-12976) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.289:                             // %true_bb644.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#12)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r23 = memub(r22+r0<<#0)
	}
.LBB131_290:                            // %after_bb646.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#13)
		r24 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_292
		memw(r30+##-13104) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.291:                             // %true_bb647.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#13)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_292:                            // %after_bb649.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#14)
		memw(r30+##-17840) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_294
		memw(r30+##-13232) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.293:                             // %true_bb650.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#14)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r24 = memub(r22+r0<<#0)
	}
.LBB131_294:                            // %after_bb652.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#15)
		r25 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_296
		memw(r30+##-13360) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.295:                             // %true_bb653.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#15)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_296:                            // %after_bb655.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#16)
		memw(r30+##-17968) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_298
		memw(r30+##-13616) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.297:                             // %true_bb656.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#16)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r25 = memub(r22+r0<<#0)
	}
.LBB131_298:                            // %after_bb658.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#17)
		memw(r30+#-1072) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-13744) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_300
	}
// %bb.299:                             // %true_bb659.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#17)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_300:                            // %after_bb661.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#18)
		memw(r30+##-18096) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_302
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.301:                             // %true_bb662.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#18)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1072) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_302:                            // %after_bb664.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#19)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-14256) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_304
	}
// %bb.303:                             // %true_bb665.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#19)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_304:                            // %after_bb667.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#20)
		memw(r30+##-18224) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_306
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.305:                             // %true_bb668.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#20)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_306:                            // %after_bb670.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#21)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-14512) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_308
	}
// %bb.307:                             // %true_bb671.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#21)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_308:                            // %after_bb673.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#22)
		memw(r30+##-18232) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_310
		memw(r30+##-14640) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.309:                             // %true_bb674.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#22)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_310:                            // %after_bb676.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#23)
		memw(r30+#-3376) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-14896) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_312
	}
// %bb.311:                             // %true_bb677.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#23)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_312:                            // %after_bb679.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#24)
		memw(r30+##-18480) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_314
		memw(r30+##-15024) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.313:                             // %true_bb680.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#24)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-3376) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_314:                            // %after_bb682.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#25)
		memw(r30+##-4656) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-15152) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_316
	}
// %bb.315:                             // %true_bb683.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#25)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_316:                            // %after_bb685.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#26)
		memw(r30+##-18608) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_318
		memw(r30+##-15280) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.317:                             // %true_bb686.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#26)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-4656) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_318:                            // %after_bb688.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#27)
		memw(r30+##-5680) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-15664) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_320
	}
// %bb.319:                             // %true_bb689.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#27)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_320:                            // %after_bb691.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#28)
		memw(r30+##-18736) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_322
		memw(r30+##-15792) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.321:                             // %true_bb692.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#28)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-5680) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_322:                            // %after_bb694.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#29)
		memw(r30+##-5936) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-15920) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_324
	}
// %bb.323:                             // %true_bb695.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#29)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_324:                            // %after_bb697.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#30)
		memw(r30+##-18864) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_326
		memw(r30+##-16048) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.325:                             // %true_bb698.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#30)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-5936) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_326:                            // %after_bb700.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#31)
		memw(r30+##-6192) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-16176) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_328
	}
// %bb.327:                             // %true_bb701.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#31)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_328:                            // %after_bb703.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#32)
		memw(r30+##-18992) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_330
		memw(r30+##-16304) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.329:                             // %true_bb704.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#32)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6192) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_330:                            // %after_bb706.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#33)
		memw(r30+##-6448) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-16432) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_332
	}
// %bb.331:                             // %true_bb707.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#33)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_332:                            // %after_bb709.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#34)
		memw(r30+##-19000) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_334
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.333:                             // %true_bb710.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#34)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6448) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_334:                            // %after_bb712.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#35)
		memw(r30+##-6960) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-16688) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_336
	}
// %bb.335:                             // %true_bb713.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#35)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_336:                            // %after_bb715.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#36)
		memw(r30+##-21096) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_338
		memw(r30+##-19504) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.337:                             // %true_bb716.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#36)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6960) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_338:                            // %after_bb718.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#37)
		memw(r30+##-7216) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-19760) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_340
	}
// %bb.339:                             // %true_bb719.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#37)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_340:                            // %after_bb721.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#38)
		memw(r30+##-21104) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_342
		memw(r30+##-21080) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.341:                             // %true_bb722.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#38)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7216) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_342:                            // %after_bb724.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#39)
		memw(r30+##-7472) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21088) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_344
	}
// %bb.343:                             // %true_bb725.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#39)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_344:                            // %after_bb727.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#40)
		memw(r30+##-21464) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_346
		memw(r30+##-21112) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.345:                             // %true_bb728.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#40)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7472) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_346:                            // %after_bb730.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#41)
		memw(r30+##-7728) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21120) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_348
	}
// %bb.347:                             // %true_bb731.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#41)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_348:                            // %after_bb733.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#42)
		memw(r30+##-21816) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_350
		memw(r30+##-21128) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.349:                             // %true_bb734.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#42)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7728) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_350:                            // %after_bb736.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#43)
		memw(r30+##-7984) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21136) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_352
	}
// %bb.351:                             // %true_bb737.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#43)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_352:                            // %after_bb739.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#44)
		memw(r30+##-22064) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_354
		memw(r30+##-21152) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.353:                             // %true_bb740.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#44)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7984) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_354:                            // %after_bb742.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#45)
		memw(r30+##-8240) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21160) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_356
	}
// %bb.355:                             // %true_bb743.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#45)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_356:                            // %after_bb745.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#46)
		memw(r30+##-22192) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_358
		memw(r30+##-21168) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.357:                             // %true_bb746.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#46)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8240) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_358:                            // %after_bb748.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#47)
		memw(r30+##-8496) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21176) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_360
	}
// %bb.359:                             // %true_bb749.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#47)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_360:                            // %after_bb751.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#48)
		memw(r30+##-22200) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_362
		memw(r30+##-21184) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.361:                             // %true_bb752.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#48)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8496) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_362:                            // %after_bb754.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#49)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21192) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_364
	}
// %bb.363:                             // %true_bb755.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#49)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_364:                            // %after_bb757.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#50)
		memw(r30+##-22208) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_366
		memw(r30+##-21200) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.365:                             // %true_bb758.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#50)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_366:                            // %after_bb760.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#51)
		memw(r30+##-9008) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21208) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_368
	}
// %bb.367:                             // %true_bb761.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#51)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_368:                            // %after_bb763.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#52)
		memw(r30+##-22216) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_370
		memw(r30+##-21216) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.369:                             // %true_bb764.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#52)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9008) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_370:                            // %after_bb766.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#53)
		memw(r30+##-9264) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21224) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_372
	}
// %bb.371:                             // %true_bb767.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#53)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_372:                            // %after_bb769.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#54)
		memw(r30+##-22224) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_374
		memw(r30+##-21232) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.373:                             // %true_bb770.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#54)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9264) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_374:                            // %after_bb772.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#55)
		memw(r30+##-9520) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21240) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_376
	}
// %bb.375:                             // %true_bb773.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#55)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_376:                            // %after_bb775.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#56)
		memw(r30+##-22232) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_378
		memw(r30+##-21248) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.377:                             // %true_bb776.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#56)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9520) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_378:                            // %after_bb778.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#57)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21256) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_380
	}
// %bb.379:                             // %true_bb779.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#57)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_380:                            // %after_bb781.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#58)
		memw(r30+##-22240) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_382
		memw(r30+##-21432) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.381:                             // %true_bb782.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#58)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_382:                            // %after_bb784.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#59)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = p0
		r0 = #0
		memw(r30+##-21440) = r4.new
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_384
	}
// %bb.383:                             // %true_bb785.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#59)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_384:                            // %after_bb787.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#60)
		memw(r30+##-22248) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_386
		memw(r30+##-21448) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.385:                             // %true_bb788.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#60)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_386:                            // %after_bb790.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#61)
		r6 = #0
		r0 = #0
	}
	{
		r4 = p0
		if (!p0) jump:nt .LBB131_388
		memw(r30+##-21456) = r4.new
	}                                       // 4-byte Folded Spill
// %bb.387:                             // %true_bb791.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#61)
	}
	{
		r0 = and(r0,r27)
		r6 = #0
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_388:                            // %after_bb793.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#62)
		memw(r30+##-22256) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = p0
		if (!p0) jump:nt .LBB131_390
		memw(r30+##-21808) = r0.new
	}                                       // 4-byte Folded Spill
// %bb.389:                             // %true_bb794.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#62)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r6 = memub(r22+r0<<#0)
	}
.LBB131_390:                            // %after_bb796.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-20272)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		if (!p0.new) jump:t .LBB131_392
	}
// %bb.391:                             // %true_bb797.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#63)
		r26 = r6
	}
	{
		r0 = and(r0,r27)
		r6 = r26
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_392:                            // %after_bb799.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r2 = add(r30,#-19248)
		r4 = #60
		v5 = vxor(v5,v5)
	}
	{
		r1 = ##16843009
		v0 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		q0 = vand(v0,r1)
		r2 = add(r30,#-5424)
		r3 = #0
		v4 = vror(v5,r4)
	}
	{
		r1 = ##16843009
		v3 = v5
	}
	{
		v4.w = vinsert(r3)
		r7 = #36
		v30 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		q3 = vand(v30,r1)
		r26 = #-1
		r1 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r1 = vsplatb(r1)
		r28 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v2 = vand(q3,r26)
		r2 = add(r28,##-40832)
		v4 = valign(v4,v4,#4)
	}
	{
		v3.w = vinsert(r1)
		v1 = vand(q0,r26)
		q1 = vcmp.gt(v30.w,v30.w)
	}
	{
		v0 = vand(q1,r26)
		v2.b = vpacke(v5.h,v2.h)
		v3 = vor(v3,v4)
		vmem(r2+#0) = v3.new
	}
	{
		v1.b = vpacke(v5.h,v1.h)
		r1 = memw(r2+#0)
		r2 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#8)
		r2 = add(r30,#-17200)
		v0.b = vpacke(v5.h,v0.h)
	}
	{
		v3.w = vinsert(r1)
		vmemu(r2+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		r1 = vextract(v3,r3)
	}
	{
		r2 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#16)
	}
	{
		v3.w = vinsert(r1)
		r1 = #0
		memw(r30+#-816) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r1 = vextract(v3,r3)
	}
	{
		r2 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r3:2 = combine(#124,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		r1 = vextract(v3,r2)
	}
	{
		r1 = insert(r16,#8,#0)
	}
	{
		r16 = add(r30,#-16944)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r1 = insert(r17,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		r3:2 = combine(#120,#8)
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r1 = insert(r19,#8,#0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r1 = insert(r20,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-17712)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		r3:2 = combine(#12,#116)
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r3)
	}
	{
		r1 = insert(r23,#8,#0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r3)
	}
	{
		r5 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r3)
	}
	{
		r1 = insert(r24,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r3)
	}
	{
		r3 = memw(r30+##-17968)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		r3:2 = combine(#112,#16)
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r1 = insert(r25,#8,#0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #20
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #108
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-18480)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #24
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #104
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-4656)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #28
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-5680)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #100
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-18864)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-5936)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-18992)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #32
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-6192)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #96
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-21096)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r7)
	}
	{
		v3 = valign(v3,v3,#4)
		r2 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#0)
		r2 = #92
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r7)
	}
	{
		r3 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r7)
	}
	{
		r3 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r7)
	}
	{
		r3 = memw(r30+##-21464)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#24)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		r2 = #40
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #88
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-21816)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-22064)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #44
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #84
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-22192)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-22200)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #48
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #80
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-22208)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-22216)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #52
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #76
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-22224)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r2 = memw(r30+##-22232)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
		r2 = #56
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v3 = valign(v3,v3,#4)
		r3 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#0)
		r3 = #72
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-22240)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		r5 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r5,#8,#16)
		r5 = #64
	}
	{
		v3.w = vinsert(r1)
		v2 = vror(v2,r5)
	}
	{
		r5 = add(r28,#-31360)
		v0 = vror(v0,r5)
		v1 = vor(v2,v1)
	}
	{
		q2 = vand(v1,r26)
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r2)
	}
	{
		v1 = vand(q2,r26)
		r2 = memw(r30+##-22248)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#24)
	}
	{
		v3.w = vinsert(r1)
		v1.b = vpacke(v5.h,v1.h)
	}
	{
		v4 = vror(v3,r3)
	}
	{
		r1 = vextract(v4,r4)
	}
	{
		v3 = valign(v3,v3,#4)
		r2 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r2,#8,#0)
		r2 = #68
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r4)
	}
	{
		r3 = memw(r30+##-22256)
	}                                       // 4-byte Folded Reload
	{
		r1 = insert(r3,#8,#8)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r4)
	}
	{
		r1 = insert(r6,#8,#16)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v4 = vror(v3,r2)
	}
	{
		r1 = vextract(v4,r4)
	}
	{
		r1 = insert(r0,#8,#24)
		r4 = add(r28,##-41472)
	}
	{
		v3.w = vinsert(r1)
	}
	{
		v3 = vror(v3,r2)
		vmem(r4+#0) = v3.new
	}
	{
		r0 = memw(r4+#0)
		r1 = memw(r4+#4)
	}
	{
		r2 = memw(r4+#8)
		r3 = memw(r4+#12)
	}
	{
		r6 = memw(r4+#16)
		r7 = memw(r4+#20)
	}
	{
		r8 = memw(r4+#24)
		r9 = memw(r4+#28)
	}
	{
		r12 = memw(r4+#32)
		r13 = memw(r4+#36)
	}
	{
		r14 = memw(r4+#40)
		r15 = memw(r4+#44)
	}
	{
		r10 = memw(r4+#48)
	}
	{
		vmemu(r16+#0) = v5
	}                                       // 128-byte Folded Spill
	{
		r11 = memw(r4+#52)
		r16 = memw(r4+#56)
	}
	{
		r4 = add(r30,#-17072)
		r17 = memw(r4+#60)
		memd(r5+#48) = r11:10

	} :mem_noshuf
	{
		memd(r5+#40) = r15:14
		memd(r5+#32) = r13:12
	}
	{
		memd(r5+#24) = r9:8
	}
	{
		v0 = vor(v0,v1)
		vmemu(r4+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		q3 = vand(v0,r26)
		r16 = #0
		memd(r5+#56) = r17:16
		memd(r5+#16) = r7:6
	}
	{
		memd(r5+#8) = r3:2
		memd(r5+#0) = r1:0
	}
	{
		r7 = memw(r30+#-560)
		v31 = vmem(r5+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r7,#-64)
	}
	{
		r0 = memw(r30+##-10288)
		if (q3) vmem(r0+#0) = v31
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (p0.new) r0 = memw(r30+##-304)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		if (p0) r0 = memub(r0+#0)
		if (p0) memw(r30+##-816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_394
	}
// %bb.393:                             // %true_bb1382.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#2)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_394:                            // %after_bb1384.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-10928)
		memw(r30+##-21096) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_396
	}
// %bb.395:                             // %true_bb1385.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#3)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r16 = memub(r22+r0<<#0)
	}
.LBB131_396:                            // %after_bb1387.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r4 = memw(r30+##-11056)
		memw(r30+##-21464) = r16
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		if (!p0.new) jump:t .LBB131_398
		r16 = #0
		r0 = #0
	}
// %bb.397:                             // %true_bb1388.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#4)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_398:                            // %after_bb1390.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-11184)
		memw(r30+##-17328) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_400
	}
// %bb.399:                             // %true_bb1391.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#5)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r16 = memub(r22+r0<<#0)
	}
.LBB131_400:                            // %after_bb1393.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-11568)
		memw(r30+#-1072) = r0.new
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		if (!p0.new) jump:t .LBB131_402
	}
// %bb.401:                             // %true_bb1394.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#6)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_402:                            // %after_bb1396.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-11824)
		memw(r30+##-17456) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_404
	}
// %bb.403:                             // %true_bb1397.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#7)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1072) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_404:                            // %after_bb1399.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-12080)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		if (!p0.new) jump:t .LBB131_406
	}
// %bb.405:                             // %true_bb1400.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#8)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_406:                            // %after_bb1402.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-12336)
		memw(r30+##-17584) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_408
	}
// %bb.407:                             // %true_bb1403.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#9)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1584) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_408:                            // %after_bb1405.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-12592)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		if (!p0.new) jump:t .LBB131_410
	}
// %bb.409:                             // %true_bb1406.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#10)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_410:                            // %after_bb1408.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-12848)
		memw(r30+##-17712) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_412
	}
// %bb.411:                             // %true_bb1409.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#11)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-1840) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_412:                            // %after_bb1411.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-12976)
		memw(r30+#-3376) = r0.new
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		if (!p0.new) jump:t .LBB131_414
	}
// %bb.413:                             // %true_bb1412.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#12)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_414:                            // %after_bb1414.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-13104)
		memw(r30+##-17840) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_416
	}
// %bb.415:                             // %true_bb1415.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#13)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+#-3376) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_416:                            // %after_bb1417.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-4656) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_418
	}
// %bb.417:                             // %true_bb1418.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#14)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_418:                            // %after_bb1420.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-13360)
		memw(r30+##-17968) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_420
		memw(r30+##-21104) = r16
	}                                       // 4-byte Folded Spill
// %bb.419:                             // %true_bb1421.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#15)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-4656) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_420:                            // %after_bb1423.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r16 = #0
		memw(r30+##-5680) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_422
	}
// %bb.421:                             // %true_bb1424.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#16)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r16 = memub(r22+r0<<#0)
	}
.LBB131_422:                            // %after_bb1426.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_424
	}
// %bb.423:                             // %true_bb1427.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#17)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-5680) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_424:                            // %after_bb1429.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-5936) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_426
	}
// %bb.425:                             // %true_bb1430.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#18)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_426:                            // %after_bb1432.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-14256)
		memw(r30+##-18096) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_428
	}
// %bb.427:                             // %true_bb1433.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#19)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-5936) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_428:                            // %after_bb1435.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-6192) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_430
	}
// %bb.429:                             // %true_bb1436.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#20)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_430:                            // %after_bb1438.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-14512)
		memw(r30+##-18224) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_432
	}
// %bb.431:                             // %true_bb1439.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#21)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6192) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_432:                            // %after_bb1441.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-6448) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_434
	}
// %bb.433:                             // %true_bb1442.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#22)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_434:                            // %after_bb1444.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-14896)
		memw(r30+##-18232) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_436
	}
// %bb.435:                             // %true_bb1445.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#23)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6448) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_436:                            // %after_bb1447.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-6960) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_438
	}
// %bb.437:                             // %true_bb1448.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#24)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_438:                            // %after_bb1450.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-15152)
		memw(r30+##-18480) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_440
	}
// %bb.439:                             // %true_bb1451.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#25)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-6960) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_440:                            // %after_bb1453.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-7216) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_442
	}
// %bb.441:                             // %true_bb1454.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#26)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_442:                            // %after_bb1456.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-15664)
		memw(r30+##-18608) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_444
	}
// %bb.443:                             // %true_bb1457.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#27)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7216) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_444:                            // %after_bb1459.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-7472) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_446
	}
// %bb.445:                             // %true_bb1460.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#28)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_446:                            // %after_bb1462.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-15920)
		memw(r30+##-18736) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_448
	}
// %bb.447:                             // %true_bb1463.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#29)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7472) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_448:                            // %after_bb1465.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-7728) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_450
	}
// %bb.449:                             // %true_bb1466.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#30)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_450:                            // %after_bb1468.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-16176)
		memw(r30+##-18864) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_452
	}
// %bb.451:                             // %true_bb1469.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#31)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7728) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_452:                            // %after_bb1471.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-7984) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_454
	}
// %bb.453:                             // %true_bb1472.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#32)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_454:                            // %after_bb1474.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-16432)
		memw(r30+##-18992) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_456
	}
// %bb.455:                             // %true_bb1475.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#33)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-7984) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_456:                            // %after_bb1477.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-8240) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_458
	}
// %bb.457:                             // %true_bb1478.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#34)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_458:                            // %after_bb1480.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-16688)
		memw(r30+##-19000) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_460
	}
// %bb.459:                             // %true_bb1481.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#35)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8240) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_460:                            // %after_bb1483.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-19504)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-8496) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_462
	}
// %bb.461:                             // %true_bb1484.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#36)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_462:                            // %after_bb1486.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-19760)
		memw(r30+##-19504) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_464
	}
// %bb.463:                             // %true_bb1487.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#37)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8496) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_464:                            // %after_bb1489.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21080)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-8752) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_466
	}
// %bb.465:                             // %true_bb1490.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#38)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_466:                            // %after_bb1492.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21088)
		memw(r30+##-19760) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_468
	}
// %bb.467:                             // %true_bb1493.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#39)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_468:                            // %after_bb1495.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21112)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-9008) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_470
	}
// %bb.469:                             // %true_bb1496.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#40)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_470:                            // %after_bb1498.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21120)
		memw(r30+##-21080) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_472
	}
// %bb.471:                             // %true_bb1499.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#41)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9008) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_472:                            // %after_bb1501.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-9264) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_474
	}
// %bb.473:                             // %true_bb1502.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#42)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_474:                            // %after_bb1504.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21136)
		memw(r30+##-21088) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_476
	}
// %bb.475:                             // %true_bb1505.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#43)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9264) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_476:                            // %after_bb1507.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21152)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-9520) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_478
	}
// %bb.477:                             // %true_bb1508.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#44)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_478:                            // %after_bb1510.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21160)
		memw(r30+##-21112) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_480
	}
// %bb.479:                             // %true_bb1511.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#45)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9520) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_480:                            // %after_bb1513.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21168)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-9776) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_482
	}
// %bb.481:                             // %true_bb1514.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#46)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_482:                            // %after_bb1516.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21176)
		memw(r30+##-21120) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_484
	}
// %bb.483:                             // %true_bb1517.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#47)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-9776) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_484:                            // %after_bb1519.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21184)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-10032) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_486
	}
// %bb.485:                             // %true_bb1520.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#48)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_486:                            // %after_bb1522.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21192)
		memw(r30+##-21128) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_488
	}
// %bb.487:                             // %true_bb1523.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#49)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-10032) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_488:                            // %after_bb1525.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r19 = #0
		memw(r30+##-10288) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-21200)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_490
	}
// %bb.489:                             // %true_bb1526.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#50)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r19 = memub(r22+r0<<#0)
	}
.LBB131_490:                            // %after_bb1528.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21208)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_492
	}
// %bb.491:                             // %true_bb1529.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#51)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-10288) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_492:                            // %after_bb1531.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21216)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-10800) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_494
	}
// %bb.493:                             // %true_bb1532.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#52)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_494:                            // %after_bb1534.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21224)
		memw(r30+##-21136) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_496
	}
// %bb.495:                             // %true_bb1535.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#53)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-10800) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_496:                            // %after_bb1537.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r20 = #0
		memw(r30+##-10928) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-21232)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_498
	}
// %bb.497:                             // %true_bb1538.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#54)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r20 = memub(r22+r0<<#0)
	}
.LBB131_498:                            // %after_bb1540.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21240)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_500
	}
// %bb.499:                             // %true_bb1541.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#55)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-10928) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_500:                            // %after_bb1543.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21248)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-11056) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_502
	}
// %bb.501:                             // %true_bb1544.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#56)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_502:                            // %after_bb1546.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21256)
		memw(r30+##-21152) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_504
	}
// %bb.503:                             // %true_bb1547.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#57)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-11056) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_504:                            // %after_bb1549.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21432)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-11184) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_506
	}
// %bb.505:                             // %true_bb1550.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#58)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_506:                            // %after_bb1552.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21440)
		memw(r30+##-21160) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_508
	}
// %bb.507:                             // %true_bb1553.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#59)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-11184) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_508:                            // %after_bb1555.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21448)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-11568) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_510
	}
// %bb.509:                             // %true_bb1556.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#60)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_510:                            // %after_bb1558.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-21456)
		memw(r30+##-21168) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_512
	}
// %bb.511:                             // %true_bb1559.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#61)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-11568) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_512:                            // %after_bb1561.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r4 = memw(r30+##-21808)
	}                                       // 4-byte Folded Reload
	{
		p0 = r4
		r0 = #0
		memw(r30+##-11824) = r0
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_514
	}
// %bb.513:                             // %true_bb1562.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#62)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_514:                            // %after_bb1564.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = memw(r30+##-20272)
		memw(r30+##-21176) = r0
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_516
	}
// %bb.515:                             // %true_bb1565.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#63)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-11824) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_516:                            // %after_bb1567.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		r17 = #0
		memw(r30+##-12080) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-23232)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:t .LBB131_518
	}
// %bb.517:                             // %true_bb1568.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#64)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r17 = memub(r22+r0<<#0)
	}
.LBB131_518:                            // %after_bb1570.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#65)
		if (!p0.new) jump:t .LBB131_520
	}
// %bb.519:                             // %true_bb1571.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#65)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-12080) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_520:                            // %after_bb1573.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#66)
		memw(r30+##-12336) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_522
	}
// %bb.521:                             // %true_bb1574.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#66)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_522:                            // %after_bb1576.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#67)
		if (!p0.new) jump:t .LBB131_524
		memw(r30+##-21184) = r0
	}                                       // 4-byte Folded Spill
// %bb.523:                             // %true_bb1577.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#67)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-12336) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_524:                            // %after_bb1579.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#68)
		memw(r30+##-12592) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_526
	}
// %bb.525:                             // %true_bb1580.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#68)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_526:                            // %after_bb1582.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#69)
		if (!p0.new) jump:t .LBB131_528
		memw(r30+##-21192) = r0
	}                                       // 4-byte Folded Spill
// %bb.527:                             // %true_bb1583.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#69)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-12592) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_528:                            // %after_bb1585.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#70)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_530
	}
// %bb.529:                             // %true_bb1586.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#70)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_530:                            // %after_bb1588.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#71)
		if (!p0.new) jump:t .LBB131_532
		memw(r30+##-21200) = r0
	}                                       // 4-byte Folded Spill
// %bb.531:                             // %true_bb1589.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#71)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_532:                            // %after_bb1591.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#72)
		memw(r30+##-12976) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_534
	}
// %bb.533:                             // %true_bb1592.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#72)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_534:                            // %after_bb1594.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#73)
		if (!p0.new) jump:t .LBB131_536
		memw(r30+##-21208) = r0
	}                                       // 4-byte Folded Spill
// %bb.535:                             // %true_bb1595.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#73)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-12976) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_536:                            // %after_bb1597.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#74)
		memw(r30+##-13104) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_538
	}
// %bb.537:                             // %true_bb1598.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#74)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_538:                            // %after_bb1600.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#75)
		if (!p0.new) jump:t .LBB131_540
		memw(r30+##-21216) = r0
	}                                       // 4-byte Folded Spill
// %bb.539:                             // %true_bb1601.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#75)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-13104) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_540:                            // %after_bb1603.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#76)
		memw(r30+##-13232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_542
	}
// %bb.541:                             // %true_bb1604.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#76)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_542:                            // %after_bb1606.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#77)
		if (!p0.new) jump:t .LBB131_544
		memw(r30+##-21224) = r0
	}                                       // 4-byte Folded Spill
// %bb.543:                             // %true_bb1607.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#77)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-13232) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_544:                            // %after_bb1609.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#78)
		memw(r30+##-13360) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_546
	}
// %bb.545:                             // %true_bb1610.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#78)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_546:                            // %after_bb1612.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#79)
		if (!p0.new) jump:t .LBB131_548
		memw(r30+##-21232) = r0
	}                                       // 4-byte Folded Spill
// %bb.547:                             // %true_bb1613.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#79)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-13360) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_548:                            // %after_bb1615.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#80)
		memw(r30+##-13616) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_550
	}
// %bb.549:                             // %true_bb1616.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#80)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_550:                            // %after_bb1618.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#81)
		if (!p0.new) jump:t .LBB131_552
		memw(r30+##-21240) = r0
	}                                       // 4-byte Folded Spill
// %bb.551:                             // %true_bb1619.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#81)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-13616) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_552:                            // %after_bb1621.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#82)
		memw(r30+##-13744) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_554
	}
// %bb.553:                             // %true_bb1622.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#82)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_554:                            // %after_bb1624.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#83)
		if (!p0.new) jump:t .LBB131_556
		memw(r30+##-21248) = r0
	}                                       // 4-byte Folded Spill
// %bb.555:                             // %true_bb1625.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#83)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-13744) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_556:                            // %after_bb1627.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#84)
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_558
	}
// %bb.557:                             // %true_bb1628.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#84)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_558:                            // %after_bb1630.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#85)
		if (!p0.new) jump:t .LBB131_560
		memw(r30+##-21256) = r0
	}                                       // 4-byte Folded Spill
// %bb.559:                             // %true_bb1631.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#85)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_560:                            // %after_bb1633.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#86)
		memw(r30+##-14256) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_562
	}
// %bb.561:                             // %true_bb1634.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#86)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_562:                            // %after_bb1636.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#87)
		if (!p0.new) jump:t .LBB131_564
		memw(r30+##-21432) = r0
	}                                       // 4-byte Folded Spill
// %bb.563:                             // %true_bb1637.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#87)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14256) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_564:                            // %after_bb1639.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#88)
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_566
	}
// %bb.565:                             // %true_bb1640.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#88)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_566:                            // %after_bb1642.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#89)
		if (!p0.new) jump:t .LBB131_568
		memw(r30+##-21440) = r0
	}                                       // 4-byte Folded Spill
// %bb.567:                             // %true_bb1643.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#89)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_568:                            // %after_bb1645.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#90)
		memw(r30+##-14512) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_570
	}
// %bb.569:                             // %true_bb1646.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#90)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_570:                            // %after_bb1648.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#91)
		if (!p0.new) jump:t .LBB131_572
		memw(r30+##-21448) = r0
	}                                       // 4-byte Folded Spill
// %bb.571:                             // %true_bb1649.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#91)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14512) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_572:                            // %after_bb1651.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#92)
		memw(r30+##-14640) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_574
	}
// %bb.573:                             // %true_bb1652.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#92)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_574:                            // %after_bb1654.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#93)
		if (!p0.new) jump:t .LBB131_576
		memw(r30+##-21456) = r0
	}                                       // 4-byte Folded Spill
// %bb.575:                             // %true_bb1655.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#93)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14640) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_576:                            // %after_bb1657.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#94)
		memw(r30+##-14896) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_578
	}
// %bb.577:                             // %true_bb1658.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#94)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_578:                            // %after_bb1660.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#95)
		if (!p0.new) jump:t .LBB131_580
		memw(r30+##-21808) = r0
	}                                       // 4-byte Folded Spill
// %bb.579:                             // %true_bb1661.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#95)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-14896) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_580:                            // %after_bb1663.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#96)
		memw(r30+##-15024) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_582
	}
// %bb.581:                             // %true_bb1664.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#96)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_582:                            // %after_bb1666.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#97)
		if (!p0.new) jump:t .LBB131_584
		memw(r30+##-21816) = r0
	}                                       // 4-byte Folded Spill
// %bb.583:                             // %true_bb1667.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#97)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15024) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_584:                            // %after_bb1669.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#98)
		memw(r30+##-15152) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_586
	}
// %bb.585:                             // %true_bb1670.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#98)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_586:                            // %after_bb1672.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#99)
		if (!p0.new) jump:t .LBB131_588
		memw(r30+##-22064) = r0
	}                                       // 4-byte Folded Spill
// %bb.587:                             // %true_bb1673.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#99)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15152) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_588:                            // %after_bb1675.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#100)
		memw(r30+##-15280) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_590
	}
// %bb.589:                             // %true_bb1676.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#100)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_590:                            // %after_bb1678.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#101)
		if (!p0.new) jump:t .LBB131_592
		memw(r30+##-22192) = r0
	}                                       // 4-byte Folded Spill
// %bb.591:                             // %true_bb1679.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#101)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15280) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_592:                            // %after_bb1681.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#102)
		memw(r30+##-15664) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_593
	}
// %bb.635:                             // %true_bb1682.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#102)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#103)
		r0 = memub(r22+r0<<#0)
		memw(r30+##-22200) = r0.new
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_636
	}
.LBB131_594:                            // %after_bb1687.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#104)
		r25 = #0
		r0 = #0
		if (!p0.new) jump:t .LBB131_595
	}
.LBB131_637:                            // %true_bb1688.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#104)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#105)
		r0 = memub(r22+r0<<#0)
		memw(r30+##-22216) = r0.new
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_638
	}
.LBB131_596:                            // %after_bb1693.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#106)
		if (!p0.new) jump:t .LBB131_597
		r23 = #0
		r0 = #0
	}
.LBB131_639:                            // %true_bb1694.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#106)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#107)
		r0 = memub(r22+r0<<#0)
		memw(r30+##-22232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_640
	}
.LBB131_598:                            // %after_bb1699.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#108)
		r24 = #0
		r0 = #0
		if (!p0.new) jump:t .LBB131_599
	}
.LBB131_641:                            // %true_bb1700.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#108)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#109)
		r0 = memub(r22+r0<<#0)
		memw(r30+##-22248) = r0.new
	}                                       // 4-byte Folded Spill
	{
		if (p0) jump:nt .LBB131_600
	}
	{
		jump .LBB131_601
	}
	.p2align	4
.LBB131_593:                            // %after_bb1684.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#103)
		if (!p0.new) jump:t .LBB131_594
		memw(r30+##-22200) = r0
	}                                       // 4-byte Folded Spill
.LBB131_636:                            // %true_bb1685.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#103)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#104)
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15664) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25 = #0
		r0 = #0
		if (p0) jump:nt .LBB131_637
	}
.LBB131_595:                            // %after_bb1690.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#105)
		if (!p0.new) jump:t .LBB131_596
		memw(r30+##-22216) = r0
	}                                       // 4-byte Folded Spill
.LBB131_638:                            // %true_bb1691.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#105)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#106)
		r23 = #0
		r0 = #0
		r25 = memub(r22+r0<<#0)
	}
	{
		if (p0) jump:nt .LBB131_639
	}
.LBB131_597:                            // %after_bb1696.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#107)
		if (!p0.new) jump:t .LBB131_598
		memw(r30+##-22232) = r0
	}                                       // 4-byte Folded Spill
.LBB131_640:                            // %true_bb1697.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#107)
	}
	{
		r0 = and(r0,r27)
	}
	{
		p0 = cmp.gt(r18,#108)
		r24 = #0
		r0 = #0
		r23 = memub(r22+r0<<#0)
	}
	{
		if (p0) jump:nt .LBB131_641
	}
.LBB131_599:                            // %after_bb1702.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#109)
		if (!p0.new) jump:t .LBB131_601
		memw(r30+##-22248) = r0
	}                                       // 4-byte Folded Spill
.LBB131_600:                            // %true_bb1703.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#109)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r24 = memub(r22+r0<<#0)
	}
.LBB131_601:                            // %after_bb1705.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#110)
		memw(r30+##-15792) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_603
	}
// %bb.602:                             // %true_bb1706.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#110)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_603:                            // %after_bb1708.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#111)
		if (!p0.new) jump:t .LBB131_605
		memw(r30+##-22256) = r0
	}                                       // 4-byte Folded Spill
// %bb.604:                             // %true_bb1709.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#111)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15792) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_605:                            // %after_bb1711.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#112)
		memw(r30+##-15920) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_607
	}
// %bb.606:                             // %true_bb1712.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#112)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_607:                            // %after_bb1714.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#113)
		if (!p0.new) jump:t .LBB131_609
		memw(r30+##-22264) = r0
	}                                       // 4-byte Folded Spill
// %bb.608:                             // %true_bb1715.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#113)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-15920) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_609:                            // %after_bb1717.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#114)
		memw(r30+##-16048) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_611
	}
// %bb.610:                             // %true_bb1718.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#114)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_611:                            // %after_bb1720.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#115)
		if (!p0.new) jump:t .LBB131_613
		memw(r30+##-22272) = r0
	}                                       // 4-byte Folded Spill
// %bb.612:                             // %true_bb1721.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#115)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16048) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_613:                            // %after_bb1723.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#116)
		memw(r30+##-16176) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_615
	}
// %bb.614:                             // %true_bb1724.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#116)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_615:                            // %after_bb1726.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#117)
		if (!p0.new) jump:t .LBB131_617
		memw(r30+##-22280) = r0
	}                                       // 4-byte Folded Spill
// %bb.616:                             // %true_bb1727.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#117)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16176) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_617:                            // %after_bb1729.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#118)
		memw(r30+##-16304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_619
	}
// %bb.618:                             // %true_bb1730.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#118)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_619:                            // %after_bb1732.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#119)
		if (!p0.new) jump:t .LBB131_621
		memw(r30+##-22448) = r0
	}                                       // 4-byte Folded Spill
// %bb.620:                             // %true_bb1733.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#119)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16304) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_621:                            // %after_bb1735.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#120)
		memw(r30+##-16432) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_623
	}
// %bb.622:                             // %true_bb1736.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#120)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_623:                            // %after_bb1738.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#121)
		if (!p0.new) jump:t .LBB131_625
		memw(r30+##-22456) = r0
	}                                       // 4-byte Folded Spill
// %bb.624:                             // %true_bb1739.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#121)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16432) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_625:                            // %after_bb1741.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#122)
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_627
	}
// %bb.626:                             // %true_bb1742.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#122)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_627:                            // %after_bb1744.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#123)
		if (!p0.new) jump:t .LBB131_629
		memw(r30+##-22704) = r0
	}                                       // 4-byte Folded Spill
// %bb.628:                             // %true_bb1745.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#123)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_629:                            // %after_bb1747.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		r0 = #0
		p0 = cmp.gt(r18,#124)
		memw(r30+##-16688) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = #0
		if (!p0) jump:nt .LBB131_631
	}
// %bb.630:                             // %true_bb1748.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#124)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
	}
.LBB131_631:                            // %after_bb1750.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#125)
		if (!p0.new) jump:t .LBB131_633
		memw(r30+##-22832) = r0
	}                                       // 4-byte Folded Spill
// %bb.632:                             // %true_bb1751.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#125)
	}
	{
		r0 = and(r0,r27)
	}
	{
		r0 = memub(r22+r0<<#0)
		memw(r30+##-16688) = r0.new
	}                                       // 4-byte Folded Spill
.LBB131_633:                            // %after_bb1753.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		p0 = cmp.gt(r18,#126)
		r0 = #0
		memw(r30+##-22240) = r24
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-22224) = r23
		memw(r30+##-22208) = r25
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_267
	}
// %bb.634:                             // %true_bb1754.us
                                        //   in Loop: Header=BB131_268 Depth=3
	{
		call ##__hexagon_divsi3
		r1:0 = combine(r21,#126)
	}
	{
		r0 = and(r0,r27)
	}
	{
		jump .LBB131_267
		r0 = memub(r22+r0<<#0)
	}
	.p2align	4
.LBB131_642:                            // %"consume resampled_input1760"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r0 = memw(r30+##-23664)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_247
	}
// %bb.643:                             // %"for output.s0.y.yo1761.preheader"
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r0 = memw(r30+##-10312)
		r3 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-10320)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r2,#-27392)
		r1 = memw(r30+##-23624)
		v0 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-23432)
		v1 = vmem(r4+#0)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r2,r1)
		r5 = memw(r30+##-23456)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-23464)
		v2 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r3,r4)
		r2 = add(r3,r5)
		r4 = memw(r30+##-7232)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r30,#-19504)
		r5 = add(r30,#-19376)
		memw(r30+##-21104) = r2
	}                                       // 4-byte Folded Spill
	{
		r3 = add(r30,#-19760)
		v3 = vmem(r4+#0)
		memw(r30+##-21112) = r0
	}                                       // 4-byte Folded Spill
	{
		r2 = #0
		vmemu(r2+#0) = v0
	}                                       // 256-byte Folded Spill
	{
		vmemu(r5+#0) = v1
	}                                       // 256-byte Folded Spill
	{
		r5 = memw(r30+##-23640)
		memw(r30+##-21120) = r2
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r30,#-19632)
		vmemu(r3+#0) = v2
	}                                       // 256-byte Folded Spill
	{
		r0 = mpyi(r1,r5)
		memw(r30+##-21128) = r0.new
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_645
		vmemu(r2+#0) = v3
	}                                       // 256-byte Folded Spill
	.p2align	4
.LBB131_644:                            // %"end for output.s0.x.xo1765"
                                        //   in Loop: Header=BB131_645 Depth=2
	{
		r1 = memw(r30+##-21120)
		r0 = memw(r30+##-21144)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r1,#1)
		r3 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r1,r0)
		memw(r30+##-21120) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-23328)
		r5 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r5,r0)
		r20 = memw(r30+##-23272)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-21112)
		memw(r30+##-21104) = r1
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r4,r0)
		if (p0) jump:nt .LBB131_247
		memw(r30+##-21112) = r1.new
	}                                       // 4-byte Folded Spill
.LBB131_645:                            // %"for output.s0.y.yo1761"
                                        //   Parent Loop BB131_248 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB131_648 Depth 3
                                        //         Child Loop BB131_710 Depth 4
                                        //           Child Loop BB131_713 Depth 5
	{
		r0 = memw(r30+##-23376)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p0.new) jump:nt .LBB131_644
	}
// %bb.646:                             // %"for output.s0.x.xo1764.preheader"
                                        //   in Loop: Header=BB131_645 Depth=2
	{
		r2 = add(r30,#-18864)
		r0 = memw(r30+##-23360)
	}                                       // 4-byte Folded Reload
	{
		r6 = #0
		r5 = memw(r30+##-21128)
	}                                       // 4-byte Folded Reload
	{
		r1 = memw(r30+##-21120)
		r3 = memw(r30+##-23296)
	}                                       // 4-byte Folded Reload
	{
		v0 = vsplat(r3)
		r0 = add(r1,r0)
		r3 = memw(r30+##-23384)
	}                                       // 4-byte Folded Reload
	{
		v31 = vsplat(r3)
		r3 = memw(r30+##-23352)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r30,#-18992)
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		r1 = mpyi(r0,r3)
		r3 = memw(r30+##-23312)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r3,r1)
		r4 = memw(r30+##-23280)
	}                                       // 4-byte Folded Reload
	{
		r4 = sub(r1,r4)
		vmemu(r2+#0) = v31
	}                                       // 128-byte Folded Spill
	{
		r2 = memw(r30+##-23304)
		memw(r30+##-4656) = r4
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r2,r1)
		r1 = memw(r30+##-23368)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r0,r1)
		r0 = memw(r30+#-3632)
		memw(r30+##-19000) = r5.new
	}                                       // 4-byte Folded Reload
	{
		r5 = memw(r30+##-21112)
		r7 = memw(r30+##-21104)
	}                                       // 4-byte Folded Reload
	{
		r2 = mpyi(r2,r0)
		memw(r30+##-21080) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r2 = mpyi(r3,r0)
		r0 = mpyi(r4,r0)
		memw(r30+##-21088) = r2.new
	}                                       // 4-byte Folded Spill
	{
		jump .LBB131_648
		memw(r30+##-21096) = r0
	}                                       // 4-byte Folded Spill
	.p2align	4
.LBB131_647:                            // %else1056
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r5:4 = extractu(r3:2,#8,#40)
		p1 = !tstbit(r1,#29)
		r6 = add(r6,#1)
	}
	{
		p2 = !tstbit(r1,#31)
		r4 = memw(r30+##-19768)
		if (!p1) memb(r0+#29) = r4
	}                                       // 4-byte Folded Reload
	{
		r5:4 = extractu(r3:2,#8,#48)
		p0 = !tstbit(r1,#30)
		p1 = cmp.eq(r6,r4)
	}
	{
		r3:2 = extractu(r3:2,#8,#56)
	}
	{
		r2 = memw(r30+##-21072)
		if (!p2) memb(r0+#31) = r2
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r7,r2)
		r5 = memw(r30+##-5936)
		if (!p0) memb(r0+#30) = r4
	}                                       // 4-byte Folded Reload
	{
		r5 = add(r5,r2)
		if (p1) jump:nt .LBB131_644
	}
.LBB131_648:                            // %"for output.s0.x.xo1764"
                                        //   Parent Loop BB131_248 Depth=1
                                        //     Parent Loop BB131_645 Depth=2
                                        // =>    This Loop Header: Depth=3
                                        //         Child Loop BB131_710 Depth 4
                                        //           Child Loop BB131_713 Depth 5
	{
		r0 = memw(r30+##-23568)
		memw(r30+##-5680) = r6
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		r2 = memw(r30+##-20664)
	}                                       // 4-byte Folded Reload
	{
		p0 = r2
		memw(r30+##-5936) = r5
	}                                       // 4-byte Folded Spill
	{
		if (!p0) jump:nt .LBB131_706
		memw(r30+##-6192) = r7
	}                                       // 4-byte Folded Spill
// %bb.649:                             // %then_bb1769
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		v12 = vxor(v12,v12)
		r8 = memw(r30+##-21064)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r9 = add(r6,r4)
		r0 = add(r8,#-1792)
		r4 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		r3 = memw(r30+#-2864)
		v6 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r3,#1024)
		r2 = add(r3,#1280)
		r0 = memw(r30+##-23256)
	}                                       // 4-byte Folded Reload
	{
		r4 = mpyi(r9,r4)
		r27 = r8
		r13 = memw(r30+##-23264)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r4,r0)
		r13 = add(r4,r13)
		r12 = memw(r30+##-21080)
	}                                       // 4-byte Folded Reload
	{
		r14 = add(r4,r12)
		r21 = memw(r30+##-20912)
	}                                       // 4-byte Folded Reload
	{
		r14 = sub(r14,r21)
		r15 = add(r0,r12)
		r28 = add(r13,r12)
		r12 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r14 = addasl(r12,r14,#7)
		r17 = asl(r14,#7)
		r18 = memw(r30+##-21088)
	}                                       // 4-byte Folded Reload
	{
		r14 = add(r4,r18)
		r20 = memw(r30+##-21096)
		memw(r30+#-560) = r14
	}                                       // 4-byte Folded Reload
	{
		r10 = asl(r15,#7)
		r1 = add(r30,#-7984)
		r4 = add(r4,r20)
		v1 = vmem(r1+#0)
	}
	{
		r5 = addasl(r12,r15,#7)
		r14 = sub(r14,r21)
		r15 = add(r0,r18)
	}
	{
		r4 = addasl(r12,r14,#7)
		r1 = sub(r4,r21)
		vmemu(r1+#0) = v1
	}                                       // 128-byte Folded Spill
	{
		r0 = add(r0,r20)
		v4 = vmem(r2+#0)
		memw(r30+##-13232) = r4
	}                                       // 4-byte Folded Spill
	{
		r4 = asl(r0,#7)
		v7 = vmem(r27+#0)
		memw(r30+##-12848) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r30,#-7728)
		r4 = #64
		r11 = add(r13,r18)
		r6 = add(r3,#64)
	}
	{
		r20 = asl(r15,#7)
		r7 = add(r3,#1792)
		r13 = add(r13,r20)
		v2 = valign(v1,v1,r4)
	}
	{
		r18 = addasl(r12,r15,#7)
		r15 = asl(r11,#7)
		v14 = valign(v7,v7,r4)
		v9 = vmem(r7+#0)
	}
	{
		r22 = add(r3,#1536)
		v16 = v12
		vmemu(r2+#0) = v2
	}                                       // 128-byte Folded Spill
	{
		v19:18.w = vunpack(v7.h)
		v17 = v12
		r2 = memw(r30+#-3912)
		v7 = vmem(r6+#0)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r30,#-10928)
		r19 = add(r8,#-1664)
		r25:24 = memd(r12+r10<<#0)
		v15 = vmem(r3+#4)
	}
	{
		v3 = valign(v4,v4,r4)
	}
	{
		v16.w = vinsert(r24)
		r8 = add(r3,#1152)
		vmemu(r6+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		r26 = add(r3,#1664)
		v11 = valign(v9,v9,r4)
		r2 = memw(r30+#-3904)
		v10 = vmem(r2+#0)
	}                                       // 4-byte Folded Reload
	{
		r10 = asl(r14,#7)
		v2 = valign(v15,v15,r4)
		r7:6 = memd(r12+r15<<#0)
		v0 = vmem(r3+#6)
	}
	{
		v1 = valign(v0,v0,r4)
		r7:6 = memd(r5+#16)
		memd(r30+##-11568) = r7:6
	}                                       // 8-byte Folded Spill
	{
		v17.w = vinsert(r6)
		v20 = v12
		q0 = vcmp.eq(v13.b,v12.b)
		v13.cur = vmem(r2+#0)
	}
	{
		v13 = valign(v6,v6,r4)
		v8 = vmem(r22+#0)
	}
	{
		r2 = add(r30,#-7216)
		v5 = valign(v8,v8,r4)
		memw(r30+##-6448) = r9
	}                                       // 4-byte Folded Spill
	{
		r4 = add(r30,#-1584)
		r9 = add(r3,#1408)
		v31 = vror(v12,r4)
		v7 = vmem(r19+#0)
	}
	{
		r23 = addasl(r12,r28,#7)
		v29:28.w = vunpack(v14.h)
		v25 = vmem(r3+#0)
		memw(r30+#-2864) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = addasl(r12,r0,#7)
		r16 = asl(r28,#7)
		v16 = valign(v16,v16,#4)
		r15:14 = memd(r5+#24)
	}
	{
		v16.w = vinsert(r25)
		r4 = add(r30,#-10800)
		vmemu(r4+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		r28 = #116
		v17 = valign(v17,v17,#4)
		v7 = vmem(r8+#0)
		memw(r30+#-304) = r0
	}                                       // 4-byte Folded Spill
	{
		v17.w = vinsert(r7)
		r4 = add(r30,#-10672)
		vmemu(r4+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		r0 = asl(r13,#7)
		r4 = add(r30,#-1072)
		vmemu(r4+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		r11 = addasl(r12,r11,#7)
		r2 = add(r30,#-7088)
		vmemu(r2+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		v19:18 = vcombine(v12,v12)
		vmemu(r2+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-1840)
		vmemu(r4+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		v14 = valign(v16,v16,#4)
		v16 = v12
		r3:2 = memd(r5+#8)
		v7 = vmem(r9+#0)
	}
	{
		v14.w = vinsert(r2)
		v27:26.w = vunpack(v9.h)
		r9:8 = memd(r5+#32)
		r7:6 = memd(r5+#48)
	}
	{
		v16.w = vinsert(r8)
		v20.w = vinsert(r6)
		v9 = valign(v17,v17,#4)
		r25:24 = memd(r5+#40)
	}
	{
		v9.w = vinsert(r14)
		v14 = valign(v14,v14,#4)
		memw(r30+##-12976) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = addasl(r12,r13,#7)
		v14.w = vinsert(r3)
		v16 = valign(v16,v16,#4)
	}
	{
		r0 = asl(r1,#7)
		v16.w = vinsert(r9)
		memw(r30+##-12592) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = addasl(r12,r1,#7)
		v9 = valign(v9,v9,#4)
		memw(r30+##-11824) = r0
	}                                       // 4-byte Folded Spill
	{
		v9.w = vinsert(r15)
		v14 = vror(v14,r28)
		memw(r30+##-12336) = r0
	}                                       // 4-byte Folded Spill
	{
		r2 = add(r30,#-6960)
		v17 = valign(v20,v20,#4)
		v14 = vor(v14,v31)
		r1:0 = memd(r12+r16<<#0)
	}
	{
		v17.w = vinsert(r7)
		v16 = valign(v16,v16,#4)
		memw(r30+##-11184) = r5
	}                                       // 4-byte Folded Spill
	{
		v16.w = vinsert(r24)
		r7:6 = memd(r23+#16)
		memw(r30+##-8752) = r23
	}                                       // 4-byte Folded Spill
	{
		v9 = vror(v9,r28)
		r9:8 = memd(r23+#40)
		memw(r30+##-7472) = r11
	}                                       // 4-byte Folded Spill
	{
		v21:20.uh = vunpack(v14.ub)
		v9 = vor(v9,v31)
		r19 = memw(r30+#-560)
		r17:16 = memd(r12+r17<<#0)
	}                                       // 4-byte Folded Reload
	{
		r4 = add(r30,#-816)
		vmemu(r4+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		v16 = valign(v16,v16,#4)
		r27:26 = memd(r5+#56)
		v7 = vmem(r26+#0)
	}
	{
		v16.w = vinsert(r25)
		v14 = valign(v17,v17,#4)
		v17 = v12
		r25:24 = memd(r23+#24)
	}
	{
		v14.w = vinsert(r26)
		v23:22.uh = vunpack(v9.ub)
		v9 = v12
		r15:14 = memd(r19+#56)
	}
	{
		v17.w = vinsert(r6)
		v9.w = vinsert(r0)
		r0 = add(r30,#-3376)
		v16 = vror(v16,r28)
	}
	{
		v14 = valign(v14,v14,#4)
		r21:20 = memd(r12+r20<<#0)
		r13 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		v14.w = vinsert(r27)
		v17 = valign(v17,v17,#4)
		r27:26 = memd(r23+#32)
	}
	{
		v17.w = vinsert(r7)
		v23:22.uw = vunpack(v22.uh)
		r7:6 = memd(r11+#8)
	}
	{
		v29:28.w = vunpack(v11.h)
		v11 = vor(v16,v31)
	}
	{
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r1)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v16 = vdelta(v22,v10)
	}
	{
		v14 = vror(v14,r28)
		v30 = vmux(q0,v16,v20)
	}
	{
		v21:20.uh = vunpack(v11.ub)
		v14 = vor(v14,v31)
	}
	{
		v11 = valign(v17,v17,#4)
		v17 = v12
	}
	{
		v11.w = vinsert(r24)
		r2 = add(r30,#-6832)
		vmemu(r2+#0) = v26
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v27
	}                                       // 256-byte Folded Spill
	{
		v9 = valign(v9,v9,#4)
		r3:2 = memd(r23+#8)
	}
	{
		v9.w = vinsert(r2)
		r2 = add(r30,#-11056)
		v23:22.uh = vunpack(v14.ub)
		v14 = v12
	}
	{
		v14.w = vinsert(r26)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r25)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r3)
		v27:26.uw = vunpack(v22.uh)
	}
	{
		vmemu(r4+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		v11 = vror(v11,r28)
		r5:4 = memd(r12+r10<<#0)
	}
	{
		v21:20.uw = vunpack(v20.uh)
		r5:4 = memd(r23+#48)
		memd(r30+##-13104) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r0 = add(r30,#-3248)
		v11 = vor(v11,v31)
		vmemu(r0+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v17.w = vinsert(r4)
		vmemu(r0+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v16 = vdelta(v26,v10)
		r1:0 = memd(r23+#56)
		r23:22 = memd(r19+#8)
	}
	{
		v29:28.w = vunpack(v8.h)
		v7 = vmux(q0,v16,v20)
	}
	{
		v8 = vror(v9,r28)
	}
	{
		v9 = valign(v14,v14,#4)
		v8 = vor(v8,v31)
	}
	{
		v9.w = vinsert(r27)
		v21:20.uh = vunpack(v11.ub)
		v11 = v12
		r27:26 = memd(r19+#24)
	}
	{
		v11.w = vinsert(r16)
		v14 = valign(v17,v17,#4)
	}
	{
		v14.w = vinsert(r5)
		v27:26.uh = vunpack(v8.ub)
		r5:4 = memd(r19+#16)
	}
	{
		v8 = valign(v9,v9,#4)
	}
	{
		v8.w = vinsert(r8)
		r2 = add(r30,#-9264)
		vmemu(r2+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		r2 = add(r30,#-9136)
		vmemu(r2+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v9 = valign(v14,v14,#4)
		v14 = v12
	}
	{
		v9.w = vinsert(r0)
		r0 = add(r30,#-9008)
		vmemu(r2+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v14.w = vinsert(r4)
		v29:28.w = vunpack(v5.h)
		r3:2 = memd(r19+#40)
	}
	{
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r9)
		v17:16.uw = vunpack(v26.uh)
		r9:8 = memd(r18+#40)
	}
	{
		v5 = vdelta(v20,v10)
	}
	{
		r0 = add(r30,#-8880)
		v5 = vmux(q0,v5,v16)
		vmemu(r0+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r1)
		r0 = add(r30,#-8496)
		vmemu(r0+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		vmemu(r0+#0) = v5
	}                                       // 128-byte Folded Spill
	{
		v5 = vror(v8,r28)
		r1:0 = memd(r19+#32)
	}
	{
		v9 = vror(v9,r28)
		v5 = vor(v5,v31)
	}
	{
		v8 = valign(v11,v11,#4)
	}
	{
		v8.w = vinsert(r17)
		v11 = valign(v14,v14,#4)
		r17:16 = memd(r18+#16)
	}
	{
		v11.w = vinsert(r5)
		v27:26.uh = vunpack(v5.ub)
		v5 = vor(v9,v31)
		r5:4 = memd(r19+#48)
	}
	{
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r22)
		v21:20.uh = vunpack(v5.ub)
	}
	{
		v9 = valign(v11,v11,#4)
		v11 = v12
	}
	{
		v9.w = vinsert(r26)
		v11.w = vinsert(r4)
		r4 = add(r30,#-9520)
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v5 = valign(v8,v8,#4)
	}
	{
		v5.w = vinsert(r23)
		v8 = valign(v9,v9,#4)
		v9 = v12
		r23:22 = memd(r18+#8)
	}
	{
		v9.w = vinsert(r0)
		v8.w = vinsert(r27)
		r0 = add(r30,#-8240)
		v17:16.uw = vunpack(v26.uh)
	}
	{
		v27:26.w = vunpack(v4.h)
		r27:26 = memd(r18+#48)
	}
	{
		v4 = vdelta(v20,v10)
	}
	{
		v5 = vror(v5,r28)
		v4 = vmux(q0,v4,v16)
	}
	{
		v8 = vror(v8,r28)
		v5 = vor(v5,v31)
	}
	{
		v8 = vor(v8,v31)
		vmemu(r0+#0) = v4
	}                                       // 128-byte Folded Spill
	{
		v4 = valign(v9,v9,#4)
	}
	{
		v4.w = vinsert(r1)
		v9 = valign(v11,v11,#4)
		v11 = v12
		r1:0 = memd(r18+#24)
	}
	{
		v9.w = vinsert(r5)
		v11.w = vinsert(r16)
		v29:28.uh = vunpack(v5.ub)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r2)
		v5 = valign(v9,v9,#4)
		v9 = v12
	}
	{
		v5.w = vinsert(r14)
		v9.w = vinsert(r20)
		v21:20.uh = vunpack(v8.ub)
	}
	{
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r3)
		v21:20.uw = vunpack(v20.uh)
		r3:2 = memd(r18+#32)
	}
	{
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r15)
		v17:16.uw = vunpack(v28.uh)
	}
	{
		v8 = vdelta(v20,v10)
	}
	{
		v29:28.w = vunpack(v3.h)
		v7 = vmux(q0,v8,v16)
	}
	{
		v3 = vror(v4,r28)
	}
	{
		v5 = vror(v5,r28)
		v3 = vor(v3,v31)
	}
	{
		r4 = add(r30,#-10288)
		v5 = vor(v5,v31)
		vmemu(r4+#0) = v7
	}                                       // 128-byte Folded Spill
	{
		v17:16.uh = vunpack(v3.ub)
	}
	{
		v8 = valign(v11,v11,#4)
		v11 = v12
	}
	{
		v8.w = vinsert(r17)
		r4 = add(r30,#-10160)
		vmemu(r4+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		r4 = add(r30,#-9776)
		vmemu(r4+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v4 = valign(v9,v9,#4)
	}
	{
		v4.w = vinsert(r21)
		v29:28.uh = vunpack(v5.ub)
		r21:20 = memd(r18+#56)
	}
	{
		v17:16.uw = vunpack(v16.uh)
	}
	{
		v5 = valign(v8,v8,#4)
	}
	{
		v5.w = vinsert(r0)
		r4 = add(r30,#-9648)
		vmemu(r4+#0) = v16
	}                                       // 256-byte Folded Spill
	{
		v9:8.uw = vunpack(v28.uh)
	}
	{
		v3 = valign(v4,v4,#4)
		v4 = v12
	}
	{
		v3.w = vinsert(r22)
		r4 = add(r30,#-10032)
		vmemu(r4+#0) = v17
	}                                       // 256-byte Folded Spill
	{
		v4.w = vinsert(r2)
		r4 = add(r30,#-9904)
		vmemu(r4+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v9:8 = vcombine(v12,v12)
		vmemu(r4+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v8.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r11+#16)
	}
	{
		v3.w = vinsert(r23)
		v4 = valign(v4,v4,#4)
		r23:22 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		v11.w = vinsert(r4)
		v9.w = vinsert(r22)
		v8 = valign(v8,v8,#4)
	}
	{
		v4.w = vinsert(r3)
		v8.w = vinsert(r27)
		v5 = valign(v5,v5,#4)
		r3:2 = memd(r11+#24)
	}
	{
		v5.w = vinsert(r1)
		v9 = valign(v9,v9,#4)
		r27:26 = memd(r11+#32)
		r25:24 = memd(r11+#56)
	}
	{
		v9.w = vinsert(r23)
		v8 = valign(v8,v8,#4)
		r23:22 = memd(r11+#40)
	}
	{
		v8.w = vinsert(r20)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r5)
		v4 = valign(v4,v4,#4)
		r5:4 = memd(r11+#48)
	}
	{
		v4.w = vinsert(r8)
		v9 = valign(v9,v9,#4)
		r11 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		v9.w = vinsert(r6)
		r19 = r11
		v5 = vror(v5,r28)
	}
	{
		v8 = valign(v8,v8,#4)
		v5 = vor(v5,v31)
		r17:16 = memd(r11+#8)
		r1:0 = memd(r11+#24)
	}
	{
		v8.w = vinsert(r21)
		v11 = valign(v11,v11,#4)
		r15:14 = memd(r11+#40)
		r21:20 = memd(r11+#48)
	}
	{
		v11.w = vinsert(r2)
		r2 = add(r30,#-11568)
		v3 = vror(v3,r28)
	}
	{
		v4 = valign(v4,v4,#4)
		v3 = vor(v3,v31)
	}
	{
		v4.w = vinsert(r9)
		v9 = valign(v9,v9,#4)
		r9:8 = memd(r11+#16)
	}
	{
		v9.w = vinsert(r7)
		v8 = vror(v8,r28)
		r7:6 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		v19.w = vinsert(r8)
		v29:28.uh = vunpack(v5.ub)
		v8 = vor(v8,v31)
	}
	{
		v18.w = vinsert(r6)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r3)
		v4 = vror(v4,r28)
	}
	{
		v21:20.uh = vunpack(v3.ub)
		v4 = vor(v4,v31)
	}
	{
		v9 = vror(v9,r28)
	}
	{
		v17:16.uw = vunpack(v28.uh)
		v5 = vor(v9,v31)
	}
	{
		v9:8.uh = vunpack(v8.ub)
	}
	{
		v3 = vror(v11,r28)
	}
	{
		v21:20.uw = vunpack(v20.uh)
		v3 = vor(v3,v31)
	}
	{
		v9 = vdelta(v16,v10)
	}
	{
		v29:28.uh = vunpack(v4.ub)
		v27 = vmux(q0,v9,v20)
	}
	{
		v9:8.uw = vunpack(v8.uh)
	}
	{
		v21:20.uh = vunpack(v3.ub)
	}
	{
		v17:16.uw = vunpack(v28.uh)
	}
	{
		v3 = vdelta(v8,v10)
	}
	{
		v5:4.uh = vunpack(v5.ub)
		v3 = vmux(q0,v3,v16)
	}
	{
		v9:8.uw = vunpack(v20.uh)
		v5 = v12
	}
	{
		v5.w = vinsert(r4)
		v3 = v12
		vmemu(r2+#0) = v3
	}                                       // 128-byte Folded Spill
	{
		v3.w = vinsert(r26)
		r2 = add(r30,#-12080)
		v29:28.w = vunpack(v0.h)
	}
	{
		v0 = vdelta(v8,v10)
		r4 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		v9:8.uw = vunpack(v4.uh)
		r6 = memw(r30+##-12976)
		r8 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
		v24 = vmux(q0,v0,v8)
	}
	{
		v3.w = vinsert(r27)
		v0 = valign(v5,v5,#4)
		v5 = v12
		r27:26 = memd(r11+#56)
	}
	{
		v0.w = vinsert(r5)
		v5.w = vinsert(r20)
		v23:22.w = vunpack(v1.h)
		r5:4 = memd(r12+r4<<#0)
	}
	{
		v1 = valign(v3,v3,#4)
	}
	{
		v1.w = vinsert(r22)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r24)
		v3 = valign(v18,v18,#4)
	}
	{
		v3.w = vinsert(r7)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r23)
		v0 = valign(v0,v0,#4)
		r23:22 = memd(r8+#24)
	}
	{
		v0.w = vinsert(r25)
		v4 = valign(v19,v19,#4)
		r25:24 = memd(r8+#40)
	}
	{
		v4.w = vinsert(r9)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r16)
		v1 = vror(v1,r28)
	}
	{
		v0 = vror(v0,r28)
		v1 = vor(v1,v31)
	}
	{
		v4 = valign(v4,v4,#4)
		v0 = vor(v0,v31)
	}
	{
		v4.w = vinsert(r0)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r17)
		r2 = add(r30,#-11952)
		vmemu(r2+#0) = v28
	}                                       // 256-byte Folded Spill
	{
		vmemu(r2+#0) = v29
	}                                       // 256-byte Folded Spill
	{
		v29:28.uh = vunpack(v1.ub)
		r3:2 = memd(r11+#32)
		r17:16 = memd(r8+#16)
	}
	{
		v19:18.uh = vunpack(v0.ub)
		r11:10 = memd(r12+r6<<#0)
		r7:6 = memd(r8+#8)
	}
	{
		v1 = vror(v3,r28)
		v3 = v12
	}
	{
		v3.w = vinsert(r2)
		r2 = add(r30,#-12848)
		v4 = valign(v4,v4,#4)
		v1 = vor(v1,v31)
	}
	{
		v4.w = vinsert(r1)
		v21:20.uw = vunpack(v28.uh)
		r1:0 = memd(r8+#32)
	}
	{
		v29:28.uw = vunpack(v18.uh)
	}
	{
		v0 = vror(v4,r28)
	}
	{
		v19:18.uh = vunpack(v1.ub)
		v0 = vor(v0,v31)
	}
	{
		v1 = vdelta(v28,v10)
	}
	{
		v1 = valign(v3,v3,#4)
		v11 = vmux(q0,v1,v20)
	}
	{
		v1.w = vinsert(r3)
		v3 = valign(v5,v5,#4)
		v5 = v12
	}
	{
		v3.w = vinsert(r21)
		v17:16.uh = vunpack(v0.ub)
		v0 = v12
		r21:20 = memd(r13+#8)
	}
	{
		v0.w = vinsert(r4)
		v5.w = vinsert(r16)
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r14)
		v29:28.w = vunpack(v15.h)
	}
	{
		v15:14.uw = vunpack(v18.uh)
	}
	{
		v19:18.uw = vunpack(v16.uh)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r26)
		v9:8.w = vunpack(v2.h)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r15)
		v2 = vdelta(v18,v10)
		r15:14 = memd(r13+#32)
	}
	{
		v0 = valign(v0,v0,#4)
		v23 = vmux(q0,v2,v14)
	}
	{
		v0.w = vinsert(r5)
		v2 = valign(v5,v5,#4)
		r5:4 = memd(r8+#56)
	}
	{
		v2.w = vinsert(r17)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r27)
		v1 = vror(v1,r28)
		r27:26 = memd(r13+#16)
	}
	{
		v0 = valign(v0,v0,#4)
		v1 = vor(v1,v31)
	}
	{
		v0.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r22)
		v3 = vror(v3,r28)
	}
	{
		v15:14.uh = vunpack(v1.ub)
		v1 = vor(v3,v31)
		v3 = v12
	}
	{
		v3.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r7)
		v2 = valign(v2,v2,#4)
		r7:6 = memd(r13+#24)
	}
	{
		v2.w = vinsert(r23)
		r2 = add(r30,#-12720)
		vmemu(r2+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v9 = v12
		vmemu(r2+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v19:18.uh = vunpack(v1.ub)
		r3:2 = memd(r8+#48)
		r9:8 = memd(r13+#40)
	}
	{
		v9.w = vinsert(r2)
		v0 = vror(v0,r28)
		r2 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		v1 = vror(v2,r28)
		v0 = vor(v0,v31)
	}
	{
		v2 = valign(v3,v3,#4)
		v15 = vor(v1,v31)
		r23:22 = memd(r12+r2<<#0)
	}
	{
		v2.w = vinsert(r1)
		v5:4.uw = vunpack(v18.uh)
		r12 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v9,v9,#4)
		r1:0 = memd(r13+#48)
	}
	{
		v3.w = vinsert(r3)
		v17:16.uw = vunpack(v14.uh)
		r17:16 = memd(r12+#8)
		r3:2 = memd(r12+#32)
	}
	{
		v21:20.w = vunpack(v6.h)
	}
	{
		v1 = vdelta(v4,v10)
	}
	{
		v7:6.uh = vunpack(v0.ub)
		v21 = vmux(q0,v1,v16)
	}
	{
		v1 = valign(v2,v2,#4)
		v2 = v12
	}
	{
		v1.w = vinsert(r24)
		v2.w = vinsert(r10)
		v0 = valign(v3,v3,#4)
	}
	{
		v0.w = vinsert(r4)
		r4 = add(r30,#-10928)
		v15:14.uh = vunpack(v15.ub)
	}
	{
		v5:4.uw = vunpack(v6.uh)
	}
	{
		v1 = valign(v1,v1,#4)
		v5 = v12
	}
	{
		v1.w = vinsert(r25)
		v5.w = vinsert(r26)
		v17:16.uw = vunpack(v14.uh)
		r25:24 = memd(r13+#56)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r5)
		v19:18.w = vunpack(v13.h)
	}
	{
		v3 = vdelta(v16,v10)
	}
	{
		v1 = vror(v1,r28)
		v19 = vmux(q0,v3,v4)
	}
	{
		v3 = valign(v5,v5,#4)
		v1 = vor(v1,v31)
	}
	{
		v3.w = vinsert(r27)
		v2 = valign(v2,v2,#4)
		r27:26 = memd(r12+#16)
	}
	{
		v2.w = vinsert(r11)
		v0 = vror(v0,r28)
		r11:10 = memd(r12+#40)
	}
	{
		v15:14.uh = vunpack(v1.ub)
		v0 = vor(v0,v31)
	}
	{
		v1 = valign(v3,v3,#4)
	}
	{
		v1.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r20)
		v17:16.uh = vunpack(v0.ub)
	}
	{
		v0 = valign(v1,v1,#4)
	}
	{
		v0.w = vinsert(r7)
		v7:6.uw = vunpack(v16.uh)
		r7:6 = memd(r12+#56)
	}
	{
		v5:4.uw = vunpack(v14.uh)
		v7 = v12
	}
	{
		v7.w = vinsert(r0)
		v2 = valign(v2,v2,#4)
		v5 = v12
	}
	{
		v2.w = vinsert(r21)
		v5.w = vinsert(r14)
		v0 = vror(v0,r28)
		r21:20 = memd(r12+#24)
	}
	{
		v3 = vdelta(v6,v10)
		v0 = vor(v0,v31)
	}
	{
		v1 = vror(v2,r28)
		v29 = vmux(q0,v3,v4)
	}
	{
		v6 = valign(v5,v5,#4)
		v1 = vor(v1,v31)
	}
	{
		v6.w = vinsert(r15)
		v3 = valign(v7,v7,#4)
		v7 = v12
	}
	{
		v3.w = vinsert(r1)
		v5:4.uh = vunpack(v0.ub)
		r1:0 = memd(r12+#48)
	}
	{
		v7.w = vinsert(r0)
		r0 = add(r30,#-7216)
		v15:14.uh = vunpack(v1.ub)
	}
	{
		v1 = valign(v6,v6,#4)
		v6 = v12
	}
	{
		v1.w = vinsert(r8)
		v6.w = vinsert(r26)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r24)
		v5:4.uw = vunpack(v4.uh)
	}
	{
		v6 = valign(v6,v6,#4)
		v5 = v12
	}
	{
		v6.w = vinsert(r27)
		v5.w = vinsert(r22)
		v0 = vdelta(v4,v10)
	}
	{
		v4 = valign(v1,v1,#4)
	}
	{
		v4.w = vinsert(r9)
		v1 = valign(v3,v3,#4)
		v3 = v12
	}
	{
		v3.w = vinsert(r2)
		v1.w = vinsert(r25)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r1)
		v6 = valign(v6,v6,#4)
		r2 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		v6.w = vinsert(r20)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r3)
		r3 = add(r30,#-7984)
		v5 = valign(v5,v5,#4)
		r25:24 = memd(r2+#80)
	}
	{
		v5.w = vinsert(r23)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r10)
		v1 = vror(v1,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v1 = vor(v1,v31)
	}
	{
		v6.w = vinsert(r21)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r11)
		v15:14.uw = vunpack(v14.uh)
	}
	{
		v4 = vror(v4,r28)
	}
	{
		v5 = valign(v5,v5,#4)
		v4 = vor(v4,v31)
	}
	{
		v5.w = vinsert(r16)
		v17:16.w = vunpack(v25.h)
		v25 = vmux(q0,v0,v14)
	}
	{
		r4 = r2
		v0 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r7)
		v6 = vror(v6,r28)
		r7:6 = memd(r2+#88)
	}
	{
		v15:14.w = vunpack(v0.h)
		v6 = vor(v6,v31)
	}
	{
		v1:0.uh = vunpack(v1.ub)
	}
	{
		v3 = vror(v3,r28)
	}
	{
		v5 = valign(v5,v5,#4)
		v1 = vor(v3,v31)
	}
	{
		v5.w = vinsert(r17)
		v3:2.uh = vunpack(v4.ub)
	}
	{
		v3 = vror(v7,r28)
	}
	{
		v9:8.uw = vunpack(v0.uh)
		v0 = vor(v3,v31)
	}
	{
		v9.w = vmpyieo(v27.h,v26.h)
		v3:2.uw = vunpack(v2.uh)
	}
	{
		v9.w += vmpyie(v27.w,v26.h)
		v7:6.uh = vunpack(v6.ub)
	}
	{
		v5 = vror(v5,r28)
	}
	{
		v3 = vdelta(v8,v10)
		v5 = vor(v5,v31)
	}
	{
		v3:2.uw = vunpack(v6.uh)
		v13 = vmux(q0,v3,v2)
	}
	{
		v7:6.uh = vunpack(v0.ub)
	}
	{
		v5:4.uh = vunpack(v5.ub)
	}
	{
		v0 = vdelta(v2,v10)
	}
	{
		v3:2.uw = vunpack(v6.uh)
	}
	{
		r0 = add(r30,#-7088)
		v6 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-10800)
		v7 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7.w = vmpyieo(v30.h,v6.h)
		v5:4.uw = vunpack(v4.uh)
	}
	{
		r0 = add(r30,#-10672)
		v0 = vmux(q0,v0,v4)
		v26 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7.w += vmpyie(v30.w,v6.h)
		r0 = add(r30,#-11056)
		v27 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v27.w = vmpyieo(v0.h,v16.h)
		v5:4.uh = vunpack(v1.ub)
	}
	{
		v27.w += vmpyie(v0.w,v16.h)
		r0 = add(r30,#-7216)
		v5 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = vdelta(v2,v10)
	}
	{
		v6.w = vmpyieo(v5.h,v26.h)
		v3:2.uw = vunpack(v4.uh)
	}
	{
		v6.w += vmpyie(v5.w,v26.h)
		v4 = vmux(q0,v1,v2)
	}
	{
		v3.w = vmpyieo(v25.h,v20.h)
		v2.w = vmpyieo(v13.h,v18.h)
	}
	{
		v26.w = vmpyieo(v4.h,v14.h)
		r0 = add(r30,#-7088)
		vmemu(r0+#0) = v6
	}                                       // 256-byte Folded Spill
	{
		v26.w += vmpyie(v4.w,v14.h)
		r0 = add(r30,#-12848)
		vmemu(r0+#0) = v7
	}                                       // 256-byte Folded Spill
	{
		v1.w = vmpyieo(v19.h,v28.h)
		r0 = add(r30,#-12720)
		v6 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-19760)
		v8 = v6
		v7 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v29.h,v6.h)
		r0 = add(r30,#-19632)
		v6 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v3.w += vmpyie(v25.w,v20.h)
		r0 = add(r30,#-12080)
		v7 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w += vmpyie(v13.w,v18.h)
		v7:6.w = vadd(v27:26.w,v7:6.w)
	}
	{
		r0 = add(r30,#-11952)
		v3:2.w = vadd(v7:6.w,v3:2.w)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v19.w,v28.h)
		v7 = v12
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w += vmpyie(v29.w,v8.h)
		r2 = add(r30,#-9264)
		v4 = v12
		r1:0 = memd(r2+#64)
	}
	{
		v3:2.w = vadd(v1:0.w,v3:2.w)
		v0 = vmemu(r3+#0)
	}                                       // 128-byte Folded Reload
	{
		v7.w = vinsert(r0)
		v4.w = vinsert(r24)
		r0 = add(r30,#-10288)
	}
	{
		v5.w = vmpyieo(v23.h,v14.h)
		v1:0.w = vunpack(v0.h)
	}
	{
		v5.w += vmpyie(v23.w,v14.h)
		r0 = add(r30,#-10160)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w = vmpyieo(v24.h,v0.h)
		r0 = add(r30,#-11568)
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1.w += vmpyie(v24.w,v0.h)
		v6 = valign(v4,v4,#4)
	}
	{
		v6.w = vinsert(r25)
		r0 = add(r30,#-7728)
		v13 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v4.w = vmpyieo(v21.h,v22.h)
		v0 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v4.w += vmpyie(v21.w,v22.h)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r6)
		v8.w = vmpyieo(v13.h,v16.h)
		v27:26.w = vunpack(v0.h)
	}
	{
		v7 = valign(v7,v7,#4)
		v5:4.w = vadd(v3:2.w,v5:4.w)
	}
	{
		v7.w = vinsert(r1)
		r2 = add(r30,#-9136)
		v14 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w = vmpyieo(v11.h,v26.h)
		r2 = add(r30,#-9520)
		v15 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v0.w += vmpyie(v11.w,v26.h)
		v6 = valign(v6,v6,#4)
		r1:0 = memd(r4+#72)
	}
	{
		v6.w = vinsert(r7)
		v11 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v13.w,v16.h)
		v2 = valign(v7,v7,#4)
		r7:6 = memd(r4+#112)
	}
	{
		v2.w = vinsert(r0)
		v7.w = vmpyieo(v11.h,v14.h)
		r0 = add(r30,#-10032)
		v3 = vror(v6,r28)
	}
	{
		v7.w += vmpyie(v11.w,v14.h)
		r0 = add(r30,#-9904)
		v14 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-9776)
		v3 = vor(v3,v31)
		v15 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2 = valign(v2,v2,#4)
		v6 = v12
		v19 = v7
		v11 = v12
	}
	{
		v6.w = vinsert(r6)
		r0 = add(r30,#-9648)
		v16 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v2.w = vinsert(r1)
		v7 = vdelta(v14,v10)
		r6 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		v29:28.uh = vunpack(v3.ub)
		v3 = vmux(q0,v7,v16)
		v7 = v12
	}
	{
		v17 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v15:14.uw = vunpack(v28.uh)
		v17:16.w = vadd(v5:4.w,v1:0.w)
		r1:0 = memd(r4+#96)
		r3:2 = memd(r6+#80)
	}
	{
		v7.w = vinsert(r0)
		v11.w = vinsert(r2)
		v13 = vror(v2,r28)
	}
	{
		v6 = valign(v6,v6,#4)
		v9:8.w = vadd(v9:8.w,v17:16.w)
		v13 = vor(v13,v31)
	}
	{
		v6.w = vinsert(r7)
		v7 = valign(v7,v7,#4)
		v17 = v12
	}
	{
		v7.w = vinsert(r1)
		v2 = vdelta(v14,v10)
		v14 = v12
		r1:0 = memd(r6+#64)
	}
	{
		v14.w = vinsert(r0)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r4+#120)
		r5:4 = memd(r4+#104)
	}
	{
		v6.w = vinsert(r2)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r1)
		v11 = valign(v11,v11,#4)
		r1:0 = memd(r6+#88)
	}
	{
		v11.w = vinsert(r0)
		r0 = add(r30,#-9008)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r4)
		r4 = r6
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		r0 = add(r30,#-8880)
		v4 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v14 = valign(v14,v14,#4)
		r3:2 = memd(r6+#72)
	}
	{
		v18.w = vmpyieo(v3.h,v4.h)
		v14.w = vinsert(r2)
		r2 = add(r30,#-6960)
		v7 = valign(v7,v7,#4)
	}
	{
		v18.w += vmpyie(v3.w,v4.h)
		v6 = vror(v6,r28)
	}
	{
		v7.w = vinsert(r5)
		v1 = vor(v6,v31)
		v5 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		r0 = add(r30,#-7728)
		r2 = add(r30,#-6832)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v11 = valign(v11,v11,#4)
		r5:4 = memd(r4+#104)
	}
	{
		v11.w = vinsert(r1)
		r0 = add(r30,#-7600)
		vmemu(r0+#0) = v18
	}                                       // 256-byte Folded Spill
	{
		r2 = add(r30,#-8496)
		v5 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = vror(v7,r28)
		v7 = v12
	}
	{
		v0 = vor(v3,v31)
		vmemu(r0+#0) = v19
	}                                       // 256-byte Folded Spill
	{
		v27:26.uh = vunpack(v1.ub)
		r1:0 = memd(r6+#112)
	}
	{
		v7.w = vinsert(r0)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v25:24.uh = vunpack(v13.ub)
		v13 = v12
	}
	{
		v21.w = vmpyieo(v1.h,v4.h)
		v3 = valign(v7,v7,#4)
		v7 = v12
	}
	{
		v21.w += vmpyie(v1.w,v4.h)
		v11 = vror(v11,r28)
	}
	{
		v3.w = vinsert(r1)
		v5:4.uw = vunpack(v24.uh)
	}
	{
		v6 = valign(v14,v14,#4)
		v5 = vor(v11,v31)
		v11 = v12
	}
	{
		v6.w = vinsert(r3)
		v19:18.uh = vunpack(v0.ub)
		r3:2 = memd(r6+#96)
		r7:6 = memd(r6+#120)
	}
	{
		v7.w = vinsert(r2)
		r2 = add(r30,#-7984)
		v29:28.uh = vunpack(v5.ub)
	}
	{
		v5 = valign(v3,v3,#4)
	}
	{
		v5.w = vinsert(r6)
		v0 = valign(v7,v7,#4)
		r6 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r3)
		v6 = vror(v6,r28)
	}
	{
		v7 = valign(v5,v5,#4)
		v6 = vor(v6,v31)
		r1:0 = memd(r6+#80)
	}
	{
		v11.w = vinsert(r0)
		v7.w = vinsert(r7)
		r0 = add(r30,#-6960)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r4)
		v15:14.uw = vunpack(v26.uh)
	}
	{
		v7 = vror(v7,r28)
	}
	{
		v11 = valign(v11,v11,#4)
		v7 = vor(v7,v31)
	}
	{
		v11.w = vinsert(r1)
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r5)
		r0 = add(r30,#-6832)
		vmemu(r0+#0) = v8
	}                                       // 256-byte Folded Spill
	{
		v27:26.uh = vunpack(v7.ub)
		r5:4 = memd(r6+#64)
	}
	{
		v7 = valign(v11,v11,#4)
		v11 = v12
	}
	{
		v11.w = vinsert(r4)
		v25:24.uh = vunpack(v6.ub)
	}
	{
		vmemu(r0+#0) = v9
	}                                       // 256-byte Folded Spill
	{
		v9:8.uw = vunpack(v24.uh)
		r1:0 = memd(r6+#88)
	}
	{
		v7.w = vinsert(r0)
		r0 = add(r30,#-3376)
		v9 = valign(v11,v11,#4)
	}
	{
		v9.w = vinsert(r5)
		v11 = vror(v0,r28)
		r5:4 = memd(r6+#72)
	}
	{
		r0 = add(r30,#-3248)
		v11 = vor(v11,v31)
		v0 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r1)
		r0 = add(r30,#-8240)
		v1 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		v1 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v9 = valign(v9,v9,#4)
		r1:0 = memd(r6+#112)
	}
	{
		v13.w = vinsert(r0)
		v9.w = vinsert(r4)
		v23:22.uw = vunpack(v18.uh)
	}
	{
		v20.w = vmpyieo(v1.h,v0.h)
		v19:18.uw = vunpack(v28.uh)
	}
	{
		v20.w += vmpyie(v1.w,v0.h)
		v29:28.uw = vunpack(v26.uh)
	}
	{
		v7 = vror(v7,r28)
	}
	{
		v29 = valign(v13,v13,#4)
		v7 = vor(v7,v31)
	}
	{
		v29.w = vinsert(r1)
		v9 = valign(v9,v9,#4)
		r1:0 = memd(r18+#80)
	}
	{
		v9.w = vinsert(r5)
		v1:0.uh = vunpack(v11.ub)
		v11 = v12
		r5:4 = memd(r6+#120)
	}
	{
		v11.w = vinsert(r0)
		r0 = add(r30,#-8240)
		v25:24.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v29,v29,#4)
	}
	{
		v7.w = vinsert(r4)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r1)
		v3 = vdelta(v14,v10)
	}
	{
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r5)
		v15:14.uw = vunpack(v0.uh)
		v0 = vmux(q0,v2,v4)
		r5:4 = memd(r18+#88)
	}
	{
		v9 = vror(v9,r28)
		v2 = v12
	}
	{
		v13 = vor(v9,v31)
		v0 = vmux(q0,v3,v22)
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v11 = valign(v11,v11,#4)
		v3 = v12
		r1:0 = memd(r18+#64)
	}
	{
		v2.w = vinsert(r0)
		v11.w = vinsert(r4)
		r0 = add(r30,#-3376)
		v7 = vror(v7,r28)
	}
	{
		v27:26.uh = vunpack(v13.ub)
		v7 = vor(v7,v31)
	}
	{
		v13 = valign(v2,v2,#4)
	}
	{
		v13.w = vinsert(r1)
		v6 = vdelta(v28,v10)
	}
	{
		v11 = valign(v11,v11,#4)
		v6 = vmux(q0,v6,v14)
	}
	{
		v11.w = vinsert(r5)
		v5 = vdelta(v18,v10)
		r5:4 = memd(r18+#72)
	}
	{
		v19:18.uw = vunpack(v24.uh)
	}
	{
		vmemu(r0+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v29:28.uh = vunpack(v7.ub)
		r1:0 = memd(r18+#112)
	}
	{
		v3.w = vinsert(r0)
		v9 = vdelta(v18,v10)
	}
	{
		v19:18.uw = vunpack(v28.uh)
	}
	{
		v30 = valign(v13,v13,#4)
		v13 = v12
	}
	{
		v30.w = vinsert(r4)
		v11 = vror(v11,r28)
		r4 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
		v11 = vor(v11,v31)
	}
	{
		v3.w = vinsert(r1)
		v0 = vdelta(v18,v10)
		r1:0 = memd(r18+#120)
	}
	{
		v7 = valign(v30,v30,#4)
		v30 = vmux(q0,v5,v8)
		v8 = v12
	}
	{
		v7.w = vinsert(r5)
		v23:22.uh = vunpack(v11.ub)
	}
	{
		v11 = valign(v3,v3,#4)
	}
	{
		v11.w = vinsert(r0)
		vmemu(r2+#0) = v0
	}                                       // 128-byte Folded Spill
	{
		v7 = vror(v7,r28)
		r3:2 = memd(r4+#80)
	}
	{
		v13.w = vinsert(r2)
		v15 = valign(v11,v11,#4)
		v7 = vor(v7,v31)
	}
	{
		v15.w = vinsert(r1)
		v1:0.uw = vunpack(v22.uh)
		r1:0 = memd(r18+#96)
	}
	{
		v8.w = vinsert(r0)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r3)
		v3:2.uh = vunpack(v7.ub)
		r3:2 = memd(r4+#88)
	}
	{
		v8 = valign(v8,v8,#4)
	}
	{
		v8.w = vinsert(r1)
		v13 = valign(v13,v13,#4)
		r1:0 = memd(r4+#64)
	}
	{
		v13.w = vinsert(r2)
		v1 = vror(v15,r28)
		v15 = v12
	}
	{
		v23:22.uw = vunpack(v2.uh)
		v2 = v12
		v7 = vor(v1,v31)
	}
	{
		v2.w = vinsert(r0)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r3)
		v25:24.uw = vunpack(v26.uh)
		r3:2 = memd(r18+#104)
	}
	{
		v27:26.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v2,v2,#4)
	}
	{
		v7.w = vinsert(r1)
		v8 = valign(v8,v8,#4)
		r1:0 = memd(r4+#112)
	}
	{
		v8.w = vinsert(r2)
		v15.w = vinsert(r0)
		v13 = vror(v13,r28)
	}
	{
		v11 = vdelta(v0,v10)
		v13 = vor(v13,v31)
	}
	{
		v8 = valign(v8,v8,#4)
		v11 = vmux(q0,v11,v22)
	}
	{
		v8.w = vinsert(r3)
		v1:0.uh = vunpack(v13.ub)
		r3:2 = memd(r4+#72)
	}
	{
		v13 = valign(v15,v15,#4)
	}
	{
		v13.w = vinsert(r1)
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r4+#120)
	}
	{
		v7.w = vinsert(r2)
		v8 = vror(v8,r28)
	}
	{
		v13 = valign(v13,v13,#4)
		v15 = vor(v8,v31)
		v8 = vmux(q0,v9,v24)
		v9 = v12
	}
	{
		v13.w = vinsert(r0)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r3)
		v19:18.uw = vunpack(v0.uh)
		r3:2 = memd(r19+#80)
	}
	{
		v9.w = vinsert(r2)
		v13 = valign(v13,v13,#4)
	}
	{
		v13.w = vinsert(r1)
		v3:2.uh = vunpack(v15.ub)
		r1:0 = memd(r4+#96)
	}
	{
		v29:28.uw = vunpack(v26.uh)
	}
	{
		v15 = vdelta(v18,v10)
		v18 = v12
	}
	{
		v18.w = vinsert(r0)
		v9 = valign(v9,v9,#4)
	}
	{
		v9.w = vinsert(r3)
		v14 = vdelta(v28,v10)
		r3:2 = memd(r4+#104)
		r5:4 = memd(r19+#64)
	}
	{
		v25:24.uw = vunpack(v2.uh)
	}
	{
		v13 = vror(v13,r28)
	}
	{
		v3 = valign(v18,v18,#4)
		v18 = vor(v13,v31)
		v13 = vmux(q0,v14,v24)
		v14 = v12
	}
	{
		v14.w = vinsert(r4)
		v3.w = vinsert(r1)
		v7 = vror(v7,r28)
		r1:0 = memd(r19+#88)
	}
	{
		v9 = valign(v9,v9,#4)
		v7 = vor(v7,v31)
	}
	{
		v9.w = vinsert(r0)
		v19:18.uh = vunpack(v18.ub)
	}
	{
		v23:22.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v3,v3,#4)
		v3:2 = vcombine(v12,v12)
	}
	{
		v7.w = vinsert(r2)
		v14 = valign(v14,v14,#4)
	}
	{
		v14.w = vinsert(r5)
		v19:18.uw = vunpack(v18.uh)
		r5:4 = memd(r19+#72)
	}
	{
		v9 = valign(v9,v9,#4)
		v19 = v12
	}
	{
		v9.w = vinsert(r1)
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r19+#112)
	}
	{
		v19.w = vinsert(r0)
		v7.w = vinsert(r3)
		v14 = valign(v14,v14,#4)
		r3:2 = memd(r19+#120)
	}
	{
		v14.w = vinsert(r4)
		v9 = vror(v9,r28)
		r4 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		r7 = r4
		v19 = valign(v19,v19,#4)
		v9 = vor(v9,v31)
	}
	{
		v19.w = vinsert(r1)
		v7 = vror(v7,r28)
		r1:0 = memd(r4+#80)
	}
	{
		v17.w = vinsert(r0)
		v23:22.uw = vunpack(v22.uh)
		v7 = vor(v7,v31)
	}
	{
		v29:28.uh = vunpack(v9.ub)
		v9 = vmux(q0,v15,v22)
	}
	{
		v15 = valign(v19,v19,#4)
	}
	{
		v15.w = vinsert(r2)
		v23:22.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v17,v17,#4)
	}
	{
		v7.w = vinsert(r1)
		v14 = valign(v14,v14,#4)
		r1:0 = memd(r4+#112)
	}
	{
		v14.w = vinsert(r5)
		v2.w = vinsert(r0)
		v15 = valign(v15,v15,#4)
	}
	{
		v15.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r4+#88)
	}
	{
		v7.w = vinsert(r2)
		v14 = vror(v14,r28)
	}
	{
		v18 = vdelta(v18,v10)
		v14 = vor(v14,v31)
	}
	{
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r3)
		v1:0.uh = vunpack(v14.ub)
		r3:2 = memd(r19+#96)
	}
	{
		v3.w = vinsert(r2)
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v14 = vror(v15,r28)
	}
	{
		v7 = vror(v7,r28)
		v23 = vor(v14,v31)
		v14 = vmux(q0,v18,v22)
	}
	{
		v18 = valign(v2,v2,#4)
		v7 = vor(v7,v31)
	}
	{
		v18.w = vinsert(r1)
		v25:24.uw = vunpack(v28.uh)
		r1:0 = memd(r4+#120)
	}
	{
		v17:16.uh = vunpack(v7.ub)
	}
	{
		v28 = valign(v18,v18,#4)
		v18 = v12
	}
	{
		v28.w = vinsert(r0)
		v23:22.uh = vunpack(v23.ub)
	}
	{
		v19 = vdelta(v24,v10)
	}
	{
		v25:24.uw = vunpack(v0.uh)
	}
	{
		v25:24.uw = vunpack(v16.uh)
		v15 = vmux(q0,v19,v24)
	}
	{
		v23:22.uw = vunpack(v22.uh)
		v25 = v12
	}
	{
		v19 = valign(v3,v3,#4)
	}
	{
		v19.w = vinsert(r3)
		v7 = valign(v28,v28,#4)
		r3:2 = memd(r4+#64)
		r5:4 = memd(r19+#104)
	}
	{
		v7.w = vinsert(r1)
		v23 = vdelta(v24,v10)
		v24 = v12
		r1:0 = memd(r7+#96)
	}
	{
		v24.w = vinsert(r0)
		v18.w = vinsert(r2)
		v19 = valign(v19,v19,#4)
	}
	{
		v19.w = vinsert(r4)
		v7 = vror(v7,r28)
	}
	{
		v24 = valign(v24,v24,#4)
		v7 = vor(v7,v31)
	}
	{
		v24.w = vinsert(r1)
		v18 = valign(v18,v18,#4)
		r1:0 = memd(r13+#80)
	}
	{
		v18.w = vinsert(r3)
		v19 = valign(v19,v19,#4)
		r3:2 = memd(r7+#72)
	}
	{
		v19.w = vinsert(r5)
		r7 = #64
		v29:28.uh = vunpack(v7.ub)
		r5:4 = memd(r7+#104)
	}
	{
		v29 = valign(v24,v24,#4)
	}
	{
		v29.w = vinsert(r4)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r2)
		v19 = vror(v19,r28)
	}
	{
		v7 = valign(v29,v29,#4)
		v19 = vor(v19,v31)
	}
	{
		v7.w = vinsert(r5)
		v1:0.uw = vunpack(v28.uh)
		v28 = v12
		r5:4 = memd(r13+#112)
	}
	{
		v28.w = vinsert(r0)
		v25.w = vinsert(r4)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r3)
		v7 = vror(v7,r28)
		r3:2 = memd(r13+#64)
	}
	{
		v3:2.uh = vunpack(v19.ub)
		v7 = vor(v7,v31)
	}
	{
		v24 = valign(v28,v28,#4)
	}
	{
		v24.w = vinsert(r1)
		v18 = vror(v18,r28)
		r1:0 = memd(r13+#88)
	}
	{
		v27:26.uw = vunpack(v2.uh)
		v18 = vor(v18,v31)
	}
	{
		v3:2.uh = vunpack(v7.ub)
	}
	{
		v3 = valign(v24,v24,#4)
	}
	{
		v3.w = vinsert(r0)
		v25 = valign(v25,v25,#4)
	}
	{
		v25.w = vinsert(r5)
		v17:16.uh = vunpack(v18.ub)
		r5:4 = memd(r13+#120)
	}
	{
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v29:28.uw = vunpack(v16.uh)
		v16 = v12
		r1:0 = memd(r13+#96)
	}
	{
		v16.w = vinsert(r0)
		v5:4.uw = vunpack(v2.uh)
		v29 = v12
	}
	{
		v29.w = vinsert(r2)
		v2 = valign(v25,v25,#4)
		v24 = vmux(q0,v23,v28)
	}
	{
		v2.w = vinsert(r4)
		v3 = vror(v3,r28)
	}
	{
		v7 = valign(v29,v29,#4)
		v3 = vor(v3,v31)
	}
	{
		v7.w = vinsert(r3)
		v2 = valign(v2,v2,#4)
		r3:2 = memd(r13+#72)
	}
	{
		v2.w = vinsert(r5)
		v16 = valign(v16,v16,#4)
		r5:4 = memd(r13+#104)
	}
	{
		v16.w = vinsert(r1)
		v7 = valign(v7,v7,#4)
		r1:0 = memd(r12+#80)
	}
	{
		v7.w = vinsert(r2)
		v2 = vror(v2,r28)
	}
	{
		v29:28.uh = vunpack(v3.ub)
		v2 = vor(v2,v31)
	}
	{
		v3 = valign(v16,v16,#4)
	}
	{
		v3.w = vinsert(r4)
		v22 = vdelta(v22,v10)
	}
	{
		v7 = valign(v7,v7,#4)
		v25 = vmux(q0,v22,v26)
	}
	{
		v7.w = vinsert(r3)
		v27:26.uh = vunpack(v2.ub)
		r3:2 = memd(r12+#112)
	}
	{
		v2 = valign(v3,v3,#4)
	}
	{
		v2.w = vinsert(r5)
		v17:16.uw = vunpack(v28.uh)
		r5:4 = memd(r12+#64)
	}
	{
		v3 = vror(v7,r28)
	}
	{
		v2 = vror(v2,r28)
		v3 = vor(v3,v31)
	}
	{
		v7 = vdelta(v16,v10)
		v16 = v12
		v2 = vor(v2,v31)
	}
	{
		v16.w = vinsert(r0)
		v19 = vdelta(v0,v10)
	}
	{
		v19:18.uw = vunpack(v26.uh)
		v4 = vmux(q0,v19,v4)
	}
	{
		v27:26.uh = vunpack(v2.ub)
		v2 = v12
		v19 = v12
	}
	{
		v29:28.uh = vunpack(v3.ub)
	}
	{
		v12.w = vinsert(r2)
		v3 = valign(v16,v16,#4)
		v16 = v12
	}
	{
		v3.w = vinsert(r1)
		v16.w = vinsert(r4)
		v17 = vdelta(v18,v10)
		r1:0 = memd(r12+#96)
	}
	{
		v19.w = vinsert(r0)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r3)
		v16 = valign(v16,v16,#4)
		r3:2 = memd(r12+#88)
	}
	{
		v16.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5:4 = memd(r12+#120)
	}
	{
		v3.w = vinsert(r2)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r4)
		v18 = valign(v19,v19,#4)
	}
	{
		v18.w = vinsert(r1)
		v3 = valign(v3,v3,#4)
		r1:0 = memd(r12+#72)
	}
	{
		v3.w = vinsert(r3)
		v16 = valign(v16,v16,#4)
		r3:2 = memd(r12+#104)
	}
	{
		v16.w = vinsert(r0)
		v12 = valign(v12,v12,#4)
	}
	{
		v12.w = vinsert(r5)
		r5 = add(r30,#-1840)
		v18 = valign(v18,v18,#4)
	}
	{
		v18.w = vinsert(r2)
		v3 = vror(v3,r28)
	}
	{
		v16 = valign(v16,v16,#4)
		v3 = vor(v3,v31)
	}
	{
		v16.w = vinsert(r1)
		v12 = vror(v12,r28)
		r1 = memw(r30+#-2864)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r1,#2176)
		r2 = r1
		v18 = valign(v18,v18,#4)
		v12 = vor(v12,v31)
	}
	{
		v18.w = vinsert(r3)
		r3 = add(r30,#-19376)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v27:26.uh = vunpack(v3.ub)
		v17 = vmux(q0,v17,v26)
	}
	{
		v3 = vror(v16,r28)
		r0 = memw(r30+##-21064)
		v16 = vmem(r0+#0)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-1920)
		v1:0.uw = vunpack(v28.uh)
		v3 = vor(v3,v31)
	}
	{
		v29:28.uh = vunpack(v12.ub)
		v7 = vmux(q0,v7,v0)
	}
	{
		v12 = vror(v18,r28)
	}
	{
		v12 = vor(v12,v31)
		v0 = vmemu(r5+#0)
	}                                       // 128-byte Folded Reload
	{
		v27:26.uw = vunpack(v26.uh)
		r5:4 = memd(r6+#96)
	}
	{
		v2.w = vinsert(r4)
		v19:18.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v3.ub)
	}
	{
		v3 = vdelta(v26,v10)
	}
	{
		v27:26.uh = vunpack(v12.ub)
	}
	{
		v12 = valign(v0,v0,r7)
	}
	{
		v10 = vdelta(v18,v10)
	}
	{
		v19:18.uw = vunpack(v28.uh)
	}
	{
		v29:28.w = vunpack(v0.h)
	}
	{
		v1:0.w = vunpack(v12.h)
	}
	{
		r1 = add(r30,#-8240)
		v23:22.w = vunpack(v16.h)
		v1 = vmux(q0,v3,v18)
		v3 = vmem(r1+#5)
	}
	{
		v19.w = vmpyieo(v11.h,v28.h)
		v18.w = vmpyieo(v13.h,v0.h)
		v5 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		v19.w += vmpyie(v11.w,v28.h)
		v27:26.uw = vunpack(v26.uh)
	}
	{
		v11.w = vmpyieo(v5.h,v22.h)
		v12 = valign(v3,v3,r7)
		v10 = vmux(q0,v10,v26)
	}
	{
		r0 = add(r30,#-1584)
		v27:26.w = vunpack(v3.h)
		v3 = vmem(r0+#0)
	}
	{
		v18.w += vmpyie(v13.w,v0.h)
		v29:28.w = vunpack(v3.h)
	}
	{
		v11.w += vmpyie(v5.w,v22.h)
		v0 = valign(v2,v2,#4)
	}
	{
		v13.w = vmpyieo(v24.h,v26.h)
		v0.w = vinsert(r5)
		v23:22.w = vunpack(v12.h)
		r5:4 = memd(r6+#104)
	}
	{
		v23.w = vmpyieo(v1.h,v28.h)
		r6 = add(r30,#-1072)
		v2 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v13.w += vmpyie(v24.w,v26.h)
		v0 = valign(v0,v0,#4)
	}
	{
		v23.w += vmpyie(v1.w,v28.h)
		v1 = valign(v2,v2,r7)
	}
	{
		v12.w = vmpyieo(v4.h,v22.h)
		v0.w = vinsert(r4)
		r4 = add(r30,#-19504)
		v27:26.w = vunpack(v2.h)
	}
	{
		v12.w += vmpyie(v4.w,v22.h)
		v2 = valign(v3,v3,r7)
	}
	{
		v3.w = vmpyieo(v7.h,v26.h)
		v29:28.w = vunpack(v1.h)
		v1 = vmem(r2+#7)
	}
	{
		v3.w += vmpyie(v7.w,v26.h)
		v5:4.w = vunpack(v2.h)
	}
	{
		v2.w = vmpyieo(v17.h,v28.h)
		v0 = valign(v0,v0,#4)
	}
	{
		v22.w = vmpyieo(v10.h,v4.h)
		v0.w = vinsert(r5)
		v26 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	{
		v22.w += vmpyie(v10.w,v4.h)
		r5 = add(r30,#-816)
		r4 = add(r2,#1920)
		v5:4.w = vunpack(v1.h)
	}
	{
		v2.w += vmpyie(v17.w,v28.h)
		r3 = add(r30,#-7984)
		v27 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v5.w = vmpyieo(v15.h,v4.h)
		r2 = add(r30,#-7728)
		v1 = valign(v1,v1,r7)
	}
	{
		v5.w += vmpyie(v15.w,v4.h)
		r6 = add(r30,#-6960)
		v4 = vmemu(r6+#0)
	}                                       // 128-byte Folded Reload
	{
		v0 = vror(v0,r28)
		v23:22.w = vadd(v23:22.w,v27:26.w)
	}
	{
		v23:22.w = vunpack(v1.h)
		v3:2.w = vadd(v23:22.w,v3:2.w)
	}
	{
		v1 = valign(v4,v4,r7)
		v3:2.w = vadd(v13:12.w,v3:2.w)
		v0 = vor(v0,v31)
	}
	{
		v4.w = vmpyieo(v25.h,v22.h)
		v27:26.w = vunpack(v4.h)
	}
	{
		v4.w += vmpyie(v25.w,v22.h)
		r5 = add(r30,#-6832)
		v27 = vmemu(r5+#0)
	}                                       // 128-byte Folded Reload
	{
		v13:12.uh = vunpack(v0.ub)
		v3:2.w = vadd(v3:2.w,v5:4.w)
	}
	{
		v13.w = vmpyieo(v9.h,v26.h)
		v1:0.w = vunpack(v1.h)
	}
	{
		r4 = add(r30,#-3376)
		v23:22.uw = vunpack(v12.uh)
		v1 = vmem(r4+#0)
	}
	{
		v12.w = vmpyieo(v14.h,v0.h)
		v7 = valign(v27,v27,r7)
	}
	{
		v12.w += vmpyie(v14.w,v0.h)
		v5:4.w = vunpack(v1.h)
	}
	{
		v13.w += vmpyie(v9.w,v26.h)
		r3 = add(r30,#-7216)
		v0 = vmemu(r3+#0)
	}                                       // 128-byte Folded Reload
	{
		v29:28.w = vunpack(v27.h)
		v0 = vmux(q0,v0,v22)
	}
	{
		v1 = valign(v1,v1,r7)
		v3:2.w = vadd(v3:2.w,v13:12.w)
	}
	{
		v9.w = vmpyieo(v8.h,v28.h)
		r7 = add(r30,#-7600)
		v5 = valign(v16,v16,r7)
	}
	{
		v9.w += vmpyie(v8.w,v28.h)
		v25:24.w = vunpack(v7.h)
	}
	{
		v25.w = vmpyieo(v30.h,v4.h)
		v27:26.w = vunpack(v1.h)
	}
	{
		v8.w = vmpyieo(v0.h,v24.h)
		v29:28.w = vunpack(v5.h)
	}
	{
		v8.w += vmpyie(v0.w,v24.h)
		v1 = vmemu(r4+#0)
	}                                       // 128-byte Folded Reload
	{
		v24.w = vmpyieo(v6.h,v26.h)
		v3:2.w = vadd(v19:18.w,v3:2.w)
	}
	{
		v25.w += vmpyie(v30.w,v4.h)
		r2 = add(r30,#-7088)
		v4 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v10.w = vmpyieo(v1.h,v28.h)
		v5 = vmemu(r7+#0)
	}                                       // 256-byte Folded Reload
	{
		v3:2.w = vadd(v3:2.w,v9:8.w)
		v16 = vmemu(r6+#0)
	}                                       // 256-byte Folded Reload
	{
		v24.w += vmpyie(v6.w,v26.h)
		v17 = vmemu(r5+#0)
	}                                       // 256-byte Folded Reload
	{
		v5:4.w = vadd(v17:16.w,v5:4.w)
		v31:30.w = vadd(v3:2.w,v25:24.w)
	}
	{
		v10.w += vmpyie(v1.w,v28.h)
		v5:4.w = vadd(v5:4.w,v21:20.w)
	}
	{
		v9:8.w = vadd(v11:10.w,v31:30.w)
		v2 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v3 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v23:22.w = vadd(v3:2.w,v5:4.w)
	}
.LBB131_650:                            // %"consume convolved1777"
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r16 = ##-1073741825
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r17 = ##2147483647
		r26 = ##1073741824
	}
	{
		r0 = add(r1,##-40960)
		r3 = add(r1,##-41088)
	}
	{
		memw(r30+#-1584) = r0
		memw(r30+##-7728) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = #32767
		r2 = add(r1,#-32256)
		vmem(r0+#0) = v9
	}
	{
		v10 = vsplat(r0)
		r0 = #-32768
		memw(r30+#-1072) = r2
	}                                       // 4-byte Folded Spill
	{
		v9 = vsplat(r0)
		r18 = add(r1,##-41216)
		vmem(r3+#0) = v8
	}
	{
		r0 = add(r1,#-31872)
		r6 = add(r1,##-41344)
		memw(r30+#-560) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = setbit(r2,#7)
		vmem(r18+#0) = v23
	}
	{
		r27 = #0
		r0 = add(r1,##-41728)
		memw(r30+#-816) = r0
	}                                       // 4-byte Folded Spill
	{
		r1 = add(pc,##.LCPI131_6@PCREL)
		memw(r30+#-304) = r0
		vmem(r6+#0) = v22
	}                                       // 4-byte Folded Spill
	{
		r24 = ##-2147483648
		r25 = #-1
		r2 = memw(r6+#120)
	}
	{
		v6 = vxor(v6,v6)
		r0 = memw(r6+#124)
		memw(r30+##-10800) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r3 = asr(r0,#31)
		v1:0 = vcombine(v6,v6)
		v17 = v6
	}
	{
		v15:14 = vcombine(v6,v6)
		v7 = vmem(r1+#0)
		memw(r30+##-9008) = r3
	}                                       // 4-byte Folded Spill
	{
		r3 = asr(r2,#31)
		r4 = memw(r6+#112)
		memw(r30+##-11056) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r5 = asr(r4,#31)
		v13:12 = vcombine(v6,v6)
		r0 = memw(r6+#116)
	}
	{
		v11 = v6
		memw(r30+##-10928) = r0
	}                                       // 4-byte Folded Spill
	{
		r4 = asr(r0,#31)
		v8 = v6
		memw(r30+##-9520) = r5
	}                                       // 4-byte Folded Spill
	{
		r5 = memw(r6+#104)
		memw(r30+##-9264) = r4

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r7 = asr(r5,#31)
		r22 = memw(r30+##-6456)
		r0 = memw(r6+#108)
	}                                       // 4-byte Folded Reload
	{
		r4 = asr(r0,#31)
		memw(r30+##-11184) = r0
		r14 = memw(r6+#96)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r9:8 = mpyu(r22,r2)
		r13:12 = mpyu(r22,r5)
		memw(r30+##-12336) = r14
	}                                       // 4-byte Folded Spill
	{
		r13 += mpyi(r22,r7)
		r9 += mpyi(r22,r3)
		memw(r30+##-9776) = r4
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r6+#100)
		memw(r30+##-11824) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r4 = asr(r0,#31)
		r0 = asr(r14,#31)
		memw(r30+##-10032) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r4 = memw(r6+#88)
		memw(r30+##-12976) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r15 = memw(r30+##-21056)
		memw(r30+##-10288) = r0
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r6+#92)
		memw(r30+##-12592) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r13 += mpyi(r5,r15)
		r1 = asr(r0,#31)
		memw(r30+##-11568) = r1.new
	}                                       // 4-byte Folded Spill
	{
		r9 += mpyi(r2,r15)
		r13:12 = min(r13:12,r17:16)
		r5 = memw(r6+#80)
	}
	{
		r1:0 = min(r9:8,r17:16)
		r2 = asr(r4,#31)
		memw(r30+##-13744) = r5
	}                                       // 4-byte Folded Spill
	{
		r21:20 = add(r13:12,r27:26)
		r3:2 = add(r1:0,r27:26)
		memw(r30+##-12080) = r2
	}                                       // 4-byte Folded Spill
	{
		r3:2 = asr(r3:2,#31)
		r12 = ##2147483647
		r13 = #0
	}
	{
		r9:8 = asr(r21:20,#31)
		r4 = memw(r6+#84)
		memw(r30+##-14128) = r4.new
	}                                       // 4-byte Folded Spill
	{
		r3:2 = min(r3:2,r13:12)
		r0 = asr(r5,#31)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r9:8 = min(r9:8,r13:12)
		r0 = asr(r4,#31)
		memw(r30+##-13104) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r3:2,r25:24)
		r3:2 = max(r9:8,r25:24)
	}
	{
		memd(r30+#-1840) = r5:4
		memd(r30+#-3376) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r2 = memw(r6+#72)
		memw(r30+##-14512) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r6+#76)
		memw(r30+##-14384) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r6+#64)
	}
	{
		memw(r30+##-13360) = r0
		memw(r30+##-14640) = r3
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-13232) = r1
		r1 = memw(r6+#68)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-14896) = r1
		r2 = memw(r6+#56)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+##-13616) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15152) = r2
		memw(r30+##-14256) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r6+#60)
		memw(r30+##-15024) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r6+#48)
	}
	{
		memw(r30+##-15664) = r3
		memw(r30+##-6960) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-7216) = r0
		r0 = memw(r6+#52)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r0,#31)
		memw(r30+##-15280) = r0
		r1 = memw(r6+#40)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-7472) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15920) = r1
		memw(r30+##-7984) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r6+#44)
		memw(r30+##-15792) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r0,#31)
		r20 = memw(r6+#32)
		memw(r30+##-8240) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+##-8496) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r23 = memw(r6+#36)
		r28 = memw(r6+#24)
	}
	{
		r0 = asr(r23,#31)
		memw(r30+##-8752) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r20,#31)
		memw(r30+##-16048) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r10 = memw(r6+#28)
		r7 = memw(r6+#16)
	}
	{
		r0 = asr(r10,#31)
		r2 = asr(r7,#31)
		memw(r30+##-16176) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r28,#31)
		memw(r30+##-16304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r21 = memw(r6+#20)
		r9 = memw(r6+#8)
	}
	{
		r0 = asr(r21,#31)
		r11 = asr(r9,#31)
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1:0 = mpyu(r22,r7)
		r12 = memw(r6+#12)
		r8 = memw(r6+#0)
	}
	{
		r1 += mpyi(r22,r2)
		r13 = asr(r8,#31)
		r6 = memw(r6+#4)
	}
	{
		r5:4 = mpyu(r22,r8)
		r14 = asr(r6,#31)
	}
	{
		r3:2 = mpyu(r22,r6)
		r1 += mpyi(r7,r15)
		r7 = #0
	}
	{
		r3 += mpyi(r22,r14)
		r5 += mpyi(r22,r13)
	}
	{
		r3 += mpyi(r6,r15)
		r5 += mpyi(r8,r15)
		r6 = ##2147483647
	}
	{
		r1:0 = min(r1:0,r17:16)
		r3:2 = min(r3:2,r17:16)
	}
	{
		r1:0 = add(r1:0,r27:26)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r5:4 = min(r5:4,r17:16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r1:0 = min(r1:0,r7:6)
		r5:4 = asr(r5:4,#31)
	}
	{
		r3:2 = min(r3:2,r7:6)
		r5:4 = min(r5:4,r7:6)
	}
	{
		r7:6 = max(r3:2,r25:24)
		r1:0 = max(r1:0,r25:24)
	}
	{
		r1:0 = max(r5:4,r25:24)
		memd(r30+##-16432) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r9)
		v1.w = vinsert(r0)
		r7 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r22,r21)
	}
	{
		r1 += mpyi(r22,r7)
		r5 += mpyi(r22,r11)
		r7 = #0
	}
	{
		r5 += mpyi(r9,r15)
		r1 += mpyi(r21,r15)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = min(r5:4,r17:16)
		r1:0 = min(r1:0,r17:16)
	}
	{
		r3:2 = mpyu(r22,r12)
		r19 = asr(r12,#31)
	}
	{
		r3 += mpyi(r22,r19)
		r5:4 = add(r5:4,r27:26)
	}
	{
		r3 += mpyi(r12,r15)
		r1:0 = add(r1:0,r27:26)
	}
	{
		r5:4 = asr(r5:4,#31)
		v1.w = vinsert(r6)
		r6 = ##2147483647
	}
	{
		r3:2 = min(r3:2,r17:16)
		r1:0 = asr(r1:0,#31)
	}
	{
		r5:4 = min(r5:4,r7:6)
		r1:0 = min(r1:0,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = max(r5:4,r25:24)
		r3:2 = add(r3:2,r27:26)
	}
	{
		r1:0 = max(r1:0,r25:24)
		v1.w = vinsert(r4)
	}
	{
		r5:4 = mpyu(r22,r10)
		r3:2 = asr(r3:2,#31)
		r1 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r7:6)
		r7:6 = mpyu(r22,r28)
		v1 = valign(v1,v1,#4)
	}
	{
		r7 += mpyi(r22,r1)
		r3:2 = max(r3:2,r25:24)
		r1 = memw(r30+##-16176)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r22,r20)
		v1.w = vinsert(r2)
	}
	{
		r5 += mpyi(r22,r1)
		r7 += mpyi(r28,r15)
		r1 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r10,r15)
		v1 = valign(v1,v1,#4)
		r11:10 = memd(r30+##-16432)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r13 += mpyi(r22,r1)
	}
	{
		r13 += mpyi(r20,r15)
		v1.w = vinsert(r10)
		r14 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r5:4 = min(r13:12,r17:16)
		r21:20 = combine(r27,r26)
	}
	{
		r7:6 = add(r7:6,r27:26)
		r3:2 = add(r3:2,r27:26)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = add(r5:4,r27:26)
		r26 = ##2147483647
		r27 = #0
	}
	{
		r7:6 = asr(r7:6,#31)
		v1.w = vinsert(r0)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = max(r7:6,r25:24)
		r13:12 = min(r3:2,r27:26)
	}
	{
		r3:2 = max(r5:4,r25:24)
		v1.w = vinsert(r0)
		r0 = memw(r30+##-8752)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r13:12,r25:24)
		r13:12 = mpyu(r22,r23)
		r3 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r22,r0)
		r11:10 = mpyu(r22,r14)
		r0 = memw(r30+##-8496)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r3)
		r13 += mpyi(r23,r15)
		v1 = valign(v1,v1,#4)
	}
	{
		r11 += mpyi(r22,r0)
		r0 = memw(r30+##-8240)
	}                                       // 4-byte Folded Reload
	{
		v1.w = vinsert(r4)
		memd(r30+##-8752) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r11 += mpyi(r14,r15)
		r7 += mpyi(r22,r0)
		r5 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r3,r15)
		v16 = valign(v1,v1,#4)
		memd(r30+##-8496) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r5)
		r3 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r12 = memw(r30+##-15280)
		memd(r30+##-8240) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r22,r3)
		v16.w = vinsert(r2)
		r9 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r5,r15)
		r5:4 = mpyu(r22,r12)
		r8 = memw(r30+##-15024)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r9)
		v3 = valign(v16,v16,#4)
		r3 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r22,r8)
		memd(r30+#-7984) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r22,r3)
		r3 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r12,r15)
		r13:12 = combine(r17,r16)
	}
	{
		r7 += mpyi(r22,r3)
		r3 = memw(r30+##-6960)
		memd(r30+#-7472) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r9,r15)
	}
	{
		r1 += mpyi(r22,r3)
		r6 = memw(r30+##-14896)
		memd(r30+#-7216) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5:4 = combine(r1,r0)
		r2 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r22,r6)
		r5 += mpyi(r8,r15)
		r8 = memw(r30+##-14640)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r22,r2)
		r9 = memw(r30+##-14512)
		memd(r30+#-6960) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r6,r15)
		r3:2 = mpyu(r22,r8)
		r6 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r22,r9)
		r1:0 = min(r1:0,r17:16)
	}
	{
		r3 += mpyi(r22,r6)
		r1:0 = add(r1:0,r21:20)
	}
	{
		r3 += mpyi(r8,r15)
		r1:0 = asr(r1:0,#31)
		r8 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r17:16)
		r1:0 = min(r1:0,r27:26)
		r28 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r8)
		r3:2 = add(r3:2,r21:20)
		r14 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r9,r15)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r17:16)
		r3:2 = min(r3:2,r27:26)
	}
	{
		r3:2 = max(r3:2,r25:24)
		r5:4 = add(r5:4,r21:20)
	}
	{
		r1:0 = max(r1:0,r25:24)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		v0.w = vinsert(r2)
		r1 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r5:4,r25:24)
		r7:6 = mpyu(r22,r14)
		r5:4 = combine(r25,r24)
	}
	{
		r5:4 = mpyu(r22,r28)
		v0 = valign(v0,v0,#4)
		r3 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r1)
		v0.w = vinsert(r0)
		r1 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r22,r3)
		r5 += mpyi(r28,r15)
	}
	{
		r7 += mpyi(r22,r1)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r5:4,r13:12)
		r7 += mpyi(r14,r15)
	}
	{
		r11 += mpyi(r22,r1)
		v0.w = vinsert(r2)
	}
	{
		r1:0 = min(r7:6,r13:12)
		r11 += mpyi(r3,r15)
	}
	{
		r7:6 = min(r11:10,r13:12)
		r1:0 = add(r1:0,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r7:6 = add(r7:6,r21:20)
	}
	{
		r3:2 = add(r5:4,r21:20)
		r5:4 = asr(r7:6,#31)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = max(r1:0,r25:24)
	}
	{
		r3:2 = min(r3:2,r27:26)
		r5:4 = max(r5:4,r25:24)
		r1 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r25:24)
		r5 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r1)
		v0.w = vinsert(r2)
		r3 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r22,r5)
	}
	{
		r7 += mpyi(r22,r3)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r1,r15)
		v0.w = vinsert(r0)
		r1 = memw(r30+##-11568)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r7:6,r13:12)
	}
	{
		r17 += mpyi(r22,r1)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r17 += mpyi(r5,r15)
		r3:2 = add(r3:2,r21:20)
		r5 = memw(r30+##-10288)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r1)
		r3:2 = asr(r3:2,#31)
		r8 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r7 += mpyi(r22,r5)
		r5 = memw(r30+##-10032)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r1,r15)
		v0.w = vinsert(r4)
	}
	{
		r3:2 = max(r3:2,r25:24)
		r7:6 = mpyu(r22,r8)
		memd(r30+##-10288) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r17:16,r13:12)
		r7 += mpyi(r22,r5)
		v0 = valign(v0,v0,#4)
	}
	{
		r7 += mpyi(r8,r15)
		v0.w = vinsert(r2)
		r8 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r1:0,r21:20)
		r9 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		v18 = valign(v0,v0,#4)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r8)
		r1:0 = min(r1:0,r27:26)
		r3 = memw(r30+##-9776)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r9)
		r1:0 = max(r1:0,r25:24)
		r2 = memw(r30+##-9520)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r3)
		v18.w = vinsert(r0)
	}
	{
		r7 += mpyi(r22,r2)
		r5 += mpyi(r8,r15)
		r8 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r9,r15)
		v2 = valign(v18,v18,#4)
		memd(r30+##-9776) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r6 = memw(r30+##-10800)
		memd(r30+##-9520) = r7:6
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r22,r8)
		r1 = memw(r30+##-9264)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r22,r6)
		r0 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r1)
	}
	{
		r5 += mpyi(r8,r15)
		r3 += mpyi(r22,r0)
	}
	{
		r3 += mpyi(r6,r15)
		memd(r30+##-9264) = r5:4
		r1 = memw(r18+#120)

	} :mem_noshuf                           // 8-byte Folded Spill
	{
		memw(r30+##-13104) = r1
		r0 = memw(r18+#124)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r2 = asr(r0,#31)
		memd(r30+##-9008) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r0 = asr(r1,#31)
		memw(r30+##-12080) = r0
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-11184) = r2
		memw(r30+##-12592) = r0
	}                                       // 4-byte Folded Spill
	{
		r2 = memw(r18+#112)
		memw(r30+##-14512) = r2.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r18+#116)
		memw(r30+##-13360) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r18+#104)
	}
	{
		memw(r30+##-16560) = r3
		memw(r30+##-13232) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-13616) = r0
		r0 = memw(r18+#108)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-15792) = r0
		r2 = memw(r18+#96)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-16816) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15664) = r1
		memw(r30+##-15920) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r18+#100)
		memw(r30+##-16688) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r18+#88)
	}
	{
		memw(r30+##-17200) = r3
		memw(r30+##-14384) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-15152) = r0
		r0 = memw(r18+#92)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-17072) = r0
		r2 = memw(r18+#80)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-17584) = r2
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-16944) = r1
		memw(r30+##-16176) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r18+#84)
		memw(r30+##-17456) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r2,#31)
		r3 = memw(r18+#72)
	}
	{
		memw(r30+##-17968) = r3
		memw(r30+##-16304) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-17328) = r0
		r0 = memw(r18+#76)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		memw(r30+##-17840) = r0
		r2 = memw(r18+#64)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r0 = asr(r3,#31)
		memw(r30+##-17712) = r1
	}                                       // 4-byte Folded Spill
	{
		memw(r30+##-18232) = r2
		memw(r30+##-16048) = r0
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r2,#31)
		r0 = memw(r18+#68)
		memw(r30+##-18480) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r3 = memw(r18+#56)
		memw(r30+##-18096) = r1

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		memw(r30+##-18736) = r3
		memw(r30+##-18224) = r1
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r18+#60)
		memw(r30+##-18608) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r1 = asr(r0,#31)
		r0 = asr(r3,#31)
		r26 = memw(r18+#48)
	}
	{
		memw(r30+##-10800) = r1
		memw(r30+##-10928) = r0
	}                                       // 4-byte Folded Spill
	{
		r7 = memw(r18+#52)
		r11 = memw(r18+#40)
	}
	{
		r0 = asr(r7,#31)
		memw(r30+##-11056) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r26,#31)
		memw(r30+##-11568) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r17 = memw(r18+#44)
		r27 = memw(r18+#32)
	}
	{
		r0 = asr(r17,#31)
		memw(r30+##-11824) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r11,#31)
		memw(r30+##-12336) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r25 = memw(r18+#36)
		r12 = memw(r18+#24)
	}
	{
		r0 = asr(r25,#31)
		memw(r30+##-12848) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = asr(r27,#31)
		memw(r30+##-12976) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r13 = memw(r18+#28)
		r16 = memw(r18+#16)
	}
	{
		r0 = asr(r13,#31)
		r14 = asr(r16,#31)
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r21:20 = mpyu(r22,r16)
		r0 = asr(r12,#31)
		memw(r30+##-13744) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r21 += mpyi(r22,r14)
		r9 = memw(r18+#20)
		r8 = memw(r18+#8)
	}
	{
		r21 += mpyi(r16,r15)
		r24 = asr(r9,#31)
		r28 = memw(r18+#12)
		r6 = memw(r18+#0)
	}
	{
		r2 = asr(r6,#31)
		r18 = asr(r8,#31)
		r10 = memw(r18+#4)
	}
	{
		r23 = asr(r10,#31)
		r19 = asr(r28,#31)
		memd(r30+##-14640) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r10)
		r1:0 = mpyu(r22,r28)
	}
	{
		r5 += mpyi(r22,r23)
		r1 += mpyi(r22,r19)
		r23 = r15
	}
	{
		r5 += mpyi(r10,r15)
	}
	{
		r5:4 = mpyu(r22,r6)
		memd(r30+##-15280) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r22,r2)
		r3:2 = mpyu(r22,r8)
	}
	{
		r5 += mpyi(r6,r15)
		r3 += mpyi(r22,r18)
	}
	{
		r5:4 = combine(r3,r2)
		r3:2 = combine(r1,r0)
		memd(r30+##-16432) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r9)
		r5 += mpyi(r8,r15)
	}
	{
		r3 += mpyi(r28,r15)
		memd(r30+##-15024) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r22,r24)
		r4 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r24 = ##-1073741825
		memd(r30+##-14896) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r22,r12)
		r1 += mpyi(r9,r15)
	}
	{
		r3 += mpyi(r22,r4)
		r5:4 = mpyu(r22,r13)
		memd(r30+##-14256) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r12,r15)
		r1:0 = mpyu(r22,r25)
	}
	{
		r2 = memw(r30+##-14128)
		memd(r30+##-13744) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r2)
	}
	{
		r5:4 = mpyu(r22,r27)
		r3:2 = combine(r5,r4)
	}
	{
		r3 += mpyi(r13,r15)
	}
	{
		r2 = memw(r30+##-12976)
		memd(r30+##-14128) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r2)
		r2 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r27,r15)
		r27 = #0
	}
	{
		r4 = memw(r30+##-12336)
		memd(r30+##-12976) = r5:4
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r22,r2)
		r3:2 = mpyu(r22,r11)
	}
	{
		r1 += mpyi(r25,r15)
		r3 += mpyi(r22,r4)
		r25 = ##2147483647
	}
	{
		r5:4 = mpyu(r22,r17)
		r3 += mpyi(r11,r15)
		memd(r30+##-12848) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r7)
		memd(r30+##-12336) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r2 = memw(r30+##-11824)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r2)
	}
	{
		r5:4 = mpyu(r22,r26)
		r3:2 = combine(r5,r4)
	}
	{
		r3 += mpyi(r17,r15)
	}
	{
		r2 = memw(r30+##-11568)
		memd(r30+##-11824) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r2)
		r2 = memw(r30+##-11056)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r26,r15)
		r6 = memw(r30+##-18736)
	}                                       // 4-byte Folded Reload
	{
		r26 = ##2147483647
		memd(r30+##-11568) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r22,r2)
		r3:2 = mpyu(r22,r6)
		r4 = memw(r30+##-10928)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r7,r15)
		r7 = memw(r30+##-18608)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r22,r4)
		memd(r30+##-11056) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r6,r15)
		r5:4 = mpyu(r22,r7)
		r6 = memw(r30+##-18232)
	}                                       // 4-byte Folded Reload
	{
		r2 = memw(r30+##-10800)
		memd(r30+##-10928) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r8 = memw(r30+##-18480)
		r0 = memw(r30+##-18224)
	}                                       // 4-byte Folded Reload
	{
		r5 += mpyi(r22,r2)
	}
	{
		r19:18 = mpyu(r22,r8)
		r5:4 = mpyu(r22,r6)
		r3:2 = combine(r5,r4)
	}
	{
		r19 += mpyi(r22,r0)
		r3 += mpyi(r7,r15)
		r0 = memw(r30+##-18096)
	}                                       // 4-byte Folded Reload
	{
		r7 = memw(r30+##-17968)
		memd(r30+##-10800) = r3:2
	}                                       // 4-byte Folded Reload
	{
		r19 += mpyi(r8,r15)
		r5 += mpyi(r22,r0)
		r2 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = mpyu(r22,r7)
		r5 += mpyi(r6,r15)
		r6 = memw(r30+##-17840)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r22,r2)
		r5:4 = min(r5:4,r25:24)
	}
	{
		r1 += mpyi(r7,r15)
		r3:2 = mpyu(r22,r6)
	}
	{
		r0 = memw(r30+##-17712)
		memd(r30+##-16048) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r12 = memw(r30+##-17584)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r22,r0)
		r9 = memw(r30+##-17456)
	}                                       // 4-byte Folded Reload
	{
		r3 += mpyi(r6,r15)
		r1:0 = mpyu(r22,r12)
		r6 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r11:10 = mpyu(r22,r9)
		r3:2 = min(r3:2,r25:24)
	}
	{
		r1 += mpyi(r22,r6)
		r6 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r12,r15)
		r8 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-16176)
		memd(r30+##-16304) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r22,r6)
		r7:6 = mpyu(r22,r8)
		r28 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r9,r15)
		r7 += mpyi(r22,r0)
		r14 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r22,r28)
		r7 += mpyi(r8,r15)
		r0 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r22,r14)
		memd(r30+##-16176) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r13 += mpyi(r22,r0)
		r0 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r28,r15)
		r1 = memw(r30+##-15152)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r0)
	}
	{
		r21 += mpyi(r22,r1)
		r1 = memw(r30+##-14384)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r14,r15)
		r28 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r22,r1)
		memd(r30+##-15152) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r0,r15)
		r9:8 = mpyu(r22,r28)
	}
	{
		r7:6 = memd(r30+##-16432)
		memd(r30+##-14384) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r25:24)
		r14 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r0 = memw(r30+##-15920)
	}                                       // 4-byte Folded Reload
	{
		r17:16 = mpyu(r22,r14)
	}
	{
		r9 += mpyi(r22,r0)
		r0 = memw(r30+##-15664)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = combine(r9,r8)
		r8 = ##1073741824
		r9 = #0
	}
	{
		r17 += mpyi(r22,r0)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r17 += mpyi(r14,r15)
		r7:6 = asr(r7:6,#31)
		r14 = memw(r30+##-14512)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r28,r15)
		r9:8 = min(r7:6,r27:26)
		memd(r30+##-15664) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r16 = ##-2147483648
		r7:6 = memd(r30+##-15280)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = mpyu(r22,r14)
		r17 = #-1
		memd(r30+##-15920) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r21:20 = max(r9:8,r17:16)
		r7:6 = min(r7:6,r25:24)
		r8 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		v17.w = vinsert(r20)
		r9 = #0
		r21:20 = memd(r30+##-15024)
	}                                       // 8-byte Folded Reload
	{
		r1 += mpyi(r22,r8)
		r21:20 = min(r21:20,r25:24)
		r8 = ##1073741824
	}
	{
		r1 += mpyi(r14,r15)
		r7:6 = add(r7:6,r9:8)
		r14 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = asr(r7:6,#31)
		v0 = valign(v17,v17,#4)
		r28 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r21:20 = add(r21:20,r9:8)
		memd(r30+##-15024) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r14)
		r7:6 = max(r7:6,r17:16)
	}
	{
		r1 += mpyi(r22,r28)
		v0.w = vinsert(r6)
	}
	{
		r7:6 = asr(r21:20,#31)
		r21:20 = memd(r30+##-14896)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r1 += mpyi(r14,r15)
		r14 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = min(r21:20,r25:24)
		r7:6 = max(r7:6,r17:16)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = add(r21:20,r9:8)
		v0.w = vinsert(r6)
		r28 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = mpyu(r22,r14)
		r1:0 = add(r5:4,r9:8)
		memd(r30+##-14896) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r22,r28)
		r7:6 = asr(r7:6,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r21 += mpyi(r14,r15)
	}
	{
		r5:4 = min(r19:18,r25:24)
		r7:6 = max(r7:6,r17:16)
		r19:18 = memd(r30+##-14640)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r17:16)
		r21:20 = combine(r17,r16)
		memd(r30+##-14512) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r17:16 = min(r19:18,r25:24)
		v0.w = vinsert(r6)
	}
	{
		r7:6 = add(r5:4,r9:8)
		r5:4 = add(r17:16,r9:8)
	}
	{
		r1:0 = asr(r5:4,#31)
		v15.w = vinsert(r0)
	}
	{
		r7:6 = asr(r7:6,#31)
		v0 = valign(v0,v0,#4)
		r5:4 = memd(r30+##-16048)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = min(r5:4,r25:24)
		v1 = valign(v15,v15,#4)
	}
	{
		r7:6 = max(r7:6,r21:20)
		r17:16 = add(r5:4,r9:8)
	}
	{
		r1:0 = min(r1:0,r27:26)
		v1.w = vinsert(r6)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r5:4 = asr(r17:16,#31)
	}
	{
		r7:6 = min(r5:4,r27:26)
		v0.w = vinsert(r0)
		r5:4 = memd(r30+##-14256)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r7:6,r21:20)
		r7:6 = min(r5:4,r25:24)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = add(r3:2,r9:8)
		v1.w = vinsert(r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = add(r7:6,r9:8)
		r7:6 = asr(r3:2,#31)
		r2 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = min(r7:6,r27:26)
		r1:0 = asr(r1:0,#31)
		r3 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = mpyu(r22,r2)
		r1:0 = min(r1:0,r27:26)
		v1 = valign(v1,v1,#4)
	}
	{
		r5 += mpyi(r22,r3)
		r7:6 = max(r7:6,r21:20)
	}
	{
		r5 += mpyi(r2,r15)
		r1:0 = max(r1:0,r21:20)
		r3:2 = memd(r30+##-16304)
	}                                       // 8-byte Folded Reload
	{
		v0.w = vinsert(r0)
		v1.w = vinsert(r6)
		r7:6 = memd(r30+##-13744)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r3:2,r25:24)
		r3:2 = min(r7:6,r25:24)
		memd(r30+##-14640) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = add(r1:0,r9:8)
		r7:6 = add(r3:2,r9:8)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		r7:6 = asr(r7:6,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r5:4 = min(r11:10,r25:24)
	}
	{
		r1:0 = max(r1:0,r21:20)
		r3:2 = min(r7:6,r27:26)
		r7:6 = memd(r30+##-16176)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = add(r5:4,r9:8)
		v1.w = vinsert(r0)
	}
	{
		r1:0 = max(r3:2,r21:20)
		r3:2 = min(r7:6,r25:24)
	}
	{
		r5:4 = asr(r5:4,#31)
		r7:6 = add(r3:2,r9:8)
		r3:2 = combine(r27,r26)
		v1 = valign(v1,v1,#4)
	}
	{
		r1:0 = asr(r7:6,#31)
		v0.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r1:0 = max(r1:0,r21:20)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = min(r13:12,r25:24)
		v1.w = vinsert(r4)
		r5:4 = memd(r30+##-14128)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		r25:24 = combine(r3,r2)
		r1 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = add(r7:6,r9:8)
		r5:4 = add(r5:4,r9:8)
		v1 = valign(v1,v1,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		v1.w = vinsert(r0)
		r8 = ##-1073741825
	}
	{
		r17:16 = asr(r5:4,#31)
		r0 = memw(r1+#120)
		memw(r30+##-12080) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r3:2 = min(r17:16,r3:2)
		r0 = memw(r1+#124)
		memw(r30+##-11184) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r13:12 = min(r7:6,r25:24)
		r0 = memw(r1+#112)
		memw(r30+##-13104) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r19:18 = max(r3:2,r21:20)
		r17:16 = max(r13:12,r21:20)
		r0 = memw(r1+#116)
	}
	{
		v1 = valign(v1,v1,#4)
		memw(r30+##-12592) = r0
	}                                       // 4-byte Folded Spill
	{
		r9 = ##2147483647
		r0 = memw(r1+#104)
	}
	{
		v1.w = vinsert(r16)
		v0.w = vinsert(r18)
		memw(r30+##-13360) = r0
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#108)
		memw(r30+##-13232) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r0 = memw(r1+#96)
		memw(r30+##-13744) = r0.new
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r0 = memw(r1+#100)
		memw(r30+##-13616) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#88)
		memw(r30+##-14256) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#92)
		memw(r30+##-14128) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#80)
		memw(r30+##-15792) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#84)
		memw(r30+##-15280) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#72)
		memw(r30+##-16176) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#76)
		memw(r30+##-16048) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#64)
		memw(r30+##-16304) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#68)
		memw(r30+##-16432) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#56)
		memw(r30+##-16560) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#60)
		memw(r30+##-16688) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#48)
		memw(r30+##-16944) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#52)
		memw(r30+##-16816) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#40)
		memw(r30+##-17200) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r1+#44)
		memw(r30+##-17072) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r11 = memw(r1+#32)
		r0 = memw(r1+#36)
	}
	{
		memw(r30+##-17328) = r0
		r14 = memw(r1+#24)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		r28 = memw(r1+#28)
		r10 = memw(r1+#16)
	}
	{
		r26 = memw(r1+#20)
		r19 = memw(r1+#8)
	}
	{
		r27 = memw(r1+#12)
		r0 = memw(r1+#0)
	}
	{
		r3 = memw(r1+#4)
	}
	{
		r17 = asr(r3,#31)
	}
	{
		r7:6 = mpyu(r22,r3)
	}
	{
		r7 += mpyi(r22,r17)
	}
	{
		r13:12 = combine(r7,r6)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = min(r7:6,r9:8)
		r13 += mpyi(r3,r15)
		r7:6 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r9:8)
		r7:6 = memd(r30+##-15152)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		memd(r30+##-15152) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r8 = ##1073741824
		r9 = #0
	}
	{
		r17:16 = add(r17:16,r9:8)
		r5:4 = add(r5:4,r9:8)
		r13:12 = combine(r9,r8)
	}
	{
		r7:6 = add(r7:6,r9:8)
		r17:16 = asr(r17:16,#31)
	}
	{
		r5:4 = asr(r5:4,#31)
		r7:6 = asr(r7:6,#31)
	}
	{
		r17:16 = min(r17:16,r25:24)
		r5:4 = min(r5:4,r25:24)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r5:4 = max(r5:4,r21:20)
	}
	{
		r17:16 = max(r17:16,r21:20)
		v0.w = vinsert(r4)
	}
	{
		r7:6 = max(r7:6,r21:20)
		v2.w = vinsert(r16)
	}
	{
		r17:16 = mpyu(r22,r0)
		r4 = asr(r0,#31)
		v0 = valign(v0,v0,#4)
	}
	{
		r17 += mpyi(r22,r4)
		v1.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		r7:6 = mpyu(r22,r19)
		r5 = asr(r19,#31)
	}
	{
		r7 += mpyi(r22,r5)
		r17 += mpyi(r0,r15)
		r5:4 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r19,r15)
		v1 = valign(v1,v1,#4)
		memd(r30+##-17456) = r17:16
	}                                       // 8-byte Folded Spill
	{
		r16 = ##-1073741825
		r17 = ##2147483647
	}
	{
		r1:0 = min(r5:4,r17:16)
		memd(r30+##-10288) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = add(r1:0,r9:8)
		r7:6 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r7:6,r17:16)
		r9:8 = combine(r25,r24)
		r19:18 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r19:18,r17:16)
		r7:6 = add(r3:2,r13:12)
		r3:2 = memd(r30+##-14384)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r19:18 = add(r1:0,r13:12)
	}
	{
		r5:4 = min(r5:4,r25:24)
		r7:6 = asr(r7:6,#31)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r19:18 = asr(r19:18,#31)
	}
	{
		r1:0 = min(r7:6,r25:24)
		r3:2 = min(r3:2,r17:16)
	}
	{
		r7:6 = min(r19:18,r25:24)
		v3.w = vinsert(r4)
	}
	{
		r5:4 = max(r1:0,r21:20)
		r1:0 = add(r3:2,r13:12)
	}
	{
		r5:4 = asr(r1:0,#31)
		v2.w = vinsert(r4)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = max(r7:6,r21:20)
		r5:4 = min(r5:4,r25:24)
	}
	{
		r3 = asr(r10,#31)
		v0.w = vinsert(r6)
		v2 = valign(v2,v2,#4)
	}
	{
		r7:6 = mpyu(r22,r10)
		r5:4 = max(r5:4,r21:20)
	}
	{
		r7 += mpyi(r22,r3)
		v1.w = vinsert(r4)
		r5:4 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r10,r15)
		r3:2 = min(r5:4,r17:16)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = memd(r30+##-12336)
		memd(r30+##-10032) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r19:18 = mpyu(r22,r27)
		r0 = asr(r27,#31)
	}
	{
		r7:6 = min(r7:6,r17:16)
		r19 += mpyi(r22,r0)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = add(r3:2,r13:12)
		r7:6 = add(r7:6,r13:12)
	}
	{
		r3:2 = mpyu(r22,r26)
		r0 = asr(r26,#31)
	}
	{
		r3 += mpyi(r22,r0)
		r7:6 = asr(r7:6,#31)
	}
	{
		r3 += mpyi(r26,r15)
		r5:4 = asr(r5:4,#31)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r5:4 = min(r5:4,r25:24)
		memd(r30+##-12336) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = max(r7:6,r21:20)
		r19 += mpyi(r27,r15)
		r7:6 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		v2.w = vinsert(r6)
		memd(r30+##-12976) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r7:6 = mpyu(r22,r28)
		r3 = asr(r28,#31)
	}
	{
		r7 += mpyi(r22,r3)
		v3.w = vinsert(r4)
		r5:4 = memd(r30+##-15920)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = mpyu(r22,r14)
		r0 = asr(r14,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r19 += mpyi(r22,r0)
		r1:0 = min(r5:4,r17:16)
		v3 = valign(v3,v3,#4)
	}
	{
		r7 += mpyi(r28,r15)
		r5:4 = add(r1:0,r13:12)
	}
	{
		v0.w = vinsert(r2)
		r7:6 = memd(r30+##-9776)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		r7:6 = min(r7:6,r17:16)
	}
	{
		r1:0 = min(r3:2,r17:16)
		r5:4 = max(r5:4,r21:20)
		r3:2 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r1:0,r13:12)
		v1.w = vinsert(r4)
		v0 = valign(v0,v0,#4)
	}
	{
		r19 += mpyi(r14,r15)
		r5:4 = asr(r1:0,#31)
	}
	{
		r3:2 = min(r3:2,r17:16)
		r7:6 = add(r7:6,r13:12)
		memd(r30+##-12848) = r19:18
	}                                       // 8-byte Folded Spill
	{
		r5:4 = min(r5:4,r25:24)
		r19:18 = add(r3:2,r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r7:6 = asr(r7:6,#31)
	}
	{
		r1:0 = asr(r19:18,#31)
		v3.w = vinsert(r4)
	}
	{
		r5:4 = min(r1:0,r25:24)
		r7:6 = min(r7:6,r25:24)
		r1 = memw(r30+##-17328)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = max(r7:6,r21:20)
		r3 = asr(r1,#31)
		r25:24 = combine(r17,r16)
		r28 = r1
	}
	{
		r7:6 = mpyu(r22,r1)
		v2.w = vinsert(r6)
		v3 = valign(v3,v3,#4)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r0 = asr(r11,#31)
		r1 = memw(r30+##-17200)
	}                                       // 4-byte Folded Reload
	{
		r27:26 = mpyu(r22,r11)
		r7 += mpyi(r22,r3)
		r3:2 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r27 += mpyi(r22,r0)
		v0.w = vinsert(r4)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = mpyu(r22,r1)
		r0 = asr(r1,#31)
	}
	{
		r5 += mpyi(r22,r0)
		r7 += mpyi(r28,r23)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = memd(r30+##-15664)
		memd(r30+##-8240) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r1,r23)
		r27 += mpyi(r11,r23)
	}
	{
		r5:4 = min(r3:2,r17:16)
		r7:6 = min(r7:6,r17:16)
		r3:2 = memd(r30+#-7984)
		memd(r30+#-3376) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r3:2,r17:16)
		r5:4 = add(r5:4,r13:12)
		r3:2 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = add(r7:6,r13:12)
		r5:4 = asr(r5:4,#31)
		memd(r30+##-8752) = r27:26
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r3:2,r17:16)
		r11:10 = add(r15:14,r13:12)
		r27:26 = combine(r9,r8)
	}
	{
		r5:4 = min(r5:4,r9:8)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r9:8)
		r15:14 = add(r1:0,r13:12)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r11:10 = asr(r11:10,#31)
	}
	{
		r1:0 = asr(r15:14,#31)
		v2.w = vinsert(r4)
	}
	{
		r5:4 = min(r1:0,r9:8)
		r7:6 = max(r7:6,r21:20)
		r1 = memw(r30+##-17072)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r11:10,r9:8)
		v1.w = vinsert(r6)
		r28 = r1
		r9:8 = combine(r13,r12)
	}
	{
		r7:6 = max(r3:2,r21:20)
		r0 = asr(r1,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r7:6 = mpyu(r22,r1)
		v3.w = vinsert(r6)
		r1 = memw(r30+##-16944)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = max(r5:4,r21:20)
		r3 = asr(r1,#31)
		r15 = r1
		v1 = valign(v1,v1,#4)
	}
	{
		r11:10 = mpyu(r22,r1)
		r7 += mpyi(r22,r0)
		r1 = memw(r30+##-16816)
	}                                       // 4-byte Folded Reload
	{
		r0 = asr(r1,#31)
		v0.w = vinsert(r4)
		r14 = r1
		v3 = valign(v3,v3,#4)
	}
	{
		r5:4 = mpyu(r22,r1)
		r11 += mpyi(r22,r3)
		r3:2 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r7 += mpyi(r28,r23)
		r5 += mpyi(r22,r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r7:6 = combine(r5,r4)
		memd(r30+##-8496) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = min(r3:2,r17:16)
		r5:4 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r5:4,r17:16)
		r1:0 = add(r1:0,r13:12)
		r5:4 = memd(r30+##-15024)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		r3:2 = add(r3:2,r13:12)
	}
	{
		r5:4 = add(r5:4,r13:12)
		r1:0 = asr(r1:0,#31)
		r13 = memw(r30+##-16688)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r3:2 = min(r3:2,r27:26)
	}
	{
		r1:0 = max(r1:0,r21:20)
		v1.w = vinsert(r4)
		r4 = memw(r30+##-16560)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = max(r3:2,r21:20)
		v3.w = vinsert(r0)
		r12 = r4
	}
	{
		r0 = asr(r13,#31)
		v2.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = mpyu(r22,r4)
		r1 = asr(r4,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r5:4 = mpyu(r22,r13)
		r7 += mpyi(r14,r23)
		v2 = valign(v2,v2,#4)
	}
	{
		r5 += mpyi(r22,r0)
		r3 += mpyi(r22,r1)
		r7:6 = memd(r30+#-7216)
		memd(r30+#-7472) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r13,r23)
		r3 += mpyi(r12,r23)
	}
	{
		r5:4 = memd(r30+##-14896)
		memd(r30+##-9264) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r7:6,r17:16)
		r11 += mpyi(r15,r23)
		memd(r30+#-7216) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r3:2 = min(r5:4,r17:16)
		r1:0 = add(r1:0,r9:8)
		r7:6 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r17:16)
		r3:2 = add(r3:2,r9:8)
		memd(r30+#-7984) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r5:4 = add(r5:4,r9:8)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r27:26)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r1:0 = asr(r1:0,#31)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r1:0 = min(r1:0,r27:26)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r1:0 = max(r1:0,r21:20)
		r3 = memw(r30+##-16432)
	}                                       // 4-byte Folded Reload
	{
		v0.w = vinsert(r4)
		v3.w = vinsert(r0)
		r12 = r3
		r5:4 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = mpyu(r22,r3)
		r1 = asr(r3,#31)
		r0 = memw(r30+##-16176)
	}                                       // 4-byte Folded Reload
	{
		r11 += mpyi(r22,r1)
		v1.w = vinsert(r2)
		r1 = memw(r30+##-16304)
	}                                       // 4-byte Folded Reload
	{
		r2 = asr(r0,#31)
		v2.w = vinsert(r4)
		r7:6 = combine(r1,r0)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = mpyu(r22,r0)
		r3 = asr(r1,#31)
		v18 = valign(v3,v3,#4)
	}
	{
		r15:14 = mpyu(r22,r1)
		r5 += mpyi(r22,r2)
		v1 = valign(v1,v1,#4)
	}
	{
		r15 += mpyi(r22,r3)
		v17 = valign(v2,v2,#4)
		r3:2 = memd(r30+##-10928)
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r6,r23)
		r1:0 = min(r3:2,r17:16)
	}
	{
		r15 += mpyi(r7,r23)
		r3:2 = add(r1:0,r9:8)
		r5:4 = memd(r30+#-6960)
		memd(r30+#-1840) = r5:4
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r17:16)
		r1:0 = combine(r21,r20)
		r7:6 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r3:2 = asr(r3:2,#31)
	}
	{
		r3:2 = min(r3:2,r27:26)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r3:2 = max(r3:2,r21:20)
		r5:4 = add(r5:4,r9:8)
	}
	{
		r7:6 = asr(r7:6,#31)
		v0.w = vinsert(r2)
		r3 = memw(r30+##-16048)
	}                                       // 4-byte Folded Reload
	{
		r2 = asr(r3,#31)
		r5:4 = asr(r5:4,#31)
		r28 = r3
	}
	{
		r19:18 = mpyu(r22,r3)
		r5:4 = min(r5:4,r27:26)
		v16 = valign(v0,v0,#4)
	}
	{
		r19 += mpyi(r22,r2)
		r3:2 = min(r7:6,r27:26)
		r7:6 = memd(r30+##-14512)
	}                                       // 8-byte Folded Reload
	{
		r19 += mpyi(r28,r23)
		r3:2 = max(r3:2,r21:20)
	}
	{
		r5:4 = max(r5:4,r21:20)
		r19:18 = memd(r30+##-14640)
		memd(r30+#-6960) = r19:18
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r17:16)
		r21:20 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r19:18,r17:16)
		r7:6 = add(r7:6,r9:8)
	}
	{
		r21:20 = min(r21:20,r17:16)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r27:26)
		r21:20 = add(r21:20,r9:8)
	}
	{
		r21:20 = asr(r21:20,#31)
		v17.w = vinsert(r2)
		r2 = memw(r30+##-15792)
	}                                       // 4-byte Folded Reload
	{
		r21:20 = min(r21:20,r27:26)
		r19:18 = add(r19:18,r9:8)
	}
	{
		r7:6 = max(r7:6,r1:0)
		r21:20 = max(r21:20,r1:0)
		v5 = valign(v17,v17,#4)
	}
	{
		r11 += mpyi(r12,r23)
		r19:18 = asr(r19:18,#31)
		r12 = memw(r30+##-15280)
	}                                       // 4-byte Folded Reload
	{
		v1.w = vinsert(r6)
		v16.w = vinsert(r20)
	}
	{
		r21:20 = mpyu(r22,r2)
		r6 = asr(r2,#31)
	}
	{
		r21 += mpyi(r22,r6)
		r19:18 = min(r19:18,r27:26)
		r6 = memw(r30+##-14256)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r2,r23)
		r19:18 = max(r19:18,r1:0)
		v15 = valign(v1,v1,#4)
	}
	{
		r7 = asr(r12,#31)
		v18.w = vinsert(r4)
		r3:2 = memd(r30+##-17456)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = mpyu(r22,r12)
		r17:16 = min(r3:2,r17:16)
		memd(r30+##-9520) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r22,r7)
		v15.w = vinsert(r18)
		r18 = r12
	}
	{
		r13:12 = mpyu(r22,r6)
		r28 = asr(r6,#31)
		r7 = memw(r30+##-14128)
	}                                       // 4-byte Folded Reload
	{
		r13 += mpyi(r22,r28)
		r17:16 = add(r17:16,r9:8)
	}
	{
		r5 += mpyi(r18,r23)
		r13 += mpyi(r6,r23)
		r19:18 = memd(r30+##-15152)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = asr(r17:16,#31)
		r28 = asr(r7,#31)
		r6 = memw(r30+##-13744)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r22,r7)
		r21:20 = min(r17:16,r27:26)
		memd(r30+##-9008) = r13:12
	}                                       // 8-byte Folded Spill
	{
		r13:12 = min(r19:18,r25:24)
		r3 += mpyi(r22,r28)
		r16 = r6
	}
	{
		r19:18 = max(r21:20,r1:0)
		r28 = asr(r6,#31)
	}
	{
		r21:20 = mpyu(r22,r6)
		r13:12 = add(r13:12,r9:8)
	}
	{
		r21 += mpyi(r22,r28)
		r3 += mpyi(r7,r23)
	}
	{
		r21 += mpyi(r16,r23)
		r13:12 = asr(r13:12,#31)
		memd(r30+##-10928) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r13:12 = min(r13:12,r27:26)
		v14.w = vinsert(r18)
		r3:2 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r3:2,r25:24)
		r7:6 = max(r13:12,r1:0)
		memd(r30+##-10800) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r11:10 = min(r11:10,r25:24)
		r1:0 = add(r21:20,r9:8)
		r13:12 = combine(r1,r0)
		v0 = valign(v14,v14,#4)
	}
	{
		r1:0 = asr(r1:0,#31)
		v0.w = vinsert(r6)
		r7:6 = combine(r9,r8)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r8 = memw(r30+##-13616)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = max(r1:0,r13:12)
		r28 = asr(r8,#31)
		r19:18 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = mpyu(r22,r8)
		v0 = valign(v0,v0,#4)
		r1 = memw(r30+##-13360)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r19:18,r25:24)
		r21 += mpyi(r22,r28)
	}
	{
		r19:18 = add(r3:2,r7:6)
		r28 = asr(r1,#31)
	}
	{
		r3:2 = mpyu(r22,r1)
		v0.w = vinsert(r0)
	}
	{
		r3 += mpyi(r22,r28)
		r19:18 = asr(r19:18,#31)
	}
	{
		r3 += mpyi(r1,r23)
		r19:18 = min(r19:18,r27:26)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = memd(r30+##-10032)
		memd(r30+##-9776) = r3:2
	}                                       // 8-byte Folded Reload
	{
		r19:18 = max(r19:18,r13:12)
		r21 += mpyi(r8,r23)
	}
	{
		r1:0 = min(r3:2,r25:24)
		v0.w = vinsert(r18)
	}
	{
		r17:16 = add(r1:0,r7:6)
		r0 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = asr(r17:16,#31)
		r28 = asr(r0,#31)
		r9 = r0
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = mpyu(r22,r0)
		r19:18 = min(r19:18,r27:26)
		r1:0 = memd(r30+##-12336)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r25:24)
		r19:18 = max(r19:18,r13:12)
		r8 = memw(r30+##-13104)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = add(r1:0,r7:6)
		v0.w = vinsert(r18)
	}
	{
		r3 += mpyi(r22,r28)
		r1:0 = asr(r1:0,#31)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r3 += mpyi(r9,r23)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r28 = asr(r8,#31)
		r19:18 = combine(r3,r2)
	}
	{
		r1:0 = min(r15:14,r25:24)
		v0.w = vinsert(r0)
		r3:2 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = mpyu(r22,r8)
		r1:0 = add(r1:0,r7:6)
	}
	{
		r17 += mpyi(r22,r28)
		r15:14 = min(r3:2,r25:24)
		r28 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r15:14 = add(r15:14,r7:6)
		v0 = valign(v0,v0,#4)
	}
	{
		r1:0 = min(r1:0,r27:26)
		r17 += mpyi(r8,r23)
	}
	{
		r9:8 = add(r11:10,r7:6)
		r15:14 = asr(r15:14,#31)
	}
	{
		r1:0 = max(r1:0,r13:12)
		r9:8 = asr(r9:8,#31)
	}
	{
		r9:8 = min(r9:8,r27:26)
		v13.w = vinsert(r0)
	}
	{
		r15:14 = min(r15:14,r27:26)
		r9:8 = max(r9:8,r13:12)
	}
	{
		r15:14 = max(r15:14,r13:12)
		r9 = asr(r28,#31)
		v1 = valign(v13,v13,#4)
	}
	{
		r1:0 = mpyu(r22,r28)
		v0.w = vinsert(r14)
		r15:14 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r1 += mpyi(r22,r9)
		v1.w = vinsert(r8)
		r11:10 = memd(r30+#-6960)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r15:14,r25:24)
		r15:14 = min(r11:10,r25:24)
		v0 = valign(v0,v0,#4)
		r3:2 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r11:10 = min(r3:2,r25:24)
		r9:8 = add(r9:8,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r9:8 = asr(r9:8,#31)
		r11:10 = add(r11:10,r7:6)
	}
	{
		r9:8 = min(r9:8,r27:26)
		r15:14 = add(r15:14,r7:6)
	}
	{
		r9:8 = max(r9:8,r13:12)
		r11:10 = asr(r11:10,#31)
	}
	{
		r15:14 = asr(r15:14,#31)
		v1.w = vinsert(r8)
	}
	{
		r3:2 = min(r11:10,r27:26)
		r15:14 = min(r15:14,r27:26)
	}
	{
		r9:8 = max(r3:2,r13:12)
		r3:2 = min(r5:4,r25:24)
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		r15:14 = max(r15:14,r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = add(r5:4,r7:6)
		v1.w = vinsert(r14)
		r15:14 = memd(r30+##-8752)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = min(r15:14,r25:24)
		v0.w = vinsert(r8)
		r14 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = asr(r5:4,#31)
		r3:2 = add(r3:2,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r9:8 = add(r9:8,r7:6)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = max(r5:4,r13:12)
		r3:2 = asr(r3:2,#31)
	}
	{
		r9:8 = asr(r9:8,#31)
		v1.w = vinsert(r4)
	}
	{
		r5:4 = min(r9:8,r27:26)
		r3:2 = min(r3:2,r27:26)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r5:4 = max(r5:4,r13:12)
		v1 = valign(v1,v1,#4)
	}
	{
		r3 = asr(r14,#31)
		v0.w = vinsert(r4)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		r9:8 = mpyu(r22,r14)
		v1.w = vinsert(r2)
	}
	{
		r9 += mpyi(r22,r3)
		r3:2 = min(r5:4,r25:24)
		r5:4 = memd(r30+##-10928)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r5:4,r25:24)
		r3:2 = add(r3:2,r7:6)
		v0 = valign(v0,v0,#4)
	}
	{
		r3:2 = asr(r3:2,#31)
		v1 = valign(v1,v1,#4)
		r11:10 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r5:4 = add(r5:4,r7:6)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r5:4 = asr(r5:4,#31)
	}
	{
		r5:4 = min(r5:4,r27:26)
		v1.w = vinsert(r2)
	}
	{
		r1 += mpyi(r28,r23)
		r15:14 = min(r11:10,r25:24)
		r28 = r14
	}
	{
		r5:4 = max(r5:4,r13:12)
		r15:14 = add(r15:14,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r9 += mpyi(r28,r23)
		v1.w = vinsert(r4)
		r5:4 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		r15:14 = asr(r15:14,#31)
	}
	{
		r3:2 = min(r15:14,r27:26)
		r5:4 = min(r5:4,r25:24)
		r11:10 = memd(r30+#-3376)
	}                                       // 8-byte Folded Reload
	{
		r3:2 = max(r3:2,r13:12)
		r5:4 = add(r5:4,r7:6)
		v1 = valign(v1,v1,#4)
	}
	{
		r3:2 = min(r21:20,r25:24)
		v0.w = vinsert(r2)
	}
	{
		r15:14 = min(r11:10,r25:24)
		r5:4 = asr(r5:4,#31)
	}
	{
		r3:2 = add(r3:2,r7:6)
		r15:14 = add(r15:14,r7:6)
		v0 = valign(v0,v0,#4)
	}
	{
		r5:4 = min(r5:4,r27:26)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = max(r5:4,r13:12)
		r15:14 = asr(r15:14,#31)
	}
	{
		r5:4 = min(r15:14,r27:26)
		v1.w = vinsert(r4)
		r14 = memw(r30+##-11184)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = min(r3:2,r27:26)
		r5:4 = max(r5:4,r13:12)
	}
	{
		r3:2 = max(r3:2,r13:12)
		r21:20 = mpyu(r22,r14)
		v1 = valign(v1,v1,#4)
	}
	{
		r3 = asr(r14,#31)
		v1.w = vinsert(r2)
	}
	{
		r21 += mpyi(r22,r3)
		r3:2 = min(r1:0,r25:24)
		r1:0 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r1:0,r25:24)
		v0.w = vinsert(r4)
		r1:0 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r17:16,r25:24)
		r21 += mpyi(r14,r23)
		v1 = valign(v1,v1,#4)
		r17:16 = memd(r30+#-7216)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = min(r19:18,r25:24)
		r19:18 = memd(r30+##-8496)
		memd(r30+#-1840) = r21:20
	}                                       // 8-byte Folded Reload
	{
		r15:14 = min(r17:16,r25:24)
		r11:10 = min(r19:18,r25:24)
		r19:18 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r1:0,r25:24)
		r17:16 = min(r19:18,r25:24)
		v0 = valign(v0,v0,#4)
		r19:18 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = min(r19:18,r25:24)
		r11:10 = add(r11:10,r7:6)
	}
	{
		r17:16 = add(r17:16,r7:6)
		r19:18 = add(r19:18,r7:6)
	}
	{
		r15:14 = add(r15:14,r7:6)
		r13:12 = add(r13:12,r7:6)
	}
	{
		r5:4 = add(r5:4,r7:6)
		r3:2 = add(r3:2,r7:6)
	}
	{
		r19:18 = asr(r19:18,#31)
		r21:20 = add(r21:20,r7:6)
	}
	{
		r1:0 = add(r1:0,r7:6)
		r3:2 = asr(r3:2,#31)
	}
	{
		r5:4 = asr(r5:4,#31)
		r13:12 = asr(r13:12,#31)
	}
	{
		r15:14 = asr(r15:14,#31)
		r11:10 = asr(r11:10,#31)
	}
	{
		r9:8 = min(r9:8,r25:24)
		r17:16 = asr(r17:16,#31)
		r25:24 = combine(r7,r6)
	}
	{
		r19:18 = min(r19:18,r27:26)
		r7:6 = asr(r21:20,#31)
		r20 = ##-2147483648
	}
	{
		r11:10 = min(r11:10,r27:26)
		r13:12 = min(r13:12,r27:26)
		r21 = #-1
	}
	{
		r7:6 = min(r7:6,r27:26)
		r5:4 = min(r5:4,r27:26)
	}
	{
		r3:2 = min(r3:2,r27:26)
		r17:16 = min(r17:16,r27:26)
	}
	{
		r15:14 = min(r15:14,r27:26)
		r27:26 = max(r19:18,r21:20)
	}
	{
		r7:6 = max(r7:6,r21:20)
		r5:4 = max(r5:4,r21:20)
		r27 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = max(r13:12,r21:20)
	}
	{
		r3:2 = max(r3:2,r21:20)
		v1.w = vinsert(r12)
		r7 = memw(r27+#120)
	}
	{
		r11:10 = max(r11:10,r21:20)
		r1:0 = asr(r1:0,#31)
		memw(r30+##-6960) = r7
	}                                       // 4-byte Folded Spill
	{
		v0.w = vinsert(r10)
		v1 = valign(v1,v1,#4)
		r7 = memw(r27+#124)
		memw(r30+#-3376) = r7.new
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r6)
		r5 = memw(r27+#112)
		memw(r30+##-7472) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r11:10 = add(r9:8,r25:24)
		r5 = memw(r27+#116)
		memw(r30+##-7216) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r7:6 = max(r17:16,r21:20)
		v0 = valign(v0,v0,#4)
		r5 = memw(r27+#104)
	}
	{
		v0.w = vinsert(r26)
		v1 = valign(v1,v1,#4)
		memw(r30+##-7984) = r5
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r4)
		r5 = memw(r27+#108)
		memw(r30+##-7728) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r7 = #0
		r5 = memw(r27+#96)
		memw(r30+##-12848) = r5.new
	}                                       // 4-byte Folded Spill
	{
		r11:10 = asr(r11:10,#31)
		v0 = valign(v0,v0,#4)
		r5 = memw(r27+#100)
	}
	{
		v0.w = vinsert(r6)
		v1 = valign(v1,v1,#4)
		memw(r30+##-9008) = r5
	}                                       // 4-byte Folded Spill
	{
		v1.w = vinsert(r2)
		r3 = memw(r27+#88)
		memw(r30+##-13232) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r5:4 = max(r15:14,r21:20)
		r3 = memw(r27+#92)
		memw(r30+##-12976) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r6 = ##2147483647
		v0 = valign(v0,v0,#4)
		r3 = memw(r27+#80)
	}
	{
		v0.w = vinsert(r4)
		memw(r30+##-13360) = r3
	}                                       // 4-byte Folded Spill
	{
		r4 = ##-2147483648
		r3 = memw(r27+#84)
	}
	{
		r11:10 = min(r11:10,r7:6)
		r5 = #-1
		memw(r30+##-12336) = r3
	}                                       // 4-byte Folded Spill
	{
		r1:0 = min(r1:0,r7:6)
		r3 = memw(r27+#72)
		memw(r30+##-12592) = r3.new
	}                                       // 4-byte Folded Spill
	{
		r11:10 = max(r11:10,r5:4)
		r3 = memw(r27+#76)
		memw(r30+##-12080) = r3.new
	}                                       // 4-byte Folded Spill
	{
		v0 = valign(v0,v0,#4)
		r2 = memw(r27+#64)
		memw(r30+##-13616) = r2.new
	}                                       // 4-byte Folded Spill
	{
		v1 = valign(v1,v1,#4)
		r19 = memw(r27+#68)
	}
	{
		v1.w = vinsert(r10)
		r24 = memw(r27+#56)
		r2 = memw(r27+#60)
	}
	{
		r3:2 = max(r1:0,r5:4)
		memw(r30+##-10800) = r2
		r9 = memw(r27+#48)

	} :mem_noshuf                           // 4-byte Folded Spill
	{
		v0.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
		r8 = memw(r27+#52)
		r13 = memw(r27+#40)
	}
	{
		r12 = memw(r27+#44)
		r15 = memw(r27+#32)
	}
	{
		r14 = memw(r27+#36)
		r28 = memw(r27+#24)
	}
	{
		r18 = memw(r27+#28)
		r20 = memw(r27+#16)
	}
	{
		r21 = memw(r27+#20)
		r25 = memw(r27+#8)
	}
	{
		r26 = memw(r27+#12)
		r16 = memw(r27+#0)
	}
	{
		r7:6 = mpyu(r22,r25)
		r11 = asr(r16,#31)
		r17 = memw(r27+#4)
	}
	{
		r5:4 = mpyu(r22,r16)
		r3 = asr(r17,#31)
	}
	{
		r1:0 = mpyu(r22,r17)
		r5 += mpyi(r22,r11)
	}
	{
		r1 += mpyi(r22,r3)
		r5 += mpyi(r16,r23)
	}
	{
		r1 += mpyi(r17,r23)
		r16 = asr(r25,#31)
		memd(r30+##-13104) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r26)
		r17 = asr(r26,#31)
		memd(r30+##-10928) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r7 += mpyi(r22,r16)
		r5 += mpyi(r22,r17)
	}
	{
		r17:16 = mpyu(r22,r20)
		r27 = asr(r20,#31)
	}
	{
		r17 += mpyi(r22,r27)
		r5 += mpyi(r26,r23)
	}
	{
		r7 += mpyi(r25,r23)
		r17 += mpyi(r20,r23)
		memd(r30+##-11056) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r21)
		r20 = asr(r21,#31)
		memd(r30+##-11568) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r28)
		r25 = asr(r28,#31)
	}
	{
		r1 += mpyi(r22,r20)
		r5 += mpyi(r22,r25)
	}
	{
		r7:6 = mpyu(r22,r18)
		r20 = asr(r18,#31)
	}
	{
		r7 += mpyi(r22,r20)
		r1 += mpyi(r21,r23)
	}
	{
		r5 += mpyi(r28,r23)
		r7 += mpyi(r18,r23)
		memd(r30+##-11184) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r5:4 = mpyu(r22,r15)
		r28 = asr(r15,#31)
		memd(r30+##-10288) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r14)
		r18 = asr(r14,#31)
		memd(r30+##-9520) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r5 += mpyi(r22,r28)
		r1 += mpyi(r22,r18)
	}
	{
		r7:6 = mpyu(r22,r13)
		r28 = asr(r13,#31)
	}
	{
		r7 += mpyi(r22,r28)
		r1 += mpyi(r14,r23)
	}
	{
		r7 += mpyi(r13,r23)
		r13 = asr(r12,#31)
		memd(r30+##-9264) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r1:0 = mpyu(r22,r12)
		r14 = asr(r9,#31)
		memd(r30+#-1584) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1 += mpyi(r22,r13)
		r7:6 = mpyu(r22,r9)
	}
	{
		r1 += mpyi(r12,r23)
		r13 = asr(r8,#31)
	}
	{
		r21:20 = mpyu(r22,r8)
		r7 += mpyi(r22,r14)
		memd(r30+##-8240) = r1:0
	}                                       // 8-byte Folded Spill
	{
		r21 += mpyi(r22,r13)
		r7 += mpyi(r9,r23)
		r0 = memw(r30+##-10800)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r8,r23)
		r5 += mpyi(r15,r23)
		memd(r30+##-8496) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r3:2 = mpyu(r22,r24)
		r8 = asr(r24,#31)
		r6 = r0
	}
	{
		memd(r30+##-9776) = r5:4
		memd(r30+##-8752) = r21:20
	}                                       // 8-byte Folded Spill
	{
		r3 += mpyi(r22,r8)
		r9 = asr(r0,#31)
	}
	{
		r5:4 = mpyu(r22,r0)
		r8 = asr(r19,#31)
	}
	{
		r1:0 = mpyu(r22,r19)
		r5 += mpyi(r22,r9)
		r9 = ##2147483647
	}
	{
		r1 += mpyi(r22,r8)
		r5 += mpyi(r6,r23)
	}
	{
		r1 += mpyi(r19,r23)
		r3 += mpyi(r24,r23)
		memd(r30+##-10800) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r0 = memw(r30+##-13616)
		memd(r30+##-11824) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r7 = asr(r0,#31)
		r6 = r0
		memd(r30+##-10032) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r21:20 = mpyu(r22,r0)
		r0 = memw(r30+##-12592)
	}                                       // 4-byte Folded Reload
	{
		r21 += mpyi(r22,r7)
		r12 = asr(r0,#31)
		r4 = memw(r30+##-12080)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = mpyu(r22,r0)
		r7 = asr(r4,#31)
		r5 = r0
	}
	{
		r1:0 = mpyu(r22,r4)
		r3 += mpyi(r22,r12)
	}
	{
		r1 += mpyi(r22,r7)
		r3 += mpyi(r5,r23)
	}
	{
		r1 += mpyi(r4,r23)
		r21 += mpyi(r6,r23)
		memd(r30+##-12592) = r3:2
	}                                       // 8-byte Folded Spill
	{
		r0 = memw(r30+##-13360)
		memd(r30+##-12080) = r1:0
	}                                       // 4-byte Folded Reload
	{
		r4 = asr(r0,#31)
		r8 = r0
		r2 = memw(r30+##-12336)
	}                                       // 4-byte Folded Reload
	{
		r13:12 = mpyu(r22,r0)
		r5 = asr(r2,#31)
		r3 = r2
	}
	{
		r7:6 = mpyu(r22,r2)
		r13 += mpyi(r22,r4)
		r2 = memw(r30+##-13232)
	}                                       // 4-byte Folded Reload
	{
		r7 += mpyi(r22,r5)
		r28 = asr(r2,#31)
	}
	{
		r5:4 = mpyu(r22,r2)
		r7 += mpyi(r3,r23)
		r3 = #0
	}
	{
		r5 += mpyi(r22,r28)
		r13 += mpyi(r8,r23)
		r8 = ##-1073741825
	}
	{
		r7:6 = memd(r30+##-13104)
		memd(r30+##-12336) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r5 += mpyi(r2,r23)
		r17:16 = min(r17:16,r9:8)
		r2 = ##1073741824
	}
	{
		r1:0 = min(r7:6,r9:8)
		r13:12 = min(r13:12,r9:8)
		memd(r30+##-13104) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r25:24 = add(r1:0,r3:2)
		r13:12 = add(r13:12,r3:2)
		r0 = memw(r30+##-12976)
	}                                       // 4-byte Folded Reload
	{
		r28 = asr(r0,#31)
		r27:26 = asr(r25:24,#31)
		r1 = r0
	}
	{
		r15:14 = mpyu(r22,r0)
		r13:12 = asr(r13:12,#31)
		r0 = memw(r30+##-12848)
	}                                       // 4-byte Folded Reload
	{
		r15 += mpyi(r22,r28)
		r18 = asr(r0,#31)
	}
	{
		r7:6 = mpyu(r22,r0)
		r11:10 = combine(r15,r14)
		r14 = ##2147483647
	}
	{
		r7 += mpyi(r22,r18)
		r11 += mpyi(r1,r23)
		r15 = #0
	}
	{
		r7 += mpyi(r0,r23)
		r27:26 = min(r27:26,r15:14)
		memd(r30+##-12976) = r11:10
	}                                       // 8-byte Folded Spill
	{
		r10 = ##-2147483648
		r11 = #-1
	}
	{
		r7:6 = memd(r30+##-10928)
		memd(r30+##-12848) = r7:6
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r13:12,r15:14)
		r19:18 = max(r27:26,r11:10)
	}
	{
		r27:26 = min(r7:6,r9:8)
		r6 = memw(r30+##-9008)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = add(r27:26,r3:2)
		v12.w = vinsert(r18)
		r5:4 = memd(r30+##-11568)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r9:8)
		r28 = asr(r6,#31)
	}
	{
		r25:24 = mpyu(r22,r6)
		r27:26 = add(r1:0,r3:2)
		r0 = memw(r30+##-7984)
	}                                       // 4-byte Folded Reload
	{
		r25 += mpyi(r22,r28)
		r19:18 = asr(r19:18,#31)
		v2 = valign(v12,v12,#4)
	}
	{
		r5:4 = mpyu(r22,r0)
		r28 = asr(r0,#31)
		v12 = valign(v15,v15,#4)
	}
	{
		r19:18 = min(r19:18,r15:14)
		r5 += mpyi(r22,r28)
	}
	{
		r19:18 = max(r19:18,r11:10)
		r5 += mpyi(r0,r23)
	}
	{
		r19:18 = asr(r27:26,#31)
		v2.w = vinsert(r18)
		memd(r30+#-7984) = r5:4
	}                                       // 8-byte Folded Spill
	{
		r5:4 = memd(r30+##-11056)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r9:8)
		r19:18 = min(r19:18,r15:14)
		r4 = memw(r30+##-7728)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = max(r19:18,r11:10)
		r1:0 = add(r1:0,r3:2)
		v2 = valign(v2,v2,#4)
	}
	{
		r19:18 = asr(r1:0,#31)
		v2.w = vinsert(r18)
	}
	{
		r25 += mpyi(r6,r23)
		r1:0 = add(r17:16,r3:2)
		r16 = memw(r30+##-7472)
	}                                       // 4-byte Folded Reload
	{
		r7:6 = mpyu(r22,r4)
		r28 = asr(r4,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r7 += mpyi(r22,r28)
		r19:18 = min(r19:18,r15:14)
	}
	{
		r19:18 = max(r19:18,r11:10)
		r7 += mpyi(r4,r23)
	}
	{
		r27:26 = mpyu(r22,r16)
		r28 = asr(r16,#31)
		memd(r30+#-7728) = r7:6
	}                                       // 8-byte Folded Spill
	{
		r1:0 = asr(r1:0,#31)
		v2.w = vinsert(r18)
		r7:6 = memd(r30+##-11184)
	}                                       // 8-byte Folded Reload
	{
		r27 += mpyi(r22,r28)
		r1:0 = min(r1:0,r15:14)
	}
	{
		r27 += mpyi(r16,r23)
		r17:16 = min(r7:6,r9:8)
		v2 = valign(v2,v2,#4)
	}
	{
		r7:6 = min(r21:20,r9:8)
		r1:0 = max(r1:0,r11:10)
		r20 = memw(r30+##-7216)
	}                                       // 4-byte Folded Reload
	{
		r19:18 = add(r17:16,r3:2)
		v2.w = vinsert(r0)
		r5:4 = memd(r30+##-12592)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = add(r7:6,r3:2)
		r28 = asr(r20,#31)
		r7:6 = memd(r30+##-11824)
	}                                       // 8-byte Folded Reload
	{
		r17:16 = mpyu(r22,r20)
		r1:0 = asr(r1:0,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r17 += mpyi(r22,r28)
		r1:0 = min(r1:0,r15:14)
	}
	{
		r17 += mpyi(r20,r23)
		r21:20 = min(r7:6,r9:8)
		r7:6 = memd(r30+##-10288)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = max(r1:0,r11:10)
		r21:20 = add(r21:20,r3:2)
		r28 = memw(r30+##-6960)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = min(r5:4,r9:8)
		v11.w = vinsert(r0)
		r5:4 = memd(r30+##-12080)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = asr(r21:20,#31)
		r19:18 = asr(r19:18,#31)
	}
	{
		r21:20 = min(r21:20,r15:14)
		r1:0 = add(r1:0,r3:2)
		v3 = valign(v11,v11,#4)
	}
	{
		r19:18 = min(r19:18,r15:14)
		r1:0 = asr(r1:0,#31)
	}
	{
		r21:20 = max(r21:20,r11:10)
		r1:0 = min(r1:0,r15:14)
	}
	{
		r19:18 = max(r19:18,r11:10)
		v3.w = vinsert(r20)
	}
	{
		r7:6 = min(r7:6,r9:8)
		v2.w = vinsert(r18)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r19:18 = add(r7:6,r3:2)
		v3 = valign(v3,v3,#4)
	}
	{
		r19:18 = asr(r19:18,#31)
		r1 = asr(r28,#31)
		r7:6 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = mpyu(r22,r28)
		r19:18 = min(r19:18,r15:14)
		v2 = valign(v2,v2,#4)
	}
	{
		r21 += mpyi(r22,r1)
		v3.w = vinsert(r0)
	}
	{
		r1:0 = min(r5:4,r9:8)
		r19:18 = max(r19:18,r11:10)
	}
	{
		r5:4 = min(r7:6,r9:8)
		r1:0 = add(r1:0,r3:2)
		v3 = valign(v3,v3,#4)
	}
	{
		r19:18 = add(r5:4,r3:2)
		v2.w = vinsert(r18)
		r5:4 = memd(r30+##-13104)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		r19:18 = asr(r19:18,#31)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r7:6 = min(r19:18,r15:14)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r13:12 = max(r13:12,r11:10)
	}
	{
		r1:0 = max(r7:6,r11:10)
		v3.w = vinsert(r0)
		r7:6 = memd(r30+##-12336)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		v2.w = vinsert(r0)
	}
	{
		r1:0 = min(r5:4,r9:8)
		r7:6 = add(r7:6,r3:2)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = asr(r7:6,#31)
		v3.w = vinsert(r12)
		r5:4 = memd(r30+##-9776)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r15:14)
		r1:0 = add(r1:0,r3:2)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = min(r5:4,r9:8)
		r1:0 = asr(r1:0,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = max(r7:6,r11:10)
		r13:12 = add(r5:4,r3:2)
	}
	{
		r1:0 = min(r1:0,r15:14)
		v3.w = vinsert(r6)
	}
	{
		r21 += mpyi(r28,r23)
		r13:12 = asr(r13:12,#31)
		r28 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r5:4 = min(r13:12,r15:14)
		r1:0 = max(r1:0,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = max(r5:4,r11:10)
		r1 = asr(r28,#31)
		r5:4 = memd(r30+##-12976)
	}                                       // 8-byte Folded Reload
	{
		r19:18 = mpyu(r22,r28)
		v3.w = vinsert(r0)
	}
	{
		r19 += mpyi(r22,r1)
		r1:0 = min(r5:4,r9:8)
	}
	{
		r1:0 = add(r1:0,r3:2)
		v2.w = vinsert(r6)
		r7:6 = memd(r30+##-12848)
	}                                       // 8-byte Folded Reload
	{
		r5:4 = min(r7:6,r9:8)
		r1:0 = asr(r1:0,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		r1:0 = min(r1:0,r15:14)
		r7:6 = memd(r30+##-9264)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r5:4 = add(r5:4,r3:2)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r1:0,r11:10)
		r7:6 = add(r7:6,r3:2)
	}
	{
		r5:4 = asr(r5:4,#31)
		v3.w = vinsert(r0)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r7:6 = asr(r7:6,#31)
	}
	{
		r7:6 = min(r7:6,r15:14)
		r5:4 = max(r5:4,r11:10)
		v3 = valign(v3,v3,#4)
	}
	{
		r1:0 = max(r7:6,r11:10)
		v3.w = vinsert(r4)
		r5:4 = memd(r30+#-1840)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r5:4,r9:8)
		v2.w = vinsert(r0)
	}
	{
		r5:4 = min(r25:24,r9:8)
		r7:6 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r7:6,r9:8)
		r5:4 = add(r5:4,r3:2)
		v3 = valign(v3,v3,#4)
	}
	{
		r7:6 = add(r7:6,r3:2)
		r5:4 = asr(r5:4,#31)
		v2 = valign(v2,v2,#4)
	}
	{
		r5:4 = min(r5:4,r15:14)
		r7:6 = asr(r7:6,#31)
	}
	{
		r25:24 = min(r17:16,r9:8)
		r7:6 = min(r7:6,r15:14)
		r17:16 = memd(r30+#-7728)
	}                                       // 8-byte Folded Reload
	{
		r19 += mpyi(r28,r23)
		r5:4 = max(r5:4,r11:10)
		r28 = #68
	}
	{
		r1:0 = add(r1:0,r3:2)
		v3.w = vinsert(r4)
		v4 = vror(v18,r28)
	}
	{
		r7:6 = max(r7:6,r11:10)
		r5:4 = min(r19:18,r9:8)
		v11 = vror(v16,r28)
		r19:18 = memd(r30+#-7984)
	}                                       // 8-byte Folded Reload
	{
		r7:6 = min(r21:20,r9:8)
		v2.w = vinsert(r6)
		r21:20 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r1:0,#31)
		v3 = valign(v3,v3,#4)
		r23:22 = memd(r30+##-10032)
	}                                       // 8-byte Folded Reload
	{
		r13:12 = min(r17:16,r9:8)
		r1:0 = min(r1:0,r15:14)
		r15:14 = combine(r11,r10)
		v0 = vror(v0,r28)
	}
	{
		r17:16 = min(r23:22,r9:8)
		r1:0 = max(r1:0,r11:10)
		v2 = valign(v2,v2,#4)
	}
	{
		r11:10 = min(r21:20,r9:8)
		v4 = vor(v4,v5)
		r21:20 = memd(r30+##-8240)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = min(r27:26,r9:8)
		r23:22 = memd(r30+##-8752)
		memd(r30+#-1584) = r1:0
	}                                       // 8-byte Folded Reload
	{
		r27:26 = min(r19:18,r9:8)
		r19:18 = min(r21:20,r9:8)
	}
	{
		r21:20 = min(r23:22,r9:8)
		v5 = vor(v11,v12)
		r23:22 = memd(r30+##-8496)
	}                                       // 8-byte Folded Reload
	{
		r23:22 = min(r23:22,r9:8)
		r9:8 = add(r27:26,r3:2)
		r27:26 = combine(r3,r2)
	}
	{
		r25:24 = add(r25:24,r3:2)
		r19:18 = add(r19:18,r3:2)
	}
	{
		r23:22 = add(r23:22,r3:2)
		r21:20 = add(r21:20,r3:2)
	}
	{
		r17:16 = add(r17:16,r3:2)
		r13:12 = add(r13:12,r3:2)
	}
	{
		r7:6 = add(r7:6,r3:2)
		r1:0 = add(r1:0,r3:2)
	}
	{
		r11:10 = add(r11:10,r3:2)
		r3:2 = asr(r25:24,#31)
		r24 = ##2147483647
	}
	{
		r9:8 = asr(r9:8,#31)
		r13:12 = asr(r13:12,#31)
		r25 = #0
	}
	{
		r9:8 = min(r9:8,r25:24)
		r19:18 = asr(r19:18,#31)
	}
	{
		r9:8 = max(r9:8,r15:14)
		r19:18 = min(r19:18,r25:24)
	}
	{
		r13:12 = min(r13:12,r25:24)
		v3.w = vinsert(r8)
	}
	{
		r19:18 = max(r19:18,r15:14)
		r13:12 = max(r13:12,r15:14)
	}
	{
		r1:0 = asr(r1:0,#31)
		v2.w = vinsert(r18)
		v3 = valign(v3,v3,#4)
	}
	{
		r23:22 = asr(r23:22,#31)
		v3.w = vinsert(r12)
	}
	{
		r9:8 = min(r23:22,r25:24)
		r1:0 = min(r1:0,r25:24)
		v2 = valign(v2,v2,#4)
	}
	{
		r9:8 = max(r9:8,r15:14)
		r1:0 = max(r1:0,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		v2.w = vinsert(r8)
		v3.w = vinsert(r0)
	}
	{
		r3:2 = min(r3:2,r25:24)
		r21:20 = asr(r21:20,#31)
	}
	{
		r23:22 = min(r21:20,r25:24)
		r3:2 = max(r3:2,r15:14)
		v2 = valign(v2,v2,#4)
	}
	{
		r13:12 = max(r23:22,r15:14)
		r7:6 = asr(r7:6,#31)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r2)
		v2.w = vinsert(r12)
	}
	{
		r7:6 = min(r7:6,r25:24)
		r17:16 = asr(r17:16,#31)
	}
	{
		r1:0 = min(r17:16,r25:24)
		r3:2 = add(r5:4,r27:26)
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = max(r1:0,r15:14)
		r7:6 = max(r7:6,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		v2.w = vinsert(r0)
		v3.w = vinsert(r6)
		r7:6 = memd(r30+#-1584)
	}                                       // 8-byte Folded Reload
	{
		r1:0 = asr(r3:2,#31)
		r5:4 = asr(r11:10,#31)
	}
	{
		r7:6 = min(r5:4,r25:24)
		v1.w = vinsert(r6)
		r5 = #64
		v2 = valign(v2,v2,#4)
	}
	{
		r1:0 = min(r1:0,r25:24)
		r3:2 = max(r7:6,r15:14)
		v3 = valign(v3,v3,#4)
	}
	{
		r1:0 = max(r1:0,r15:14)
		v2.w = vinsert(r2)
		v1 = valign(v1,v1,#4)
	}
	{
		v3.w = vinsert(r0)
		r0 = add(r30,#-18864)
		v0 = vor(v0,v1)
	}
	{
		r0 = add(r30,#-21424)
		v11 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1 = valign(v3,v3,#4)
		v3.w = vadd(v5.w,v11.w):sat
		v4.w = vadd(v4.w,v11.w):sat
		v0.w = vadd(v0.w,v11.w):sat
	}
	{
		v2 = vror(v2,r28)
	}
	{
		r0 = add(r30,#-20528)
		v1 = vor(v2,v1)
		v5 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v2.w = vasr(v4.w,v5.w)
	}
	{
		v3.w = vasr(v3.w,v5.w)
		v2.w = vmin(v10.w,v2.w)
	}
	{
		v0.w = vasr(v0.w,v5.w)
		v3.w = vmin(v3.w,v10.w)
		v2.w = vmax(v9.w,v2.w)
	}
	{
		v1.w = vasr(v1.w,v5.w)
		v0.w = vmin(v10.w,v0.w)
		v3.w = vmax(v9.w,v3.w)
	}
	{
		v1.w = vmin(v1.w,v10.w)
		v0.w = vmax(v9.w,v0.w)
	}
	{
		v2.h = vpacke(v2.w,v3.w)
		v1.w = vmax(v9.w,v1.w)
	}
	{
		r0 = add(r30,#-18992)
		v3 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.h = vpacke(v0.w,v1.w)
		v2.h = vadd(v3.h,v2.h):sat
	}
	{
		v0.h = vadd(v3.h,v0.h):sat
		v4 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		v3:2.w = vsub(v3:2.w,v3:2.w)
		v1.h = vmin(v2.h,v4.h)
		v0.h = vmin(v4.h,v0.h)
		r0 = memw(r30+#-816)
	}                                       // 4-byte Folded Reload
	{
		v1.h = vmax(v2.h,v1.h)
		v0.h = vmax(v3.h,v0.h)
		vmem(r0+#0) = v1.new
	}
	{
		v1 = vmux(q3,v7,v6)
		r4 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		r1:0 = memd(r4+#248)
		vmem(r4+#0) = v0
	}
	{
		memd(r30+#-816) = r1:0
		r1:0 = memd(r4+#240)

	} :mem_noshuf
	{
		memd(r30+#-1584) = r1:0
		r1:0 = memd(r4+#232)

	} :mem_noshuf
	{
		memd(r30+#-1840) = r1:0
		r1:0 = memd(r4+#224)

	} :mem_noshuf
	{
		memd(r30+#-3376) = r1:0
		r1:0 = memd(r4+#216)

	} :mem_noshuf
	{
		memd(r30+#-6960) = r1:0
		r1:0 = memd(r4+#208)

	} :mem_noshuf
	{
		memd(r30+#-7216) = r1:0
		r1:0 = memd(r4+#200)

	} :mem_noshuf
	{
		memd(r30+#-7472) = r1:0
		r1:0 = memd(r4+#192)

	} :mem_noshuf
	{
		memd(r30+#-7728) = r1:0
		r1:0 = memd(r4+#184)

	} :mem_noshuf
	{
		memd(r30+#-7984) = r1:0
		r1:0 = memd(r4+#176)

	} :mem_noshuf
	{
		r11 = memw(r4+#124)
		memd(r30+##-8240) = r1:0

	} :mem_noshuf
	{
		r1:0 = memd(r4+#168)
	}
	{
		memd(r30+##-8496) = r1:0
		r1:0 = memd(r4+#160)

	} :mem_noshuf
	{
		r10 = memw(r4+#120)
		memd(r30+##-8752) = r1:0

	} :mem_noshuf
	{
		r1:0 = memd(r4+#152)
	}
	{
		memd(r30+##-9008) = r1:0
		r1:0 = memd(r4+#144)

	} :mem_noshuf
	{
		memd(r30+##-9264) = r1:0
		r1:0 = memd(r4+#136)

	} :mem_noshuf
	{
		memd(r30+##-9520) = r1:0
		r1:0 = memd(r4+#128)

	} :mem_noshuf
	{
		memd(r30+##-9776) = r1:0
		r1:0 = memd(r4+#112)

	} :mem_noshuf
	{
		memd(r30+##-10032) = r1:0
		r1:0 = memd(r4+#104)

	} :mem_noshuf
	{
		memd(r30+##-10288) = r1:0
		r1:0 = memd(r4+#96)

	} :mem_noshuf
	{
		memd(r30+##-10800) = r1:0
		r13:12 = memd(r4+#88)

	} :mem_noshuf
	{
		r3:2 = memd(r4+#80)
		r7:6 = memd(r4+#72)
	}
	{
		r9:8 = memd(r4+#64)
		r1:0 = memd(r4+#0)
	}
	{
		r15:14 = memd(r4+#8)
		r19:18 = memd(r4+#16)
	}
	{
		r21:20 = memd(r4+#24)
		r23:22 = memd(r4+#32)
	}
	{
		r25:24 = memd(r4+#48)
		r27:26 = memd(r4+#56)
	}
	{
		r17:16 = memd(r4+#40)
		r4 = memw(r30+#-560)
	}
	{
		r26 = #-1
		memd(r4+#56) = r27:26
		memd(r4+#48) = r25:24
	}
	{
		memd(r4+#40) = r17:16
		memd(r4+#32) = r23:22
	}
	{
		memd(r4+#24) = r21:20
		memd(r4+#16) = r19:18
	}
	{
		memd(r4+#8) = r15:14
		memd(r4+#0) = r1:0
	}
	{
		r4 = memw(r30+#-304)
		v0 = vmem(r4+#0)
	}
	{
		vmem(r4+#0) = v0
	}
	{
		r0 = memw(r4+#56)
		r1 = memw(r4+#60)
	}
	{
		r0 = vtrunehb(r1:0)
		r14 = memw(r4+#48)
		r15 = memw(r4+#52)
	}
	{
		r14 = vtrunehb(r15:14)
		r16 = memw(r4+#40)
		r17 = memw(r4+#44)
	}
	{
		r18 = memw(r4+#32)
		r19 = memw(r4+#36)
	}
	{
		r20 = memw(r4+#24)
		r21 = memw(r4+#28)
	}
	{
		r22 = memw(r4+#16)
		r23 = memw(r4+#20)
	}
	{
		r24 = memw(r4+#0)
		r25 = memw(r4+#4)
	}
	{
		r28 = vtrunehb(r25:24)
		r24 = memw(r4+#8)
	}
	{
		v8.w = vinsert(r28)
		r25 = memw(r4+#12)
	}
	{
		r28 = vtrunehb(r25:24)
	}
	{
		v0 = valign(v8,v8,#4)
	}
	{
		r28 = vtrunehb(r23:22)
		v0.w = vinsert(r28)
		v8 = vror(v6,r5)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r28 = vtrunehb(r21:20)
		v0.w = vinsert(r28)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r28 = vtrunehb(r19:18)
		v0.w = vinsert(r28)
		r19 = ##16843009
	}
	{
		v1.uw = vrmpy(v1.ub,r19.ub)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r28 = vtrunehb(r17:16)
		v0.w = vinsert(r28)
		v2 = valign(v1,v1,#4)
	}
	{
		v1 = vor(v1,v2)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r28)
		r28 = #0
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r14)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r0)
		r1:0 = combine(#73,#100)
	}
	{
		r0 = add(r30,#-20656)
		v0 = vror(v0,r0)
	}
	{
		v0 = vor(v0,v8)
		v4 = vmemu(r0+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r30,#-5168)
		v3:2 = vdeal(v0,v1,r1)
		v0.ub = vmin(v0.ub,v4.ub)
	}
	{
		r0 = add(r0,##-41856)
		v1 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r1 = #18
		v0.ub = vmax(v0.ub,v1.ub)
		vmem(r0+#0) = v0.new
	}
	{
		r1 = #36
		v31:30 = vdeal(v3,v2,r1)
		r22 = memw(r0+#24)
		r23 = memw(r0+#28)
	}
	{
		v1:0 = vdeal(v31,v30,r1)
		r20 = memw(r0+#16)
		r21 = memw(r0+#20)
	}
	{
		r24 = memw(r0+#8)
		r25 = memw(r0+#12)
	}
	{
		r0 = #-1
		v1:0 = vshuff(v1,v0,r5)
		r14 = memw(r0+#0)
		r15 = memw(r0+#4)
	}
	{
		r1 = vextract(v0,r28)
	}
	{
		p1 = bitsclr(r1,#2)
		v1 = vand(q3,r0)
		r0 = memw(r30+##-19000)
	}                                       // 4-byte Folded Reload
	{
		r28 = vextract(v1,r28)
	}
	{
		p2 = bitsclr(r1,#4)
		r16 = memw(r30+##-21264)
	}                                       // 4-byte Folded Reload
	{
		r28 = and(r28,#255)
		r17 = memw(r30+##-6448)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.gtu(r28,#0)
		r28 = memw(r30+##-21048)
	}                                       // 4-byte Folded Reload
	{
		r0 += mpyi(r17,r16)
		r17 = add(r30,#-5424)
		r16 = ##16843009
	}
	{
		r0 = sub(r0,r28)
		r28 = memw(r30+##-7224)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r28,r0)
		v31 = vmemu(r17+#0)
	}                                       // 256-byte Folded Reload
	{
		r17:16 = extractu(r15:14,#8,#8)
		q0 = vand(v31,r16)
		if (p0) memb(r0+#0) = r14
	}
	{
		r17:16 = extractu(r15:14,#8,#16)
		p0 = bitsclr(r1,#8)
		if (!p1) memb(r0+#1) = r16
	}
	{
		if (p0) jump:nt .LBB131_652
		if (!p2) memb(r0+#2) = r16
	}
// %bb.651:                             // %cond.store810
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r17:16 = extractu(r15:14,#8,#24)
	}
	{
		memb(r0+#3) = r16
	}
.LBB131_652:                            // %else811
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r17:16 = extractu(r15:14,#8,#32)
		p1 = bitsclr(r1,#16)
	}
	{
		r17:16 = extractu(r15:14,#8,#40)
		p2 = bitsclr(r1,#32)
		if (!p1) memb(r0+#4) = r16
	}
	{
		r17:16 = extractu(r15:14,#8,#48)
		p0 = !tstbit(r1,#7)
		if (!p2) memb(r0+#5) = r16
	}
	{
		p1 = !tstbit(r1,#6)
		if (p0) jump:nt .LBB131_654
		if (!p1.new) memb(r0+#6) = r16
	}
// %bb.653:                             // %cond.store818
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r15:14,#8,#56)
	}
	{
		memb(r0+#7) = r14
	}
.LBB131_654:                            // %else819
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r25:24,#8,#8)
		p0 = !tstbit(r1,#8)
	}
	{
		p1 = !tstbit(r1,#9)
		p0 = !tstbit(r1,#11)
		if (!p0) memb(r0+#8) = r24
		if (!p1.new) memb(r0+#9) = r14
	}
	{
		r15:14 = extractu(r25:24,#8,#16)
		p2 = !tstbit(r1,#10)
	}
	{
		if (p0) jump:nt .LBB131_656
		if (!p2) memb(r0+#10) = r14
	}
// %bb.655:                             // %cond.store826
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r25:24,#8,#24)
	}
	{
		memb(r0+#11) = r14
	}
.LBB131_656:                            // %else827
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r25:24,#8,#32)
		p1 = !tstbit(r1,#12)
	}
	{
		r15:14 = extractu(r25:24,#8,#40)
		p2 = !tstbit(r1,#13)
		if (!p1) memb(r0+#12) = r14
	}
	{
		p0 = !tstbit(r1,#15)
		p1 = !tstbit(r1,#14)
		if (!p2) memb(r0+#13) = r14
	}
	{
		r15:14 = extractu(r25:24,#8,#48)
	}
	{
		if (p0) jump:nt .LBB131_658
		if (!p1) memb(r0+#14) = r14
	}
// %bb.657:                             // %cond.store834
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r25:24,#8,#56)
	}
	{
		memb(r0+#15) = r14
	}
.LBB131_658:                            // %else835
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r21:20,#8,#8)
		p0 = !tstbit(r1,#16)
	}
	{
		p1 = !tstbit(r1,#17)
		p0 = !tstbit(r1,#19)
		if (!p0) memb(r0+#16) = r20
		if (!p1.new) memb(r0+#17) = r14
	}
	{
		r15:14 = extractu(r21:20,#8,#16)
		p2 = !tstbit(r1,#18)
	}
	{
		if (p0) jump:nt .LBB131_660
		if (!p2) memb(r0+#18) = r14
	}
// %bb.659:                             // %cond.store842
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r21:20,#8,#24)
	}
	{
		memb(r0+#19) = r14
	}
.LBB131_660:                            // %else843
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r21:20,#8,#32)
		p1 = !tstbit(r1,#20)
	}
	{
		r15:14 = extractu(r21:20,#8,#40)
		p2 = !tstbit(r1,#21)
		if (!p1) memb(r0+#20) = r14
	}
	{
		p0 = !tstbit(r1,#23)
		p1 = !tstbit(r1,#22)
		if (!p2) memb(r0+#21) = r14
	}
	{
		r15:14 = extractu(r21:20,#8,#48)
	}
	{
		if (p0) jump:nt .LBB131_662
		if (!p1) memb(r0+#22) = r14
	}
// %bb.661:                             // %cond.store850
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r21:20,#8,#56)
	}
	{
		memb(r0+#23) = r14
	}
.LBB131_662:                            // %else851
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r23:22,#8,#8)
		p0 = !tstbit(r1,#24)
	}
	{
		p1 = !tstbit(r1,#25)
		p0 = !tstbit(r1,#27)
		if (!p0) memb(r0+#24) = r22
		if (!p1.new) memb(r0+#25) = r14
	}
	{
		r15:14 = extractu(r23:22,#8,#16)
		p2 = !tstbit(r1,#26)
	}
	{
		if (p0) jump:nt .LBB131_664
		if (!p2) memb(r0+#26) = r14
	}
// %bb.663:                             // %cond.store858
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r23:22,#8,#24)
	}
	{
		memb(r0+#27) = r14
	}
.LBB131_664:                            // %else859
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r23:22,#8,#32)
		p1 = !tstbit(r1,#28)
		r18 = memw(r30+##-19776)
	}                                       // 4-byte Folded Reload
	{
		r15:14 = extractu(r23:22,#8,#40)
		p2 = !tstbit(r1,#29)
		r20 = #0
		if (!p1) memb(r0+#28) = r14
	}
	{
		p0 = !tstbit(r1,#31)
		r21 = memw(r30+##-21040)
		if (!p2) memb(r0+#29) = r14
	}                                       // 4-byte Folded Reload
	{
		r15:14 = extractu(r23:22,#8,#48)
		p1 = !tstbit(r1,#30)
	}
	{
		if (p0) jump:nt .LBB131_666
		if (!p1) memb(r0+#30) = r14
	}
// %bb.665:                             // %cond.store866
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r23:22,#8,#56)
	}
	{
		memb(r0+#31) = r14
	}
.LBB131_666:                            // %else867
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		v1 = v6
		v0 = vmux(q0,v7,v6)
		r15 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r15,#-32384)
		r14 = add(r15,##-41984)
	}
	{
		v0.uw = vrmpy(v0.ub,r19.ub)
		memd(r1+#0) = r9:8
		memd(r1+#8) = r7:6
	}
	{
		memd(r1+#16) = r3:2
		memd(r1+#24) = r13:12
	}
	{
		r3:2 = memd(r30+##-10800)
	}                                       // 8-byte Folded Reload
	{
		memd(r1+#32) = r3:2
		r3:2 = memd(r30+##-10288)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#40) = r3:2
		r3:2 = memd(r30+##-10032)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r1+#56) = r11:10
		memd(r1+#48) = r3:2
	}
	{
		v2.cur = vmem(r1+#0)
		vmem(r14+#0) = v2
	}
	{
		v2 = valign(v0,v0,#4)
		r2 = memw(r14+#56)
	}
	{
		v0 = vor(v0,v2)
		r3 = memw(r14+#60)
		r4 = memw(r14+#48)
	}
	{
		r5 = memw(r14+#52)
		r6 = memw(r14+#40)
	}
	{
		r7 = memw(r14+#44)
		r8 = memw(r14+#32)
	}
	{
		r9 = memw(r14+#36)
		r12 = memw(r14+#24)
	}
	{
		r13 = memw(r14+#28)
		r10 = memw(r14+#16)
	}
	{
		r11 = memw(r14+#20)
		r16 = memw(r14+#0)
	}
	{
		r17 = memw(r14+#4)
	}
	{
		r1 = vtrunehb(r17:16)
		r16 = memw(r14+#8)
	}
	{
		v1.w = vinsert(r1)
		r17 = memw(r14+#12)
	}
	{
		r1 = vtrunehb(r17:16)
		r17 = #100
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r11:10)
		v1.w = vinsert(r1)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r13:12)
		v1.w = vinsert(r1)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r9:8)
		v1.w = vinsert(r1)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r7:6)
		v1.w = vinsert(r1)
		r7 = #73
	}
	{
		r7 = #18
		v3:2 = vdeal(v0,v0,r7)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r5:4)
		v1.w = vinsert(r1)
		r7 = #36
		v3:2 = vdeal(v3,v2,r7)
	}
	{
		r5 = #64
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		r1 = vtrunehb(r3:2)
		v1.w = vinsert(r1)
		r2 = add(r30,#-5168)
		v31:30 = vdeal(v3,v2,r7)
	}
	{
		v1 = valign(v1,v1,#4)
	}
	{
		v1.w = vinsert(r1)
		r1 = #100
	}
	{
		r1 = add(r15,##-42112)
		v1 = vror(v1,r1)
	}
	{
		v0 = vor(v1,v8)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v0.ub = vmin(v0.ub,v4.ub)
	}
	{
		v0.ub = vmax(v0.ub,v1.ub)
		vmem(r1+#0) = v0.new
	}
	{
		v1:0 = vshuff(v31,v30,r5)
		r2 = memw(r1+#24)
		r3 = memw(r1+#28)
	}
	{
		v1 = vand(q0,r26)
		r6 = memw(r1+#16)
		r7 = memw(r1+#20)
	}
	{
		r8 = memw(r1+#8)
		r9 = memw(r1+#12)
	}
	{
		r12 = memw(r1+#0)
		r13 = memw(r1+#4)
	}
	{
		r4 = vextract(v0,r20)
	}
	{
		r1 = vextract(v1,r20)
	}
	{
		p1 = bitsclr(r4,#2)
		p2 = bitsclr(r4,#4)
		r5 = and(r1,#255)
		r1 = add(r0,#32)
	}
	{
		r15:14 = extractu(r13:12,#8,#8)
		p0 = cmp.gtu(r5,#0)
	}
	{
		if (p2) jump:nt .LBB131_668
		if (!p1) memb(r1+#1) = r14
		if (p0) memb(r1+#0) = r12
	}
// %bb.667:                             // %cond.store873
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#16)
	}
	{
		memb(r1+#2) = r14
	}
.LBB131_668:                            // %else874
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#24)
		p1 = bitsclr(r4,#8)
	}
	{
		r15:14 = extractu(r13:12,#8,#32)
		p2 = bitsclr(r4,#16)
		if (!p1) memb(r1+#3) = r14
	}
	{
		r15:14 = extractu(r13:12,#8,#40)
		p0 = !tstbit(r4,#6)
		if (!p2) memb(r1+#4) = r14
	}
	{
		p1 = bitsclr(r4,#32)
		if (p0) jump:nt .LBB131_670
		if (!p1.new) memb(r1+#5) = r14
	}
// %bb.669:                             // %cond.store881
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#48)
	}
	{
		memb(r1+#6) = r14
	}
.LBB131_670:                            // %else882
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r13:12,#8,#56)
		p1 = !tstbit(r4,#8)
	}
	{
		p0 = !tstbit(r4,#7)
		p1 = !tstbit(r4,#10)
		if (!p1) memb(r1+#8) = r8
		if (!p0.new) memb(r1+#7) = r12
	}
	{
		r13:12 = extractu(r9:8,#8,#8)
		p2 = !tstbit(r4,#9)
	}
	{
		if (p1) jump:nt .LBB131_672
		if (!p2) memb(r1+#9) = r12
	}
// %bb.671:                             // %cond.store889
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#16)
	}
	{
		memb(r1+#10) = r12
	}
.LBB131_672:                            // %else890
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#24)
		p1 = !tstbit(r4,#11)
	}
	{
		r13:12 = extractu(r9:8,#8,#32)
		p2 = !tstbit(r4,#12)
		if (!p1) memb(r1+#11) = r12
	}
	{
		r13:12 = extractu(r9:8,#8,#40)
		p0 = !tstbit(r4,#14)
		if (!p2) memb(r1+#12) = r12
	}
	{
		p1 = !tstbit(r4,#13)
		if (p0) jump:nt .LBB131_674
		if (!p1.new) memb(r1+#13) = r12
	}
// %bb.673:                             // %cond.store897
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#48)
	}
	{
		memb(r1+#14) = r12
	}
.LBB131_674:                            // %else898
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r9:8,#8,#56)
		p1 = !tstbit(r4,#16)
	}
	{
		p0 = !tstbit(r4,#15)
		p1 = !tstbit(r4,#18)
		if (!p1) memb(r1+#16) = r6
		if (!p0.new) memb(r1+#15) = r8
	}
	{
		r9:8 = extractu(r7:6,#8,#8)
		p2 = !tstbit(r4,#17)
	}
	{
		if (p1) jump:nt .LBB131_676
		if (!p2) memb(r1+#17) = r8
	}
// %bb.675:                             // %cond.store905
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#16)
	}
	{
		memb(r1+#18) = r8
	}
.LBB131_676:                            // %else906
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#24)
		p1 = !tstbit(r4,#19)
	}
	{
		r9:8 = extractu(r7:6,#8,#32)
		p2 = !tstbit(r4,#20)
		if (!p1) memb(r1+#19) = r8
	}
	{
		r9:8 = extractu(r7:6,#8,#40)
		p0 = !tstbit(r4,#22)
		if (!p2) memb(r1+#20) = r8
	}
	{
		p1 = !tstbit(r4,#21)
		if (p0) jump:nt .LBB131_678
		if (!p1.new) memb(r1+#21) = r8
	}
// %bb.677:                             // %cond.store913
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#48)
	}
	{
		memb(r1+#22) = r8
	}
.LBB131_678:                            // %else914
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r7:6,#8,#56)
		p1 = !tstbit(r4,#24)
	}
	{
		p0 = !tstbit(r4,#23)
		p1 = !tstbit(r4,#26)
		if (!p1) memb(r1+#24) = r2
		if (!p0.new) memb(r1+#23) = r6
	}
	{
		r7:6 = extractu(r3:2,#8,#8)
		p2 = !tstbit(r4,#25)
	}
	{
		if (p1) jump:nt .LBB131_680
		if (!p2) memb(r1+#25) = r6
	}
// %bb.679:                             // %cond.store921
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#16)
	}
	{
		memb(r1+#26) = r6
	}
.LBB131_680:                            // %else922
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#24)
		p1 = !tstbit(r4,#27)
	}
	{
		r7:6 = extractu(r3:2,#8,#32)
		p2 = !tstbit(r4,#28)
		if (!p1) memb(r1+#27) = r6
	}
	{
		r7:6 = extractu(r3:2,#8,#40)
		p0 = !tstbit(r4,#30)
		if (!p2) memb(r1+#28) = r6
	}
	{
		p1 = !tstbit(r4,#29)
		if (p0) jump:nt .LBB131_682
		if (!p1.new) memb(r1+#29) = r6
	}
// %bb.681:                             // %cond.store929
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#48)
	}
	{
		memb(r1+#30) = r6
	}
.LBB131_682:                            // %else930
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r3:2 = extractu(r3:2,#8,#56)
		p0 = !tstbit(r4,#31)
		r16 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r3 = add(r16,#-32512)
		r5:4 = memd(r30+##-9776)
		if (!p0) memb(r1+#31) = r2
	}                                       // 8-byte Folded Reload
	{
		r1 = add(r16,##-42240)
		v1 = vmux(q2,v7,v6)
		memd(r3+#0) = r5:4
	}
	{
		r5:4 = memd(r30+##-9520)
	}                                       // 8-byte Folded Reload
	{
		v1.uw = vrmpy(v1.ub,r19.ub)
		memd(r3+#8) = r5:4
		r5:4 = memd(r30+##-9264)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		v2 = valign(v1,v1,#4)
		memd(r3+#16) = r5:4
	}
	{
		v1 = vor(v1,v2)
		r5:4 = memd(r30+##-9008)
	}                                       // 8-byte Folded Reload
	{
		memd(r3+#24) = r5:4
		r5:4 = memd(r30+##-8752)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#32) = r5:4
		r5:4 = memd(r30+##-8496)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#40) = r5:4
		r5:4 = memd(r30+##-8240)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#48) = r5:4
		r5:4 = memd(r30+#-7984)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r3+#56) = r5:4
	}
	{
		v0.cur = vmem(r3+#0)
		vmem(r1+#0) = v0
	}
	{
		v0 = v6
		r2 = memw(r1+#56)
		r3 = memw(r1+#60)
	}
	{
		r4 = memw(r1+#48)
		r5 = memw(r1+#52)
	}
	{
		r6 = memw(r1+#40)
		r7 = memw(r1+#44)
	}
	{
		r8 = memw(r1+#32)
		r9 = memw(r1+#36)
	}
	{
		r12 = memw(r1+#24)
		r13 = memw(r1+#28)
	}
	{
		r14 = memw(r1+#16)
		r15 = memw(r1+#20)
	}
	{
		r10 = memw(r1+#0)
		r11 = memw(r1+#4)
	}
	{
		r28 = vtrunehb(r11:10)
		r10 = memw(r1+#8)
	}
	{
		v0.w = vinsert(r28)
		r11 = memw(r1+#12)
	}
	{
		r1 = vtrunehb(r11:10)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r15:14)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r13:12)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r9:8)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r7:6)
		v0.w = vinsert(r1)
		r7 = #73
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r5:4)
		v0.w = vinsert(r1)
		r5 = #64
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r3:2)
		v0.w = vinsert(r1)
		r2 = add(r30,#-5168)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r1)
		r1 = add(r16,##-42368)
	}
	{
		v0 = vror(v0,r17)
	}
	{
		r7 = #18
		v3:2 = vdeal(v0,v1,r7)
		v0 = vor(v0,v8)
	}
	{
		v0.ub = vmin(v0.ub,v4.ub)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r7 = #36
		v3:2 = vdeal(v3,v2,r7)
		v0.ub = vmax(v0.ub,v1.ub)
		vmem(r1+#0) = v0.new
	}
	{
		r2 = memw(r1+#24)
	}
	{
		v31:30 = vdeal(v3,v2,r7)
		r3 = memw(r1+#28)
		r6 = memw(r1+#16)
	}
	{
		r7 = memw(r1+#20)
		r8 = memw(r1+#8)
	}
	{
		v1:0 = vshuff(v31,v30,r5)
		r9 = memw(r1+#12)
		r12 = memw(r1+#0)
	}
	{
		v1 = vand(q2,r26)
		r13 = memw(r1+#4)
	}
	{
		r4 = vextract(v0,r20)
	}
	{
		r1 = vextract(v1,r20)
	}
	{
		p1 = bitsclr(r4,#2)
		r1 = and(r1,#255)
	}
	{
		p0 = cmp.gtu(r1,#0)
		r1 = add(r0,#64)
	}
	{
		if (p1) jump:nt .LBB131_684
		if (p0) memb(r1+#0) = r12
	}
// %bb.683:                             // %cond.store936
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#8)
	}
	{
		memb(r1+#1) = r14
	}
.LBB131_684:                            // %else937
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#16)
		p1 = bitsclr(r4,#4)
	}
	{
		r15:14 = extractu(r13:12,#8,#24)
		p2 = bitsclr(r4,#8)
		if (!p1) memb(r1+#2) = r14
	}
	{
		p0 = bitsclr(r4,#32)
		p1 = bitsclr(r4,#16)
		if (!p2) memb(r1+#3) = r14
	}
	{
		r15:14 = extractu(r13:12,#8,#32)
	}
	{
		if (p0) jump:nt .LBB131_686
		if (!p1) memb(r1+#4) = r14
	}
// %bb.685:                             // %cond.store944
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#40)
	}
	{
		memb(r1+#5) = r14
	}
.LBB131_686:                            // %else945
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r15:14 = extractu(r13:12,#8,#48)
		p0 = !tstbit(r4,#8)
	}
	{
		p1 = !tstbit(r4,#6)
		p0 = !tstbit(r4,#9)
		if (!p0) memb(r1+#8) = r8
	}
	{
		r13:12 = extractu(r13:12,#8,#56)
		p2 = !tstbit(r4,#7)
		if (!p1) memb(r1+#6) = r14
	}
	{
		if (p0) jump:nt .LBB131_688
		if (!p2) memb(r1+#7) = r12
	}
// %bb.687:                             // %cond.store952
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#8)
	}
	{
		memb(r1+#9) = r12
	}
.LBB131_688:                            // %else953
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#16)
		p1 = !tstbit(r4,#10)
	}
	{
		r13:12 = extractu(r9:8,#8,#24)
		p2 = !tstbit(r4,#11)
		if (!p1) memb(r1+#10) = r12
	}
	{
		p0 = !tstbit(r4,#13)
		p1 = !tstbit(r4,#12)
		if (!p2) memb(r1+#11) = r12
	}
	{
		r13:12 = extractu(r9:8,#8,#32)
	}
	{
		if (p0) jump:nt .LBB131_690
		if (!p1) memb(r1+#12) = r12
	}
// %bb.689:                             // %cond.store960
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#40)
	}
	{
		memb(r1+#13) = r12
	}
.LBB131_690:                            // %else961
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#48)
		p0 = !tstbit(r4,#16)
	}
	{
		p1 = !tstbit(r4,#14)
		p0 = !tstbit(r4,#17)
		if (!p0) memb(r1+#16) = r6
	}
	{
		r9:8 = extractu(r9:8,#8,#56)
		p2 = !tstbit(r4,#15)
		if (!p1) memb(r1+#14) = r12
	}
	{
		if (p0) jump:nt .LBB131_692
		if (!p2) memb(r1+#15) = r8
	}
// %bb.691:                             // %cond.store968
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#8)
	}
	{
		memb(r1+#17) = r8
	}
.LBB131_692:                            // %else969
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#16)
		p1 = !tstbit(r4,#18)
	}
	{
		r9:8 = extractu(r7:6,#8,#24)
		p2 = !tstbit(r4,#19)
		if (!p1) memb(r1+#18) = r8
	}
	{
		p0 = !tstbit(r4,#21)
		p1 = !tstbit(r4,#20)
		if (!p2) memb(r1+#19) = r8
	}
	{
		r9:8 = extractu(r7:6,#8,#32)
	}
	{
		if (p0) jump:nt .LBB131_694
		if (!p1) memb(r1+#20) = r8
	}
// %bb.693:                             // %cond.store976
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#40)
	}
	{
		memb(r1+#21) = r8
	}
.LBB131_694:                            // %else977
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#48)
		p0 = !tstbit(r4,#24)
	}
	{
		p1 = !tstbit(r4,#22)
		p0 = !tstbit(r4,#25)
		if (!p0) memb(r1+#24) = r2
	}
	{
		r7:6 = extractu(r7:6,#8,#56)
		p2 = !tstbit(r4,#23)
		if (!p1) memb(r1+#22) = r8
	}
	{
		if (p0) jump:nt .LBB131_696
		if (!p2) memb(r1+#23) = r6
	}
// %bb.695:                             // %cond.store984
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#8)
	}
	{
		memb(r1+#25) = r6
	}
.LBB131_696:                            // %else985
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#16)
		p1 = !tstbit(r4,#26)
	}
	{
		r7:6 = extractu(r3:2,#8,#24)
		p2 = !tstbit(r4,#27)
		if (!p1) memb(r1+#26) = r6
	}
	{
		p0 = !tstbit(r4,#29)
		p1 = !tstbit(r4,#28)
		if (!p2) memb(r1+#27) = r6
	}
	{
		r7:6 = extractu(r3:2,#8,#32)
	}
	{
		if (p0) jump:nt .LBB131_698
		if (!p1) memb(r1+#28) = r6
	}
// %bb.697:                             // %cond.store992
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#40)
	}
	{
		memb(r1+#29) = r6
	}
.LBB131_698:                            // %else993
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r3:2,#8,#48)
		p1 = !tstbit(r4,#31)
		v1 = vmux(q1,v7,v6)
		r16 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r3:2 = extractu(r3:2,#8,#56)
		p0 = !tstbit(r4,#30)
		r4 = add(r16,#-32640)
		r0 = add(r0,#96)
	}
	{
		v1.uw = vrmpy(v1.ub,r19.ub)
		r3:2 = memd(r30+#-7728)
		if (!p1) memb(r1+#31) = r2
	}                                       // 8-byte Folded Reload
	{
		r1 = add(r16,##-42496)
		if (!p0) memb(r1+#30) = r6
		memd(r4+#0) = r3:2
	}
	{
		v2 = valign(v1,v1,#4)
		r3:2 = memd(r30+#-7472)
	}                                       // 8-byte Folded Reload
	{
		v1 = vor(v1,v2)
		memd(r4+#8) = r3:2
		r3:2 = memd(r30+#-7216)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#16) = r3:2
		r3:2 = memd(r30+#-6960)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#24) = r3:2
		r3:2 = memd(r30+#-3376)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#32) = r3:2
		r3:2 = memd(r30+#-1840)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#40) = r3:2
		r3:2 = memd(r30+#-1584)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#48) = r3:2
		r3:2 = memd(r30+#-816)

	} :mem_noshuf                           // 8-byte Folded Reload
	{
		memd(r4+#56) = r3:2
	}
	{
		v0.cur = vmem(r4+#0)
		vmem(r1+#0) = v0
	}
	{
		v0 = v6
		r2 = memw(r1+#56)
		r3 = memw(r1+#60)
	}
	{
		r4 = memw(r1+#48)
		r5 = memw(r1+#52)
	}
	{
		r6 = memw(r1+#40)
		r7 = memw(r1+#44)
	}
	{
		r8 = memw(r1+#32)
		r9 = memw(r1+#36)
	}
	{
		r12 = memw(r1+#24)
		r13 = memw(r1+#28)
	}
	{
		r14 = memw(r1+#16)
		r15 = memw(r1+#20)
	}
	{
		r10 = memw(r1+#0)
		r11 = memw(r1+#4)
	}
	{
		r28 = vtrunehb(r11:10)
		r10 = memw(r1+#8)
	}
	{
		v0.w = vinsert(r28)
		r11 = memw(r1+#12)
	}
	{
		r1 = vtrunehb(r11:10)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r15:14)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r13:12)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r9:8)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r7:6)
		v0.w = vinsert(r1)
		r7 = #73
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r5:4)
		v0.w = vinsert(r1)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		r1 = vtrunehb(r3:2)
		v0.w = vinsert(r1)
		r2 = add(r30,#-5168)
	}
	{
		v0 = valign(v0,v0,#4)
	}
	{
		v0.w = vinsert(r1)
		r1 = add(r16,##-42624)
	}
	{
		v0 = vror(v0,r17)
	}
	{
		r7 = #18
		v3:2 = vdeal(v0,v1,r7)
		v0 = vor(v0,v8)
	}
	{
		v0.ub = vmin(v0.ub,v4.ub)
		v1 = vmemu(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		r7 = #36
		v3:2 = vdeal(v3,v2,r7)
		v0.ub = vmax(v0.ub,v1.ub)
		vmem(r1+#0) = v0.new
	}
	{
		r2 = memw(r1+#24)
	}
	{
		r7 = #64
		v1:0 = vdeal(v3,v2,r7)
		r3 = memw(r1+#28)
		r4 = memw(r1+#16)
	}
	{
		r5 = memw(r1+#20)
		r6 = memw(r1+#8)
	}
	{
		v1:0 = vshuff(v1,v0,r7)
		r7 = memw(r1+#12)
		r8 = memw(r1+#0)
	}
	{
		v1 = vand(q1,r26)
		r9 = memw(r1+#4)
	}
	{
		r1 = vextract(v0,r20)
	}
	{
		r12 = vextract(v1,r20)
	}
	{
		p1 = bitsclr(r1,#2)
		p2 = bitsclr(r1,#4)
		r12 = and(r12,#255)
	}
	{
		r13:12 = extractu(r9:8,#8,#8)
		p0 = cmp.gtu(r12,#0)
	}
	{
		r13:12 = extractu(r9:8,#8,#16)
		p0 = bitsclr(r1,#16)
		if (!p1) memb(r0+#1) = r12
		if (p0) memb(r0+#0) = r8
	}
	{
		r13:12 = extractu(r9:8,#8,#24)
		p1 = bitsclr(r1,#8)
		if (!p2) memb(r0+#2) = r12
	}
	{
		if (p0) jump:nt .LBB131_700
		if (!p1) memb(r0+#3) = r12
	}
// %bb.699:                             // %cond.store1007
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#32)
	}
	{
		memb(r0+#4) = r12
	}
.LBB131_700:                            // %else1008
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r13:12 = extractu(r9:8,#8,#40)
		p1 = bitsclr(r1,#32)
	}
	{
		r13:12 = extractu(r9:8,#8,#48)
		p1 = !tstbit(r1,#7)
		if (!p1) memb(r0+#5) = r12
	}
	{
		r9:8 = extractu(r9:8,#8,#56)
		p2 = !tstbit(r1,#6)
	}
	{
		r9:8 = extractu(r7:6,#8,#8)
		p1 = !tstbit(r1,#9)
		if (!p1) memb(r0+#7) = r8
		if (!p2) memb(r0+#6) = r12
	}
	{
		p0 = !tstbit(r1,#8)
		p2 = !tstbit(r1,#10)
		if (!p1) memb(r0+#9) = r8
	}
	{
		r9:8 = extractu(r7:6,#8,#16)
		p0 = !tstbit(r1,#12)
		if (!p0) memb(r0+#8) = r6
	}
	{
		r9:8 = extractu(r7:6,#8,#24)
		p1 = !tstbit(r1,#11)
		if (!p2) memb(r0+#10) = r8
	}
	{
		if (p0) jump:nt .LBB131_702
		if (!p1) memb(r0+#11) = r8
	}
// %bb.701:                             // %cond.store1023
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#32)
	}
	{
		memb(r0+#12) = r8
	}
.LBB131_702:                            // %else1024
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r9:8 = extractu(r7:6,#8,#40)
		p1 = !tstbit(r1,#13)
	}
	{
		r9:8 = extractu(r7:6,#8,#48)
		p1 = !tstbit(r1,#15)
		if (!p1) memb(r0+#13) = r8
	}
	{
		r7:6 = extractu(r7:6,#8,#56)
		p2 = !tstbit(r1,#14)
	}
	{
		r7:6 = extractu(r5:4,#8,#8)
		p1 = !tstbit(r1,#17)
		if (!p1) memb(r0+#15) = r6
		if (!p2) memb(r0+#14) = r8
	}
	{
		p0 = !tstbit(r1,#16)
		p2 = !tstbit(r1,#18)
		if (!p1) memb(r0+#17) = r6
	}
	{
		r7:6 = extractu(r5:4,#8,#16)
		p0 = !tstbit(r1,#20)
		if (!p0) memb(r0+#16) = r4
	}
	{
		r7:6 = extractu(r5:4,#8,#24)
		p1 = !tstbit(r1,#19)
		if (!p2) memb(r0+#18) = r6
	}
	{
		if (p0) jump:nt .LBB131_704
		if (!p1) memb(r0+#19) = r6
	}
// %bb.703:                             // %cond.store1039
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r5:4,#8,#32)
	}
	{
		memb(r0+#20) = r6
	}
.LBB131_704:                            // %else1040
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r7:6 = extractu(r5:4,#8,#40)
		p1 = !tstbit(r1,#21)
	}
	{
		r7:6 = extractu(r5:4,#8,#48)
		p1 = !tstbit(r1,#23)
		if (!p1) memb(r0+#21) = r6
	}
	{
		r5:4 = extractu(r5:4,#8,#56)
		p2 = !tstbit(r1,#22)
	}
	{
		r5:4 = extractu(r3:2,#8,#8)
		p1 = !tstbit(r1,#25)
		if (!p1) memb(r0+#23) = r4
		if (!p2) memb(r0+#22) = r6
	}
	{
		p0 = !tstbit(r1,#24)
		r6 = memw(r30+##-5680)
		if (!p1) memb(r0+#25) = r4
	}                                       // 4-byte Folded Reload
	{
		r5:4 = extractu(r3:2,#8,#16)
		p2 = !tstbit(r1,#26)
		if (!p0) memb(r0+#24) = r2
	}
	{
		p0 = !tstbit(r1,#28)
		r7 = memw(r30+##-6192)
		if (!p2) memb(r0+#26) = r4
	}                                       // 4-byte Folded Reload
	{
		r5:4 = extractu(r3:2,#8,#24)
		p1 = !tstbit(r1,#27)
	}
	{
		if (p0) jump:nt .LBB131_647
		if (!p1) memb(r0+#27) = r4
	}
// %bb.705:                             // %cond.store1055
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r5:4 = extractu(r3:2,#8,#32)
	}
	{
		jump .LBB131_647
		memb(r0+#28) = r4
	}
	.p2align	4
.LBB131_706:                            // %next_bb1770
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r0 = memw(r30+##-10296)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r6,r0)
		r0 = memw(r30+##-23584)
	}                                       // 4-byte Folded Reload
	{
		p0 = r0
		if (!p1) jump:nt .LBB131_716
		memw(r30+##-6448) = r1
	}                                       // 4-byte Folded Spill
// %bb.707:                             // %"for convolved.s1.r19$y1771.preheader"
                                        //   in Loop: Header=BB131_648 Depth=3
	{
		r4 = #64
		r2 = #116
		if (!p0) jump:nt .LBB131_716
	}
// %bb.708:                             //   in Loop: Header=BB131_648 Depth=3
	{
		r16 = #0
		r0 = memw(r30+##-10304)
	}                                       // 4-byte Folded Reload
	{
		r24 = r5
		r13 = r7
		r6 = memw(r30+##-23240)
	}                                       // 4-byte Folded Reload
	{
		r0 = mpyi(r1,r0)
		r1 = add(r30,#-19504)
	}
	{
		r1 = add(r30,#-19376)
		v8 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-19760)
		v9 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		r1 = add(r30,#-19632)
		v0 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v1 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v23:22 = vcombine(v1,v0)
		r1 = memw(r30+##-5688)
	}                                       // 4-byte Folded Reload
	{
		loop1(.LBB131_710,r1)
		r1 = memw(r30+##-20912)
	}                                       // 4-byte Folded Reload
	{
		r0 = sub(r0,r1)
		memw(r30+#-3376) = r0.new
	}                                       // 4-byte Folded Spill
	{
		r0 = memw(r30+##-23224)
	}                                       // 4-byte Folded Reload
	{
		p1 = r0
		jump .LBB131_710
	}
	.p2align	4
.LBB131_709:                            // %"end for convolved.s1.r19$x1775.loopexit.us"
                                        //   in Loop: Header=BB131_710 Depth=4
	{
		r16 = add(r16,#1)
		r1 = memw(r30+#-2608)
		r0 = memw(r30+#-2096)
	}                                       // 4-byte Folded Reload
	{
		r13 = add(r13,r1)
		r24 = add(r24,r1)
		r6 = memw(r30+#-1840)
	}                                       // 4-byte Folded Reload
	{
		r6 = add(r6,r0)
		nop
		nop
	} :endloop1
	{
		jump .LBB131_650
	}
.Ltmp35:                                // Block address taken
.LBB131_710:                            // %"for convolved.s1.r19$y1771.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        //     Parent Loop BB131_645 Depth=2
                                        //       Parent Loop BB131_648 Depth=3
                                        // =>      This Loop Header: Depth=4
                                        //           Child Loop BB131_713 Depth 5
	{
		if (!p1) jump:nt .LBB131_712
		memw(r30+#-1584) = r16
		memw(r30+#-1840) = r6
	}                                       // 4-byte Folded Spill
// %bb.711:                             //   in Loop: Header=BB131_710 Depth=4
	{
		r5 = #0 ; jump .LBB131_714
	}
	.p2align	4
.LBB131_712:                            //   in Loop: Header=BB131_710 Depth=4
	{
		r0 = memw(r30+##-4784)
		memw(r30+#-1072) = r13
	}                                       // 4-byte Folded Reload
	{
		r5 = lsr(r0,#1)
		r0 = #0
	}
	{
		loop0(.LBB131_713,r5)
		r5 = #0
	}
	.p2align	4
.Ltmp36:                                // Block address taken
.LBB131_713:                            // %"for convolved.s1.r19$x1774.us"
                                        //   Parent Loop BB131_248 Depth=1
                                        //     Parent Loop BB131_645 Depth=2
                                        //       Parent Loop BB131_648 Depth=3
                                        //         Parent Loop BB131_710 Depth=4
                                        // =>        This Inner Loop Header: Depth=5
	{
		r1 = add(r30,#-816)
		r12 = add(r13,r0)
		v10 = vxor(v10,v10)
		v28 = vmem(r6+#0)
	}
	{
		r1 = add(r30,#-688)
		v5 = v10
		vmemu(r1+#0) = v22
	}                                       // 256-byte Folded Spill
	{
		v17:16 = vcombine(v10,v10)
		v20 = v10
		vmemu(r1+#0) = v23
	}                                       // 256-byte Folded Spill
	{
		r28 = add(r24,r0)
		v19:18 = vcombine(v10,v10)
		r1 = memw(r30+#-3912)
		r9:8 = memd(r12+#-96)
	}                                       // 4-byte Folded Reload
	{
		v5.w = vinsert(r8)
		v4 = valign(v1,v1,r4)
		r15:14 = memd(r12+#-88)
		v1.cur = vmem(r6+#-2)
	}
	{
		r3 = r24
		v23:22 = vcombine(v10,v10)
		v0 = vmem(r1+#0)
		memw(r30+#-560) = r5
	}                                       // 4-byte Folded Spill
	{
		v15:14.w = vunpack(v4.h)
		v11 = v10
		r1 = memw(r30+#-3904)
		v6 = vmem(r6+#-1)
	}                                       // 4-byte Folded Reload
	{
		v21 = valign(v6,v6,r4)
		v27:26 = vcombine(v10,v10)
		r11:10 = memd(r12+#-80)
		memw(r30+#-304) = r0
	}                                       // 4-byte Folded Spill
	{
		v17.w = vinsert(r10)
		q0 = vcmp.eq(v2.b,v10.b)
		r21:20 = memd(r12+#-64)
		v2.cur = vmem(r1+#0)
	}
	{
		v19.w = vinsert(r20)
		v3:2.w = vunpack(v28.h)
		v28 = v10
		r23:22 = memd(r12+#-56)
	}
	{
		v3 = valign(v5,v5,#4)
		v15 = v10
		r17:16 = memd(r12+#-72)
		r25:24 = memd(r13+r0<<#0)
	}
	{
		v3.w = vinsert(r9)
		v5:4.w = vunpack(v21.h)
		r9:8 = memd(r12+#-32)
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		v22.w = vinsert(r8)
		r7 = add(r1,#-31616)
		v7:6.w = vunpack(v6.h)
		r27:26 = memd(r12+#-40)
	}
	{
		v29 = valign(v3,v3,#4)
		v3 = v10
	}
	{
		v29.w = vinsert(r14)
		v7 = valign(v17,v17,#4)
	}
	{
		v7.w = vinsert(r11)
		v17 = valign(v19,v19,#4)
		r11:10 = memd(r12+#-24)
	}
	{
		v17.w = vinsert(r21)
		v5 = valign(v29,v29,#4)
		r21:20 = memd(r12+#-8)
		r1:0 = memd(r12+#8)
	}
	{
		v5.w = vinsert(r15)
		v25:24.w = vunpack(v1.h)
	}
	{
		v1 = vror(v10,r4)
		v25 = v10
		r5:4 = memd(r12+#-48)
	}
	{
		v20.w = vinsert(r4)
		v25.w = vinsert(r24)
		v5 = vror(v5,r2)
		r19:18 = memd(r12+#-16)
	}
	{
		r24 = r3
		v5 = valign(v7,v7,#4)
		v21 = vor(v5,v1)
		r3:2 = memd(r12+#16)
	}
	{
		v5.w = vinsert(r16)
		v26.w = vinsert(r2)
		v7 = valign(v17,v17,#4)
		r2 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v7.w = vinsert(r22)
		v23.w = vinsert(r18)
		r18 = #116
		v19 = valign(v20,v20,#4)
	}
	{
		v19.w = vinsert(r5)
		v17 = valign(v22,v22,#4)
		r5:4 = memd(r12+#24)
		r15:14 = memd(r24+r2<<#0)
	}
	{
		v17.w = vinsert(r9)
		v27.w = vinsert(r14)
		v7 = valign(v7,v7,#4)
		r9:8 = memd(r28+#16)
	}
	{
		v7.w = vinsert(r23)
		v28.w = vinsert(r8)
		v5 = valign(v5,v5,#4)
		r23:22 = memd(r6+#184)
	}
	{
		v5.w = vinsert(r17)
		v19 = valign(v19,v19,#4)
		r13:12 = memd(r28+#8)
		r17:16 = memd(r28+#24)
	}
	{
		v19.w = vinsert(r26)
		v17 = valign(v17,v17,#4)
		memd(r7+#120) = r23:22
		r23:22 = memd(r6+#176)

	} :mem_noshuf
	{
		v17.w = vinsert(r10)
		v5 = vror(v5,r18)
		memd(r7+#112) = r23:22
	}
	{
		v20 = valign(v19,v19,#4)
		v22 = vor(v5,v1)
		r23:22 = memd(r6+#168)
	}
	{
		v20.w = vinsert(r27)
		v5 = valign(v17,v17,#4)
		memd(r7+#104) = r23:22
	}
	{
		v5.w = vinsert(r11)
		v17 = valign(v23,v23,#4)
		r27:26 = memd(r6+#160)
	}
	{
		v17.w = vinsert(r19)
		v23 = valign(v26,v26,#4)
		memd(r7+#96) = r27:26
	}
	{
		v23.w = vinsert(r3)
		v7 = vror(v7,r18)
		r11:10 = memd(r6+#152)
	}
	{
		v7 = vror(v20,r18)
		v19 = vor(v7,v1)
		memd(r7+#88) = r11:10
	}
	{
		v7 = valign(v25,v25,#4)
		v29 = vor(v7,v1)
		r3:2 = memd(r6+#144)
	}
	{
		v7.w = vinsert(r25)
		v5 = vror(v5,r18)
		memd(r7+#80) = r3:2
	}
	{
		v5 = valign(v17,v17,#4)
		v20 = vor(v5,v1)
		r3:2 = memd(r6+#136)
	}
	{
		v5.w = vinsert(r20)
		v7 = valign(v7,v7,#4)
		memd(r7+#72) = r3:2
	}
	{
		v7.w = vinsert(r0)
		v17 = valign(v27,v27,#4)
		r3:2 = memd(r6+#128)
	}
	{
		v17.w = vinsert(r15)
		v23 = valign(v23,v23,#4)
		memd(r7+#64) = r3:2
	}
	{
		v23.w = vinsert(r4)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r6+#120)
	}
	{
		v7.w = vinsert(r1)
		v5 = valign(v5,v5,#4)
		memd(r7+#56) = r3:2
	}
	{
		v5.w = vinsert(r21)
		v17 = valign(v17,v17,#4)
		r1:0 = memd(r6+#112)
	}
	{
		v17.w = vinsert(r12)
		v23 = valign(v23,v23,#4)
		memd(r7+#48) = r1:0
	}
	{
		v23.w = vinsert(r5)
		v31:30.uh = vunpack(v22.ub)
		r1:0 = memd(r6+#104)
	}
	{
		v13:12.uh = vunpack(v21.ub)
		memd(r7+#40) = r1:0
	}
	{
		v26 = valign(v28,v28,#4)
		r21:20 = memd(r6+#96)
	}
	{
		v26.w = vinsert(r9)
		v29:28.uh = vunpack(v29.ub)
		memd(r7+#32) = r21:20
	}
	{
		v5 = vror(v5,r18)
		r1:0 = memd(r6+#88)
	}
	{
		v31:30.uw = vunpack(v30.uh)
		v25 = vor(v5,v1)
		memd(r7+#24) = r1:0
	}
	{
		v5 = valign(v17,v17,#4)
		r1:0 = memd(r6+#80)
	}
	{
		v5.w = vinsert(r13)
		v17 = vror(v23,r18)
		memd(r7+#16) = r1:0
	}
	{
		v13:12.uw = vunpack(v12.uh)
		v23 = vor(v17,v1)
	}
	{
		v13 = vdelta(v30,v0)
		r1:0 = memd(r6+#72)
	}
	{
		v29:28.uw = vunpack(v28.uh)
		v12 = vmux(q0,v13,v12)
		memd(r7+#8) = r1:0
	}
	{
		v22 = valign(v26,v26,#4)
		r1:0 = memd(r6+#64)
	}
	{
		v22.w = vinsert(r16)
		v17.w = vmpyieo(v12.h,v24.h)
		v21 = vdelta(v28,v0)
		memd(r7+#0) = r1:0
	}
	{
		v17.w += vmpyie(v12.w,v24.h)
		v29:28.uh = vunpack(v23.ub)
		r1:0 = memd(r28+#48)
		r23:22 = memd(r28+#32)
	}
	{
		v18.w = vinsert(r0)
		v16.w = vinsert(r22)
		v31:30.uh = vunpack(v25.ub)
		r15:14 = memd(r28+#56)
	}
	{
		v7 = vror(v7,r18)
		r3:2 = memd(r28+#80)
		r27:26 = memd(r28+#64)
	}
	{
		v15.w = vinsert(r2)
		v13 = valign(v22,v22,#4)
		v7 = vor(v7,v1)
		r5:4 = memd(r28+#112)
	}
	{
		v13.w = vinsert(r17)
		v10.w = vinsert(r4)
		v25:24.uw = vunpack(v28.uh)
		r13:12 = memd(r28+#40)
	}
	{
		v11.w = vinsert(r26)
		v29:28.uh = vunpack(v19.ub)
	}
	{
		v31:30.uw = vunpack(v30.uh)
	}
	{
		v27:26.uw = vunpack(v28.uh)
	}
	{
		v12 = vror(v13,r18)
		v13 = vmux(q0,v21,v26)
	}
	{
		v22 = vdelta(v30,v0)
		v12 = vor(v12,v1)
	}
	{
		v31:30.uh = vunpack(v20.ub)
	}
	{
		v21:20.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v18,v18,#4)
	}
	{
		v7.w = vinsert(r1)
		v16.w = vmpyieo(v13.h,v14.h)
		v18 = valign(v16,v16,#4)
		r1:0 = memd(r28+#96)
	}
	{
		v18.w = vinsert(r23)
		v3.w = vinsert(r0)
		v5 = vror(v5,r18)
	}
	{
		v16.w += vmpyie(v13.w,v14.h)
		v7 = valign(v7,v7,#4)
		v13 = vmem(r7+#0)
	}
	{
		v7.w = vinsert(r14)
		v29:28.uh = vunpack(v12.ub)
		v12 = vor(v5,v1)
	}
	{
		v19 = vdelta(v24,v0)
	}
	{
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r15)
		v25:24.uw = vunpack(v30.uh)
	}
	{
		v31:30.uh = vunpack(v12.ub)
	}
	{
		v12 = valign(v18,v18,#4)
	}
	{
		v12.w = vinsert(r12)
		v18 = valign(v15,v15,#4)
	}
	{
		v18.w = vinsert(r3)
		v7 = vror(v7,r18)
		r3:2 = memd(r28+#88)
	}
	{
		v15:14.w = vunpack(v13.h)
		v7 = vor(v7,v1)
	}
	{
		v13 = valign(v18,v18,#4)
		v15 = vmux(q0,v22,v24)
	}
	{
		v13.w = vinsert(r2)
		v27:26.uw = vunpack(v28.uh)
	}
	{
		v29:28.uh = vunpack(v7.ub)
	}
	{
		v7 = valign(v11,v11,#4)
	}
	{
		v7.w = vinsert(r27)
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r5)
		v11 = valign(v13,v13,#4)
		r5:4 = memd(r28+#72)
	}
	{
		v11.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r28+#120)
	}
	{
		v7.w = vinsert(r4)
		r4 = #64
		v10 = valign(v10,v10,#4)
	}
	{
		v10.w = vinsert(r2)
		r2 = #116
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r1)
		v11 = vror(v11,r18)
		r1:0 = memd(r28+#104)
	}
	{
		v10 = valign(v10,v10,#4)
		v11 = vor(v11,v1)
	}
	{
		v10.w = vinsert(r3)
		v7 = valign(v7,v7,#4)
	}
	{
		v7.w = vinsert(r5)
		v3 = valign(v3,v3,#4)
		r5 = memw(r30+#-560)
	}                                       // 4-byte Folded Reload
	{
		v3.w = vinsert(r0)
		r5 = add(r5,#2)
		v29:28.uw = vunpack(v28.uh)
		r0 = memw(r30+#-304)
	}                                       // 4-byte Folded Reload
	{
		v21:20.uw = vunpack(v20.uh)
	}
	{
		v21.w = vmpyieo(v15.h,v6.h)
		v13 = vdelta(v28,v0)
	}
	{
		v21.w += vmpyie(v15.w,v6.h)
		v29:28.uh = vunpack(v11.ub)
	}
	{
		v6 = vror(v10,r2)
	}
	{
		v12 = valign(v12,v12,#4)
		v6 = vor(v6,v1)
	}
	{
		v12.w = vinsert(r13)
		v7 = vror(v7,r2)
		r13 = memw(r30+#-1072)
	}                                       // 4-byte Folded Reload
	{
		v3 = valign(v3,v3,#4)
		v7 = vor(v7,v1)
	}
	{
		v3.w = vinsert(r1)
		r1 = add(r30,#-816)
		v11:10.uw = vunpack(v28.uh)
	}
	{
		v12 = vror(v12,r18)
	}
	{
		v5 = vdelta(v26,v0)
		v12 = vor(v12,v1)
	}
	{
		v27:26.uw = vunpack(v30.uh)
	}
	{
		v31:30.uh = vunpack(v6.ub)
		v5 = vmux(q0,v5,v26)
	}
	{
		v6 = vdelta(v10,v0)
	}
	{
		v11:10.uh = vunpack(v7.ub)
		v7 = vmux(q0,v19,v20)
	}
	{
		v3 = vror(v3,r2)
	}
	{
		r6 = add(r6,#512)
		v11:10.uw = vunpack(v10.uh)
		v1 = vor(v3,v1)
		v3 = vmem(r6+#1)
	}
	{
		v11.w = vmpyieo(v5.h,v2.h)
		v20.w = vmpyieo(v7.h,v4.h)
		v23:22.uh = vunpack(v12.ub)
	}
	{
		v11.w += vmpyie(v5.w,v2.h)
		v25:24.uw = vunpack(v30.uh)
	}
	{
		v19:18.uh = vunpack(v1.ub)
		v2 = vmux(q0,v6,v10)
	}
	{
		v20.w += vmpyie(v7.w,v4.h)
		v29:28.w = vunpack(v3.h)
	}
	{
		v3 = valign(v3,v3,r4)
		v9:8.w = vadd(v21:20.w,v9:8.w)
	}
	{
		v5.w = vmpyieo(v2.h,v28.h)
		v23:22.uw = vunpack(v22.uh)
	}
	{
		v5.w += vmpyie(v2.w,v28.h)
		v0 = vdelta(v24,v0)
		v29 = vmux(q0,v13,v22)
	}
	{
		v31:30.uw = vunpack(v18.uh)
	}
	{
		v10.w = vmpyieo(v29.h,v14.h)
		v7:6.w = vunpack(v3.h)
		v0 = vmux(q0,v0,v30)
	}
	{
		v10.w += vmpyie(v29.w,v14.h)
		r1 = add(r30,#-688)
		v2 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w = vmpyieo(v0.h,v6.h)
		v3 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		v4.w += vmpyie(v0.w,v6.h)
		v3:2.w = vadd(v17:16.w,v3:2.w)
		r1 = memw(r30+#-1328)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,r1)
		v23:22.w = vadd(v11:10.w,v3:2.w)
		v9:8.w = vadd(v5:4.w,v9:8.w)
	} :endloop0
.LBB131_714:                            // %"end for convolved.s1.r19$x1775.loopexit.us.unr-lcssa"
                                        //   in Loop: Header=BB131_710 Depth=4
	{
		r0 = memw(r30+#-2352)
	}                                       // 4-byte Folded Reload
	{
		p0 = cmp.eq(r0,#0); if (p0.new) jump:t .LBB131_709
		r16 = memw(r30+#-1584)
	}                                       // 4-byte Folded Reload
// %bb.715:                             // %"for convolved.s1.r19$x1774.us.epil"
                                        //   in Loop: Header=BB131_710 Depth=4
	{
		v2 = vxor(v2,v2)
		r0 = memw(r30+#-3912)
		r2 = memw(r30+##-5696)
	}                                       // 4-byte Folded Reload
	{
		r28 = #116
		v7:6 = vcombine(v2,v2)
		v12 = v2
		r3 = memw(r30+#-3904)
	}                                       // 4-byte Folded Reload
	{
		r2 = mpyi(r16,r2)
		v14 = v2
		v0 = vmem(r0+#0)
	}
	{
		v28 = vror(v2,r4)
		r0 = memw(r30+##-4656)
		r1 = memw(r30+#-3376)
	}                                       // 4-byte Folded Reload
	{
		r2 = add(r5,r2)
		q0 = vcmp.eq(v1.b,v2.b)
		r6 = memw(r30+#-2864)
		v1.cur = vmem(r3+#0)
	}                                       // 4-byte Folded Reload
	{
		r17 = r13
		v5:4 = vcombine(v2,v2)
		v3 = v2
	}
	{
		r6 += asl(r2,#8)
		r3 = memw(r30+##-4400)
		r2 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r7 = add(r2,#-31744)
		r2 = memw(r30+#-3632)
	}                                       // 4-byte Folded Reload
	{
		r0 += mpyi(r16,r3)
		v11:10.w = vunpack(v27.h)
		v27.cur = vmem(r6+#0)
	}
	{
		r1 += mpyi(r0,r2)
		r0 = memw(r30+#-3888)
		r3:2 = memd(r6+#184)
	}                                       // 4-byte Folded Reload
	{
		r4 = memw(r30+#-3120)
	}                                       // 4-byte Folded Reload
	{
		r1 += mpyi(r5,r0)
	}
	{
		r5 = addasl(r4,r1,#7)
		r0 = asl(r1,#7)
	}
	{
		r1:0 = memd(r4+r0<<#0)
	}
	{
		v7.w = vinsert(r0)
		r9:8 = memd(r5+#16)
		r23:22 = memd(r5+#8)
	}
	{
		v12.w = vinsert(r8)
		r15:14 = memd(r5+#24)
		memd(r7+#120) = r3:2

	} :mem_noshuf
	{
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r6+#176)
	}
	{
		v7.w = vinsert(r1)
		v11 = valign(v12,v12,#4)
		memd(r7+#112) = r3:2
		r3:2 = memd(r6+#168)

	} :mem_noshuf
	{
		v11.w = vinsert(r9)
		memd(r7+#104) = r3:2
	}
	{
		v7 = valign(v7,v7,#4)
		r3:2 = memd(r6+#160)
	}
	{
		v7.w = vinsert(r22)
		v11 = valign(v11,v11,#4)
		memd(r7+#96) = r3:2
	}
	{
		v11.w = vinsert(r14)
		r1:0 = memd(r6+#152)
	}
	{
		v7 = valign(v7,v7,#4)
		memd(r7+#88) = r1:0
	}
	{
		v7.w = vinsert(r23)
		v11 = valign(v11,v11,#4)
		r1:0 = memd(r6+#144)
	}
	{
		v11.w = vinsert(r15)
		memd(r7+#80) = r1:0
	}
	{
		v7 = vror(v7,r28)
		r1:0 = memd(r6+#136)
	}
	{
		v11 = vror(v11,r28)
		v7 = vor(v7,v28)
		memd(r7+#72) = r1:0
	}
	{
		v11 = vor(v11,v28)
		r1:0 = memd(r6+#128)
	}
	{
		v25:24.uh = vunpack(v7.ub)
		memd(r7+#64) = r1:0
	}
	{
		v17:16.uh = vunpack(v11.ub)
		r1:0 = memd(r6+#120)
	}
	{
		v13:12.uw = vunpack(v24.uh)
		memd(r7+#56) = r1:0
	}
	{
		v17:16.uw = vunpack(v16.uh)
		r1:0 = memd(r6+#112)
	}
	{
		memd(r7+#48) = r1:0
	}
	{
		v7 = vdelta(v16,v0)
		r1:0 = memd(r6+#104)
	}
	{
		v11 = vmux(q0,v7,v12)
		memd(r7+#40) = r1:0
		r1:0 = memd(r6+#96)

	} :mem_noshuf
	{
		v7.w = vmpyieo(v11.h,v10.h)
		memd(r7+#32) = r1:0
	}
	{
		v7.w += vmpyie(v11.w,v10.h)
		r1:0 = memd(r6+#88)
	}
	{
		memd(r7+#24) = r1:0
		r1:0 = memd(r6+#80)

	} :mem_noshuf
	{
		memd(r7+#16) = r1:0
		r1:0 = memd(r6+#72)

	} :mem_noshuf
	{
		memd(r7+#8) = r1:0
		r1:0 = memd(r6+#64)

	} :mem_noshuf
	{
		memd(r7+#0) = r1:0
		r27:26 = memd(r5+#80)

	} :mem_noshuf
	{
		v3.w = vinsert(r26)
		r19:18 = memd(r5+#32)
		r21:20 = memd(r5+#48)
	}
	{
		v14.w = vinsert(r18)
		v6.w = vinsert(r20)
		r11:10 = memd(r5+#64)
		r1:0 = memd(r5+#96)
	}
	{
		v5.w = vinsert(r10)
		v4.w = vinsert(r0)
		r3:2 = memd(r5+#112)
	}
	{
		v2.w = vinsert(r2)
		v3 = valign(v3,v3,#4)
		v10 = vmem(r7+#0)
	}
	{
		v3.w = vinsert(r27)
		v6 = valign(v6,v6,#4)
		v12 = vmem(r6+#1)
	}
	{
		v6.w = vinsert(r21)
		v11:10.w = vunpack(v10.h)
	}
	{
		v11 = valign(v14,v14,#4)
	}
	{
		v11.w = vinsert(r19)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r11)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r1)
		v2 = valign(v2,v2,#4)
		r1:0 = memd(r5+#40)
	}
	{
		v2.w = vinsert(r3)
		v6 = valign(v6,v6,#4)
		r3:2 = memd(r5+#56)
		r23:22 = memd(r5+#72)
	}
	{
		v6.w = vinsert(r2)
		v11 = valign(v11,v11,#4)
		r13:12 = memd(r5+#88)
		r27:26 = memd(r5+#104)
	}
	{
		v11.w = vinsert(r0)
		r2 = #116
		v5 = valign(v5,v5,#4)
		r5:4 = memd(r5+#120)
	}
	{
		v5.w = vinsert(r22)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r12)
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r4)
		r4 = #64
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r26)
		v6 = valign(v6,v6,#4)
	}
	{
		v6.w = vinsert(r3)
		v11 = valign(v11,v11,#4)
	}
	{
		v11.w = vinsert(r1)
		v5 = valign(v5,v5,#4)
	}
	{
		v5.w = vinsert(r23)
		v3 = valign(v3,v3,#4)
	}
	{
		v3.w = vinsert(r13)
		r13 = r17
		v2 = valign(v2,v2,#4)
	}
	{
		v2.w = vinsert(r5)
		v4 = valign(v4,v4,#4)
	}
	{
		v4.w = vinsert(r27)
		v6 = vror(v6,r2)
	}
	{
		v11 = vror(v11,r2)
		v6 = vor(v6,v28)
	}
	{
		v5 = vror(v5,r2)
		v11 = vor(v11,v28)
	}
	{
		v3 = vror(v3,r2)
		v5 = vor(v5,v28)
	}
	{
		v2 = vror(v2,r2)
		v3 = vor(v3,v28)
	}
	{
		v4 = vror(v4,r2)
		v2 = vor(v2,v28)
	}
	{
		v27:26.uh = vunpack(v6.ub)
		v1 = vor(v4,v28)
	}
	{
		v19:18.uh = vunpack(v3.ub)
	}
	{
		v3:2.uh = vunpack(v2.ub)
	}
	{
		v21:20.uh = vunpack(v5.ub)
	}
	{
		v17:16.uh = vunpack(v11.ub)
	}
	{
		v5:4.uw = vunpack(v26.uh)
	}
	{
		v29:28.uh = vunpack(v1.ub)
	}
	{
		v29 = vdelta(v4,v0)
	}
	{
		v5:4.uw = vunpack(v16.uh)
	}
	{
		v17:16.uw = vunpack(v18.uh)
		v1 = vmux(q0,v29,v4)
	}
	{
		v3:2.uw = vunpack(v2.uh)
	}
	{
		v6.w = vmpyieo(v1.h,v10.h)
		v31 = valign(v12,v12,r4)
	}
	{
		v6.w += vmpyie(v1.w,v10.h)
		v3 = vdelta(v16,v0)
	}
	{
		v19:18.uw = vunpack(v20.uh)
		v23:22.w = vadd(v7:6.w,v23:22.w)
	}
	{
		v0 = vdelta(v2,v0)
		v30 = vmux(q0,v3,v18)
	}
	{
		v5:4.uw = vunpack(v28.uh)
	}
	{
		v17:16.w = vunpack(v12.h)
		v0 = vmux(q0,v0,v4)
	}
	{
		v13:12.w = vunpack(v31.h)
	}
	{
		v5.w = vmpyieo(v30.h,v16.h)
	}
	{
		v4.w = vmpyieo(v0.h,v12.h)
	}
	{
		v5.w += vmpyie(v30.w,v16.h)
	}
	{
		v4.w += vmpyie(v0.w,v12.h)
	}
	{
		jump .LBB131_709
		v9:8.w = vadd(v5:4.w,v9:8.w)
	}
.LBB131_716:                            //   in Loop: Header=BB131_648 Depth=3
	{
		r3 = add(r30,#-19760)
		r2 = add(r30,#-19632)
		r0 = add(r30,#-19504)
		r4 = add(r30,#-19376)
	}
	{
		v0 = vmemu(r3+#0)
	}                                       // 256-byte Folded Reload
	{
		v1 = vmemu(r2+#0)
	}                                       // 256-byte Folded Reload
	{
		v23:22 = vcombine(v1,v0)
		v8 = vmemu(r0+#0)
	}                                       // 256-byte Folded Reload
	{
		jump .LBB131_650
		v9 = vmemu(r4+#0)
	}                                       // 256-byte Folded Reload
	.p2align	4
.LBB131_717:                            // %if.then.i2015
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r1 = #16384
		if (!cmp.gtu(r0,r1.new)) jump:nt ##.LBB131_250
	}
// %bb.718:                             // %if.then3.i2019
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		call ##halide_free
		r1:0 = combine(r2,#0)
	}
	{
		r1 = add(r30,#-23216)
		r0 = ##16843009
	}
	{
		r20 = memw(r30+##-23272)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r30,#-19248)
		v4 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		q1 = vand(v4,r0)
		v4 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		r0 = ##16843009
		r1 = add(r30,#-23088)
	}
	{
		q3 = vand(v4,r0)
		r0 = ##16843009
	}
	{
		r3 = memw(r30+##-23336)
	}                                       // 4-byte Folded Reload
	{
		r1 = add(r30,#-5424)
		v4 = vmemu(r1+#0)
	}                                       // 128-byte Folded Reload
	{
		q2 = vand(v4,r0)
		r0 = ##16843009
	}
	{
		v4 = vmemu(r1+#0)
	}                                       // 256-byte Folded Reload
	{
		q0 = vand(v4,r0)
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = add(r0,#-27404)
	}
	{
		jump .LBB131_250
		r0 = memw(r0+#8)
	}
.LBB131_719:                            // %then_bb206
                                        //   in Loop: Header=BB131_248 Depth=1
	{
		r17 = add(#7,asl(r17,#2))
		r1 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r0 = and(r17,#-8)
		r1 = add(r1,#-27404)
	}
	{
		r2 = sub(r29,r0)
		r29 = sub(r29,r0)
	}
	{
		r2 = and(r2,#-128)
		r29 = and(r29,#-128)
		jump .LBB131_254
		memw(r1+#0) = r2.new
	}
.LBB131_720:                            // %if.then.i.loopexit
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r0,#-28044)
		r19 = add(r0,#-27404)
	}
	{
		r1 = memw(r19+#0)
		if (!cmp.eq(r1.new,#0)) jump:t ##.LBB131_84
	}
.LBB131_721:                            // %pseudostack_free.exit
	{
		memw(r19+#0) = #0
		memw(r19+#4) = #0
	}
	{
		memw(r19+#8) = #0
	}
.LBB131_722:                            // %if.then.i1933
	{
		r1 = memw(r16+#0)
		if (cmp.eq(r1.new,#0)) jump:nt .LBB131_725
	}
.LBB131_723:                            // %land.lhs.true.i2040
	{
		r0 = memw(r16+#8)
	}
	{
		r2 = #16384
		if (!cmp.gtu(r0,r2.new)) jump:t .LBB131_725
	}
// %bb.724:                             // %if.then.i2041
	{
		call ##halide_free
		r0 = #0
	}
.LBB131_725:                            // %pseudostack_free.exit2045
	{
		memw(r16+#0) = #0
		memw(r16+#4) = #0
	}
	{
		memw(r16+#8) = #0
	}
.LBB131_726:                            // %call_destructor.exit1937
	{
		call ##halide_qurt_hvx_unlock_as_destructor
		r1:0 = combine(#1,#0)
	}
	{
		r0 = #0
		r17:16 = memd(r30+#-8)
		r19:18 = memd(r30+#-16)
	}                                       // 8-byte Folded Reload
	{
		r21:20 = memd(r30+#-24)
		r23:22 = memd(r30+#-32)
	}                                       // 8-byte Folded Reload
	{
		r25:24 = memd(r30+#-40)
		r27:26 = memd(r30+#-48)
	}                                       // 8-byte Folded Reload
	{
		r31:30 = dealloc_return(r30):raw
	}
.LBB131_727:
	{
		r0 = memw(r30+#-3896)
	}                                       // 4-byte Folded Reload
	{
		r16 = add(r0,#-28044)
	}
	{
		r1 = memw(r16+#0)
		if (!cmp.eq(r1.new,#0)) jump:t .LBB131_723
	}
	{
		jump .LBB131_725
	}
.Ltmp37:                                // Address of block that was removed by CodeGen
.Ltmp38:                                // Address of block that was removed by CodeGen
.Ltmp39:                                // Address of block that was removed by CodeGen
.Ltmp40:                                // Address of block that was removed by CodeGen
.Ltmp41:                                // Address of block that was removed by CodeGen
.Ltmp42:                                // Address of block that was removed by CodeGen
.Ltmp43:                                // Address of block that was removed by CodeGen
.Ltmp44:                                // Address of block that was removed by CodeGen
.Ltmp45:                                // Address of block that was removed by CodeGen
.Ltmp46:                                // Address of block that was removed by CodeGen
.Ltmp47:                                // Address of block that was removed by CodeGen
.Ltmp48:                                // Address of block that was removed by CodeGen
.Ltmp49:                                // Address of block that was removed by CodeGen
.Ltmp50:                                // Address of block that was removed by CodeGen
.Ltmp51:                                // Address of block that was removed by CodeGen
.Lfunc_end131:
	.size	depthwise_conv_hvx128, .Lfunc_end131-depthwise_conv_hvx128
                                        // -- End function
	.section	.text.depthwise_conv_hvx128_argv,"ax",@progbits
	.globl	depthwise_conv_hvx128_argv      // -- Begin function depthwise_conv_hvx128_argv
	.p2align	4
	.type	depthwise_conv_hvx128_argv,@function
depthwise_conv_hvx128_argv:             // @depthwise_conv_hvx128_argv
// %bb.0:                               // %entry
	{
		allocframe(r29,#40):raw
	}
	{
		r6 = memw(r0+#0)
		r2 = memw(r0+#8)
	}
	{
		r4 = memw(r0+#16)
		r1 = memw(r0+#4)
	}
	{
		r3 = memw(r0+#12)
		r5 = memw(r0+#20)
	}
	{
		r7 = memw(r0+#24)
		r9 = memw(r0+#32)
	}
	{
		r8 = memw(r0+#28)
		r13 = memw(r0+#40)
	}
	{
		r15 = memw(r0+#48)
		r12 = memw(r0+#36)
	}
	{
		r14 = memw(r0+#44)
		r28 = memw(r0+#52)
	}
	{
		r10 = memw(r0+#56)
		r5 = memw(r5+#0)
	}
	{
		r0 = memw(r0+#60)
		r7 = memw(r7+#0)
	}
	{
		r8 = memw(r8+#0)
		r9 = memw(r9+#0)
	}
	{
		r12 = memw(r12+#0)
		r13 = memw(r13+#0)
	}
	{
		r14 = memw(r14+#0)
		r3 = memub(r3+#0)
	}
	{
		r1 = memub(r1+#0)
		r15 = memub(r15+#0)
	}
	{
		r28 = memub(r28+#0)
		r10 = memub(r10+#0)
	}
	{
		r0 = r6
		memw(r29+#36) = r0
		memw(r29+#32) = r10
	}
	{
		memw(r29+#28) = r28
		memw(r29+#24) = r15
	}
	{
		memw(r29+#20) = r14
		memw(r29+#16) = r13
	}
	{
		memw(r29+#12) = r12
		memw(r29+#8) = r9
	}
	{
		call ##depthwise_conv_hvx128
		memw(r29+#4) = r8
		memw(r29+#0) = r7
	}
	{
		r0 = #0
		dealloc_return
	}
.Lfunc_end132:
	.size	depthwise_conv_hvx128_argv, .Lfunc_end132-depthwise_conv_hvx128_argv
                                        // -- End function
	.section	.text.depthwise_conv_hvx128_metadata,"ax",@progbits
	.globl	depthwise_conv_hvx128_metadata  // -- Begin function depthwise_conv_hvx128_metadata
	.p2align	4
	.type	depthwise_conv_hvx128_metadata,@function
depthwise_conv_hvx128_metadata:         // @depthwise_conv_hvx128_metadata
// %bb.0:                               // %entry
	{
		r0 = add(pc,##.Ldepthwise_conv_hvx128_metadata_storage@PCREL)
		jumpr r31
	}
.Lfunc_end133:
	.size	depthwise_conv_hvx128_metadata, .Lfunc_end133-depthwise_conv_hvx128_metadata
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.0,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.0 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.0
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.0,@function
hydride.node.depthwise_conv_hvx_depth3.0: // @hydride.node.depthwise_conv_hvx_depth3.0
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end134:
	.size	hydride.node.depthwise_conv_hvx_depth3.0, .Lfunc_end134-hydride.node.depthwise_conv_hvx_depth3.0
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.1,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.1 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.1
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.1,@function
hydride.node.depthwise_conv_hvx_depth3.1: // @hydride.node.depthwise_conv_hvx_depth3.1
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end135:
	.size	hydride.node.depthwise_conv_hvx_depth3.1, .Lfunc_end135-hydride.node.depthwise_conv_hvx_depth3.1
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.2,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.2 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.2
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.2,@function
hydride.node.depthwise_conv_hvx_depth3.2: // @hydride.node.depthwise_conv_hvx_depth3.2
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end136:
	.size	hydride.node.depthwise_conv_hvx_depth3.2, .Lfunc_end136-hydride.node.depthwise_conv_hvx_depth3.2
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.3,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.3 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.3
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.3,@function
hydride.node.depthwise_conv_hvx_depth3.3: // @hydride.node.depthwise_conv_hvx_depth3.3
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end137:
	.size	hydride.node.depthwise_conv_hvx_depth3.3, .Lfunc_end137-hydride.node.depthwise_conv_hvx_depth3.3
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.4,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.4 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.4
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.4,@function
hydride.node.depthwise_conv_hvx_depth3.4: // @hydride.node.depthwise_conv_hvx_depth3.4
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end138:
	.size	hydride.node.depthwise_conv_hvx_depth3.4, .Lfunc_end138-hydride.node.depthwise_conv_hvx_depth3.4
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.5,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.5 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.5
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.5,@function
hydride.node.depthwise_conv_hvx_depth3.5: // @hydride.node.depthwise_conv_hvx_depth3.5
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end139:
	.size	hydride.node.depthwise_conv_hvx_depth3.5, .Lfunc_end139-hydride.node.depthwise_conv_hvx_depth3.5
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.6,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.6 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.6
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.6,@function
hydride.node.depthwise_conv_hvx_depth3.6: // @hydride.node.depthwise_conv_hvx_depth3.6
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end140:
	.size	hydride.node.depthwise_conv_hvx_depth3.6, .Lfunc_end140-hydride.node.depthwise_conv_hvx_depth3.6
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.7,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.7 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.7
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.7,@function
hydride.node.depthwise_conv_hvx_depth3.7: // @hydride.node.depthwise_conv_hvx_depth3.7
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end141:
	.size	hydride.node.depthwise_conv_hvx_depth3.7, .Lfunc_end141-hydride.node.depthwise_conv_hvx_depth3.7
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.8,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.8 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.8
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.8,@function
hydride.node.depthwise_conv_hvx_depth3.8: // @hydride.node.depthwise_conv_hvx_depth3.8
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end142:
	.size	hydride.node.depthwise_conv_hvx_depth3.8, .Lfunc_end142-hydride.node.depthwise_conv_hvx_depth3.8
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.9,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.9 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.9
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.9,@function
hydride.node.depthwise_conv_hvx_depth3.9: // @hydride.node.depthwise_conv_hvx_depth3.9
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end143:
	.size	hydride.node.depthwise_conv_hvx_depth3.9, .Lfunc_end143-hydride.node.depthwise_conv_hvx_depth3.9
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.10,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.10 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.10
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.10,@function
hydride.node.depthwise_conv_hvx_depth3.10: // @hydride.node.depthwise_conv_hvx_depth3.10
// %bb.0:                               // %entry
	{
		v6 = vsplat(r0)
		v30 = vnot(v5)
		v31 = vnot(v4)
	}
	{
		v7 = vand(v2,v6)
		v6 = vand(v3,v6)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v0 = vsplat(r1)
		v5 = vand(v0,v30)
		v4 = vand(v1,v31)
	}
	{
		v1 = v0
	}
	{
		v1:0.w = vadd(v1:0.w,v5:4.w)
		jumpr r31
	}
.Lfunc_end144:
	.size	hydride.node.depthwise_conv_hvx_depth3.10, .Lfunc_end144-hydride.node.depthwise_conv_hvx_depth3.10
                                        // -- End function
	.section	.rodata,"a",@progbits
	.p2align	7                               // -- Begin function hydride.node.depthwise_conv_hvx_depth3.11
.LCPI145_0:
	.word	63                              // 0x3f
	.word	62                              // 0x3e
	.word	61                              // 0x3d
	.word	60                              // 0x3c
	.word	59                              // 0x3b
	.word	58                              // 0x3a
	.word	57                              // 0x39
	.word	56                              // 0x38
	.word	55                              // 0x37
	.word	54                              // 0x36
	.word	53                              // 0x35
	.word	52                              // 0x34
	.word	51                              // 0x33
	.word	50                              // 0x32
	.word	49                              // 0x31
	.word	48                              // 0x30
	.word	47                              // 0x2f
	.word	46                              // 0x2e
	.word	45                              // 0x2d
	.word	44                              // 0x2c
	.word	43                              // 0x2b
	.word	42                              // 0x2a
	.word	41                              // 0x29
	.word	40                              // 0x28
	.word	39                              // 0x27
	.word	38                              // 0x26
	.word	37                              // 0x25
	.word	36                              // 0x24
	.word	35                              // 0x23
	.word	34                              // 0x22
	.word	33                              // 0x21
	.word	32                              // 0x20
.LCPI145_1:
	.word	31                              // 0x1f
	.word	30                              // 0x1e
	.word	29                              // 0x1d
	.word	28                              // 0x1c
	.word	27                              // 0x1b
	.word	26                              // 0x1a
	.word	25                              // 0x19
	.word	24                              // 0x18
	.word	23                              // 0x17
	.word	22                              // 0x16
	.word	21                              // 0x15
	.word	20                              // 0x14
	.word	19                              // 0x13
	.word	18                              // 0x12
	.word	17                              // 0x11
	.word	16                              // 0x10
	.word	15                              // 0xf
	.word	14                              // 0xe
	.word	13                              // 0xd
	.word	12                              // 0xc
	.word	11                              // 0xb
	.word	10                              // 0xa
	.word	9                               // 0x9
	.word	8                               // 0x8
	.word	7                               // 0x7
	.word	6                               // 0x6
	.word	5                               // 0x5
	.word	4                               // 0x4
	.word	3                               // 0x3
	.word	2                               // 0x2
	.word	1                               // 0x1
	.word	0                               // 0x0
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.11,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.11
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.11,@function
hydride.node.depthwise_conv_hvx_depth3.11: // @hydride.node.depthwise_conv_hvx_depth3.11
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.LCPI145_0@PCREL)
		v4 = vsplat(r0)
	}
	{
		r2 = add(pc,##.LCPI145_1@PCREL)
		v5 = v4
	}
	{
		v2 = vmem(r1+#0)
	}
	{
		v3 = vmem(r2+#0)
	}
	{
		v1:0.w = vsub(v1:0.w,v3:2.w)
	}
	{
		v1:0.w = vsub(v5:4.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end145:
	.size	hydride.node.depthwise_conv_hvx_depth3.11, .Lfunc_end145-hydride.node.depthwise_conv_hvx_depth3.11
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.12,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.12 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.12
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.12,@function
hydride.node.depthwise_conv_hvx_depth3.12: // @hydride.node.depthwise_conv_hvx_depth3.12
// %bb.0:                               // %entry
	{
		v2 = vsplat(r0)
	}
	{
		v3 = v2
	}
	{
		v1:0.w = vsub(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end146:
	.size	hydride.node.depthwise_conv_hvx_depth3.12, .Lfunc_end146-hydride.node.depthwise_conv_hvx_depth3.12
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.13,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.13 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.13
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.13,@function
hydride.node.depthwise_conv_hvx_depth3.13: // @hydride.node.depthwise_conv_hvx_depth3.13
// %bb.0:                               // %entry
	{
		v6 = vsplat(r0)
		v30 = vnot(v5)
		v31 = vnot(v4)
	}
	{
		v7 = vand(v2,v6)
		v6 = vand(v3,v6)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v0 = vsplat(r1)
		v5 = vand(v0,v30)
		v4 = vand(v1,v31)
	}
	{
		v1 = v0
	}
	{
		v1:0.w = vadd(v1:0.w,v5:4.w)
		jumpr r31
	}
.Lfunc_end147:
	.size	hydride.node.depthwise_conv_hvx_depth3.13, .Lfunc_end147-hydride.node.depthwise_conv_hvx_depth3.13
                                        // -- End function
	.section	.rodata,"a",@progbits
	.p2align	7                               // -- Begin function hydride.node.depthwise_conv_hvx_depth3.14
.LCPI148_0:
	.word	63                              // 0x3f
	.word	62                              // 0x3e
	.word	61                              // 0x3d
	.word	60                              // 0x3c
	.word	59                              // 0x3b
	.word	58                              // 0x3a
	.word	57                              // 0x39
	.word	56                              // 0x38
	.word	55                              // 0x37
	.word	54                              // 0x36
	.word	53                              // 0x35
	.word	52                              // 0x34
	.word	51                              // 0x33
	.word	50                              // 0x32
	.word	49                              // 0x31
	.word	48                              // 0x30
	.word	47                              // 0x2f
	.word	46                              // 0x2e
	.word	45                              // 0x2d
	.word	44                              // 0x2c
	.word	43                              // 0x2b
	.word	42                              // 0x2a
	.word	41                              // 0x29
	.word	40                              // 0x28
	.word	39                              // 0x27
	.word	38                              // 0x26
	.word	37                              // 0x25
	.word	36                              // 0x24
	.word	35                              // 0x23
	.word	34                              // 0x22
	.word	33                              // 0x21
	.word	32                              // 0x20
.LCPI148_1:
	.word	31                              // 0x1f
	.word	30                              // 0x1e
	.word	29                              // 0x1d
	.word	28                              // 0x1c
	.word	27                              // 0x1b
	.word	26                              // 0x1a
	.word	25                              // 0x19
	.word	24                              // 0x18
	.word	23                              // 0x17
	.word	22                              // 0x16
	.word	21                              // 0x15
	.word	20                              // 0x14
	.word	19                              // 0x13
	.word	18                              // 0x12
	.word	17                              // 0x11
	.word	16                              // 0x10
	.word	15                              // 0xf
	.word	14                              // 0xe
	.word	13                              // 0xd
	.word	12                              // 0xc
	.word	11                              // 0xb
	.word	10                              // 0xa
	.word	9                               // 0x9
	.word	8                               // 0x8
	.word	7                               // 0x7
	.word	6                               // 0x6
	.word	5                               // 0x5
	.word	4                               // 0x4
	.word	3                               // 0x3
	.word	2                               // 0x2
	.word	1                               // 0x1
	.word	0                               // 0x0
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.14,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.14
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.14,@function
hydride.node.depthwise_conv_hvx_depth3.14: // @hydride.node.depthwise_conv_hvx_depth3.14
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.LCPI148_0@PCREL)
		v4 = vsplat(r0)
	}
	{
		r2 = add(pc,##.LCPI148_1@PCREL)
		v5 = v4
	}
	{
		v2 = vmem(r1+#0)
	}
	{
		v3 = vmem(r2+#0)
	}
	{
		v1:0.w = vsub(v1:0.w,v3:2.w)
	}
	{
		v1:0.w = vsub(v5:4.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end148:
	.size	hydride.node.depthwise_conv_hvx_depth3.14, .Lfunc_end148-hydride.node.depthwise_conv_hvx_depth3.14
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.15,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.15 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.15
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.15,@function
hydride.node.depthwise_conv_hvx_depth3.15: // @hydride.node.depthwise_conv_hvx_depth3.15
// %bb.0:                               // %entry
	{
		v2 = vsplat(r0)
	}
	{
		v3 = v2
	}
	{
		v1:0.w = vsub(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end149:
	.size	hydride.node.depthwise_conv_hvx_depth3.15, .Lfunc_end149-hydride.node.depthwise_conv_hvx_depth3.15
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.16,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.16 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.16
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.16,@function
hydride.node.depthwise_conv_hvx_depth3.16: // @hydride.node.depthwise_conv_hvx_depth3.16
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end150:
	.size	hydride.node.depthwise_conv_hvx_depth3.16, .Lfunc_end150-hydride.node.depthwise_conv_hvx_depth3.16
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.17,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.17 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.17
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.17,@function
hydride.node.depthwise_conv_hvx_depth3.17: // @hydride.node.depthwise_conv_hvx_depth3.17
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end151:
	.size	hydride.node.depthwise_conv_hvx_depth3.17, .Lfunc_end151-hydride.node.depthwise_conv_hvx_depth3.17
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.18,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.18 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.18
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.18,@function
hydride.node.depthwise_conv_hvx_depth3.18: // @hydride.node.depthwise_conv_hvx_depth3.18
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end152:
	.size	hydride.node.depthwise_conv_hvx_depth3.18, .Lfunc_end152-hydride.node.depthwise_conv_hvx_depth3.18
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.19,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.19 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.19
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.19,@function
hydride.node.depthwise_conv_hvx_depth3.19: // @hydride.node.depthwise_conv_hvx_depth3.19
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end153:
	.size	hydride.node.depthwise_conv_hvx_depth3.19, .Lfunc_end153-hydride.node.depthwise_conv_hvx_depth3.19
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.20,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.20 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.20
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.20,@function
hydride.node.depthwise_conv_hvx_depth3.20: // @hydride.node.depthwise_conv_hvx_depth3.20
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end154:
	.size	hydride.node.depthwise_conv_hvx_depth3.20, .Lfunc_end154-hydride.node.depthwise_conv_hvx_depth3.20
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.21,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.21 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.21
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.21,@function
hydride.node.depthwise_conv_hvx_depth3.21: // @hydride.node.depthwise_conv_hvx_depth3.21
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end155:
	.size	hydride.node.depthwise_conv_hvx_depth3.21, .Lfunc_end155-hydride.node.depthwise_conv_hvx_depth3.21
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.22,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.22 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.22
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.22,@function
hydride.node.depthwise_conv_hvx_depth3.22: // @hydride.node.depthwise_conv_hvx_depth3.22
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end156:
	.size	hydride.node.depthwise_conv_hvx_depth3.22, .Lfunc_end156-hydride.node.depthwise_conv_hvx_depth3.22
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.23,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.23 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.23
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.23,@function
hydride.node.depthwise_conv_hvx_depth3.23: // @hydride.node.depthwise_conv_hvx_depth3.23
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end157:
	.size	hydride.node.depthwise_conv_hvx_depth3.23, .Lfunc_end157-hydride.node.depthwise_conv_hvx_depth3.23
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.24,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.24 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.24
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.24,@function
hydride.node.depthwise_conv_hvx_depth3.24: // @hydride.node.depthwise_conv_hvx_depth3.24
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end158:
	.size	hydride.node.depthwise_conv_hvx_depth3.24, .Lfunc_end158-hydride.node.depthwise_conv_hvx_depth3.24
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.25,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.25 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.25
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.25,@function
hydride.node.depthwise_conv_hvx_depth3.25: // @hydride.node.depthwise_conv_hvx_depth3.25
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end159:
	.size	hydride.node.depthwise_conv_hvx_depth3.25, .Lfunc_end159-hydride.node.depthwise_conv_hvx_depth3.25
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.26,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.26 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.26
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.26,@function
hydride.node.depthwise_conv_hvx_depth3.26: // @hydride.node.depthwise_conv_hvx_depth3.26
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end160:
	.size	hydride.node.depthwise_conv_hvx_depth3.26, .Lfunc_end160-hydride.node.depthwise_conv_hvx_depth3.26
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.27,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.27 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.27
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.27,@function
hydride.node.depthwise_conv_hvx_depth3.27: // @hydride.node.depthwise_conv_hvx_depth3.27
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end161:
	.size	hydride.node.depthwise_conv_hvx_depth3.27, .Lfunc_end161-hydride.node.depthwise_conv_hvx_depth3.27
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.28,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.28 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.28
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.28,@function
hydride.node.depthwise_conv_hvx_depth3.28: // @hydride.node.depthwise_conv_hvx_depth3.28
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end162:
	.size	hydride.node.depthwise_conv_hvx_depth3.28, .Lfunc_end162-hydride.node.depthwise_conv_hvx_depth3.28
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.29,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.29 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.29
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.29,@function
hydride.node.depthwise_conv_hvx_depth3.29: // @hydride.node.depthwise_conv_hvx_depth3.29
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end163:
	.size	hydride.node.depthwise_conv_hvx_depth3.29, .Lfunc_end163-hydride.node.depthwise_conv_hvx_depth3.29
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.30,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.30 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.30
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.30,@function
hydride.node.depthwise_conv_hvx_depth3.30: // @hydride.node.depthwise_conv_hvx_depth3.30
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end164:
	.size	hydride.node.depthwise_conv_hvx_depth3.30, .Lfunc_end164-hydride.node.depthwise_conv_hvx_depth3.30
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.31,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.31 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.31
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.31,@function
hydride.node.depthwise_conv_hvx_depth3.31: // @hydride.node.depthwise_conv_hvx_depth3.31
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end165:
	.size	hydride.node.depthwise_conv_hvx_depth3.31, .Lfunc_end165-hydride.node.depthwise_conv_hvx_depth3.31
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.32,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.32 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.32
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.32,@function
hydride.node.depthwise_conv_hvx_depth3.32: // @hydride.node.depthwise_conv_hvx_depth3.32
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end166:
	.size	hydride.node.depthwise_conv_hvx_depth3.32, .Lfunc_end166-hydride.node.depthwise_conv_hvx_depth3.32
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.33,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.33 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.33
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.33,@function
hydride.node.depthwise_conv_hvx_depth3.33: // @hydride.node.depthwise_conv_hvx_depth3.33
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end167:
	.size	hydride.node.depthwise_conv_hvx_depth3.33, .Lfunc_end167-hydride.node.depthwise_conv_hvx_depth3.33
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.34,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.34 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.34
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.34,@function
hydride.node.depthwise_conv_hvx_depth3.34: // @hydride.node.depthwise_conv_hvx_depth3.34
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end168:
	.size	hydride.node.depthwise_conv_hvx_depth3.34, .Lfunc_end168-hydride.node.depthwise_conv_hvx_depth3.34
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.35,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.35 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.35
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.35,@function
hydride.node.depthwise_conv_hvx_depth3.35: // @hydride.node.depthwise_conv_hvx_depth3.35
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end169:
	.size	hydride.node.depthwise_conv_hvx_depth3.35, .Lfunc_end169-hydride.node.depthwise_conv_hvx_depth3.35
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.36,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.36 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.36
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.36,@function
hydride.node.depthwise_conv_hvx_depth3.36: // @hydride.node.depthwise_conv_hvx_depth3.36
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end170:
	.size	hydride.node.depthwise_conv_hvx_depth3.36, .Lfunc_end170-hydride.node.depthwise_conv_hvx_depth3.36
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.37,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.37 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.37
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.37,@function
hydride.node.depthwise_conv_hvx_depth3.37: // @hydride.node.depthwise_conv_hvx_depth3.37
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end171:
	.size	hydride.node.depthwise_conv_hvx_depth3.37, .Lfunc_end171-hydride.node.depthwise_conv_hvx_depth3.37
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.38,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.38 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.38
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.38,@function
hydride.node.depthwise_conv_hvx_depth3.38: // @hydride.node.depthwise_conv_hvx_depth3.38
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end172:
	.size	hydride.node.depthwise_conv_hvx_depth3.38, .Lfunc_end172-hydride.node.depthwise_conv_hvx_depth3.38
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.39,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.39 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.39
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.39,@function
hydride.node.depthwise_conv_hvx_depth3.39: // @hydride.node.depthwise_conv_hvx_depth3.39
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end173:
	.size	hydride.node.depthwise_conv_hvx_depth3.39, .Lfunc_end173-hydride.node.depthwise_conv_hvx_depth3.39
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.40,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.40 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.40
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.40,@function
hydride.node.depthwise_conv_hvx_depth3.40: // @hydride.node.depthwise_conv_hvx_depth3.40
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end174:
	.size	hydride.node.depthwise_conv_hvx_depth3.40, .Lfunc_end174-hydride.node.depthwise_conv_hvx_depth3.40
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.41,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.41 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.41
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.41,@function
hydride.node.depthwise_conv_hvx_depth3.41: // @hydride.node.depthwise_conv_hvx_depth3.41
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end175:
	.size	hydride.node.depthwise_conv_hvx_depth3.41, .Lfunc_end175-hydride.node.depthwise_conv_hvx_depth3.41
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.42,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.42 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.42
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.42,@function
hydride.node.depthwise_conv_hvx_depth3.42: // @hydride.node.depthwise_conv_hvx_depth3.42
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end176:
	.size	hydride.node.depthwise_conv_hvx_depth3.42, .Lfunc_end176-hydride.node.depthwise_conv_hvx_depth3.42
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.43,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.43 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.43
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.43,@function
hydride.node.depthwise_conv_hvx_depth3.43: // @hydride.node.depthwise_conv_hvx_depth3.43
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end177:
	.size	hydride.node.depthwise_conv_hvx_depth3.43, .Lfunc_end177-hydride.node.depthwise_conv_hvx_depth3.43
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.44,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.44 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.44
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.44,@function
hydride.node.depthwise_conv_hvx_depth3.44: // @hydride.node.depthwise_conv_hvx_depth3.44
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end178:
	.size	hydride.node.depthwise_conv_hvx_depth3.44, .Lfunc_end178-hydride.node.depthwise_conv_hvx_depth3.44
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.45,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.45 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.45
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.45,@function
hydride.node.depthwise_conv_hvx_depth3.45: // @hydride.node.depthwise_conv_hvx_depth3.45
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end179:
	.size	hydride.node.depthwise_conv_hvx_depth3.45, .Lfunc_end179-hydride.node.depthwise_conv_hvx_depth3.45
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.46,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.46 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.46
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.46,@function
hydride.node.depthwise_conv_hvx_depth3.46: // @hydride.node.depthwise_conv_hvx_depth3.46
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end180:
	.size	hydride.node.depthwise_conv_hvx_depth3.46, .Lfunc_end180-hydride.node.depthwise_conv_hvx_depth3.46
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.47,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.47 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.47
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.47,@function
hydride.node.depthwise_conv_hvx_depth3.47: // @hydride.node.depthwise_conv_hvx_depth3.47
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end181:
	.size	hydride.node.depthwise_conv_hvx_depth3.47, .Lfunc_end181-hydride.node.depthwise_conv_hvx_depth3.47
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.48,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.48 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.48
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.48,@function
hydride.node.depthwise_conv_hvx_depth3.48: // @hydride.node.depthwise_conv_hvx_depth3.48
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end182:
	.size	hydride.node.depthwise_conv_hvx_depth3.48, .Lfunc_end182-hydride.node.depthwise_conv_hvx_depth3.48
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.49,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.49 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.49
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.49,@function
hydride.node.depthwise_conv_hvx_depth3.49: // @hydride.node.depthwise_conv_hvx_depth3.49
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end183:
	.size	hydride.node.depthwise_conv_hvx_depth3.49, .Lfunc_end183-hydride.node.depthwise_conv_hvx_depth3.49
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.50,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.50 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.50
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.50,@function
hydride.node.depthwise_conv_hvx_depth3.50: // @hydride.node.depthwise_conv_hvx_depth3.50
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end184:
	.size	hydride.node.depthwise_conv_hvx_depth3.50, .Lfunc_end184-hydride.node.depthwise_conv_hvx_depth3.50
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.51,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.51 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.51
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.51,@function
hydride.node.depthwise_conv_hvx_depth3.51: // @hydride.node.depthwise_conv_hvx_depth3.51
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end185:
	.size	hydride.node.depthwise_conv_hvx_depth3.51, .Lfunc_end185-hydride.node.depthwise_conv_hvx_depth3.51
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.52,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.52 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.52
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.52,@function
hydride.node.depthwise_conv_hvx_depth3.52: // @hydride.node.depthwise_conv_hvx_depth3.52
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end186:
	.size	hydride.node.depthwise_conv_hvx_depth3.52, .Lfunc_end186-hydride.node.depthwise_conv_hvx_depth3.52
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.53,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.53 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.53
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.53,@function
hydride.node.depthwise_conv_hvx_depth3.53: // @hydride.node.depthwise_conv_hvx_depth3.53
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end187:
	.size	hydride.node.depthwise_conv_hvx_depth3.53, .Lfunc_end187-hydride.node.depthwise_conv_hvx_depth3.53
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.54,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.54 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.54
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.54,@function
hydride.node.depthwise_conv_hvx_depth3.54: // @hydride.node.depthwise_conv_hvx_depth3.54
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end188:
	.size	hydride.node.depthwise_conv_hvx_depth3.54, .Lfunc_end188-hydride.node.depthwise_conv_hvx_depth3.54
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.55,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.55 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.55
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.55,@function
hydride.node.depthwise_conv_hvx_depth3.55: // @hydride.node.depthwise_conv_hvx_depth3.55
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end189:
	.size	hydride.node.depthwise_conv_hvx_depth3.55, .Lfunc_end189-hydride.node.depthwise_conv_hvx_depth3.55
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.56,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.56 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.56
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.56,@function
hydride.node.depthwise_conv_hvx_depth3.56: // @hydride.node.depthwise_conv_hvx_depth3.56
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end190:
	.size	hydride.node.depthwise_conv_hvx_depth3.56, .Lfunc_end190-hydride.node.depthwise_conv_hvx_depth3.56
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.57,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.57 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.57
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.57,@function
hydride.node.depthwise_conv_hvx_depth3.57: // @hydride.node.depthwise_conv_hvx_depth3.57
// %bb.0:                               // %entry
	{
		jumpr r31
	}
.Lfunc_end191:
	.size	hydride.node.depthwise_conv_hvx_depth3.57, .Lfunc_end191-hydride.node.depthwise_conv_hvx_depth3.57
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.58,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.58 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.58
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.58,@function
hydride.node.depthwise_conv_hvx_depth3.58: // @hydride.node.depthwise_conv_hvx_depth3.58
// %bb.0:                               // %entry
	{
		v6 = vsplat(r0)
		v30 = vnot(v5)
		v31 = vnot(v4)
	}
	{
		v7 = vand(v2,v6)
		v6 = vand(v3,v6)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v0 = vsplat(r1)
		v5 = vand(v0,v30)
		v4 = vand(v1,v31)
	}
	{
		v1 = v0
	}
	{
		v1:0.w = vadd(v1:0.w,v5:4.w)
		jumpr r31
	}
.Lfunc_end192:
	.size	hydride.node.depthwise_conv_hvx_depth3.58, .Lfunc_end192-hydride.node.depthwise_conv_hvx_depth3.58
                                        // -- End function
	.section	.rodata,"a",@progbits
	.p2align	7                               // -- Begin function hydride.node.depthwise_conv_hvx_depth3.59
.LCPI193_0:
	.word	63                              // 0x3f
	.word	62                              // 0x3e
	.word	61                              // 0x3d
	.word	60                              // 0x3c
	.word	59                              // 0x3b
	.word	58                              // 0x3a
	.word	57                              // 0x39
	.word	56                              // 0x38
	.word	55                              // 0x37
	.word	54                              // 0x36
	.word	53                              // 0x35
	.word	52                              // 0x34
	.word	51                              // 0x33
	.word	50                              // 0x32
	.word	49                              // 0x31
	.word	48                              // 0x30
	.word	47                              // 0x2f
	.word	46                              // 0x2e
	.word	45                              // 0x2d
	.word	44                              // 0x2c
	.word	43                              // 0x2b
	.word	42                              // 0x2a
	.word	41                              // 0x29
	.word	40                              // 0x28
	.word	39                              // 0x27
	.word	38                              // 0x26
	.word	37                              // 0x25
	.word	36                              // 0x24
	.word	35                              // 0x23
	.word	34                              // 0x22
	.word	33                              // 0x21
	.word	32                              // 0x20
.LCPI193_1:
	.word	31                              // 0x1f
	.word	30                              // 0x1e
	.word	29                              // 0x1d
	.word	28                              // 0x1c
	.word	27                              // 0x1b
	.word	26                              // 0x1a
	.word	25                              // 0x19
	.word	24                              // 0x18
	.word	23                              // 0x17
	.word	22                              // 0x16
	.word	21                              // 0x15
	.word	20                              // 0x14
	.word	19                              // 0x13
	.word	18                              // 0x12
	.word	17                              // 0x11
	.word	16                              // 0x10
	.word	15                              // 0xf
	.word	14                              // 0xe
	.word	13                              // 0xd
	.word	12                              // 0xc
	.word	11                              // 0xb
	.word	10                              // 0xa
	.word	9                               // 0x9
	.word	8                               // 0x8
	.word	7                               // 0x7
	.word	6                               // 0x6
	.word	5                               // 0x5
	.word	4                               // 0x4
	.word	3                               // 0x3
	.word	2                               // 0x2
	.word	1                               // 0x1
	.word	0                               // 0x0
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.59,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.59
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.59,@function
hydride.node.depthwise_conv_hvx_depth3.59: // @hydride.node.depthwise_conv_hvx_depth3.59
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.LCPI193_0@PCREL)
		v4 = vsplat(r0)
	}
	{
		r2 = add(pc,##.LCPI193_1@PCREL)
		v5 = v4
	}
	{
		v2 = vmem(r1+#0)
	}
	{
		v3 = vmem(r2+#0)
	}
	{
		v1:0.w = vsub(v1:0.w,v3:2.w)
	}
	{
		v1:0.w = vsub(v5:4.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end193:
	.size	hydride.node.depthwise_conv_hvx_depth3.59, .Lfunc_end193-hydride.node.depthwise_conv_hvx_depth3.59
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.60,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.60 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.60
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.60,@function
hydride.node.depthwise_conv_hvx_depth3.60: // @hydride.node.depthwise_conv_hvx_depth3.60
// %bb.0:                               // %entry
	{
		v2 = vsplat(r0)
	}
	{
		v3 = v2
	}
	{
		v1:0.w = vsub(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end194:
	.size	hydride.node.depthwise_conv_hvx_depth3.60, .Lfunc_end194-hydride.node.depthwise_conv_hvx_depth3.60
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.61,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.61 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.61
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.61,@function
hydride.node.depthwise_conv_hvx_depth3.61: // @hydride.node.depthwise_conv_hvx_depth3.61
// %bb.0:                               // %entry
	{
		v6 = vsplat(r0)
		v30 = vnot(v5)
		v31 = vnot(v4)
	}
	{
		v7 = vand(v2,v6)
		v6 = vand(v3,v6)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
	}
	{
		v0 = vsplat(r1)
		v5 = vand(v0,v30)
		v4 = vand(v1,v31)
	}
	{
		v1 = v0
	}
	{
		v1:0.w = vadd(v1:0.w,v5:4.w)
		jumpr r31
	}
.Lfunc_end195:
	.size	hydride.node.depthwise_conv_hvx_depth3.61, .Lfunc_end195-hydride.node.depthwise_conv_hvx_depth3.61
                                        // -- End function
	.section	.rodata,"a",@progbits
	.p2align	7                               // -- Begin function hydride.node.depthwise_conv_hvx_depth3.62
.LCPI196_0:
	.word	63                              // 0x3f
	.word	62                              // 0x3e
	.word	61                              // 0x3d
	.word	60                              // 0x3c
	.word	59                              // 0x3b
	.word	58                              // 0x3a
	.word	57                              // 0x39
	.word	56                              // 0x38
	.word	55                              // 0x37
	.word	54                              // 0x36
	.word	53                              // 0x35
	.word	52                              // 0x34
	.word	51                              // 0x33
	.word	50                              // 0x32
	.word	49                              // 0x31
	.word	48                              // 0x30
	.word	47                              // 0x2f
	.word	46                              // 0x2e
	.word	45                              // 0x2d
	.word	44                              // 0x2c
	.word	43                              // 0x2b
	.word	42                              // 0x2a
	.word	41                              // 0x29
	.word	40                              // 0x28
	.word	39                              // 0x27
	.word	38                              // 0x26
	.word	37                              // 0x25
	.word	36                              // 0x24
	.word	35                              // 0x23
	.word	34                              // 0x22
	.word	33                              // 0x21
	.word	32                              // 0x20
.LCPI196_1:
	.word	31                              // 0x1f
	.word	30                              // 0x1e
	.word	29                              // 0x1d
	.word	28                              // 0x1c
	.word	27                              // 0x1b
	.word	26                              // 0x1a
	.word	25                              // 0x19
	.word	24                              // 0x18
	.word	23                              // 0x17
	.word	22                              // 0x16
	.word	21                              // 0x15
	.word	20                              // 0x14
	.word	19                              // 0x13
	.word	18                              // 0x12
	.word	17                              // 0x11
	.word	16                              // 0x10
	.word	15                              // 0xf
	.word	14                              // 0xe
	.word	13                              // 0xd
	.word	12                              // 0xc
	.word	11                              // 0xb
	.word	10                              // 0xa
	.word	9                               // 0x9
	.word	8                               // 0x8
	.word	7                               // 0x7
	.word	6                               // 0x6
	.word	5                               // 0x5
	.word	4                               // 0x4
	.word	3                               // 0x3
	.word	2                               // 0x2
	.word	1                               // 0x1
	.word	0                               // 0x0
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.62,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.62
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.62,@function
hydride.node.depthwise_conv_hvx_depth3.62: // @hydride.node.depthwise_conv_hvx_depth3.62
// %bb.0:                               // %entry
	{
		r1 = add(pc,##.LCPI196_0@PCREL)
		v4 = vsplat(r0)
	}
	{
		r2 = add(pc,##.LCPI196_1@PCREL)
		v5 = v4
	}
	{
		v2 = vmem(r1+#0)
	}
	{
		v3 = vmem(r2+#0)
	}
	{
		v1:0.w = vsub(v1:0.w,v3:2.w)
	}
	{
		v1:0.w = vsub(v5:4.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end196:
	.size	hydride.node.depthwise_conv_hvx_depth3.62, .Lfunc_end196-hydride.node.depthwise_conv_hvx_depth3.62
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.63,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.63 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.63
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.63,@function
hydride.node.depthwise_conv_hvx_depth3.63: // @hydride.node.depthwise_conv_hvx_depth3.63
// %bb.0:                               // %entry
	{
		v2 = vsplat(r0)
	}
	{
		v3 = v2
	}
	{
		v1:0.w = vsub(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end197:
	.size	hydride.node.depthwise_conv_hvx_depth3.63, .Lfunc_end197-hydride.node.depthwise_conv_hvx_depth3.63
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.64,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.64 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.64
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.64,@function
hydride.node.depthwise_conv_hvx_depth3.64: // @hydride.node.depthwise_conv_hvx_depth3.64
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end198:
	.size	hydride.node.depthwise_conv_hvx_depth3.64, .Lfunc_end198-hydride.node.depthwise_conv_hvx_depth3.64
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.65,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.65 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.65
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.65,@function
hydride.node.depthwise_conv_hvx_depth3.65: // @hydride.node.depthwise_conv_hvx_depth3.65
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end199:
	.size	hydride.node.depthwise_conv_hvx_depth3.65, .Lfunc_end199-hydride.node.depthwise_conv_hvx_depth3.65
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.66,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.66 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.66
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.66,@function
hydride.node.depthwise_conv_hvx_depth3.66: // @hydride.node.depthwise_conv_hvx_depth3.66
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end200:
	.size	hydride.node.depthwise_conv_hvx_depth3.66, .Lfunc_end200-hydride.node.depthwise_conv_hvx_depth3.66
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.67,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.67 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.67
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.67,@function
hydride.node.depthwise_conv_hvx_depth3.67: // @hydride.node.depthwise_conv_hvx_depth3.67
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end201:
	.size	hydride.node.depthwise_conv_hvx_depth3.67, .Lfunc_end201-hydride.node.depthwise_conv_hvx_depth3.67
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.68,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.68 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.68
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.68,@function
hydride.node.depthwise_conv_hvx_depth3.68: // @hydride.node.depthwise_conv_hvx_depth3.68
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end202:
	.size	hydride.node.depthwise_conv_hvx_depth3.68, .Lfunc_end202-hydride.node.depthwise_conv_hvx_depth3.68
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.69,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.69 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.69
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.69,@function
hydride.node.depthwise_conv_hvx_depth3.69: // @hydride.node.depthwise_conv_hvx_depth3.69
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end203:
	.size	hydride.node.depthwise_conv_hvx_depth3.69, .Lfunc_end203-hydride.node.depthwise_conv_hvx_depth3.69
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.70,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.70 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.70
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.70,@function
hydride.node.depthwise_conv_hvx_depth3.70: // @hydride.node.depthwise_conv_hvx_depth3.70
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end204:
	.size	hydride.node.depthwise_conv_hvx_depth3.70, .Lfunc_end204-hydride.node.depthwise_conv_hvx_depth3.70
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.71,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.71 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.71
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.71,@function
hydride.node.depthwise_conv_hvx_depth3.71: // @hydride.node.depthwise_conv_hvx_depth3.71
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end205:
	.size	hydride.node.depthwise_conv_hvx_depth3.71, .Lfunc_end205-hydride.node.depthwise_conv_hvx_depth3.71
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.72,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.72 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.72
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.72,@function
hydride.node.depthwise_conv_hvx_depth3.72: // @hydride.node.depthwise_conv_hvx_depth3.72
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end206:
	.size	hydride.node.depthwise_conv_hvx_depth3.72, .Lfunc_end206-hydride.node.depthwise_conv_hvx_depth3.72
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.73,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.73 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.73
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.73,@function
hydride.node.depthwise_conv_hvx_depth3.73: // @hydride.node.depthwise_conv_hvx_depth3.73
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end207:
	.size	hydride.node.depthwise_conv_hvx_depth3.73, .Lfunc_end207-hydride.node.depthwise_conv_hvx_depth3.73
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.74,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.74 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.74
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.74,@function
hydride.node.depthwise_conv_hvx_depth3.74: // @hydride.node.depthwise_conv_hvx_depth3.74
// %bb.0:                               // %entry
	{
		v0.h = vsub(v0.h,v1.h)
		jumpr r31
	}
.Lfunc_end208:
	.size	hydride.node.depthwise_conv_hvx_depth3.74, .Lfunc_end208-hydride.node.depthwise_conv_hvx_depth3.74
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.75,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.75 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.75
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.75,@function
hydride.node.depthwise_conv_hvx_depth3.75: // @hydride.node.depthwise_conv_hvx_depth3.75
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end209:
	.size	hydride.node.depthwise_conv_hvx_depth3.75, .Lfunc_end209-hydride.node.depthwise_conv_hvx_depth3.75
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.76,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.76 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.76
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.76,@function
hydride.node.depthwise_conv_hvx_depth3.76: // @hydride.node.depthwise_conv_hvx_depth3.76
// %bb.0:                               // %entry
	{
		v3:2.w = vunpack(v2.h)
	}
	{
		v1:0.w = vadd(v3:2.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end210:
	.size	hydride.node.depthwise_conv_hvx_depth3.76, .Lfunc_end210-hydride.node.depthwise_conv_hvx_depth3.76
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.77,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.77 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.77
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.77,@function
hydride.node.depthwise_conv_hvx_depth3.77: // @hydride.node.depthwise_conv_hvx_depth3.77
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end211:
	.size	hydride.node.depthwise_conv_hvx_depth3.77, .Lfunc_end211-hydride.node.depthwise_conv_hvx_depth3.77
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.78,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.78 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.78
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.78,@function
hydride.node.depthwise_conv_hvx_depth3.78: // @hydride.node.depthwise_conv_hvx_depth3.78
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vsub(v1:0.w,v7:6.w)
		jumpr r31
	}
.Lfunc_end212:
	.size	hydride.node.depthwise_conv_hvx_depth3.78, .Lfunc_end212-hydride.node.depthwise_conv_hvx_depth3.78
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.79,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.79 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.79
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.79,@function
hydride.node.depthwise_conv_hvx_depth3.79: // @hydride.node.depthwise_conv_hvx_depth3.79
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end213:
	.size	hydride.node.depthwise_conv_hvx_depth3.79, .Lfunc_end213-hydride.node.depthwise_conv_hvx_depth3.79
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.80,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.80 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.80
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.80,@function
hydride.node.depthwise_conv_hvx_depth3.80: // @hydride.node.depthwise_conv_hvx_depth3.80
// %bb.0:                               // %entry
	{
		allocframe(r29,#1152):raw
	}
	{
		r29 = and(r29,#-128)
		r2 = add(r30,#392)
		r3 = add(r30,#8)
		r11 = add(r30,#1032)
	}
	{
		r0 = add(r29,#1024)
		v5 = v0
		v17 = vmem(r2+#0)
		vmem(r29+#2) = v5
	}
	{
		r2 = add(r30,#1288)
		r3 = add(r30,#1160)
		v18 = vmem(r3+#0)
		vmem(r29+#1) = v15
	}
	{
		v25.w = vmpyieo(v1.h,v5.h)
		r0 = add(r30,#2056)
		v0 = vmem(r11+#0)
		vmem(r0+#0) = v11
	}
	{
		r1 = add(r30,#264)
		r15 = add(r30,#2312)
		v15 = v4
		vmem(r29+#5) = v6
	}                                       // 128-byte Folded Spill
	{
		r14 = add(r30,#2568)
		r10 = add(r30,#1416)
		v4 = vmem(r2+#0)
		vmem(r29+#7) = v10
	}
	{
		r9 = add(r30,#1800)
		v11:10 = vcombine(v9,v8)
		v6 = vmem(r3+#0)
		vmem(r29+#3) = v2
	}
	{
		v25.w += vmpyie(v1.w,v5.h)
		r1 = add(r30,#1544)
		r28 = add(r30,#2440)
		v16 = vmem(r1+#0)
	}
	{
		v1.w = vmpyieo(v6.h,v0.h)
		r12 = add(r30,#1928)
		r13 = add(r30,#1672)
		v20 = vmem(r0+#0)
	}
	{
		v1.w += vmpyie(v6.w,v0.h)
		r0 = add(r30,#2696)
		v21 = vmem(r0+#1)
		vmem(r29+#6) = v7
	}
	{
		v0.w = vmpyieo(v2.h,v4.h)
		r5 = add(r30,#776)
		r7 = add(r30,#520)
		v2.cur = vmem(r10+#0)
	}
	{
		r6 = add(r30,#904)
		r8 = add(r30,#648)
		v27 = vmem(r15+#0)
		vmem(r29+#4) = v3
	}
	{
		v0.w += vmpyie(v2.w,v4.h)
		r4 = add(r30,#136)
		r2 = add(r29,#1024)
		v8 = vmem(r14+#0)
	}
	{
		v3.w = vmpyieo(v28.h,v27.h)
		v28.cur = vmem(r28+#0)
	}
	{
		v2.w = vmpyieo(v7.h,v8.h)
		v7.cur = vmem(r0+#0)
	}
	{
		v3.w += vmpyie(v28.w,v27.h)
		v29 = vmem(r9+#0)
	}
	{
		v2.w += vmpyie(v7.w,v8.h)
		v31 = vmem(r1+#0)
	}
	{
		v4.w = vmpyieo(v30.h,v29.h)
		v3:2.w = vadd(v3:2.w,v21:20.w)
		v30.cur = vmem(r12+#0)
	}
	{
		v5.w = vmpyieo(v9.h,v31.h)
		v8.w = vmpyieo(v17.h,v16.h)
		v9.cur = vmem(r13+#0)
	}
	{
		v4.w += vmpyie(v30.w,v29.h)
		v19 = vmem(r5+#0)
	}
	{
		v5.w += vmpyie(v9.w,v31.h)
		v24 = vmem(r7+#0)
	}
	{
		v6.w = vmpyieo(v23.h,v19.h)
		v3:2.w = vadd(v3:2.w,v5:4.w)
		v23.cur = vmem(r6+#0)
	}
	{
		v7.w = vmpyieo(v26.h,v24.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
		v26.cur = vmem(r8+#0)
	}
	{
		v9.w = vmpyieo(v22.h,v18.h)
		v21.w = vmpyieo(v13.h,v12.h)
		v22.cur = vmem(r4+#0)
	}
	{
		v7.w += vmpyie(v26.w,v24.h)
		v2 = vmem(r29+#1)
	}                                       // 128-byte Folded Reload
	{
		v6.w += vmpyie(v23.w,v19.h)
		v4 = vmem(r2+#0)
	}                                       // 128-byte Folded Reload
	{
		v20.w = vmpyieo(v2.h,v14.h)
		v1:0.w = vadd(v1:0.w,v7:6.w)
		v27 = vmem(r29+#7)
	}                                       // 128-byte Folded Reload
	{
		v9.w += vmpyie(v22.w,v18.h)
		v28 = vmem(r29+#6)
	}                                       // 128-byte Folded Reload
	{
		v8.w += vmpyie(v17.w,v16.h)
		v26 = vmem(r29+#2)
	}                                       // 128-byte Folded Reload
	{
		v3.w = vmpyieo(v11.h,v10.h)
		v1:0.w = vadd(v1:0.w,v9:8.w)
		v29 = vmem(r29+#5)
	}                                       // 128-byte Folded Reload
	{
		v20.w += vmpyie(v2.w,v14.h)
		v30 = vmem(r29+#4)
	}                                       // 128-byte Folded Reload
	{
		v2.w = vmpyieo(v4.h,v27.h)
		v5.w = vmpyieo(v26.h,v15.h)
		v31 = vmem(r29+#3)
	}                                       // 128-byte Folded Reload
	{
		v21.w += vmpyie(v13.w,v12.h)
	}
	{
		v2.w += vmpyie(v4.w,v27.h)
		v1:0.w = vadd(v21:20.w,v1:0.w)
	}
	{
		v4.w = vmpyieo(v28.h,v29.h)
		v24.w = vmpyieo(v30.h,v31.h)
	}
	{
		v3.w += vmpyie(v11.w,v10.h)
	}
	{
		v5.w += vmpyie(v26.w,v15.h)
		v1:0.w = vadd(v1:0.w,v3:2.w)
	}
	{
		v4.w += vmpyie(v28.w,v29.h)
	}
	{
		v24.w += vmpyie(v30.w,v31.h)
		v1:0.w = vadd(v1:0.w,v5:4.w)
	}
	{
		v1:0.w = vadd(v25:24.w,v1:0.w)
		r31:30 = dealloc_return(r30):raw
	}
.Lfunc_end214:
	.size	hydride.node.depthwise_conv_hvx_depth3.80, .Lfunc_end214-hydride.node.depthwise_conv_hvx_depth3.80
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.81,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.81 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.81
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.81,@function
hydride.node.depthwise_conv_hvx_depth3.81: // @hydride.node.depthwise_conv_hvx_depth3.81
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end215:
	.size	hydride.node.depthwise_conv_hvx_depth3.81, .Lfunc_end215-hydride.node.depthwise_conv_hvx_depth3.81
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.82,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.82 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.82
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.82,@function
hydride.node.depthwise_conv_hvx_depth3.82: // @hydride.node.depthwise_conv_hvx_depth3.82
// %bb.0:                               // %entry
	{
		v7.w = vmpyieo(v3.h,v2.h)
		v6.w = vmpyieo(v5.h,v4.h)
	}
	{
		v7.w += vmpyie(v3.w,v2.h)
	}
	{
		v6.w += vmpyie(v5.w,v4.h)
	}
	{
		v1:0.w = vadd(v7:6.w,v1:0.w)
		jumpr r31
	}
.Lfunc_end216:
	.size	hydride.node.depthwise_conv_hvx_depth3.82, .Lfunc_end216-hydride.node.depthwise_conv_hvx_depth3.82
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.83,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.83 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.83
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.83,@function
hydride.node.depthwise_conv_hvx_depth3.83: // @hydride.node.depthwise_conv_hvx_depth3.83
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end217:
	.size	hydride.node.depthwise_conv_hvx_depth3.83, .Lfunc_end217-hydride.node.depthwise_conv_hvx_depth3.83
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.84,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.84 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.84
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.84,@function
hydride.node.depthwise_conv_hvx_depth3.84: // @hydride.node.depthwise_conv_hvx_depth3.84
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end218:
	.size	hydride.node.depthwise_conv_hvx_depth3.84, .Lfunc_end218-hydride.node.depthwise_conv_hvx_depth3.84
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.85,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.85 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.85
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.85,@function
hydride.node.depthwise_conv_hvx_depth3.85: // @hydride.node.depthwise_conv_hvx_depth3.85
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end219:
	.size	hydride.node.depthwise_conv_hvx_depth3.85, .Lfunc_end219-hydride.node.depthwise_conv_hvx_depth3.85
                                        // -- End function
	.section	.text.hydride.node.depthwise_conv_hvx_depth3.86,"ax",@progbits
	.globl	hydride.node.depthwise_conv_hvx_depth3.86 // -- Begin function hydride.node.depthwise_conv_hvx_depth3.86
	.p2align	4
	.type	hydride.node.depthwise_conv_hvx_depth3.86,@function
hydride.node.depthwise_conv_hvx_depth3.86: // @hydride.node.depthwise_conv_hvx_depth3.86
// %bb.0:                               // %entry
	{
		v11 = vsplat(r2)
		v10 = vsplat(r0)
		r5 = #32767
		r2 = #-32768
	}
	{
		v27 = vsplat(r3)
		v28 = vsplat(r4)
		v0.w = vadd(v0.w,v10.w):sat
		v1.w = vadd(v1.w,v11.w):sat
	}
	{
		v12 = vsplat(r1)
		v29 = vsplat(r5)
		v3.w = vadd(v3.w,v27.w):sat
		v4.w = vadd(v4.w,v28.w):sat
	}
	{
		v30 = vsplat(r2)
	}
	{
		v0.w = vasr(v0.w,v12.w)
	}
	{
		v1.w = vasr(v1.w,v12.w)
		v0.w = vmin(v29.w,v0.w)
	}
	{
		v3.w = vasr(v3.w,v12.w)
		v1.w = vmin(v1.w,v29.w)
		v0.w = vmax(v30.w,v0.w)
	}
	{
		v4.w = vasr(v4.w,v12.w)
		v3.w = vmin(v29.w,v3.w)
		v1.w = vmax(v30.w,v1.w)
	}
	{
		v4.w = vmin(v4.w,v29.w)
		v3.w = vmax(v30.w,v3.w)
	}
	{
		v0.h = vpacke(v0.w,v1.w)
		v4.w = vmax(v30.w,v4.w)
	}
	{
		v0.h = vadd(v2.h,v0.h):sat
	}
	{
		v1.h = vpacke(v3.w,v4.w)
		v0.h = vmin(v0.h,v6.h)
	}
	{
		v1.h = vadd(v5.h,v1.h):sat
	}
	{
		v31.h = vmin(v7.h,v1.h)
		v1.h = vmax(v8.h,v0.h)
	}
	{
		v0.h = vmax(v9.h,v31.h)
		jumpr r31
	}
.Lfunc_end220:
	.size	hydride.node.depthwise_conv_hvx_depth3.86, .Lfunc_end220-hydride.node.depthwise_conv_hvx_depth3.86
                                        // -- End function
	.type	_ZN6Halide7Runtime8Internal11buf_is_usedE,@object // @_ZN6Halide7Runtime8Internal11buf_is_usedE
	.section	.bss,"aw",@nobits
	.weak	_ZN6Halide7Runtime8Internal11buf_is_usedE
	.p2align	2
_ZN6Halide7Runtime8Internal11buf_is_usedE:
	.space	40
	.size	_ZN6Halide7Runtime8Internal11buf_is_usedE, 40

	.type	_ZN6Halide7Runtime8Internal7mem_bufE,@object // @_ZN6Halide7Runtime8Internal7mem_bufE
	.weak	_ZN6Halide7Runtime8Internal7mem_bufE
	.p2align	2
_ZN6Halide7Runtime8Internal7mem_bufE:
	.space	40
	.size	_ZN6Halide7Runtime8Internal7mem_bufE, 40

	.type	_ZN6Halide7Runtime8Internal13custom_mallocE,@object // @_ZN6Halide7Runtime8Internal13custom_mallocE
	.data
	.weak	_ZN6Halide7Runtime8Internal13custom_mallocE
	.p2align	2
_ZN6Halide7Runtime8Internal13custom_mallocE:
	.word	halide_default_malloc
	.size	_ZN6Halide7Runtime8Internal13custom_mallocE, 4

	.type	_ZN6Halide7Runtime8Internal11custom_freeE,@object // @_ZN6Halide7Runtime8Internal11custom_freeE
	.weak	_ZN6Halide7Runtime8Internal11custom_freeE
	.p2align	2
_ZN6Halide7Runtime8Internal11custom_freeE:
	.word	halide_default_free
	.size	_ZN6Halide7Runtime8Internal11custom_freeE, 4

	.type	.L.str,@object                  // @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.string	"custom allocators not supported on Hexagon.\n"
	.size	.L.str, 45

	.section	.fini_array,"aw",@fini_array
	.p2align	2
	.word	_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv
	.type	_ZN6Halide7Runtime8Internal14custom_do_taskE,@object // @_ZN6Halide7Runtime8Internal14custom_do_taskE
	.data
	.weak	_ZN6Halide7Runtime8Internal14custom_do_taskE
	.p2align	2
_ZN6Halide7Runtime8Internal14custom_do_taskE:
	.word	halide_default_do_task
	.size	_ZN6Halide7Runtime8Internal14custom_do_taskE, 4

	.type	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE,@object // @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE
	.weak	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE
	.p2align	2
_ZN6Halide7Runtime8Internal19custom_do_loop_taskE:
	.word	halide_default_do_loop_task
	.size	_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, 4

	.type	_ZN6Halide7Runtime8Internal17custom_do_par_forE,@object // @_ZN6Halide7Runtime8Internal17custom_do_par_forE
	.weak	_ZN6Halide7Runtime8Internal17custom_do_par_forE
	.p2align	2
_ZN6Halide7Runtime8Internal17custom_do_par_forE:
	.word	halide_default_do_par_for
	.size	_ZN6Halide7Runtime8Internal17custom_do_par_forE, 4

	.type	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE,@object // @_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE
	.weak	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE
	.p2align	2
_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE:
	.word	halide_default_do_parallel_tasks
	.size	_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE, 4

	.type	.L.str.1,@object                // @.str.1
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.1:
	.string	"halide_default_do_parallel_tasks not implemented on this platform."
	.size	.L.str.1, 67

	.type	_ZN6Halide7Runtime8Internal21custom_semaphore_initE,@object // @_ZN6Halide7Runtime8Internal21custom_semaphore_initE
	.data
	.weak	_ZN6Halide7Runtime8Internal21custom_semaphore_initE
	.p2align	2
_ZN6Halide7Runtime8Internal21custom_semaphore_initE:
	.word	halide_default_semaphore_init
	.size	_ZN6Halide7Runtime8Internal21custom_semaphore_initE, 4

	.type	.L.str.1.2,@object              // @.str.1.2
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.1.2:
	.string	"halide_default_semaphore_init not implemented on this platform."
	.size	.L.str.1.2, 64

	.type	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE,@object // @_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE
	.data
	.weak	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE
	.p2align	2
_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE:
	.word	halide_default_semaphore_try_acquire
	.size	_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE, 4

	.type	.L.str.3,@object                // @.str.3
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.3:
	.string	"halide_default_semaphore_try_acquire not implemented on this platform."
	.size	.L.str.3, 71

	.type	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE,@object // @_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE
	.data
	.weak	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE
	.p2align	2
_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE:
	.word	halide_default_semaphore_release
	.size	_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE, 4

	.type	.L.str.2,@object                // @.str.2
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.2:
	.string	"halide_default_semaphore_release not implemented on this platform."
	.size	.L.str.2, 67

	.type	_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE,@object // @_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE
	.section	.bss,"aw",@nobits
	.weak	_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE
	.p2align	2
_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE:
	.space	4
	.size	_ZN6Halide7Runtime8Internal23halide_fake_mutex_arrayE, 4

	.type	.L.str.4,@object                // @.str.4
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.4:
	.string	"halide_spawn_thread not implemented on this platform."
	.size	.L.str.4, 54

	.type	.L.str.5,@object                // @.str.5
.L.str.5:
	.string	"halide_join_thread not implemented on this platform."
	.size	.L.str.5, 53

	.type	.L.str.6,@object                // @.str.6
.L.str.6:
	.string	"halide_set_num_threads: only supports a value of 1 on this platform."
	.size	.L.str.6, 69

	.type	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE,@object // @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.section	.bss,"aw",@nobits
	.weak	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.p2align	2
_ZN6Halide7Runtime8Internal17halide_gpu_deviceE:
	.word	0                               // 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE,@object // @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE:
	.byte	0                               // 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, 1

	.type	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE,@object // @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE:
	.byte	0                               // 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, 1

	.type	.L.str.7,@object                // @.str.7
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.7:
	.string	"HL_GPU_DEVICE"
	.size	.L.str.7, 14

	.type	.L.str.8,@object                // @.str.8
.L.str.8:
	.string	"<nullptr>"
	.size	.L.str.8, 10

	.type	.L.str.1.9,@object              // @.str.1.9
.L.str.1.9:
	.string	"-nan"
	.size	.L.str.1.9, 5

	.type	.L.str.2.10,@object             // @.str.2.10
.L.str.2.10:
	.string	"nan"
	.size	.L.str.2.10, 4

	.type	.L.str.3.11,@object             // @.str.3.11
.L.str.3.11:
	.string	"-inf"
	.size	.L.str.3.11, 5

	.type	.L.str.4.12,@object             // @.str.4.12
.L.str.4.12:
	.string	"inf"
	.size	.L.str.4.12, 4

	.type	.L.str.5.13,@object             // @.str.5.13
.L.str.5.13:
	.string	"-0.000000e+00"
	.size	.L.str.5.13, 14

	.type	.L.str.6.14,@object             // @.str.6.14
.L.str.6.14:
	.string	"0.000000e+00"
	.size	.L.str.6.14, 13

	.type	.L.str.7.15,@object             // @.str.7.15
.L.str.7.15:
	.string	"-0.000000"
	.size	.L.str.7.15, 10

	.type	.L.str.8.16,@object             // @.str.8.16
.L.str.8.16:
	.string	"0.000000"
	.size	.L.str.8.16, 9

	.type	.L.str.9,@object                // @.str.9
.L.str.9:
	.string	"-"
	.size	.L.str.9, 2

	.type	.L.str.11,@object               // @.str.11
.L.str.11:
	.string	"e+"
	.size	.L.str.11, 3

	.type	.L.str.12,@object               // @.str.12
.L.str.12:
	.string	"e-"
	.size	.L.str.12, 3

	.type	.L.str.13,@object               // @.str.13
.L.str.13:
	.string	"0123456789abcdef"
	.size	.L.str.13, 17

	.type	.L.str.18,@object               // @.str.18
.L.str.18:
	.string	"bad_type_code"
	.size	.L.str.18, 14

	.type	.L.str.17,@object               // @.str.17
.L.str.17:
	.string	"handle"
	.size	.L.str.17, 7

	.type	.L.str.16,@object               // @.str.16
.L.str.16:
	.string	"float"
	.size	.L.str.16, 6

	.type	.L.str.15,@object               // @.str.15
.L.str.15:
	.string	"uint"
	.size	.L.str.15, 5

	.type	.L.str.14,@object               // @.str.14
.L.str.14:
	.string	"int"
	.size	.L.str.14, 4

	.type	.L.str.19,@object               // @.str.19
.L.str.19:
	.string	"x"
	.size	.L.str.19, 2

	.type	.L.str.20,@object               // @.str.20
.L.str.20:
	.string	"nullptr"
	.size	.L.str.20, 8

	.type	.L.str.21,@object               // @.str.21
.L.str.21:
	.string	"buffer("
	.size	.L.str.21, 8

	.type	.L.str.23,@object               // @.str.23
.L.str.23:
	.string	", {"
	.size	.L.str.23, 4

	.type	.L.str.24,@object               // @.str.24
.L.str.24:
	.string	"}"
	.size	.L.str.24, 2

	.type	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE,@object // @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE
	.data
	.weak	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE
_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE:
	.byte	1                               // 0x1
	.size	_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, 1

	.type	_ZN6Halide7Runtime8Internal21allocation_pools_lockE,@object // @_ZN6Halide7Runtime8Internal21allocation_pools_lockE
	.section	.bss,"aw",@nobits
	.weak	_ZN6Halide7Runtime8Internal21allocation_pools_lockE
	.p2align	2
_ZN6Halide7Runtime8Internal21allocation_pools_lockE:
	.space	4
	.size	_ZN6Halide7Runtime8Internal21allocation_pools_lockE, 4

	.type	_ZN6Halide7Runtime8Internal23device_allocation_poolsE,@object // @_ZN6Halide7Runtime8Internal23device_allocation_poolsE
	.weak	_ZN6Halide7Runtime8Internal23device_allocation_poolsE
	.p2align	2
_ZN6Halide7Runtime8Internal23device_allocation_poolsE:
	.word	0
	.size	_ZN6Halide7Runtime8Internal23device_allocation_poolsE, 4

	.type	_ZN6Halide7Runtime8Internal17device_copy_mutexE,@object // @_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.weak	_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.p2align	2
_ZN6Halide7Runtime8Internal17device_copy_mutexE:
	.space	4
	.size	_ZN6Halide7Runtime8Internal17device_copy_mutexE, 4

	.type	.L.str.6.17,@object             // @.str.6.17
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.6.17:
	.string	"halide_copy_to_host"
	.size	.L.str.6.17, 20

	.type	.L.str.7.18,@object             // @.str.7.18
.L.str.7.18:
	.string	"halide_copy_to_device"
	.size	.L.str.7.18, 22

	.type	.L.str.9.19,@object             // @.str.9.19
.L.str.9.19:
	.string	"halide_copy_to_device does not support switching interfaces\n"
	.size	.L.str.9.19, 61

	.type	.L.str.17.20,@object            // @.str.17.20
.L.str.17.20:
	.string	"halide_device_malloc"
	.size	.L.str.17.20, 21

	.type	.L.str.20.21,@object            // @.str.20.21
.L.str.20.21:
	.string	"halide_device_malloc doesn't support switching interfaces\n"
	.size	.L.str.20.21, 59

	.type	.L.str.16.22,@object            // @.str.16.22
.L.str.16.22:
	.string	"halide_device_sync"
	.size	.L.str.16.22, 19

	.type	.L.str.21.23,@object            // @.str.21.23
.L.str.21.23:
	.string	"halide_device_free"
	.size	.L.str.21.23, 19

	.type	.L.str.22.24,@object            // @.str.22.24
.L.str.22.24:
	.string	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:252 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.22.24, 157

	.type	.L.str.23.25,@object            // @.str.23.25
.L.str.23.25:
	.string	"halide_device_and_host_malloc"
	.size	.L.str.23.25, 30

	.type	.L.str.25.26,@object            // @.str.25.26
.L.str.25.26:
	.string	"halide_device_and_host_malloc doesn't support switching interfaces\n"
	.size	.L.str.25.26, 68

	.type	.L.str.26,@object               // @.str.26
.L.str.26:
	.string	"allocating host and device memory failed\n"
	.size	.L.str.26, 42

	.type	.L.str.27,@object               // @.str.27
.L.str.27:
	.string	"halide_device_and_host_free"
	.size	.L.str.27, 28

	.type	.L.str.28,@object               // @.str.28
.L.str.28:
	.string	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:317 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.28, 157

	.type	.L.str.29,@object               // @.str.29
.L.str.29:
	.string	"halide_default_device_and_host_malloc"
	.size	.L.str.29, 38

	.type	.L.str.30,@object               // @.str.30
.L.str.30:
	.string	"halide_default_device_and_host_free"
	.size	.L.str.30, 36

	.type	.L.str.31,@object               // @.str.31
.L.str.31:
	.string	"halide_device_wrap_native"
	.size	.L.str.31, 26

	.type	.L.str.32,@object               // @.str.32
.L.str.32:
	.string	"halide_device_wrap_native doesn't support switching interfaces\n"
	.size	.L.str.32, 64

	.type	.L.str.33,@object               // @.str.33
.L.str.33:
	.string	"halide_device_detach_native"
	.size	.L.str.33, 28

	.type	.L.str.34,@object               // @.str.34
.L.str.34:
	.string	"/home/arnoor2/Racket/TensorSynth/Rosette-experiments/frontends/halide/src/runtime/device_interface.cpp:403 halide_abort_if_false() failed: buf->device == 0\n"
	.size	.L.str.34, 157

	.type	.L.str.35,@object               // @.str.35
.L.str.35:
	.string	"halide_default_device_detach_native"
	.size	.L.str.35, 36

	.type	.L.str.41,@object               // @.str.41
.L.str.41:
	.string	"halide_buffer_copy does not support switching device interfaces"
	.size	.L.str.41, 64

	.type	.L.str.58,@object               // @.str.58
.L.str.58:
	.string	"device_interface does not support cropping\n"
	.size	.L.str.58, 44

	.type	.L.str.59,@object               // @.str.59
.L.str.59:
	.string	"device_interface does not support slicing\n"
	.size	.L.str.59, 43

	.type	.L.str.60,@object               // @.str.60
.L.str.60:
	.string	"destination buffer already has a device allocation\n"
	.size	.L.str.60, 52

	.type	.L.str.61,@object               // @.str.61
.L.str.61:
	.string	"src and dst must have identical dimensionality\n"
	.size	.L.str.61, 48

	.type	.L.str.64,@object               // @.str.64
.L.str.64:
	.string	"dst must have exactly one fewer dimension than src\n"
	.size	.L.str.64, 52

	.type	.L.str.36,@object               // @.str.36
.L.str.36:
	.string	"Bounds inference call to external stage "
	.size	.L.str.36, 41

	.type	.L.str.1.37,@object             // @.str.1.37
.L.str.1.37:
	.string	" returned non-zero value: "
	.size	.L.str.1.37, 27

	.type	.L.str.2.38,@object             // @.str.2.38
.L.str.2.38:
	.string	"Call to external stage "
	.size	.L.str.2.38, 24

	.type	.L.str.3.39,@object             // @.str.3.39
.L.str.3.39:
	.string	"Bounds given for "
	.size	.L.str.3.39, 18

	.type	.L.str.4.40,@object             // @.str.4.40
.L.str.4.40:
	.string	" in "
	.size	.L.str.4.40, 5

	.type	.L.str.5.41,@object             // @.str.5.41
.L.str.5.41:
	.string	" (from "
	.size	.L.str.5.41, 8

	.type	.L.str.6.42,@object             // @.str.6.42
.L.str.6.42:
	.string	" to "
	.size	.L.str.6.42, 5

	.type	.L.str.7.43,@object             // @.str.7.43
.L.str.7.43:
	.string	") do not cover required region (from "
	.size	.L.str.7.43, 38

	.type	.L.str.8.44,@object             // @.str.8.44
.L.str.8.44:
	.string	")"
	.size	.L.str.8.44, 2

	.type	.L.str.9.45,@object             // @.str.9.45
.L.str.9.45:
	.string	" has type "
	.size	.L.str.9.45, 11

	.type	.L.str.10.46,@object            // @.str.10.46
.L.str.10.46:
	.string	" but type of the buffer passed in is "
	.size	.L.str.10.46, 38

	.type	.L.str.11.47,@object            // @.str.11.47
.L.str.11.47:
	.string	" requires a buffer of exactly "
	.size	.L.str.11.47, 31

	.type	.L.str.12.48,@object            // @.str.12.48
.L.str.12.48:
	.string	" dimensions, but the buffer passed in has "
	.size	.L.str.12.48, 43

	.type	.L.str.13.49,@object            // @.str.13.49
.L.str.13.49:
	.string	" dimensions"
	.size	.L.str.13.49, 12

	.type	.L.str.14.50,@object            // @.str.14.50
.L.str.14.50:
	.string	" is accessed at "
	.size	.L.str.14.50, 17

	.type	.L.str.15.51,@object            // @.str.15.51
.L.str.15.51:
	.string	", which is before the min ("
	.size	.L.str.15.51, 28

	.type	.L.str.16.52,@object            // @.str.16.52
.L.str.16.52:
	.string	") in dimension "
	.size	.L.str.16.52, 16

	.type	.L.str.17.53,@object            // @.str.17.53
.L.str.17.53:
	.string	", which is beyond the max ("
	.size	.L.str.17.53, 28

	.type	.L.str.18.54,@object            // @.str.18.54
.L.str.18.54:
	.string	"Total allocation for buffer "
	.size	.L.str.18.54, 29

	.type	.L.str.19.55,@object            // @.str.19.55
.L.str.19.55:
	.string	" is "
	.size	.L.str.19.55, 5

	.type	.L.str.20.56,@object            // @.str.20.56
.L.str.20.56:
	.string	", which exceeds the maximum size of "
	.size	.L.str.20.56, 37

	.type	.L.str.21.57,@object            // @.str.21.57
.L.str.21.57:
	.string	"The extents for buffer "
	.size	.L.str.21.57, 24

	.type	.L.str.22.58,@object            // @.str.22.58
.L.str.22.58:
	.string	" dimension "
	.size	.L.str.22.58, 12

	.type	.L.str.23.59,@object            // @.str.23.59
.L.str.23.59:
	.string	" is negative ("
	.size	.L.str.23.59, 15

	.type	.L.str.24.60,@object            // @.str.24.60
.L.str.24.60:
	.string	"Product of extents for buffer "
	.size	.L.str.24.60, 31

	.type	.L.str.25.61,@object            // @.str.25.61
.L.str.25.61:
	.string	"Applying the constraints on "
	.size	.L.str.25.61, 29

	.type	.L.str.26.62,@object            // @.str.26.62
.L.str.26.62:
	.string	" to the required region made it smaller in dimension "
	.size	.L.str.26.62, 54

	.type	.L.str.27.63,@object            // @.str.27.63
.L.str.27.63:
	.string	". "
	.size	.L.str.27.63, 3

	.type	.L.str.28.64,@object            // @.str.28.64
.L.str.28.64:
	.string	"Required size: "
	.size	.L.str.28.64, 16

	.type	.L.str.29.65,@object            // @.str.29.65
.L.str.29.65:
	.string	"Constrained size: "
	.size	.L.str.29.65, 19

	.type	.L.str.30.66,@object            // @.str.30.66
.L.str.30.66:
	.string	"."
	.size	.L.str.30.66, 2

	.type	.L.str.31.67,@object            // @.str.31.67
.L.str.31.67:
	.string	"Constraint violated: "
	.size	.L.str.31.67, 22

	.type	.L.str.32.68,@object            // @.str.32.68
.L.str.32.68:
	.string	" ("
	.size	.L.str.32.68, 3

	.type	.L.str.33.69,@object            // @.str.33.69
.L.str.33.69:
	.string	") == "
	.size	.L.str.33.69, 6

	.type	.L.str.34.70,@object            // @.str.34.70
.L.str.34.70:
	.string	"Parameter "
	.size	.L.str.34.70, 11

	.type	.L.str.35.71,@object            // @.str.35.71
.L.str.35.71:
	.string	" but must be at least "
	.size	.L.str.35.71, 23

	.type	.L.str.36.72,@object            // @.str.36.72
.L.str.36.72:
	.string	" but must be at most "
	.size	.L.str.36.72, 22

	.type	.L.str.37,@object               // @.str.37
.L.str.37:
	.string	"Out of memory (halide_malloc returned nullptr)"
	.size	.L.str.37, 47

	.type	.L.str.38,@object               // @.str.38
.L.str.38:
	.string	"Buffer argument "
	.size	.L.str.38, 17

	.type	.L.str.39,@object               // @.str.39
.L.str.39:
	.string	" is nullptr"
	.size	.L.str.39, 12

	.type	.L.str.40,@object               // @.str.40
.L.str.40:
	.string	"Failed to dump function "
	.size	.L.str.40, 25

	.type	.L.str.41.73,@object            // @.str.41.73
.L.str.41.73:
	.string	" to file "
	.size	.L.str.41.73, 10

	.type	.L.str.42,@object               // @.str.42
.L.str.42:
	.string	" with error "
	.size	.L.str.42, 13

	.type	.L.str.43,@object               // @.str.43
.L.str.43:
	.string	"The host pointer of "
	.size	.L.str.43, 21

	.type	.L.str.44,@object               // @.str.44
.L.str.44:
	.string	" is not aligned to a "
	.size	.L.str.44, 22

	.type	.L.str.45,@object               // @.str.45
.L.str.45:
	.string	" bytes boundary."
	.size	.L.str.45, 17

	.type	.L.str.46,@object               // @.str.46
.L.str.46:
	.string	"The buffer "
	.size	.L.str.46, 12

	.type	.L.str.47,@object               // @.str.47
.L.str.47:
	.string	" is dirty on device, but this pipeline was compiled "
	.size	.L.str.47, 53

	.type	.L.str.48,@object               // @.str.48
.L.str.48:
	.string	"with no support for device to host copies."
	.size	.L.str.48, 43

	.type	.L.str.49,@object               // @.str.49
.L.str.49:
	.string	" is null, but the pipeline will access it on the host."
	.size	.L.str.49, 55

	.type	.L.str.50,@object               // @.str.50
.L.str.50:
	.string	"The folded storage dimension "
	.size	.L.str.50, 30

	.type	.L.str.51,@object               // @.str.51
.L.str.51:
	.string	" of "
	.size	.L.str.51, 5

	.type	.L.str.52,@object               // @.str.52
.L.str.52:
	.string	" was accessed out of order by loop "
	.size	.L.str.52, 36

	.type	.L.str.53,@object               // @.str.53
.L.str.53:
	.string	"Cannot fold dimension "
	.size	.L.str.53, 23

	.type	.L.str.54,@object               // @.str.54
.L.str.54:
	.string	" because an extern stage accesses ["
	.size	.L.str.54, 36

	.type	.L.str.55,@object               // @.str.55
.L.str.55:
	.string	", "
	.size	.L.str.55, 3

	.type	.L.str.56,@object               // @.str.56
.L.str.56:
	.string	"],"
	.size	.L.str.56, 3

	.type	.L.str.57,@object               // @.str.57
.L.str.57:
	.string	" which is outside the range currently valid: ["
	.size	.L.str.57, 47

	.type	.L.str.58.74,@object            // @.str.58.74
.L.str.58.74:
	.string	"]."
	.size	.L.str.58.74, 3

	.type	.L.str.59.75,@object            // @.str.59.75
.L.str.59.75:
	.string	" which wraps around the boundary of the fold, "
	.size	.L.str.59.75, 47

	.type	.L.str.60.76,@object            // @.str.60.76
.L.str.60.76:
	.string	"which occurs at multiples of "
	.size	.L.str.60.76, 30

	.type	.L.str.61.77,@object            // @.str.61.77
.L.str.61.77:
	.string	"The fold factor ("
	.size	.L.str.61.77, 18

	.type	.L.str.62,@object               // @.str.62
.L.str.62:
	.string	") of dimension "
	.size	.L.str.62, 16

	.type	.L.str.63,@object               // @.str.63
.L.str.63:
	.string	" is too small to store the required region accessed by loop "
	.size	.L.str.63, 61

	.type	.L.str.64.78,@object            // @.str.64.78
.L.str.64.78:
	.string	")."
	.size	.L.str.64.78, 3

	.type	.L.str.65,@object               // @.str.65
.L.str.65:
	.string	"Requirement Failed: ("
	.size	.L.str.65, 22

	.type	.L.str.66,@object               // @.str.66
.L.str.66:
	.string	") "
	.size	.L.str.66, 3

	.type	.L.str.67,@object               // @.str.67
.L.str.67:
	.string	"A schedule specialized with specialize_fail() was chosen: "
	.size	.L.str.67, 59

	.type	.L.str.68,@object               // @.str.68
.L.str.68:
	.string	"Buffer has a non-zero device but no device interface.\n"
	.size	.L.str.68, 55

	.type	.L.str.69,@object               // @.str.69
.L.str.69:
	.string	"Buffer has a non-null device_interface but device is 0.\n"
	.size	.L.str.69, 57

	.type	.L.str.70,@object               // @.str.70
.L.str.70:
	.string	"Buffer has both host and device dirty bits set.\n"
	.size	.L.str.70, 49

	.type	.L.str.71,@object               // @.str.71
.L.str.71:
	.string	"Buffer pointer passed to "
	.size	.L.str.71, 26

	.type	.L.str.72,@object               // @.str.72
.L.str.72:
	.string	" is null.\n"
	.size	.L.str.72, 11

	.type	.L.str.73,@object               // @.str.73
.L.str.73:
	.string	"The explicit allocation bound ("
	.size	.L.str.73, 32

	.type	.L.str.74,@object               // @.str.74
.L.str.74:
	.string	" is too small to store the required region ("
	.size	.L.str.74, 45

	.type	.L.str.75,@object               // @.str.75
.L.str.75:
	.string	"Buffer could not be cropped (runtime error or unimplemented device option).\n"
	.size	.L.str.75, 77

	.type	.L.str.4.91,@object             // @.str.4.91
.L.str.4.91:
	.string	"qurt_hvx_lock failed\n"
	.size	.L.str.4.91, 22

	.type	.L.str.7.92,@object             // @.str.7.92
.L.str.7.92:
	.string	"Printer buffer allocation failed.\n"
	.size	.L.str.7.92, 35

	.type	.L.str.6.93,@object             // @.str.6.93
.L.str.6.93:
	.string	"qurt_hvx_unlock failed\n"
	.size	.L.str.6.93, 24

	.type	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE,@object // @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.data
	.weak	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.p2align	2
_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE:
	.word	halide_default_can_use_target_features
	.size	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, 4

	.type	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE,@object // @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE
	.section	.bss,"aw",@nobits
	.weak	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE
	.p2align	2
_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE:
	.space	4
	.size	_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE, 4

	.type	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE,@object // @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE
	.weak	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE
_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE:
	.byte	0                               // 0x0
	.size	_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE,@object // @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE
	.weak	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE
	.p2align	3
_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE:
	.space	32
	.size	_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, 32

	.type	.L.str.94,@object               // @.str.94
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.94:
	.string	"Internal error: wrong structure size passed to halide_can_use_target_features()\n"
	.size	.L.str.94, 81

	.type	.L__unnamed_1,@object           // @0
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_1:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_1, 8

	.type	.L__unnamed_2,@object           // @1
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_2:
	.word	.L__unnamed_1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.size	.L__unnamed_2, 32

	.type	.Lstr,@object                   // @str
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr:
	.string	"input"
	.size	.Lstr, 6

	.type	.Lstr.102,@object               // @str.102
	.p2align	5
.Lstr.102:
	.string	"input_zero"
	.size	.Lstr.102, 11

	.type	.L__unnamed_3,@object           // @2
	.p2align	3
.L__unnamed_3:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_3, 8

	.type	.L__unnamed_4,@object           // @3
	.p2align	3
.L__unnamed_4:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_4, 8

	.type	.L__unnamed_5,@object           // @4
	.p2align	3
.L__unnamed_5:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_5, 8

	.type	.L__unnamed_6,@object           // @5
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_6:
	.word	.L__unnamed_3
	.word	0
	.word	.L__unnamed_4
	.word	0
	.word	.L__unnamed_5
	.word	0
	.size	.L__unnamed_6, 24

	.type	.Lstr.103,@object               // @str.103
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.103:
	.string	"filter"
	.size	.Lstr.103, 7

	.type	.Lstr.104,@object               // @str.104
	.p2align	5
.Lstr.104:
	.string	"filter_zero"
	.size	.Lstr.104, 12

	.type	.L__unnamed_7,@object           // @6
	.p2align	3
.L__unnamed_7:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_7, 8

	.type	.L__unnamed_8,@object           // @7
	.section	.data.rel.ro,"aw",@progbits
	.p2align	2
.L__unnamed_8:
	.word	.L__unnamed_7
	.word	0
	.size	.L__unnamed_8, 8

	.type	.Lstr.105,@object               // @str.105
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.105:
	.string	"bias"
	.size	.Lstr.105, 5

	.type	.Lstr.106,@object               // @str.106
	.p2align	5
.Lstr.106:
	.string	"depth_multiplier"
	.size	.Lstr.106, 17

	.type	.Lstr.107,@object               // @str.107
	.p2align	5
.Lstr.107:
	.string	"stride_x"
	.size	.Lstr.107, 9

	.type	.Lstr.108,@object               // @str.108
	.p2align	5
.Lstr.108:
	.string	"stride_y"
	.size	.Lstr.108, 9

	.type	.Lstr.109,@object               // @str.109
	.p2align	5
.Lstr.109:
	.string	"dilation_x"
	.size	.Lstr.109, 11

	.type	.Lstr.110,@object               // @str.110
	.p2align	5
.Lstr.110:
	.string	"dilation_y"
	.size	.Lstr.110, 11

	.type	.Lstr.111,@object               // @str.111
	.p2align	5
.Lstr.111:
	.string	"output_multiplier"
	.size	.Lstr.111, 18

	.type	.Lstr.112,@object               // @str.112
	.p2align	5
.Lstr.112:
	.string	"output_shift"
	.size	.Lstr.112, 13

	.type	.Lstr.113,@object               // @str.113
	.p2align	5
.Lstr.113:
	.string	"output_zero"
	.size	.Lstr.113, 12

	.type	.Lstr.114,@object               // @str.114
	.p2align	5
.Lstr.114:
	.string	"output_min"
	.size	.Lstr.114, 11

	.type	.Lstr.115,@object               // @str.115
	.p2align	5
.Lstr.115:
	.string	"output_max"
	.size	.Lstr.115, 11

	.type	.L__unnamed_9,@object           // @8
	.p2align	3
.L__unnamed_9:
	.word	0                               // 0x0
	.word	0
	.size	.L__unnamed_9, 8

	.type	.L__unnamed_10,@object          // @9
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_10:
	.word	.L__unnamed_9
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.size	.L__unnamed_10, 32

	.type	.Lstr.116,@object               // @str.116
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.116:
	.string	"output"
	.size	.Lstr.116, 7

	.type	.L__unnamed_11,@object          // @10
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_11:
	.word	.Lstr
	.word	1                               // 0x1
	.word	4                               // 0x4
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.L__unnamed_2
	.word	.Lstr.102
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.103
	.word	1                               // 0x1
	.word	3                               // 0x3
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.L__unnamed_6
	.word	.Lstr.104
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.105
	.word	1                               // 0x1
	.word	1                               // 0x1
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.L__unnamed_8
	.word	.Lstr.106
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.107
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.108
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.109
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.110
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.111
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	0                               // 0x0
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.112
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	32                              // 0x20
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.113
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.114
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.115
	.word	0                               // 0x0
	.word	0                               // 0x0
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.Lstr.116
	.word	2                               // 0x2
	.word	4                               // 0x4
	.byte	1                               // 0x1
	.byte	8                               // 0x8
	.half	1                               // 0x1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	.L__unnamed_10
	.size	.L__unnamed_11, 576

	.type	.Lstr.117,@object               // @str.117
	.section	.rodata,"a",@progbits
	.p2align	7
.Lstr.117:
	.string	"hexagon-32-noos-hvx-hvx_128-hvx_v66-no_asserts-no_bounds_query"
	.size	.Lstr.117, 63

	.type	.Lstr.118,@object               // @str.118
	.p2align	5
.Lstr.118:
	.string	"depthwise_conv_hvx128"
	.size	.Lstr.118, 22

	.type	.Ldepthwise_conv_hvx128_metadata_storage,@object // @depthwise_conv_hvx128_metadata_storage
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.Ldepthwise_conv_hvx128_metadata_storage:
	.word	1                               // 0x1
	.word	16                              // 0x10
	.word	.L__unnamed_11
	.word	.Lstr.117
	.word	.Lstr.118
	.size	.Ldepthwise_conv_hvx128_metadata_storage, 20

	.type	.Lswitch.table.halide_type_to_string,@object // @switch.table.halide_type_to_string
	.p2align	2
.Lswitch.table.halide_type_to_string:
	.word	.L.str.14
	.word	.L.str.15
	.word	.L.str.16
	.word	.L.str.17
	.size	.Lswitch.table.halide_type_to_string, 16

	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.ident	"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"
	.section	".note.GNU-stack","",@progbits
