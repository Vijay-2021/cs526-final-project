#ifndef depthwise_conv_batch_0058_sample_0000_SCHEDULE_H
#define depthwise_conv_batch_0058_sample_0000_SCHEDULE_H

// MACHINE GENERATED -- DO NOT EDIT
// This schedule was automatically generated by Adams2019
// for target=x86-64-linux-avx-avx2-avx512-avx512_sapphirerapids-avx512_skylake-disable_llvm_loop_opt-f16c-fma-sse41  // NOLINT
// with machine_params=10,16777216,40

#include "Halide.h"


inline void apply_schedule_depthwise_conv_batch_0058_sample_0000(
    ::Halide::Pipeline pipeline,
    ::Halide::Target target
) {
    using ::Halide::Func;
    using ::Halide::MemoryType;
    using ::Halide::RVar;
    using ::Halide::TailStrategy;
    using ::Halide::Var;
    Func output = pipeline.get_func(8);
    Func convolved = pipeline.get_func(7);
    Func resampled_input = pipeline.get_func(6);
    Func offset_c = pipeline.get_func(4);
    Func sum_filter = pipeline.get_func(3);
    Func filter_zeroed = pipeline.get_func(1);
    Var b(output.get_schedule().dims()[3].var);
    Var c(output.get_schedule().dims()[0].var);
    Var ci("ci");
    Var cii("cii");
    Var x(output.get_schedule().dims()[1].var);
    Var xi("xi");
    Var y(output.get_schedule().dims()[2].var);
    RVar r19_x(convolved.update(0).get_schedule().dims()[0].var);
    RVar r19_y(convolved.update(0).get_schedule().dims()[1].var);
    output
        .split(c, c, ci, 128, TailStrategy::ShiftInwards)
        .split(x, x, xi, 2, TailStrategy::ShiftInwards)
        .split(ci, ci, cii, 64, TailStrategy::ShiftInwards)
        .unroll(ci)
        .unroll(xi)
        .vectorize(cii, 512 / 64) // Added vectorization factor
        .compute_root()
        .reorder({cii, ci, xi, c, x, y, b})
        .fuse(x, y, x)
        .fuse(c, x, c)
        .parallel(c);
    convolved
        .split(c, c, ci, 16, TailStrategy::RoundUp)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .compute_at(output, c)
        .reorder({ci, c, x, y, b});
    convolved.update(0)
        .split(c, c, ci, 64, TailStrategy::GuardWithIf)
        .vectorize(ci,  512 / 64) // Added vectorization factor
        .reorder({ci, r19_x, x, r19_y, c, y, b});
    resampled_input
        .store_in(MemoryType::Stack)
        .split(c, c, ci, 64, TailStrategy::ShiftInwards)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .compute_at(convolved, r19_y)
        .reorder({ci, c, x, y, b});
    offset_c
        .split(c, c, ci, 64, TailStrategy::ShiftInwards)
        .unroll(c)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .compute_at(output, c)
        .reorder({ci, c});
    sum_filter
        .store_in(MemoryType::Stack)
        .split(c, c, ci, 16, TailStrategy::RoundUp)
        .unroll(c)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .compute_at(offset_c, c)
        .reorder({ci, c});
    sum_filter.update(0)
        .split(c, c, ci, 32, TailStrategy::RoundUp)
        .unroll(c)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .reorder({ci, c, r19_x, r19_y});
    filter_zeroed
        .split(c, c, ci, 64, TailStrategy::ShiftInwards)
        .vectorize(ci, 512 / 64) // Added vectorization factor
        .compute_at(output, c)
        .reorder({ci, c, x, y});

}

#endif  // depthwise_conv_batch_0058_sample_0000_SCHEDULE_H
